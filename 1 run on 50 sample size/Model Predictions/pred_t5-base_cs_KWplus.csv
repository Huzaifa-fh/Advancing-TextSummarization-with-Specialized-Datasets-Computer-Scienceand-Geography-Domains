Text,Abstractive,topics,topics_plus,prediction,model_dir,kw_variant
"A deep generative model is a powerful method of learning a data distribution, which has achieved tremendous success in numerous scenarios. However, it is nontrivial for a single generative model to faithfully capture the distributions of the complex data such as images with complicate structures. In this paper, we propose a novel approach of cascaded boosting for boosting generative models, where meta-models (i.e., weak learners) are cascaded together to produce a stronger model. Any hidden variable meta-model can be leveraged as long as it can support the likelihood evaluation. We derive a decomposable variational lower bound of the boosted model, which allows each meta-model to be trained separately and greedily. We can further improve the learning power of the generative models by combing our cascaded boosting framework with the multiplicative boosting framework. The past decade has witnessed tremendous success in the field of deep generative models (DGMs) in both unsupervised learning (Goodfellow et al., 2014; Kingma & Welling, 2013; Radford et al., 2015) and semi-supervised learning (Abbasnejad et al., 2017; Kingma et al., 2014; Li et al., 2018) paradigms. DGMs learn the data distribution by combining the scalability of deep learning with the generality of probabilistic reasoning. However, it is not easy for a single parametric model to learn a complex distribution, since the upper limit of a model's ability is determined by its fixed structure. If a model with low capacity was adopted, the model would be likely to have a poor performance. Straightforwardly increasing the model capacity (e.g., including more layers or more neurons) is likely to cause serious challenges, such as vanishing gradient problem (Hochreiter et al., 2001 ) and exploding gradient problem (Grosse, 2017 ). An alternative approach is to integrate multiple weak models to achieve a strong one. The early success was made on mixture models (Dempster et al., 1977; Figueiredo & Jain, 2002; Xu & Jordan, 1996) and product-of-experts (Hinton, 1999; . However, the weak models in such work are typically shallow models with very limited capacity. Recent success has been made on boosting generative models, where a set of meta-models (i.e., weak learners) are combined to construct a stronger model. In particular, Grover & Ermon (2018) propose a method of multiplicative boosting, which takes the geometric average of the meta-model distributions, with each assigned an exponentiated weight. This boosting method improves performance on density estimation and sample generation, compared to a single meta-model. However, the boosted model has an explicit partition function, which requires importance sampling (Rubinstein & Kroese, 2016) for an estimation. In general, sampling from the boosted model is conducted based on Markov chain Monte Carlo (MCMC) method (Hastings, 1970) . As a result, it requires a high time complexity of likelihood evaluation and sample generation. Rosset & Segal (2003) propose another method of additive boosting, which takes the weighted arithmetic mean of meta-models' distributions. This method can sample fast, but the improvement of performance on density estimation is not comparable to the multiplicative boosting, since additive boosting requires that the expected log-likelihood and likelihood of the current meta-model are better-or-equal than those of the previous boosted model (Grover & Ermon, 2018) , which is difficult to satisfy. In summary, it is nontrivial for both of the previous boosting methods to balance well between improving the learning power and keeping the efficiency of sampling and density estimation. To address the aforementioned issues, we propose a novel boosting framework, called cascaded boosting, where meta-models are connected in cascade. The framework is inspired by the greedy layer-wise training algorithm of DBNs (Deep Belief Networks) (Bengio et al., 2007; Hinton et al., 2006) , where an ensemble of RBMs (Restricted Boltzmann Machines) (Smolensky, 1986) are converted to a stronger model. We propose a decomposable variational lower bound, which reveals the principle behind the greedy layer-wise training algorithm. The decomposition allows us to incorporate any hidden variable meta-model, as long as it supports likelihood evaluation, and train these meta-models separately and greedily, yielding a deep boosted model. Finally, We demonstrate that our boosting framework can be integrated with the multiplicative boosting framework (Grover & Ermon, 2018) , yielding a hybrid boosting with an improved learning power of generative models. To summary, we make the following contributions: • We propose a boosting framework to boost generative models, where meta-models are cascaded together to produce a stronger model. • We give a decomposable variational lower bound of the boosted model, which reveals the principle behind the greedy layer-wise training algorithm. • We finally demonstrate that our boosting framework can be extended to a hybrid model by integrating it with the multiplicative boosting models, which further improves the learning power of generative models. We propose a framework for boosting generative models by cascading meta-models. Any hidden variable meta-model can be incorporated, as long as it supports likelihood evaluation. The decomposable lower bound allows us to train meta-models separately and greedily. Our cascaded boosting can be integrated with the multiplicative boosting. In our experiments, we first validate that the non-decreasing property of the decomposable variational lower bound (Equation 5) holds in practice, and next further promote the performance of some advanced models, which represent state-ofthe-art methods. Then, we show that our cascaded boosting has better performance of improving models' learning power, compared with naively increasing model capacity. Finally, we compare different generative boosting methods, validating the ability of the hybrid boosting in further improving learning power of generative models. A PROOF OF THEOREM 1 Proof. Using q(h 1 , · · · , h k−1 |x) as the approximate posterior, we have a variational lower bound Thus, the lower bound is equal to: Thus, Take the expection with respect to dataset D, we have B PROOF OF THEOREM 3 AND THEOREM 4 where n is the number of meta-models. Since we can omit the subscript, thereby writing q i as q. For any j ∈ [k + 1, n] ∩ Z and any m k+1 , m k+2 , · · · , m j , given i (k + 1 ≤ i ≤ j), we have Let q(h k , · · · , h j−1 |h k−1 ) be the approximate posterior of p j (h k−1 , · · · , h j−1 ), according to Theorem 1, we have we have",Propose an approach for boosting generative models by cascading hidden variable models,The past decade ; Goodfellow et al. ; Kingma & Welling ; Radford ; al. ; Abbasnejad ; Kingma ; Li ; Grosse ; Figueiredo & Jain,The past decade ; Goodfellow et al. ; Kingma & Welling ; Radford ; al. ; Abbasnejad ; Kingma ; Li ; Grosse ; Figueiredo & Jain,"A deep generative model is a powerful method of learning a data distribution. However, it is difficult for a single generative model to accurately capture complex data distributions. In this paper, we propose cascaded boosting for boosting generative models, where meta-models (i.e., weak learners) are cascaded together to produce a stronger model. The decomposable variational lower bound of the boosted model allows each meta-model to be trained separately and greedily. We can further improve the learning power of the generative models by combing cas",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"The universal approximation theorem, in one of its most general versions, says that if we consider only continuous activation functions σ, then a standard feedforward neural network with one hidden layer is able to approximate any continuous multivariate function f to any given approximation threshold ε, if and only if σ is non-polynomial. In this paper, we give a direct algebraic proof of the theorem. Furthermore we shall explicitly quantify the number of hidden units required for approximation. Specifically, if X in R^n is compact, then a neural network with n input units, m output units, and a single hidden layer with {n+d choose d} hidden units (independent of m and ε), can uniformly approximate any polynomial function f:X -> R^m whose total degree is at most d for each of its m coordinate functions. In the general case that f is any continuous function, we show there exists some N in O(ε^{-n}) (independent of m), such that N hidden units would suffice to approximate f. We also show that this uniform approximation property (UAP) still holds even under seemingly strong conditions imposed on the weights. We highlight several consequences: (i) For any δ > 0, the UAP still holds if we restrict all non-bias weights w in the last layer to satisfy |w| < δ. (ii) There exists some λ>0 (depending only on f and σ), such that the UAP still holds if we restrict all non-bias weights w in the first layer to satisfy |w|>λ. (iii) If the non-bias weights in the first layer are *fixed* and randomly chosen from a suitable range, then the UAP holds with probability 1. A standard (feedforward) neural network with n input units, m output units, and with one or more hidden layers, refers to a computational model N that can compute a certain class of functions ρ : R n → R m , where ρ = ρ W is parametrized by W (called the weights of N ). Implicitly, the definition of ρ depends on a choice of some fixed function σ : R → R, called the activation function of N . Typically, σ is assumed to be continuous, and historically, the earliest commonly used activation functions were sigmoidal. A key fundamental result justifying the use of sigmoidal activation functions was due to Cybenko (1989) , Hornik et al. (1989) , and Funahashi (1989) , who independently proved the first version of what is now famously called the universal approximation theorem. This first version says that if σ is sigmoidal, then a standard neural network with one hidden layer would be able to uniformly approximate any continuous function f : X → R m whose domain X ⊆ R n is compact. Hornik (1991) extended the theorem to the case when σ is any continuous bounded non-constant activation function. Subsequently, Leshno et al. (1993) proved that for the class of continuous activation functions, a standard neural network with one hidden layer is able to uniformly approximate any continuous function f : X → R m on any compact X ⊆ R n , if and only if σ is non-polynomial. Although a single hidden layer is sufficient for the uniform approximation property (UAP) to hold, the number of hidden units required could be arbitrarily large. Given a subclass F of real-valued continuous functions on a compact set X ⊆ R n , a fixed activation function σ, and some ε > 0, let N = N (F, σ, ε) be the minimum number of hidden units required for a single-hidden-layer neural network to be able to uniformly approximate every f ∈ F within an approximation error threshold of ε. If σ is the rectified linear unit (ReLU) x → max(0, x), then N is at least Ω( 1 √ ε ) when F is the class of C 2 non-linear functions (Yarotsky, 2017) , or the class of strongly convex differentiable functions (Liang & Srikant, 2016) ; see also (Arora et al., 2018) . If σ is any smooth non-polynomial function, then N is at most O(ε −n ) for the class of C 1 functions with bounded Sobolev norm (Mhaskar, 1996) ; cf. (Pinkus, 1999, Thm. 6.8) , (Maiorov & Pinkus, 1999) . As a key highlight of this paper, we show that if σ is an arbitrary continuous non-polynomial function, then N is at most O(ε −n ) for the entire class of continuous functions. In fact, we give an explicit upper bound for N in terms of ε and the modulus of continuity of f , so better bounds could be obtained for certain subclasses F, which we discuss further in Section 4. Furthermore, even for the wider class F of all continuous functions f : X → R m , the bound is still O(ε −n ), independent of m. To prove this bound, we shall give a direct algebraic proof of the universal approximation theorem, in its general version as stated by Leshno et al. (1993) (i.e. σ is continuous and non-polynomial). An important advantage of our algebraic approach is that we are able to glean additional information on sufficient conditions that would imply the UAP. Another key highlight we have is that if F is the subclass of polynomial functions f : X → R m with total degree at most d for each coordinate function, then n+d d hidden units would suffice. In particular, notice that our bound N ≤ n+d d does not depend on the approximation error threshold ε or the output dimension m. We shall also show that the UAP holds even under strong conditions on the weights. Given any δ > 0, we can always choose the non-bias weights in the last layer to have small magnitudes no larger than δ. Furthermore, we show that there exists some λ > 0 (depending only on σ and the function f to be approximated), such that the non-bias weights in the first layer can always be chosen to have magnitudes greater than λ. Even with these seemingly strong restrictions on the weights, we show that the UAP still holds. Thus, our main results can be collectively interpreted as a quantitative refinement of the universal approximation theorem, with extensions to restricted weight values. Outline: Section 2 covers the preliminaries, including relevant details on arguments involving dense sets. Section 3 gives precise statements of our results, while Section 4 discusses the consequences of our results. Section 5 introduces our algebraic approach and includes most details of the proofs of our results; details omitted from Section 5 can be found in the appendix. Finally, Section 6 concludes our paper with further remarks. The universal approximation theorem (version of Leshno et al. (1993) ) is an immediate consequence of Theorem 3.2 and the observation that σ must be non-polynomial for the UAP to hold, which follows from the fact that the uniform closure of P ≤d (X) is P ≤d (X) itself, for every integer d ≥ 1. Alternatively, we could infer the universal approximation theorem by applying the Stone-Weirstrass theorem (Theorem 2.1) to Theorem 3.1. Given fixed n, m, d, a compact set X ⊆ R n , and σ ∈ C(R)\P ≤d−1 (R), Theorem 3.1 says that we could use a fixed number N of hidden units (independent of ε) and still be able to approximate any function f ∈ P ≤d (X, R m ) to any desired approximation error threshold ε. Our ε-free bound, although possibly surprising to some readers, is not the first instance of an ε-free bound: Neural networks with two hidden layers of sizes 2n + 1 and 4n + 3 respectively are able to uniformly approximate any f ∈ C(X), provided that we use a (somewhat pathological) activation function (Maiorov & Pinkus, 1999) ; cf. (Pinkus, 1999) . Lin et al. (2017) showed that for fixed n, d, and a fixed smooth non-linear σ, there is a fixed N (i.e. ε-free), such that a neural network with N hidden units is able to approximate any f ∈ P ≤d (X). An explicit expression for N is not given, but we were able to infer from their constructive proof that N = 4 n+d+1 d − 4 hidden units are required, over d − 1 hidden layers (for d ≥ 2). In comparison, we require less hidden units and a single hidden layer. Our proof of Theorem 3.2 is an application of Jackson's theorem (Theorem 2.2) to Theorem 3.1, which gives an explicit bound in terms of the values of the modulus of continuity ω f of the function f to be approximated. The moduli of continuity of several classes of continuous functions have explicit characterizations. For example, given constants k > 0 and 0 < α ≤ 1, recall that a continuous function f : for all x, y ∈ X, and it is called α-Hölder if there is some constant c such that |f (x)−f (y)| ≤ c x−y α for all x, y ∈ X. The modulus of continuity of a k-Lipschitz (resp. α-Hölder) continuous function f is ω f (t) = kt (resp. ω f (t) = ct α ), hence Theorem 3.2 implies the following corollary. n → R is α-Hölder continuous, then there is a constant k such that for every ε > 0, there exists some An interesting consequence of Theorem 3.3 is the following: The freezing of lower layers of a neural network, even in the extreme case that all frozen layers are randomly initialized and the last layer is the only ""non-frozen"" layer, does not necessarily reduce the representability of the resulting model. Specifically, in the single-hidden-layer case, we have shown that if the non-bias weights in the first layer are fixed and randomly chosen from some suitable fixed range, then the UAP holds with probability 1, provided that there are sufficiently many hidden units. Of course, this representability does not reveal anything about the learnability of such a model. In practice, layers are already pre-trained before being frozen. It would be interesting to understand quantitatively the difference between having pre-trained frozen layers and having randomly initialized frozen layers. Theorem 3.3 can be viewed as a result on random features, which were formally studied in relation to kernel methods (Rahimi & Recht, 2007) . In the case of ReLU activation functions, Sun et al. (2019) proved an analog of Theorem 3.3 for the approximation of functions in a reproducing kernel Hilbert space; cf. (Rahimi & Recht, 2008) . For a good discussion on the role of random features in the representability of neural networks, see (Yehudai & Shamir, 2019) . The UAP is also studied in other contexts, most notably in relation to the depth and width of neural networks. Lu et al. (2017) proved the UAP for neural networks with hidden layers of bounded width, under the assumption that ReLU is used as the activation function. Soon after, Hanin (2017) strengthened the bounded-width UAP result by considering the approximation of continuous convex functions. Recently, the role of depth in the expressive power of neural networks has gathered much interest (Delalleau & Bengio, 2011; Eldan & Shamir, 2016; Mhaskar et al., 2017; Montúfar et al., 2014; Telgarsky, 2016) . We do not address depth in this paper, but we believe it is possible that our results can be applied iteratively to deeper neural networks, perhaps in particular for the approximation of compositional functions; cf. (Mhaskar et al., 2017) . Theorem 5.6 is rather general, and could potentially be used to prove analogs of the universal approximation theorem for other classes of neural networks, such as convolutional neural networks and recurrent neural networks. In particular, finding a single suitable set of weights (as a representative of the infinitely many possible sets of weights in the given class of neural networks), with the property that its corresponding ""non-bias Vandermonde matrix"" (see Definition 5.5) is non-singular, would serve as a straightforward criterion for showing that the UAP holds for the given class of neural networks (with certain weight constraints). We formulated this criterion to be as general as we could, with the hope that it would applicable to future classes of ""neural-like"" networks. We believe our algebraic approach could be emulated to eventually yield a unified understanding of how depth, width, constraints on weights, and other architectural choices, would influence the approximation capabilities of arbitrary neural networks. Finally, we end our paper with an open-ended question. The proofs of our results in Section 5 seem to suggest that non-bias weights and bias weights play very different roles. We could impose very strong restrictions on the non-bias weights and still have the UAP. What about the bias weights? First, we recall the notion of generalized Wronskians as given in (LeVeque, 1956, Chap. 4.3) . Let ∆ 0 , . . . , ∆ N −1 be any N differential operators of the form and let x = (x 1 , . . . , x n ). Recall that λ 1 < · · · < λ N are all the n-tuples in Λ n ≤d in the colexicographic order. For each be the coefficient of the monomial q k (x) in ∆ λi p(x). Consider an arbitrary W ∈ U, and for each 1 ≤ j ≤ N , define f j ∈ P ≤d (R n ) by the map x → p(w 1,j x 1 , . . . , w n,j x n ). Note that F p,0n (W ) = (f 1 , . . . , f N ) by definition. Next, define the matrix M W (x) := [∆ i f j (x)] 1≤i,j≤N , and note that det M W (x) is the generalized Wronskian of (f 1 , . . . , f N ) associated to ∆ 1 , . . . , ∆ N . In particular, this generalized Wronskian is well-defined, since the definition of the colexicographic order implies that λ k,1 + · · · + λ k,n ≤ k for all possible k. Similar to the univariate case, (f 1 , . . . , f N ) is linearly independent if (and only if) its generalized Wronskian is not the zero function (Wolsson, 1989) . Thus, to show that W ∈ p U ind , it suffices to show that the evaluation det M W (1 n ) of this generalized Wronskian at x = 1 n gives a non-zero value, where 1 n denotes the all-ones vector in R n . Observe that the (i, j)-th entry of M W (1 n ) equals ( w It follows from the definition of the colexicographic order that λ j − λ i necessarily contains at least one strictly negative entry whenever j < i, hence we infer that M is upper triangular. The diagonal entries of M are α 0n , . . . , α 0n , and note that α λi for each 1 ≤ i ≤ N , where λ i,1 ! · · · λ i,n ! denotes the product of the factorials of the entries of the n-tuple λ i . In particular, λ i,1 ! · · · λ i,n ! = 0, and α (1) λi , which is the coefficient of the monomial q i (x) in p(x), is non-zero. Thus, det(M ) = 0. We have come to the crucial step of our proof. If we can show that det(M ) = det(Q[W ]) = 0, then det(M W (1 n )) = det(M ) det(M ) = 0, and hence we can infer that W ∈ p U ind . This means that p U ind contains the subset U ⊆ U consisting of all W such that Q[W ] is non-singular. Note that det(Q [W ] ) is a polynomial in terms of the non-bias weights in W (1) as its variables, so we could write this polynomial as r = r(W ). Consequently, if we can find a single W ∈ U such that Q[W ] is non-singular, then r(W ) is not identically zero on U, which then implies that U = {W ∈ U : r(W ) = 0} is dense in U (w.r.t. the Euclidean metric).",A quantitative refinement of the universal approximation theorem via an algebraic approach.,one ; R^n ; N ; approximate f. We ; UAP ; first ; W ; Cybenko ; Hornik et al ; Funahashi,one ; R^n ; N ; approximate f. ; UAP ; first ; W ; Cybenko ; Hornik et al ; Funahashi,"The universal approximation theorem states that a standard feedforward neural network with one hidden layer can approximate any continuous multivariate function f to any given approximation threshold, if and only if  is non-polynomial. In particular, if X in Rn is compact, a neural network with n input units, m output units, and a single hidden layer with n+d choose d hidden units (independent of m) can uniformly approximate any polynomial function f.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"Style transfer usually refers to the task of applying color and texture information from a specific style image to a given content image while preserving the structure of the latter. Here we tackle the more generic problem of semantic style transfer: given two unpaired collections of images, we aim to learn a mapping between the corpus-level style of each collection, while preserving semantic content shared across the two domains. We introduce XGAN (""Cross-GAN""), a dual adversarial autoencoder, which captures a shared representation of the common domain semantic content in an unsupervised way, while jointly learning the domain-to-domain image translations in both directions.   We exploit ideas from the domain adaptation literature and define a semantic consistency loss which encourages the model to preserve semantics in the learned embedding space. We report promising qualitative results for the task of face-to-cartoon translation. The cartoon dataset we collected for this purpose will also be released as a new benchmark for semantic style transfer. Image-to-image translation -learning to map images from one domain to another -covers several classical computer vision tasks such as style transfer (rendering an image in the style of a given input BID3 ), colorization (mapping grayscale images to color images (Zhang et al., 2016) ), super-resolution (increasing the resolution of an input image BID9 ), or semantic segmentation (inferring pixelwise semantic labeling of a scene BID14 ). In many cases, one can rely on supervision in the form of labels or paired samples. This assumption holds for instance for colorization, where ground-truth pairs are easily obtained by generating grayscale images from colored inputs.Figure 1: On the left, we depict a high-level motivational example for semantic style transfer, the task of adapting an image to the visual appearance of an other domain without altering its semantic content. The proposed XGAN applied on the face-to-cartoon task preserves important face semantics such as hair style or face shape (right).In this work, we consider the task of semantic style transfer: learning to map an image from one domain into the style of another domain without altering its semantic content (see Figure 1) . In that sense, our goal is akin to style transfer: We aim to transfer style while keeping content consistent. The key differences with traditional techniques are that (i) we work with image collections instead of having a single style image, and (ii ) we aim to retain higher-level semantic content in the feature space rather than pixel-level structure. In particular, we experiment on the task of translating faces to cartoons while preserving their various facial attributes (hair color, eye color, etc.). Note that without loss of generality, a photo of a face can be mapped to many valid cartoons, and vice versa. Semantic style transfer is therefore a many-to-many mapping problem, for which obtaining labeled examples is ambiguous and costly. Although this paper specifically focuses on the face-to-cartoon setting, many other examples fall under this category: mapping landscape pictures to paintings (where the different scene objects and their composition describe the input semantics), transforming sketches to images, or even cross-domain tasks such as generating images from text. In this setting, we only rely on two unlabeled training image collections or corpora, one for each domain, with no known image pairings across domains. Hence, we are faced with a double domain shift, first in terms of global domain appearance, and second in terms of the content distribution of the two collections.Recent work BID6 Zhu et al., 2017; Yi et al., 2017; BID1 report good performance using GAN-based models for unsupervised image-to-image translation when the two input domains share similar pixel-level structure (e.g., horses and zebras) but fail for more general transformations (e.g., dogs and cats). Perhaps the best known recent example is CycleGAN (Zhu et al., 2017) . Given two image domains D 1 and D 2 , the model is trained with a pixel-level cycleconsistency loss which ensures that the mapping g 1→2 from D 1 to D 2 followed by its inverse, g 2→1 , yields the identity function; i.e., g 1→2 • g 2→1 = id. However, we argue that such a pixel-level constraint is not sufficient in our case; the category of transformations we are interested in requires a constraint in semantic space even though the transformation occurs in the pixel space.To this end, we propose XGAN (""Cross-GAN""), a dual adversarial autoencoder which learns a shared semantic representation of the two input domains in an unsupervised way, while jointly learning both domain-to-domain translations. In other words, the domain-to-domain translation g 1→2 consists of an encoder e 1 taking inputs in D 1 , followed by a decoder d 2 with outputs in D 2 (and likewise for g 2→1 ) such that e 1 and e 2 , as well as d 1 and d 2 , are partially shared. The main novelty lies in how we constrain the shared embedding using techniques from the domain adaptation literature, as well as a novel semantic consistency loss. The latter ensures that the domain-to-domain translations preserve the semantic representation, i.e., that e 1 ≈ e 2 •g 1→2 and e 2 ≈ e 1 •g 2→1 . Therefore, it acts as a form of self-supervision which alleviates the need for paired examples and preserves semantic featurelevel information rather than pixel-level content. In the following section, we review relevant recent work before discussing the XGAN model in more detail in Section 3. In Section 4, we introduce CARTOONSET, our dataset of cartoon faces for research on semantic style transfer, which we are currently in the process of making publicly available. Finally, in Section 5 we report experimental results of XGAN on the face-to-cartoon task, and discuss various ablation experiments. In this work, we introduced XGAN, a model for unsupervised domain translation applied to the task of semantically-consistent style transfer. In particular, we argue that learning image-to-image translation between two structurally different domains requires passing through a high-level joint semantic representation while discarding local pixel-level dependencies. Additionally, we proposed a semantic consistency loss acting on both domain translations as a form of self-supervision.We reported promising experimental results on the task of mapping the domain of face images to cartoon avatars that clearly outperform the current baseline. We also showed that additional weak supervision, such as a pretrained feature representation, can easily be added to the model in the form of teacher knowledge. While not necessary, it acts as a good regularizer for the learned embeddings and generated samples. This can be particularly useful for natural image data as offthe-shelf pretrained models are abundant. Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In ICCV, 2017.","XGAN is an unsupervised model for feature-level image-to-image translation applied to semantic style transfer problems such as the face-to-cartoon task, for which we introduce a new dataset.",two ; XGAN ; Cross-GAN ; Zhang et al. ; one ; first ; second ; Zhu et al. ; Yi et al. ; GAN,two ; XGAN ; Cross-GAN ; Zhang et al. ; one ; first ; second ; Zhu et al. ; Yi et al. ; GAN,"Style transfer refers to applying color and texture information from a specific style image to a given content image while preserving the structure of the latter. Here, we tackle the generic problem of semantic style transfer: given two unpaired collections of images, we aim to learn a mapping between the corpus-level style of each collection while preserving semantic content shared across the two domains. We introduce XGAN (""Cross-GAN""), a dual adversarial autoencoder, which captures a shared representation of the common domain semantic content, while jointly learning domain-to-domain",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"Parameter pruning is a promising approach for CNN compression and acceleration by eliminating redundant model parameters with tolerable performance loss. Despite its effectiveness, existing regularization-based parameter pruning methods usually drive weights towards zero with large and constant regularization factors, which neglects the fact that the expressiveness of CNNs is fragile and needs a more gentle way of regularization for the networks to adapt during pruning. To solve this problem, we propose a new regularization-based pruning method (named IncReg) to incrementally assign different regularization factors to different weight groups based on their relative importance, whose effectiveness is proved on popular CNNs compared with state-of-the-art methods. Recently, deep Convolutional Neural Networks (CNNs) have made a remarkable success in computer vision tasks by leveraging large-scale networks learning from big amount of data. However, CNNs usually lead to massive computation and storage consumption, thus hindering their deployment on mobile and embedded devices. To solve this problem, many research works focus on compressing the scale of CNNs. Parameter pruning is a promising approach for CNN compression and acceleration, which aims at eliminating redundant model parameters at tolerable performance loss. To avoid hardware-unfriendly irregular sparsity, structured pruning is proposed for CNN acceleration BID0 BID22 . In the im2col implementation BID1 BID3 of convolution, weight tensors are expanded into matrices, so there are generally two kinds of structured sparsity, i.e. row sparsity (or filter-wise sparsity) and column sparsity (or shape-wise sparsity) BID24 BID23 .There are mainly two categories of structured pruning. One is importance-based methods, which prune weights in groups based on some established importance criteria BID17 BID19 BID23 . The other is regularization-based methods, which add group regularization terms to learn structured sparsity BID24 BID16 BID8 . Existing group regularization approaches mainly focus on the regularization form (e.g. Group LASSO BID26 ) to learn structured sparsity, while ignoring the influence of regularization factor. In particular , they tend to use a large and constant regularization factor for all weight groups in the network BID24 BID16 , which has two problems. Firstly, this 'one-size-fit-all' regularization scheme has a hidden assumption that all weights in different groups are equally important, which however does not hold true, since weights with larger magnitude tend to be more important than those with smaller magnitude. Secondly, few works have noticed that the expressiveness of CNNs is so fragile BID25 during pruning that it cannot withstand a large penalty term from beginning, especially for large pruning ratios and compact networks (like ResNet BID7 ). AFP BID6 was proposed to solve the first problem, while ignored the second one. In this paper , we propose a new regularization-based method named IncReg to incrementally learn structured sparsity. We propose a new structured pruning method based on an incremental way of regularization, which helps CNNs to transfer their expressiveness to the rest parts during pruning by increasing the regularization factors of unimportant weight groups little by little. Our method is proved to be comparably effective on popular CNNs compared with state-of-the-art methods, especially in face of large pruning ratios and compact networks.",we propose a new regularization-based pruning method (named IncReg) to incrementally assign different regularization factors to different weight groups based on their relative importance.,CNN ; zero ; IncReg ; Convolutional Neural Networks ; two ; One ; e.g. Group LASSO ; Firstly ; Secondly ; first,CNN ; zero ; IncReg ; Convolutional Neural Networks ; two ; One ; e.g. Group LASSO ; Firstly ; Secondly ; first,"Parameter pruning is a promising approach for CNN compression and acceleration by eliminating redundant model parameters with tolerable performance loss. However, existing regularization-based parameter pruning methods drive weights towards zero with large and constant regularization factors, which neglects the fragile expressiveness of CNNs. To solve this problem, we propose a new regularization-based pruning method (named IncReg) to incrementally assign different regularization factors to different weight groups based on their relative importance, whose effectiveness is proved on popular CNNs compared with state-of-the-",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"It is well-known that  classifiers are vulnerable to adversarial perturbations. To defend against adversarial perturbations, various certified robustness results have been derived. However, existing certified robustnesses are limited to top-1 predictions. In many real-world applications, top-$k$ predictions are more relevant. In this work, we aim to derive certified robustness for top-$k$ predictions. In particular, our certified robustness is based on randomized smoothing, which turns any classifier to a new classifier via adding noise to an input example. We adopt randomized smoothing because it is scalable to large-scale neural networks and applicable to any classifier. We derive a tight robustness in $\ell_2$ norm for top-$k$ predictions  when using randomized smoothing with Gaussian noise. We find that generalizing the certified robustness  from top-1 to top-$k$ predictions faces significant technical challenges. We also empirically evaluate our method on CIFAR10 and ImageNet. For example, our method can obtain an ImageNet classifier with a certified top-5 accuracy of 62.8\% when the $\ell_2$-norms of the adversarial perturbations are less than 0.5 (=127/255). Our code is publicly available at: \url{https://github.com/jjy1994/Certify_Topk}. Classifiers are vulnerable to adversarial perturbations (Szegedy et al., 2014; Goodfellow et al., 2015; Carlini & Wagner, 2017b; Jia & Gong, 2018) . Specifically, given an example x and a classifier f , an attacker can carefully craft a perturbation δ such that f makes predictions for x + δ as the attacker desires. Various empirical defenses (e.g., Goodfellow et al. (2015) ; Svoboda et al. (2019) ; Buckman et al. (2018) ; Ma et al. (2018) ; Guo et al. (2018) ; Dhillon et al. (2018) ; Xie et al. (2018) ; Song et al. (2018) ) have been proposed to defend against adversarial perturbations. However, these empirical defenses were often soon broken by adaptive adversaries (Carlini & Wagner, 2017a; . As a response, certified robustness (e.g., Wong & Kolter (2018) ; Raghunathan et al. (2018a) ; Liu et al. (2018) ; Lecuyer et al. (2019) ; Cohen et al. (2019) ) against adversarial perturbations has been developed. In particular, a robust classifier verifiably predicts the same top-1 label for data points in a certain region around any example x. In many applications such as recommender systems, web search, and image classification cloud service (Clarifai; Google Cloud Vision), top-k predictions are more relevant. In particular, given an example, a set of k most likely labels are predicted for the example. However, existing certified robustness results are limited to top-1 predictions, leaving top-k robustness unexplored. To bridge this gap, we study certified robustness for top-k predictions in this work. Our certified top-k robustness leverages randomized smoothing (Cao & Gong, 2017; Cohen et al., 2019) , which turns any base classifier f to be a robust classifier via adding random noise to an example. For instance, Cao & Gong (2017) is the first to propose randomized smoothing with uniform noise as an empirical defense. We consider random Gaussian noise because of its certified robustness guarantee (Cohen et al., 2019) . Specifically, we denote by p i the probability that the base classifier f predicts label i for the Gaussian random variable N (x, σ 2 I). The smoothed classifier g k (x) predicts the k labels with the largest probabilities p i 's for the example x. We adopt randomized smoothing because it is scalable to large-scale neural networks and applicable to any base classifier. Our major theoretical result is a tight certified robustness bound for top-k predictions when using randomized smoothing with Gaussian noise. Specifically, given an example x, a label l is verifiably among the top-k labels predicted by the smoothed classifier g k (x + δ) when the 2 -norm of the adversarial perturbation δ is less than a threshold (called certified radius). The certified radius for top-1 predictions derived by Cohen et al. (2019) is a special case of our certified radius when k = 1. As our results and proofs show, generalizing certified robustness from top-1 to top-k predictions faces significant new challenges and requires new techniques. Our certified radius is the unique solution to an equation, which depends on σ, p l , and the k largest probabilities p i 's (excluding p l ). However, computing our certified radius in practice faces two challenges: 1) it is hard to exactly compute the probability p l and the k largest probabilities p i 's, and 2) the equation about the certified radius does not have an analytical solution. To address the first challenge, we estimate simultaneous confidence intervals of the label probabilities via the Clopper-Pearson method and Bonferroni correction in statistics. To address the second challenge, we propose an algorithm to solve the equation to obtain a lower bound of the certified radius, where the lower bound can be tuned to be arbitrarily close to the true certified radius. We evaluate our method on CIFAR10 (Krizhevsky & Hinton, 2009) and ImageNet (Deng et al., 2009) datasets. For instance, on ImageNet, our method respectively achieves approximate certified top-1, top-3, and top-5 accuracies as 46.6%, 57.8%, and 62.8% when the 2 -norms of the adversarial perturbations are less than 0.5 (127/255) and σ = 0.5. Our contributions are summarized as follows: • Theory. We derive the first certified radius for top-k predictions. Moreover, we prove our certified radius is tight for randomized smoothing with Gaussian noise. • Algorithm. We develop algorithms to estimate our certified radius in practice. • Evaluation. We empirically evaluate our method on CIFAR10 and ImageNet. Adversarial perturbation poses a fundamental security threat to classifiers. Existing certified defenses focus on top-1 predictions, leaving top-k predictions untouched. In this work, we derive the first certified radius under 2 -norm for top-k predictions. Our results are based on randomized smoothing. Moreover, we prove that our certified radius is tight for randomized smoothing with Gaussian noise. In order to compute the certified radius in practice, we further propose simultaneous confidence interval estimation methods as well as design an algorithm to estimate a lower bound of the certified radius. Interesting directions for future work include 1) deriving a tight certified radius under other norms such as 1 and ∞ , 2) studying which noise gives the tightest certified radius for randomized smoothing, and 3) studying certified robustness for top-k ranking. A PROOF OF THEOREM 1 Given an example x, we define the following two random variables: where ∼ N (0, σ 2 I). The random variables X and Y represent random samples obtained by adding isotropic Gaussian noise to the example x and its perturbed version x + δ, respectively. Moreover, we have the following lemma from Cohen et al. (2019) . Lemma 2. Given an example x, a number q ∈ [0, 1], and regions A and B defined as follows: ) Then, we have the following equations: Proof. Please refer to Cohen et al. (2019) . Based on Lemma 1 and 2, we derive the following lemma: Lemma 3. Suppose we have an arbitrary base classifier f , an example x, a set of labels which are denoted as S, two probabilities p S and p S that satisfy p S ≤ p S = Pr(f (X) ∈ S) ≤ p S , and regions A S and B S defined as follows: Proof. We know that Pr(X ∈ A S ) = p S based on Lemma 2. Combined with the condition that p S ≤ Pr(f (X) ∈ S), we obtain the first inequality in (20) . Similarly, we can obtain the second inequality in (20). We define M (z) = I(f (z) ∈ S). Based on the first inequality in (20) and Lemma 1, we have the following: which is the first inequality in (21). The second inequality in (21) can be obtained similarly. Next, we restate Theorem 1 and show our proof. Theorem 1 (Certified Radius for Top-k Predictions). Suppose we are given an example x, an arbitrary base classifier f , ∼ N (0, σ 2 I), a smoothed classifier g, an arbitrary label l ∈ {1, 2, · · · , c}, and p l , p 1 , · · · , p l−1 , p l+1 , · · · , p c ∈ [0, 1] that satisfy the following conditions: where p and p indicate lower and upper bounds of p, respectively. Let where ties are broken uniformly at random. Moreover, we denote by S t = {b 1 , b 2 , · · · , b t } the set of t labels with the smallest probability upper bounds in the k largest ones and by p St = t j=1 p bj the sum of the t probability upper bounds, where t = 1, 2, · · · , k. Then, we have: where R l is the unique solution to the following equation: where Φ and Φ −1 are the cumulative distribution function and its inverse of the standard Gaussian distribution, respectively. Proof. Roughly speaking, our idea is to make the probability that the base classifier f predicts l when taking Y as input larger than the smallest one among the probabilities that f predicts for a set of arbitrary k labels selected from all labels except l. For simplicity, we let Γ = {1, 2, · · · , c} \ {l}, i.e., all labels except l. We denote by Γ k a set of k labels in Γ. We aim to find a certified radius R l such that we have max Γ k ⊆Γ min i∈Γ k Pr(f (Y) = i) < Pr(f (Y) = l), which guarantees l ∈ g k (x + δ). We first upper bound the minimal probability min i∈Γ k Pr(f (Y) = i) for a given Γ k , and then we upper bound the maximum value of the minimal probability among all possible Γ k ⊆ Γ. Finally, we obtain the certified radius R l via letting the upper bound of the maximum value smaller than Pr(f (Y) = l). Bounding min i∈Γ k Pr(f (Y) = i) for a given Γ k : We use S to denote a non-empty subset of Γ k and use |S| to denote its size. We define p S = i∈S p i , which is the sum of the upper bounds of the probabilities for the labels in S. Moreover, we define the following region associated with the set S: We have Pr(f (Y) ∈ S) ≤ Pr(Y ∈ B S ) by applying Lemma 3 to the set S. In addition, we have . Therefore, we have: Moreover, we have: where we have the first inequality because S is a subset of Γ k and we have the second inequality because the smallest value in a set is no larger than the average value of the set. Equation 27 holds for any S ⊆ Γ k . Therefore, by taking all possible sets S into consideration, we have the following: where S t is the set of t labels in Γ k whose probability upper bounds are the smallest, where ties are broken uniformly at random. We have Equation 30 from Equation 29 because Pr(Y ∈ B S ) decreases as p S decreases. Since Pr(Y ∈ B St ) increases as p St increases, Equation 30 reaches its maximum value when Γ k = {b 1 , b 2 , · · · , b k }, i.e., when Γ k is the set of k labels in Γ with the largest probability upper bounds. Formally, we have: where Obtaining R l : According to Lemma 3, we have the following for S = {l}: Recall that our goal is to make Pr(f (Y) = l) > max Γ k ⊆Γ min i∈Γ k Pr(f (Y) = i). It suffices to let: According to Lemma 2, we have Pr( . Therefore, we have the following constraint on δ: Since the left-hand side of the above inequality 1) decreases as ||δ|| 2 increases, 2) is larger than 0 when ||δ|| 2 → −∞, and 3) is smaller than 0 when ||δ|| 2 → ∞, we have the constraint ||δ|| 2 < R l , where R l is the unique solution to the following equation: B PROOF OF THEOREM 2 Following the terminology we used in proving Theorem 1, we define a region A {l} as follows: According to Lemma 2, we have Pr(X ∈ A {l} ) = p l . We first show the following lemma, which is the key to prove our Theorem 2. Lemma 4. Assuming we have p l + k j=1 p bj ≤ 1. For any perturbation δ 2 > R l , there exists k disjoint regions C bj ⊆ R d \ A {l} , j ∈ {1, 2, · · · , k} that satisfy the following: where the random variables X and Y are defined in Equation 10 and 11, respectively; and {b 1 , b 2 , · · · , b k } and S t are defined in Theorem 1. Proof. Our proof is based on mathematical induction and the intermediate value theorem. For convenience, we defer the proof to Appendix B.1. Next, we restate Theorem 2 and show our proof. Theorem 2 (Tightness of the Certified Radius). Assuming we have p l + k j=1 p bj ≤ 1 and p l + i=1,··· ,l−1,l+1,··· ,c p i ≥ 1. Then, for any perturbation ||δ|| 2 > R l , there exists a base classifier f * consistent with (1) but we have l / ∈ g k (x + δ). Proof. Our idea is to construct a base classifier such that l is not among the top-k labels predicted by the smoothed classifier for any perturbed example x + δ when ||δ|| 2 > R l . First, according to Lemma 4, we know there exists k disjoint regions C bj ⊆ R d \ A {l} , j ∈ {1, 2, · · · , k} that satisfy Equation 37 and 38. Moreover, we divide the remaining region R d \ (A {l} ∪ k j=1 C bj ) into c−k −1 regions, which we denote as C b k+1 , C b k+2 , · · · , C bc−1 and satisfy Pr(X ∈ C bj ) ≤ p bj for j ∈ {k + 1, k + 2, · · · , c − 1}. Note that b 1 , b 2 , · · · , b c−1 is some permutation of {1, 2, · · · , c} \ {l}. We can divide the remaining region into such c−k −1 regions because p l + i=1,··· ,l−1,l+1,··· ,c p i ≥ 1. Then, based on these regions, we construct the following base classifier: Based on the definition of f * , we have the following: Pr(f Therefore, f * satisfies the conditions in (1). Next, we show that l is not among the top-k labels predicted by the smoothed classifier for any perturbed example x + δ when ||δ|| 2 > R l . Specifically, we have: where j = 1, 2, · · · , k. Since we have found k labels whose probabilities are larger than the probability of the label l, we have l / ∈ g k (x + δ) when δ 2 > R l .",We study the certified robustness for top-k predictions via randomized smoothing under Gaussian noise and derive a tight robustness bound in L_2 norm.,top-$k$ ; Gaussian ; ImageNet ; Goodfellow et al. ; Carlini & Wagner ; Jia & Gong ; Goodfellow et al ; Svoboda ; al. ; Buckman,top-$k$ ; Gaussian ; ImageNet ; Goodfellow et al. ; Carlini & Wagner ; Jia & Gong ; Goodfellow et al ; Svoboda ; al. ; Buckman,"Classifiers are vulnerable to adversarial perturbations. To defend against them, certified robustness results have been derived. However, existing certified robustnesses are limited to top-1 predictions. In many real-world applications, top-$k$ predictions are more relevant. Our certified robustness is based on randomized smoothing, which turns any classifier to a new classifier via adding noise to an input example. We derive a tight robustness in $ell_2$ norm for top-$k$ predictions when using randomized smooth",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"In information retrieval, learning to rank constructs a machine-based ranking model which given a query, sorts the search results by their degree of relevance or importance to the query. Neural networks have been successfully applied to this problem, and in this paper, we propose an attention-based deep neural network which better incorporates different embeddings of the queries and search results with an attention-based mechanism. This model also applies a decoder mechanism to learn the ranks of the search results in a listwise fashion. The embeddings are trained with convolutional neural networks or the word2vec model. We demonstrate the performance of this model with image retrieval and text querying data sets. Learning to rank applies supervised or semi-supervised machine learning to construct ranking models for information retrieval problems. In learning to rank, a query is given and a number of search results are to be ranked by their relevant importance given the query. Many problems in information retrieval can be formulated or partially solved by learning to rank. In learning to rank, there are typically three approaches: the pointwise, pairwise, and listwise approaches Liu (2011) . The pointwise approach assigns an importance score to each pair of query and search result. The pairwise approach discerns which search result is more relevant for a certain query and a pair of search results. The listwise approach outputs the ranks for all search results given a specific query, therefore being the most general. For learning to rank, neural networks are known to enjoy a success. Generally in such models, neural networks are applied to model the ranking probabilities with the features of queries and search results as the input. For instance, RankNet Burges et al. (2005) applies a neural network to calculate a probability for any search result being more relevant compared to another. Each pair of query and search result is combined into a feature vector, which is the input of the neural network, and a ranking priority score is the output. Another approach learns the matching mechanism between the query and the search result, which is particularly suitable for image retrieval. Usually the mechanism is represented by a similarity matrix which outputs a bilinear form as the ranking priority score; for instance, such a structure is applied in Severyn & Moschitti (2015) . We postulate that it could be beneficial to apply multiple embeddings of the queries and search results to a learning to rank model. It has already been observed that for training images, applying a committee of convolutional neural nets improves digit and character recognition . From such an approach, the randomness of the architecture of a single neural network can be effectively reduced. For training text data, combining different techniques such as tf-idf, latent Dirichlet allocation (LDA) Blei et al. (2003) , or word2vec Mikolov et al. (2013) , has also been explored by Das et al. (2015) . This is due to the fact that it is relatively hard to judge different models a priori. However, we have seen no literature on designing a mechanism to incorporate different embeddings for ranking. We hypothesize that applying multiple embeddings to a ranking neural network can improve the accuracy not only in terms of ""averaging out"" the error, but it can also provide a more robust solution compared to applying a single embedding. For learning to rank, we propose the application of the attention mechanism Bahdanau et al. (2015) ; , which is demonstrated to be successful in focusing on different aspects of the input so that it can incorporate distinct features. It incorporates different embeddings with weights changing over time, derived from a recurrent neural network (RNN) structure. Thus, it can help us better summarize information from the query and search results. We also apply a decoder mechanism to rank all the search results, which provides a flexible list-wise ranking approach that can be applied to both image retrieval and text querying. Our model has the following contributions: (1) it applies the attention mechanism to listwise learning to rank problems, which we think is novel in the learning to rank literature; (2) it takes different embeddings of queries and search results into account, incorporating them with the attention mechanism; (3) double attention mechanisms are applied to both queries and search results. Section 2 reviews RankNet, similarity matching, and the attention mechanism in details. Section 3 constructs the attention-based deep net for ranking, and discusses how to calibrate the model. Section 4 demonstrates the performance of our model on image retrieval and text querying data sets. Section 5 discusses about potential future research and concludes the paper. Both queries and search results can be embedded with neural networks. Given an input vector x 0 representing a query or a search result, we denote the l-th layer in a neural net as is the bias, and f is a nonlinear activation function. If the goal is classification with C categories, then (P (y = 1), . . . , , where y is a class indicator, and sof tmax(u) From training this model, we may take the softmax probabilities as the embedding, and create different embeddings with different neural network structures. For images, convolutional neural nets (CNNs) LeCun et al. (1998) are more suitable, in which each node only takes information from neighborhoods of the previous layer. Pooling over each neighborhood is also performed for each layer of a convolutional neural net. With different networks, we can obtain different embeddings c 1 , . . . , c M . In the attention mechanism below, we generate the weights α t with an RNN structure, and summarize c t in a decoder series z t , Here f AT T and φ θ are chosen as tanh layers in our experiments. Note that the attention weight α t at state t depends on the previous attention weight α t−1 , the embeddings, and the previous decoder state z t−1 , and the decoder series z t sums up information of c t up to state t. As aforementioned, given multiple embeddings, the ranking process can be viewed as applying different attention weights to the embeddings and generating the decoder series z t , offering a listwise approach. However, since there are features for both queries and search results, we consider them as separately, and apply double attention mechanisms to each of them. Our full model is described below. In this paper, we proposed a new neural network for learning-to-rank problems which applies the attention mechanism to incorporate different embeddings of queries and search results, and ranks the search results with a listwise approach. Data experiments show that our model yields improvements over state-of-the-art techniques. For the future, it would be of interest to consider improving the RNN structure in the attention mechanism, and tailoring the embedding part of the neural network to this problem. P. Wu, S. Hoi, H. Xia, P. Zhao, D. Wang, and C. Miao. Online multimodal deep similarity learning with application to image retrieval. In Proceedings of the 21st ACM international conference on Multimedia, Barcelona, Spain, 2013.",learning to rank with several embeddings and attentions,three ; Liu ; RankNet Burges ; Severyn & Moschitti ; Dirichlet ; Mikolov et al ; Das et al ; Bahdanau et al ; RNN ; RankNet,three ; Liu ; RankNet Burges ; Severyn & Moschitti ; Dirichlet ; Mikolov et al ; Das et al ; Bahdanau et al ; RNN ; RankNet,"Learning to rank constructs a machine-based ranking model which sorts the search results by their relevance or importance to the query. Neural networks have been successfully applied to this problem, and in this paper, we propose an attention-based deep neural network which incorporates different embeddings of the queries and search results with an attention-based mechanism. This model also applies a decoder mechanism to learn the ranks of the search results in a listwise fashion. Learning to rank uses supervised or semi-supervised machine learning to construct ranking models for information retrieval problems.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"This paper presents a Mutual Information Neural Estimator (MINE) that is linearly scalable in dimensionality as well as in sample size. MINE is  back-propable and we prove that it is strongly consistent. We illustrate a handful of applications in which MINE is succesfully applied  to enhance the property of generative models in both unsupervised and supervised settings. We apply our framework to estimate the information bottleneck, and apply it in tasks related to supervised classification problems. Our results  demonstrate substantial added flexibility and improvement in these settings.
 Mutual information is an important quantity for expressing and understanding the relationship between random variables. As a fundamental tool of data science, it has found application in a range of domains and tasks, including applications to biomedical sciences, blind source separation (BSS, e.g., independent component analysis, BID23 , information bottleneck (IB, BID45 , feature selection BID28 BID36 , and causality BID8 .In contrast to correlation, mutual information captures the absolute statistical dependency between two variables, and thus can act as a measure of true dependence. Put simply, mutual information is the shared information of two random variables, X and Z, defined on the same probability space, (X ⇥ Z, F), where X ⇥ Z is the domain over both variables (such as R m ⇥ R n ), and F is the set of all possible outcomes over both variables. The mutual information has the form 1 : DISPLAYFORM0 where P XZ : F ! [0, 1] is a probabilistic measure (commonly known as a joint probability distribution in this context), and P X = R Z dP XZ and P Z = R X dP XZ are the marginals. The mutual information is notoriously difficult to compute. Exact computation is only tractable with discrete variables (as the sum can be computed exactly) or with a limited family of problems where the probability distributions are known and for low dimensions. For more general problems, common approaches include binning BID18 BID14 , kernel density estimation BID32 BID28 , Edgeworth expansion based estimators BID47 and likelihood-ratio estimators based on support vector machines (SVMs, e.g., BID43 . While the mutual information can be estimated from empirical samples with these estimators, they still make critical assumptions about the underlying distribution of samples, and estimate errors can reflect this. In addition , these estimators typically do not scale well with sample size or dimension.More recently, there has been great progress in the estimation of f -divergences BID34 and integral probability metrics (IPMs, Sriperumbudur et al., 2009 ) using deep neural networks (e.g., in the context of f -divergences and the Wasserstein distance or Fisher IPMs, BID35 BID4 BID33 . These methods are at the center of generative adversarial networks (GANs Goodfellow et al., 2014) , which train a generative model without any explicit assumptions about the underlying distribution of the data. One perspective on these works is that, given the correct constraints on a neural network, the network can be used to compute a variational lower-bound on the distance or divergence of implicit probability measures.In this paper we look to extend this estimation strategy to mutual information as given in equation 1, which we note corresponds to the Kullback-Leibler (KL-) divergence BID27 between the joint, P XZ and the product of the marginal distributions, P X ⌦ P Z , i.e., D KL (P XZ || P X ⌦ P Z ). This observation can be used to help formulate variational Bayes in terms of implicit distributions BID30 or INFOMAX BID7 .We introduce an estimator for the mutual information based on the Donsker-Varadhan representation of the KL-divergence BID38 . As with those introduced by BID35 , our estimator is scalable, flexible, and is completely trainable via back-propagation. The contributions of this paper are as follows.• We introduce the mutual information neural estimator (MINE), providing its theoretical bases and generalizability to other information metrics.• We illustrate that our estimator can be used to train a model with improved support coverage and richer learned representation for training adversarial models (such as adversariallylearned inferences, ALI, Dumoulin et al., 2016 ).• We demonstrate how to use MINE to improve reconstructions and inference in Adversarially Learned Inference Dumoulin et al. FORMULA0 on large scale Datasets.• We show that our estimator provides a method of performing the Information Bottleneck method BID45 in a continuous setting, and that this approach outperforms variational bottleneck methods BID1 . We proposed a mutual information estimator, which we called the mutual information neural estimator (MINE), that is scalable in dimension and sample-size. We demonstrated the efficiency of this estimator by applying it in a number of settings. First, a term of mutual information can be introduced alleviate mode-dropping issue in generative adversarial networks (GANs, Goodfellow et al., 2014) . Mutual information can also be used to improve inference and reconstructions in adversarially-learned inference (ALI, Dumoulin et al., 2016) . Finally, we showed that our estimator allows for tractable application of Information bottleneck methods BID45 in a continuous setting.through co-occurrence. We illustrate this perspective by considering distributions on natural image manifolds.Consider a random image in [0, 1] d by randomly sampling the intensity of each pixel independently. This image will show very little structure when compared to an image sampled form the manifold of natual images, M nature ⇢ [0, 1] d , as the latter is is bound to respect a number of physically possible priors (such as smoothness). We expect the mutual information of the pixels of images arising from M nature to be high. Differently put, the larger the number of simultaneously co-occurring subset of pixels in [0, 1] d , the higher the mutual information. In the language of cumulants tensors, the larger ponderation of higher order cumulants tensor in the cumulant generating function of the joint distribution over the pixels, the higher the mutual information, and the more structure there is to be found amongst the pixels. Note that the case of mutually independent pixels corresponds to joint distribution where the only cumulants contributing the joint distribution are of order one. This is the corner case where the joint distribution equals the product of marginals. Thus in order to assess the amount of structure it is enough to score how the joint distribution is different from the product of marginals. As we show in the paper, this principle can be extended to different divergences as well.",A scalable in sample size and dimensions mutual information estimator.,Mutual Information Neural Estimator ; BSS ; two ; F ; P XZ ; P Z ; Edgeworth ; Sriperumbudur et al. ; Wasserstein ; Fisher,Mutual Information Neural Estimator ; BSS ; two ; F ; P XZ ; P Z ; Edgeworth ; Sriperumbudur et al. ; Wasserstein ; Fisher,"The Mutual Information Neural Estimator (MINE) is linearly scalable in dimensionality as well as sample size. It is back-propable and proves to be strongly consistent. The framework is successfully applied to enhance the property of generative models in unsupervised and supervised settings. Mutual information is the shared information of two random variables, X and Z, defined on the same probability space, where X  Z is the domain over both variables, and F is the set of all possible outcomes over both variables. The mutual information is notoriously difficult to compute.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"Existing methods for AI-generated artworks still struggle with generating high-quality stylized content, where high-level semantics are preserved, or separating fine-grained styles from various artists. We propose a novel Generative Adversarial Disentanglement Network which can disentangle two complementary factors of variations when only one of them is labelled in general, and fully decompose complex anime illustrations into style and content in particular. Training such model is challenging, since given a style, various content data may exist but not the other way round. Our approach is divided into two stages, one that encodes an input image into a style independent content, and one based on a dual-conditional generator. We demonstrate the ability to generate high-fidelity anime portraits with a fixed content and a large variety of styles from over a thousand artists, and vice versa, using a single end-to-end network and with applications in style transfer. We show this unique capability as well as superior output to the current state-of-the-art. Computer generated art (Hertzmann, 2018) has become a topic of focus lately, due to revolutionary advancements in deep learning. Neural style transfer (Gatys et al., 2016) is a groundbreaking approach where high-level styles from artwork can be re-targeted to photographs using deep neural networks. While there has been numerous works and extensions on this topic, there are deficiencies in existing methods. For complex artworks, the methods that rely on matching neural network features and feature statistics, do not sufficiently capture the concept of style at the semantic level. Methods based on image-toimage translation (Isola et al., 2017) are able to learn domain specific definitions of style, but do not scale well to a large number of styles. In addressing these challenges, we found that style transfer can be formulated as a particular instance of a general problem, where the dataset has two complementary factors of variation, with one of the factors labelled, and the goal is to train a generative network where the two factors can be fully disentangled and controlled independently. For the style transfer problem, we have labelled style and unlabelled content. Based on various adversarial training techniques, we propose a solution to the problem and call our method Generative Adversarial Disentangling Network. Our approach consists of two main stages. First, we train a style-independent content encoder, then we introduce a dual-conditional generator based on auxiliary classifier GANs. We demonstrate the disentanglement performance of our approach on a large dataset of anime portraits with over a thousand artist-specific styles, where our decomposition approach outperforms existing methods in terms of level of details and visual quality. Our method can faithfully generate portraits with proper style-specific shapes and appearances of facial features, including eyes, mouth, chin, hair, blushes, highlights, contours, as well as overall color saturation and contrast. To show the generality of our method, we also include results on the NIST handwritten digit dataset where we can disentangle between writer identity and digit class when only the writer is labelled, or alternatively when only the digit is labelled. We introduced a Generative Adversarial Disentangling Network which enables true semanticlevel artwork synthesis using a single generator. Our evaluations and ablation study indicate that style and content can be disentangled effectively through our a two-stage framework, where first a style independent content encoder is trained and then, a content and styleconditional GANs is used for synthesis. While we believe that our approach can be extended to a wider range of artistic styles, we have validated our technique on various styles within the context of anime illustrations. In particular, this techniques is applicable, as long as we disentangle two factors of variation in a dataset and only one of the factors is labelled and controlled. Compared to existing methods for style transfer, we show significant improvements in terms of modeling high-level artistic semantics and visual quality. In the future, we hope to extend our method to styles beyond anime artworks, and we are also interested in learning to model entire character bodies, or even entire scenes. In the top two rows, in each column are two samples from the training dataset by the same artist. In each subsequent group of three rows, the leftmost image is from the training dataset. The images to the right are style transfer results generated by three different methods, from the content of the left image in the group and from the style of the top artist in the column. In each group, first row is our method, second row is StarGAN and third row is neural style. For neural style, the style image is the topmost image in the column. As stated in (Gatys et al., 2016) , which is based on an earlier work on neural texture synthesis (Gatys et al., 2015) , the justification for using Gram matrices of neural network features as a representation of style is that it captures statistical texture information. So, in essence, ""style"" defined as such is a term for ""texture statistics"", and the style transfer is limited to texture statistics transfer. Admittedly, it does it in smart ways, as in a sense the content features are implicitly used for selecting which part of the style image to copy the texture statistics from. As discussed in section 2 above, we feel that there is more about style than just feature statistics. Consider for example the case of caricatures. The most important aspects of the style would be what facial features of the subjects are exaggerated and how they are exaggerated. Since these deformations could span a long spatial distance, they cannot be captured by local texture statistics alone. Another problem is domain dependency. Consider the problem of transferring or preserving color in style transfer. If we have a landscape photograph taken during the day and want to change it to night by transferring the style from another photo taken during the night, or if we want to change the season from spring to autumn, then color would be part of the style we want to transfer. But if we have a still photograph and want to make it an oil painting, then color is part of the content, we may want only the quality of the strokes of the artwork but keep the colors of our original photo. People are aware of this problem and in (Gatys et al., 2017) , two methods, luminance-only style transfer and color histogram matching, are developed to optionally keep the color of the content image. However, color is only one aspect of the image for which counting it as style vs. content could be an ambiguity. For more complicated aspects, the option to keep or to transfer may not be easily available. We make two observations here. First, style must be more than just feature statistics. Second, the concept of ""style"" is inherently domain-dependent. In our opinion, ""style"" means different ways of presenting the same subject. In each different domain, the set of possible subjects is different and so is the set of possible ways to present them. So, we think that any successful style transfer method must be adaptive to the intended domain and the training procedure must actively use labelled style information. Simple feature based methods will never work in the general setting. This includes previous approaches which explicitly claimed to disentangle style and content, such as in (Kazemi et al., 2019) which adopts the method in the original neural style transfer for style and content losses, and also some highly accomplished methods like (Liao et al., 2017) . As a side note, for these reasons we feel that some previous methods made questionable claims about style. In particular, works like (Huang et al., 2018) and StyleGAN (Karras et al., 2018 ) made reference to style while only being experimented on collections of real photographs. By our definition, in such dataset, without a careful definition and justification there is only one possibly style, that is, photorealistic, so the distinction between style and content does not make sense, and calling a certain subset of factors ""content"" and others ""style"" could be an arbitrary decision. This is also why we elect to not test our method on more established GAN datasets like CelebA or LSUN, which are mostly collections of real photos.","An adversarial training-based method for disentangling two complementary sets of variations in a dataset where only one of them is labelled, tested on style vs. content in anime illustrations.",AI ; Generative Adversarial Disentanglement Network ; two ; only one ; one ; over a thousand ; Hertzmann ; Gatys et al. ; Isola ; al.,AI ; Generative Adversarial Disentanglement Network ; two ; only one ; one ; over a thousand ; Hertzmann ; Gatys et al. ; Isola ; al.,"Generative Adversarial Disentanglement Network can fully decompose complex anime illustrations into style and content. The approach is divided into two stages, one that encodes an input image into a style independent content, and one based on a dual-conditional generator. Neural style transfer is a groundbreaking approach where high-level styles from artwork can be re-targeted to photographs using deep neural networks. However, existing methods do not adequately capture the concept of style at the semantic level. The goal is to train a generative model.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"Formal verification of machine learning models has attracted attention recently, and significant progress has been made on proving simple properties like robustness to small perturbations of the input features. In this context, it has also been observed that folding the verification procedure into training makes it easier to train verifiably robust models. In this paper, we extend the applicability of verified training by extending it to (1) recurrent neural network architectures and (2) complex specifications that go beyond simple adversarial robustness, particularly specifications that capture temporal properties like requiring that a robot periodically visits a charging station or that a language model always produces sentences of bounded length. Experiments show that while models trained using standard training often violate desired specifications, our verified training method produces models that both perform well (in terms of test error or reward) and can be shown to be provably consistent with specifications. While deep neural networks (DNNs) have shown immense progress on diverse tasks (Sutskever et al., 2014; Mnih et al., 2015; Silver et al., 2016) , they are often deployed without formal guarantees of their correctness and functionality. Their performance is typically evaluated using test data, or sometimes with adversarial evaluation (Carlini & Wagner, 2017; Ebrahimi et al., 2018; Wang et al., 2019) . However, such evaluation does not provide formal guarantees regarding the absence of rare but possibly catastrophic failures (Administration; Board; Ross & Swetlitz, 2018) . Researchers have therefore started investigating formal verification techniques for DNNs. Most of the focus in this direction has been restricted to feedforward networks and robustness to adversarial perturbations (Tjeng et al., 2017; Raghunathan et al., 2018b; Ko et al., 2019) . However, many practically relevant systems involve DNNs that lead to sequential outputs (e.g., an RNN that generates captions for images, or the states of an RL agent). These sequential outputs can be interpreted as real-valued, discrete-time signals. For such signals, it is of interest to provide guarantees with respect to temporal specifications (e.g., absence of repetitions in a generated sequence, or that a generated sequence halts appropriately). Temporal logic provides a compact and intuitive formalism for capturing such properties that deal with temporal abstractions. Here, we focus on Signal Temporal Logic (STL) (Donzé & Maler, 2010) as the specification language and exploit its quantitative semantics to integrate a verification procedure into training to provide guarantees with regard to temporal specifications. Our approach builds on recent work , which is based on propagating differentiable numerical bounds through DNNs, to include specifications that go beyond adversarial robustness. Additionally, we propose extensions to ; that allow us to train auto-regressive GRUs/RNNs to certifiably satisfy temporal specifications. We focus on the problem of verified training for consistency rather than post-facto verification. To summarize, our contributions are as: • We present extensions to ; that allow us to extend verified training to novel architectures and specifications, including complex temporal specifications. To handle the auto-regressive decoder often used in RNN-based systems, we leverage differentiable approximations of the non-differentiable operations. • We empirically demonstrate the applicability of our approach to ensure verifiable consistency with temporal specifications while maintaining the ability of neural networks to achieve high accuracy on the underlying tasks across domains. For supervised learning, verified training on the train-data enables us to provide similar verification guarantees for unseen test-data. • We show that verified training results in robust DNNs whose specification conformance is significantly easier to guarantee than those trained adversarially or with data augmentation. Temporal properties are commonly desired from DNNs in settings where the outputs have a sequential nature. We extend verified training to tasks that require temporal properties to be satisfied, and to architectures such as auto-regressive RNNs whose outputs have a sequential nature. Our experiments suggest that verified training leads to DNNs that are more verifiable, and often with fewer failures. Future work includes extending verification/verified training to unbounded temporal properties. Another important direction is to develop better bound propagation techniques that can be leveraged for verified training. In the RL setting, an important direction is data-driven verification in the absence of a known model of the environment.",Neural Network Verification for Temporal Properties and Sequence Generation Models,al. ; Mnih ; Silver et al. ; Carlini & Wagner ; Ebrahimi et al. ; Wang ; Board ; Ross & Swetlitz ; Tjeng et al. ; Raghunathan,al. ; Mnih ; Silver et al. ; Carlini & Wagner ; Ebrahimi et al. ; Wang ; Board ; Ross & Swetlitz ; Tjeng et al. ; Raghunathan,"Formal verification of machine learning models has attracted attention recently, and significant progress has been made on proving simple properties like robustness to small perturbations of input features. Folding the verification procedure into training makes it easier to train verifiably robust models. In this paper, we extend the applicability of verified training to (1) recurrent neural network architectures and (2) complex specifications that go beyond simple adversarial robustness. While models trained using standard training often violate desired specifications, our verified training method produces models that both perform well (in terms of test error or reward) and can be shown to be prov",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"Global feature pooling is a modern variant of feature pooling providing better interpretatability and regularization. Although alternative pooling methods exist (eg. max, lp norm, stochastic), the averaging operation is still the dominating global pooling scheme in popular models. As fine-grained recognition requires learning subtle, discriminative features, we consider the question: is average pooling the optimal strategy? We first ask: ``is there a difference between features learned by global average and max pooling?'' Visualization and quantitative analysis show that max pooling encourages learning features of different spatial scales. We then ask ``is there a single global feature pooling variant that's most suitable for fine-grained recognition?'' A thorough evaluation of nine representative pooling algorithms finds that: max pooling outperforms average pooling consistently across models, datasets, and image resolutions; it does so by reducing the generalization gap; and generalized pooling's performance increases almost monotonically as it changes from average to max. We finally ask: ``what's the best way to combine two heterogeneous pooling schemes?'' Common strategies struggle because of potential gradient conflict but the ``freeze-and-train'' trick works best. We also find that post-global batch normalization helps with faster convergence and improves model performance consistently. Deeply rooted in the works of complex cells in the visual cortex (Hubel & Wiesel, 1962) and locally orderless images (Koenderink & Van Doorn, 1999) , feature pooling has been an indispensable component of visual recognition in both traditional bag-of-words (BOW) frameworks (Csurka et al., 2004; Lazebnik et al., 2006) using hand-crafted features (e.g. SIFT (Lowe, 2004) , HOG (Dalal & Triggs, 2005) ), and modern convolutional neural networks (CNNs) (LeCun et al., 1998; Krizhevsky et al., 2012) . A recent variant of this technique, called ""global feature pooling"" (Lin et al., 2013) , distinguishes itself by defining its pooling kernel the same size as input feature map. The pooling output is a scalar value indicating the existence of certain features (or patterns). Benefits of global pooling are two-fold: allowing better interpretation of the underlying filters as feature detectors, and serving as a strong network regularizer to reduce overfitting. Global pooling is thus used in most, if not all, recent state-of-the-art deep models He et al., 2016; Szegedy et al., 2017; Huang et al., 2017; Hu et al., 2018) in visual recognition. Unless otherwise noted, all the pooling methods discussed in this paper are used as the global pooling layer. Feature pooling is also of special interests to Fine-grained Visual Categorization (FGVC) (Rosch et al., 1976; Nilsback & Zisserman, 2010; Farrell et al., 2011) , where objects are classified into subcategories rather than basic categories. Carefully designed pooling schemes can learn helpful discriminative features and yield better performance without requiring more conv-layers in the network. Wang et al. (2018) provided a good example that combines three pooling operations: average, max and cross-channel pooling to learn to capture class-specific discriminative patches. Another major research direction is higher-order pooling: Lin et al. (2015) proposed to apply bilinear pooling (also know as second-order pooling) to capture pairwise correlations between the feature channels and model part-feature interactions; Gao et al. (2016) proposed compact bilinear pooling that applies random maclaurin projection and tensor sketch projection to approximate the outer product operation, greatly reducing parameters without sacrificing accuracy; Works along this line of research include low-rank bilinear pooling (Kim et al., 2016) , grassmann pooling (Wei et al., 2018) , kernel pooling (Cui et al., 2017) , and Alpha-pooling Simon et al. (2017) , etc. Although higher-order pooling methods output a vector rather than a scalar, they're still relevant as they reside in the same location as the global pooling layer. The most common pooling operations are average, max and striding. Striding always takes the activation at a fixed location, thus is never applied as global pooling. An abundant set of pooling flavors exist for both traditional and modern feature extractors. Stochastic pooling randomly chooses an activation according to a multinomial distribution decided by activation strength in the pooling region. Fractional max pooling (Graham, 2014) can be adapted to fractional sized pooling regions. Spatial pyramid pooling (He et al., 2015) outputs the combination of multiple max pooling with different sized pooling kernels. S3Pool (Zhai et al., 2017) , or stochastic spatial sampling Pooling, randomly picks a sub-region to apply max pooling to. Detail-preserving pooling (Saeedan et al., 2018) computes the output as the linear combination of input feature pixels whose weight is proportional to differences of the input intensities. Translation invariant pooling (Zhang, 2019) borrowed the idea of anti-alias by low-pass filtering from signal processing. A major pooling family, generalized pooling, aims to find a smooth transition between average and max pooling: k-max pooling (Kalchbrenner et al., 2014) outputs the average of the k highest activations of the feature map; l p norm pooling generalizes pooling to the p-norm of the input feature map (Boureau et al., 2010) ; soft pooling (Boureau et al., 2010) , or softmax pooling, outputs the sum of feature map weighted by softmax output; mixed pooling (Lee et al., 2016 ) computes a weighted sum of the max and average pooling; gated pooling (Lee et al., 2016 ) is similar to mixed pooling but the weight is learned instead. To the best of our knowledge, these pooling operations remain largely unexplored in the global pooling scenario. An interesting observation is that all highly-ranked classification models ""happen"" to choose the same averaging operation in their global pooling layer. Is this an arbitrary choice or actually the optimal strategy? How does average pooling compare against the other pooling schemes (e.g. max) in general image classification and also fine-grained visual recognition? Research (Boureau et al., 2010; Murray & Perronnin, 2014; Scherer et al., 2010; Hu et al., 2018; has shown that the selection of feature pooling affects the algorithm's performance, whether using hand-crafted features or deep features. Specially, Murray & Perronnin (2014) showed max pooling has superior performance in the traditional recognition framework because of its better pattern discriminability, and the same conclusion was made by an experimental evaluation of Scherer et al. (2010) using LeNet-5 (LeCun et al., 1998) on the Caltech 101 (Fei-Fei et al., 2007) and NORB (Jarrett et al., 2009 ) dataset. Boureau et al. (2010) provided a theoretical proof that ""max pooling is particularly well suited to the separation of features that are very sparse."" However, in squeeze and excitation networks (Hu et al., 2018) , global max pooling is reported to achieve 0.29% higher top-1 error and 0.05% higher top-5 error than average pooling. Similar results were reported by using VGG (Simonyan & Zisserman, 2015) and GoogleNet (Szegedy et al., 2016) . It seems max pooling is less preferred as a global pooling scheme than before. These intriguing contrasts call for a careful examination of both pooling schemes. Our investigation begins with the two most common global average and max pooling. Specially, we're interested to know what features have both pooling methods helped learned. Feature map visualization indicates that max pooling produces sparser final conv-layer feature maps. This is further verified quantitatively by two perceptually-consistent sparsity metrics: discrete entropy and thresholded l 0 norm. Visualization of final conv-layer filters further helps us conclude empirically that: global average pooling encourages object-level features while global max pooling focuses more on part-level features. As class-specific features often reside in localized object parts in finegrained datasets, it's equal to say global max pooling find more discriminative features, well aligned with previous findings (Murray & Perronnin, 2014; . The second question to answer is that ""is there a single optimal pooling operation on different finegrained datasets across different models?"" We evaluate nine representative pooling schemes, which are: average, max, k-max, l p norm, soft, logavgexp, mixed, gated, and stochastic pooling, in the experiment section. We make several observations: max pooling outperforms average pooling across datasets, input resolutions, and models. The reason behind this phenomenon, besides their feature differences, is relevant to the fact that max pooling generalizes better. Most pooling methods we evaluated performs better than average pooling, with k-max (k = 2) and mixed pooling (α = 0.5) being the top two. Our k-max pooling model, when trained properly, beats all previous higher-order pooling methods using the same backbone. The fact that no single pooling works best for all models leads to the need for learnable pooling, where the pooling function is not chosen by heuristic, but optimized via gradient descent. However, our finding that model performance decrease and generalization gap increases in an almost monotonic way when generalized pooling changes from max to average casts a shadow upon the learnable generalized pooling. A pooling is better not because it minimizes training loss, but because it better regularizes the model. Throughout our experiment, post-global batch normalization is applied as another key ingredient achieving consistent performance improvement and faster convergence. Finally, we explore the integration of heterogeneous pooling. Since different features can be learned by average or max pooling, our assumption is that learning a model with heterogeneous poolings will lead to better performance, but what's the best way to integrate them? We review and evaluate three common strategies, but found their improvement upon single pooling is limied. Our hypothesis is that different pooling methods interfere and cancel each other out when learned together. We instead propose to apply the ""freeze-and-train"" trick. The intuition is that the frozen branch won't degrade during training and the gradients will be well separated. The resulting architecture only adds a tiny amount of parameters to a backbone network, but consistently outperforms single pooling models. In this paper, we focus on the global pooling layer in popular classification models as applied to the task of fine-grained recognition. By visualizing the final conv-layer filters and feature maps, we discover that max pooling produces much sparser feature maps and helps the network learn part-level features. Average pooling, on the other hand, encourages object-level features to be learned. We evaluated nine representative global pooling schemes for fine-grained recognition. K-max (k = 2) pooling outperformed all other global pooling schemes and is actually better than all higher-order pooling models. We made several observations from pooling benchmark experiments: (1) max pooling performs better than average pooling across datasets, models, and input resolution; (2) max pooling generalizes better than average pooling; and (3) model performance displays an approximately monotonically increasing characteristic when generalized pooling changes from average to max. Based on these observations, we discussed the potential risk of learning a generalized pooling: namely that minimizing training loss may lead to average pooling and thus be prone to overfitting. We highlight the importance of post-global batch normalization -which is absent from most, if not all, popular state-of-the-art models -in helping to attain faster convergence and in consistently improving model performance. We evaluated several strategies for heterogeneous pooling integration. The freeze-and-train trick performs best among all end-to-end learnable models. For future work, we suggest consideration of models learned from scratch alongside those fine-tuned from pretrained weights. In addition, experiments should be explored on a broader set of data, not just on fine-grained datasets, in order to affirm whether the findings presented here generalize to more general-purpose datasets such as ImageNet and/or MS-COCO.",A benchmark of nine representative global pooling schemes reveals some interesting findings.,max ; first ; nine ; two ; Hubel & Wiesel ; Koenderink & Van Doorn ; Csurka et al. ; Lazebnik et al. ; SIFT ; HOG (Dalal & Triggs,max ; first ; nine ; two ; Hubel & Wiesel ; Koenderink & Van Doorn ; Csurka et al. ; Lazebnik et al. ; SIFT ; HOG (Dalal & Triggs,"Global feature pooling is a modern variant of feature pooling providing better interpretatability and regularization. Although alternative pooling methods exist (eg. max, lp norm, stochastic), the averaging operation is still the dominant global pooling scheme in popular models. Visualization and quantitative analysis show that max pooling encourages learning features of different spatial scales. The freeze-and-train'' trick works best. Post-global batch normalization helps with faster convergence and improves model performance consistently.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"The celebrated Sequence to Sequence learning (Seq2Seq) technique and its numerous variants achieve excellent performance on many tasks. However, many machine learning tasks have inputs naturally represented as graphs; existing Seq2Seq models face a significant challenge in achieving accurate conversion from graph form to the appropriate sequence. To address this challenge, we introduce a general end-to-end graph-to-sequence neural encoder-decoder architecture that maps an input graph to a sequence of vectors and uses an attention-based LSTM method to decode the target sequence from these vectors. Our method first generates the node and graph embeddings using an improved graph-based neural network with a novel aggregation strategy to incorporate edge direction information in the node embeddings. We further introduce an attention mechanism that aligns node embeddings and the decoding sequence to better cope with large graphs. Experimental results on bAbI, Shortest Path, and Natural Language Generation tasks demonstrate that our model achieves state-of-the-art performance and significantly outperforms existing graph neural networks, Seq2Seq, and Tree2Seq models; using the proposed bi-directional node embedding aggregation strategy, the model can converge rapidly to the optimal performance. The celebrated Sequence to Sequence learning (Seq2Seq) technique and its numerous variants achieve excellent performance on many tasks such as Neural Machine Translation BID13 , Natural Language Generation (NLG) BID24 and Speech Recognition BID24 . Most of the proposed Seq2Seq models can be viewed as a family of encoder-decoders , where an encoder reads and encodes a source input in the form of sequences into a continuous vector representation of fixed dimension, and a decoder takes the encoded vectors and outputs a target sequence. Many other enhancements including Bidirectional Recurrent Neural Networks (Bi-RNN) BID20 or Bidirectional Long Short-Term Memory Networks (Bi-LSTM) (Graves & Schmidhuber, 2005) as encoder, and attention mechanism Luong et al., 2015) , have been proposed to further improve its practical performance for general or domain-specific applications.Despite their flexibility and expressive power, a significant limitation with the Seq2Seq models is that they can only be applied to problems whose inputs are represented as sequences. However, the sequences are probably the simplest structured data, and many important problems are best expressed with a more complex structure such as graphs that have more capacity to encode complicated pair-wise relationships in the data. For example, one task in NLG applications is to translate a graph-structured semantic representation such as Abstract Meaning Representation to a text expressing its meaning BID2 . In addition, path planning for a mobile robot (Hu & Yang, 2004) and path finding for question answering in bAbI task (Li et al., 2015) can also be cast as graph-to-sequence problems.On the other hand, even if the raw inputs are originally expressed in a sequence form, it can still benefit from the enhanced inputs with additional information (to formulate graph inputs). For example, for semantic parsing tasks (text-to-AMR or text-to-SQL), they have been shown better performance by augmenting the original sentence sequences with other structural information such as dependency parsing trees (Pust et al., 2015) . Intuitively, the ideal solution for graph-to-sequence tasks is to build a more powerful encoder which is able to learn the input representation regardless of its inherent structure.To cope with graph-to-sequence problems, a simple and straightforward approach is to directly convert more complex structured graph data into sequences (Iyer et al., 2016; BID15 Liu et al., 2017) , and apply sequence models to the resulting sequences. However, the Seq2Seq model often fails to perform as well as hoped on these problems, in part because it inevitably suffers significant information loss due to the conversion of complex structured data into a sequence, especially when the input data is naturally represented as graphs. Recently, a line of research efforts have been devoted to incorporate additional information by extracting syntactic information such as the phrase structure of a source sentence (Tree2seq) BID12 , by utilizing attention mechanisms for input sets (Set2seq) BID32 , and by encoding sentences recursively as trees (Socher et al., 2010; BID29 . Although these methods achieve promising results on certain classes of problems, most of the presented techniques largely depend on the underlying application and may not be able to generalize to a broad class of problems in a general way.To address this issue, we propose Graph2Seq, a novel attention-based neural network architecture for graph-to-sequence learning. The Graph2Seq model follows the conventional encoder-decoder approach with two main components, a graph encoder and a sequence decoder. The proposed graph encoder aims to learn expressive node embeddings and then to reassemble them into the corresponding graph embeddings. To this end, inspired by a recent graph representation learning method (Hamilton et al., 2017a) , we propose an inductive graph-based neural network to learn node embeddings from node attributes through aggregation of neighborhood information for directed and undirected graphs, which explores two distinct aggregators on each node to yield two representations that are concatenated to form the final node embedding. In addition, we further design an attention-based RNN sequence decoder that takes the graph embedding as its initial hidden state and outputs a target prediction by learning to align and translate jointly based on the context vectors associated with the corresponding nodes and all previous predictions. Our code and data are available at https://github.com/anonymous/Graph2Seq.Graph2Seq is simple yet general and is highly extensible where its two building blocks, graph encoder and sequence decoder, can be replaced by other models such as Graph Convolutional (Attention) Networks (Kipf & Welling, 2016; BID31 or their extensions BID19 , and LSTM (Hochreiter & Schmidhuber, 1997) . We highlight three main contributions of this paper as follows:• We propose a new attention-based neural networks paradigm to elegantly address graphto-sequence learning problems that learns a mapping between graph-structured inputs to sequence outputs, which current Seq2Seq and Tree2Seq may be inadequate to handle.• We propose a novel graph encoder to learn a bi-directional node embeddings for directed and undirected graphs with node attributes by employing various aggregation strategies, and to learn graph-level embedding by exploiting two different graph embedding techniques. Equally importantly, we present an attention mechanism to learn the alignments between nodes and sequence elements to better cope with large graphs.• Experimental results show that our model achieves state-of-the-art performance on three recently introduced graph-to-sequence tasks and significantly outperforms existing graph neural networks, Seq2Seq, and Tree2Seq models. In this paper, we study the graph-to-sequence problem, introducing a new general and flexible Graph2Seq model that follows the encoder-decoder architecture. We showed that, using our proposed bi-directional node embedding aggregation strategy, the graph encoder could successfully learn representations for three representative classes of directed graph, i.e., directed acyclic graphs, directed cyclic graphs and sequence-styled graphs. Experimental results on three tasks demonstrate that our model significantly outperforms existing graph neural networks, Seq2Seq, and Tree2Seq baselines on both synthetic and real application datasets. We also showed that introducing an attention mechanism over node representation into the decoding substantially enhances the ability of our model to produce correct target sequences from large graphs. Since much symbolic data is represented as graphs and many tasks express their desired outputs as sequences, we expect Graph2Seq to be broadly applicable to unify symbolic AI and beyond. A PSEUDO-CODE OF THE GRAPH-TO-SEQUENCE ALGORITHM",Graph to Sequence Learning with Attention-Based Neural Networks,Sequence ; first ; bAbI ; Shortest Path ; Natural Language Generation ; Neural Machine Translation ; Bidirectional Recurrent Neural Networks ; Bi-RNN ; Graves & Schmidhuber ; Luong et al.,Sequence ; first ; bAbI ; Shortest Path ; Natural Language Generation ; Neural Machine Translation ; Bidirectional Recurrent Neural Networks ; Bi-RNN ; Graves & Schmidhuber ; Luong et al.,"The Sequence to Sequence learning (Seq2Seq) technique and its numerous variants achieve excellent performance on many tasks. However, many machine learning tasks have inputs naturally represented as graphs. To address this challenge, we introduce a general end-to-end graph-to-sequence neural encoder-decoder architecture that maps an input graph to a sequence of vectors and uses an attention-based LSTM method to decode the target sequence from these vectors. Experimental results on bAbI, Shortest Path, and Natural Language Generation tasks demonstrate that our",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"Graphs possess exotic features like variable size and absence of natural ordering of the nodes that make them difficult to analyze and compare. To circumvent this problem and learn on graphs, graph feature representation is required. Main difficulties with feature extraction lie in the trade-off between expressiveness, consistency and efficiency, i.e. the capacity to extract features that represent the structural information of the graph while being deformation-consistent and isomorphism-invariant. While state-of-the-art methods enhance expressiveness with powerful graph neural-networks, we propose to leverage natural spectral properties of graphs to study a simple graph feature: the graph Laplacian spectrum (GLS). We analyze the representational power of this object that satisfies both isomorphism-invariance, expressiveness and deformation-consistency. In particular, we propose a theoretical analysis based on graph perturbation to understand what kind of comparison between graphs we do when comparing GLS. To do so, we derive bounds for the distance between GLS that are related to the divergence to isomorphism, a standard computationally expensive graph divergence. Finally, we experiment GLS as graph representation through consistency tests and classification tasks, and show that it is a strong graph feature representation baseline. No matter where and at which scale we look, graphs are present. Social networks, public transport, information networks, molecules, any structural dependency between elements of a global system is a graph. An important task is to extract information from these graphs in order to understand whether they contain certain structural properties that can be represented and used in downstream machine learning tasks. In general, graphs are difficult to use as input of standard algorithms because of their exotic features like variable size and absence of natural orientation. Consequently, graph feature representation with equal dimensionality and dimension-wise alignment is required to learn on graphs. Any embedding method is traditionally associated to a trade-off between preservation of structural information (expressiveness) and computation time (efficiency) (Cai et al., 2018) . In the expressiveness, we particularly consider two key attributes of graph feature representation: consistency under deformation and invariance under isomorphism. The first forces the embedding to discriminate two graphs consistently with their structural dissimilarity. The second enables to have one representation for each graph, which can be a challenge since one graph has many possible orientations. In this paper, we propose to analyze the importance of satisfying the introduced criteria through a known but unused, simple, expressive and efficient candidate graph feature representation: the graph Laplacian spectrum (GLS). The Laplacian matrix of a graph is a well-known object in spectral learning (Belkin & Niyogi, 2002) for several reasons. First, the Laplacian eigenvalues give many structural information like the presence of communities and partitions (Newman, 2013) , the regularity, the closed-walks enumeration, the diameter or the connectedness of the graph (Brouwer & Haemers, 2011) . It is also interpretable in term of physics or mechanics (Bonald et al., 2018) . It is backed by efficient and robust approximate eigen decomposition algorithms enabling to scale on large graphs and huge datasets (Halko et al., 2011) . These properties give intuition that GLS can be an appropriate candidate for graph representation. In this paper we go further and analyze additional interesting properties of the Laplacian spectrum through the following contributions: (1) we build a perturbation-based framework to analyze the representation capacity of the GLS, (2) we analyze Interpretation of the GLS The smallest non-zero eigenvalue of the Laplacian is the spectral gap, corresponding the difference between the two largest eigenvalues of the Laplacian. It contains information about the connectivity of the graph. High spectral gap means high connectivity. For example, given a number of vertices in a connected graph, a minimum spectral gap indicates that the graph is a double kite (Marsden, 2013) . The largest eigenvalue gives a lower bound of the maximal node degree of the graph. The spectral gap can also be viewed as the difference in energy between the ground state and first excited state of a dynamical system (Cubitt et al., 2015) . More generally each eigenvalue of the Laplacian corresponds to the energy level of a stable configuration of the nodes in the embedding space (Bonald et al., 2018) . The lower the energy, the stabler the configuration. In (Shuman et al., 2016) , the Laplacian eigenvalues correspond to frequencies associated to a Fourier decomposition of any signal living on the vertices of the graph. Thus, the truncation of the Fourier decomposition acts as filter on the signal. Characterizing a graph by the some eigenvalues of its Laplacian is thus comparable to characterizing a melody by some fundamental frequencies. In summary, Laplacian spectrum contains many graph structural information. Methods to get such information are generally computationally expensive. In the light of these properties, we go further and analyze in the following sections the capacity of GLS to represent graph structure. In this paper, we analyzed the graph Laplacian spectrum (GLS) as whole graph representation. In particular, we showed that comparing two GLS is a good proxy for the divergence between two graphs in term of structural information. We coupled these results to the natural invariance to isomorphism, the simplicity of implementation, the computational efficiency offered by modern randomized algorithms and the rare occurrence of detrimental L-cospectral non-isomorphic graphs to propose the GLS as a strong baseline graph feature representation. A PROOF OF LEMMA 1 Proof. with L P * = diag(P * 1) − P * = D P * − P * and 1 the unit vector. Therefore, Moreover, from Weyl's eigenvalues inequalities and since eigenvalues are isomorphism invariant: Hence: Now let (λ, x) be any eigen couple of a matrix M ∈ M n×n . We can always pick i ∈ {1 . . . n} and build x such that |x i | = 1 and |x j =i | < 1. Hence: Using previous results we get: Proof. We remind that the Forbenius norm is unitarily invariant thanks to the cyclic property of the trace. For anyP ∈ O(|V 2 |) we have: We also have that Hence:",We study theoretically the consistency the Laplacian spectrum and use it as whole-graph embeddding,Laplacian ; GLS ; al. ; two ; first ; second ; one ; Belkin & Niyogi ; Newman ; Brouwer & Haemers,Laplacian ; GLS ; al. ; two ; first ; second ; one ; Belkin & Niyogi ; Newman ; Brouwer & Haemers,"Graphs possess exotic features that make them difficult to analyze and compare. To avoid this problem, graph feature representation is required. The main difficulties with feature extraction lie in the trade-off between expressiveness, consistency and efficiency, i.e. the capacity to extract features that represent the structural information of the graph while being deformation-consistent and isomorphism-invariant. In particular, we propose a theoretical analysis based on graph perturbation to understand what kind of comparison between graphs we do when comparing GLS.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"This paper proposes a Pruning in Training (PiT) framework of learning to reduce the parameter size of networks. Different from existing works, our PiT framework employs the sparse penalties to train networks and thus help rank the importance of weights and filters. Our PiT algorithms can directly prune the network without any fine-tuning. The pruned networks can still achieve comparable performance to the original networks. In particular, we introduce the (Group) Lasso-type Penalty (L-P /GL-P), and (Group) Split LBI Penalty (S-P / GS-P) to regularize the networks, and a pruning strategy proposed  is used in help prune the network. We conduct the extensive experiments on MNIST, Cifar-10, and miniImageNet. The results validate the efficacy of our proposed methods. Remarkably, on MNIST dataset, our PiT framework can save 17.5% parameter size of LeNet-5, which achieves the 98.47% recognition accuracy. The expressive power of Deep Convolutional Neural Networks (DNNs) comes from the millions of parameters, which are optimized by various algorithms such as Stochastic Gradient Descent (SGD), and Adam BID18 . However, one has to strike a trade-off between the representation capability and computational cost, caused by the plenty of parameters in the real world applications, e.g., robotics, self-driving cars, and augmented reality. Pruning significant number of parameters would be essential to reduce the computational complexity and thus facilitate a timely and efficient fashion on a resource-limited platform, e.g. devices of Internet of Things (IoT). In addition, it has long been conjectured that the state-of-the-art DNNs may be too complicated for most specific tasks; and we may have the free lunch of ""reducing 2× connections without losing accuracy and without retraining"" BID7 .To compress DNNs, recent efforts had been made on learning the DNNs of small size. They either reduce the number and size of weights of parameters of original networks, and fine-tune the pruned networks BID0 ; BID32 , or distill the knowledge of large model , or directly learning the compact and lightweight small DNNs, such as ShuffleNet BID24 , MobileNet Howard et al. (2017) , and SqueezeNet BID13 . Note that, (1) to efficiently learn the compressed DNNs, previous works had to introduce additional computational cost in fine-tuning, or training the updated networks; (2) it is not practical nor desirable to learn the tailored, or bespoke networks for any applications, beyond computer vision tasks.To this end, the center idea of this paper is to propose a Pruning in Training (PiT) framework that enables pruning networks in the training process. Particularly , the sparsity regularizers, including lasso-type, and split LBI penalties are applied to train the networks. Such regularizers not only encourage the sparsity of DNNs, i.e., fewer (sparse) connections with non-zero values, but also can accelerate the speed of DNNs convergence. Furthermore, in the learning process, we can iteratively compute the regularization path of layer-wise parameters of DNNs. The parameters can be ranked by the regularization path in a descending order, as BID3 . The parameters in the high rank are in the high priority of not being pruned.More importantly, our PiT can learn the sparse structures of DNNs, and utilize the functionality of filters and connection weights (in fully connected layers). In the optimal cases , the weights (or filters) of each layer should be learned fully orthogonal to each other and thus formulate an orthogonal basis. The orthogonal constraint may be only enforced as the initialization (e.g., SVD Jia (2017) and BID26 ), or via the other regularization tricks, such as dropout preventing co-adaption BID27 , or batch normalization reducing the internal covariate shift of hidden layers BID14 . Therefore, our PiT can help uncover redundant information in a network by compressing less important filters and weights, and facilitate pruning out more interpretable networks. As the experiments shown in these three datasets, our PiT indeed can learn to prune networks without fine-tuning. We give some further discussion and highlight the potential future works, 1. In all our experiments, our L-P / GL-P, and S-P / GS-P are applied to, at most, four layers in one network. Theoretically, our PiT algorithms should be able to be directly applied to any layers of DNNs, since PiT only adds some sparse penalties in the loss functions. However, in practice, we found that the network training algorithm, i.e., SGD in Alg. 3, is unstable, if we apply the sparse penalties more than four layers. It will take much more time and training epochs to get the networks converged. 2. Essentially, our PiT presents a feature selection algorithm, which can dynamically learn the importance of weights and filters in the learning process; mostly importantly, we donot need any fine-tuning step, which, we believe, will destroy values and properties of selected weights and filters. Therefore, it would be very interesting to analyze the statistical properties of selected features in each layer. 3. Theoretically, we can not guarantee the orthogonality of weights and filters in the trained model. Empirically, we adapt some strategies. For example, the weights and filters of each layer can be orthogonally initialized; and we apply the common regularization tricks, e.g., dropout, and batch normalization. These can help decorrelate the learned parameters of the same layers. Practically, our PiT framework works well in selecting the important parameters and prune the networks as shown in the experiments. We also visualize the correlation between removed and none removed filters in the Appendix. 4. It is a conjecture that the capacity of DNNs may be too large to learn a small dataset;and it is essential to do network pruning. However, it is also an open question as how to numerically measure the capacity of DNNs and the complexity of one dataset.",we propose an algorithm of learning to prune network by enforcing structure sparsity penalties,Pruning in Training ( ; PiT ; Penalty ; miniImageNet ; Deep Convolutional Neural Networks ; millions ; Stochastic Gradient Descent ; Howard et al ; Pruning in Training (PiT ; LBI,Pruning in Training ( ; PiT ; Penalty ; miniImageNet ; Deep Convolutional Neural Networks ; millions ; Stochastic Gradient Descent ; Howard et al ; Pruning in Training (PiT ; LBI,"Pruning in Training (PiT) framework of learning to reduce parameter size of networks employs sparse penalties to train networks and rank the importance of weights and filters. The pruned networks can still achieve comparable performance to the original networks. The expressive power of Deep Convolutional Neural Networks (DNNs) comes from millions of parameters, which are optimized by various algorithms such as Stochastic Gradient Descent (SGD), and Adam BID18. However, there is a trade-off between the representation capability and computational cost.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"While much recent work has targeted learning deep discrete latent variable models with variational inference, this setting remains challenging, and it is often necessary to make use of potentially high-variance gradient estimators in optimizing the ELBO. As an alternative, we propose to optimize a non-ELBO objective derived from the Bethe free energy approximation to an MRF's partition function. This objective gives rise to a saddle-point learning problem, which we train inference networks to approximately optimize. The derived objective requires no sampling, and can be efficiently computed for many MRFs of interest. We evaluate the proposed approach in learning high-order neural HMMs on text, and find that it often outperforms other approximate inference schemes in terms of true held-out log likelihood. At the same time, we find that all the approximate inference-based approaches to learning high-order neural HMMs we consider underperform learning with exact inference by a significant margin. There has been much recent interest in learning deep generative models with discrete latent variables BID29 BID28 BID12 BID25 BID15 Lee et al., 2018, inter alia) , especially in the case where these latent variables have structure -that is, where the interdependence between the discrete latents is modeled. Most recent work has focused on learning these models with variational inference BID14 , and in particular with variational autoencoders (VAEs) BID17 BID32 .Variational inference has a number of convenient properties, including that it involves the maximization of the evidence lower-bound (ELBO), a lower bound on the log marginal likelihood of the data. At the same time, when learning models with discrete latent variables variational inference may require the use of potentially high-variance gradient estimators, which are obtained during learning by sampling from the variational posterior; see Appendix A for an empirical investigation into the variance of various popular estimators when learning neural text HMMs with VAEs.In this paper we investigate learning discrete latent variable models with an alternative objective to the ELBO. In particular , we propose to approximate the intractable log marginal likelihood with an objective deriving from the Bethe free energy BID1 , a quantity which is intimately related to loopy belief propagation (LBP) BID30 BID45 BID9 BID9 , and which is the basis for ""outer approximations"" to the marginal polytope BID39 . The Bethe free energy is attractive because if all the factors in the factor graph associated with the model have low degree, it can often be evaluated efficiently, without any need for approximation by sampling (see Section 2). Of course, requiring all factors in the factor graph to be of low degree severely limits the expressiveness of directed graphical models. It does not, however , limit the expressiveness of markov random fields (MRFs) (i.e., undirected graphical models) as severely, since we can simply have an extremely loopy MRF, with arbitrary pairwise factors; see FIG1 (c) and Section 2.2.We accordingly propose to learn deep, undirected graphical models with latent variables, using a saddlepoint objective that makes use of the Bethe free energy approximation to the model's partition functions. We further amortize inference by using ""inference networks"" BID36 BID17 BID13 BID38 in optimizing the saddle-point objective. Unlike the ELBO, our objective will not form a lower bound on the log marginal likelihood, but an approximation to it. At the same time (and unlike other recent work on MRFs with a variational flavor BID21 BID23 ), this objective can be optimized efficiently, without sampling, and in our experiments in learning neural HMMs on text it outperforms other approximate inference methods in terms of held out log likelihood. We emphasize, however , that despite the improvement observed when training with the proposed objective, in our experiments all approximate inference methods were found to significantly underperform learning with exact inference; see Section 4.3. We begin with the results obtained by maximizing the true log marginal likelihood of the training data under both the directed (""Full"" in Table 1 ) and undirected models (""Pairwise MRF"" in Table 1 ), by backpropagating gradients through the relevant dynamic programs. These results establish how well our models perform under exact inference, and are shown in the last row of each subtable in Table 1 . We see that perplexities are roughly comparable between the directed and undirected models when trained with exact inference.We now consider the remaining directed HMM results of Table 1 , where the models are trained with approximate inference. In the first row of each ""Full"" subtable there, we show the result of maximizing the ELBO using a mean field-style posterior approximation and the REINFORCE BID43 gradient estimator, with an input-dependent baseline to reduce variance BID29 . The results are quite poor, with this approximate inference scheme leading to a gain of almost 200 points in perplexity over exact inference. Using the tighter IWAE BID3 objectives improves performance slightly in all cases, though the most dramatic performance improvement comes from using a first-order HMM posterior in maximizing the ELBO, which can be sampled from exactly using quantities calculated with the forward algorithm BID31 BID4 BID34 BID49 . While these results are encouraging, note that in general we may not have an exact dynamic program for sampling from a lower-order structured model, and that moreover we still appear to incur a perplexity penalty of more than 100 points over exact inference; see Appendix A for an empirical comparison of the variance of these estimators.Moving to the MRF results, the second row of each ""Pairwise MRF"" subtable in Table 1 contains the results of optimizing F as a saddle point problem. While this approach too underperforms exact inference by approximately 100 points in perplexity, somewhat remarkably it manages to consistently outperform the best approximate inference results for the directed models by a fair margin. The first row of each ""Pairwise MRF"" subtable in Table 1 attempts to determine whether the jump in perplexity when moving to the F objective is due to the approximate inference or to the approximate objective, by minimizing the F objective using the exact marginals, as calculated by a dynamic program. (Note that this is not equivalent to the negative log marginal likelihood, since the factor graphs are loopy). Interestingly, we see that this performs almost as well as the exact objective, suggesting that, at least for HMM models, the F objective is reasonable, and approximate inference remains the problem.Despite these encouraging results, we note that there are several drawbacks to the proposed approach. In particular, we find that in practice F indeed can over-or under-estimate perplexity. Moreover, while ELBO values are not perfectly correlated with their corresponding true perplexities, values of F seem even less correlated, which necessitates finding correlated proxies of perplexity that may be monitored during training. Finally, we note that explicitly calculating the projection onto the nullspace of A may be prohibitive for some models (e.g., large RBMs BID35 ), and so other approaches to tackling the constrained optimization problem are likely necessary. We have presented an objective for learning latent-variable MRFs based on the Bethe approximation to the partition function, which can often be efficiently evaluated and requires no sampling. This objective leads to slightly better held-out perplexities than other approximate inference methods when learning neural HMMs. Future work will examine scaling the proposed method to larger, non-sequential MRFs, and whether F -like objectives can be made to better correlate with the true perplexity.",Learning deep latent variable MRFs with a saddle-point objective derived from the Bethe partition function approximation.,MRF ; Lee et al. ; inter alia ; Appendix ; ELBO ; Pairwise MRF ; the last row ; HMM ; the first row ; first,MRF ; Lee et al. ; inter alia ; Appendix ; ELBO ; Pairwise MRF ; the last row ; HMM ; the first row ; first,"In learning deep discrete latent variable models with variational inference, it is often necessary to use potentially high-variance gradient estimators in optimizing the ELBO. As an alternative, we propose to optimize a non-ELBO objective derived from the Bethe free energy approximation to an MRF's partition function. This objective can be efficiently computed for many MRFs of interest. We evaluate the proposed approach in learning high-order neural HMMs on text, and find that it often outperforms other approximate inference schemes in terms of true held-out log likelihood",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"It is becoming increasingly clear that many machine learning classifiers are vulnerable to adversarial examples. In attempting to explain the origin of adversarial examples, previous studies have typically focused on the fact that neural networks operate on high dimensional data, they overfit, or they are too linear. Here we show that distributions of logit differences have a universal functional form. This functional form is independent of architecture, dataset, and training protocol; nor does it change during training. This leads to adversarial error having a universal scaling, as a power-law, with respect to the size of the adversarial perturbation. We show that this universality holds for a broad range of datasets (MNIST, CIFAR10, ImageNet, and random data), models (including state-of-the-art deep networks, linear models, adversarially trained networks, and networks trained on randomly shuffled labels), and attacks (FGSM, step l.l., PGD). Motivated by these results, we study the effects of reducing prediction entropy on adversarial robustness. Finally, we study the effect of network architectures on adversarial sensitivity. To do this, we use neural architecture search with reinforcement learning to find adversarially robust architectures on CIFAR10. Our resulting architecture is more robust to white \emph{and} black box attacks compared to previous attempts.
 An intriguing aspect of deep learning models in computer vision is that while they can classify images with high accuracy, they fail catastrophically when those same images are perturbed slightly in an adversarial fashion BID17 BID1 . The prevalence of adversarial examples presents challenges to our understanding of how deep networks generalize and pose security risks in real world applications BID11 BID5 . Several techniques have been proposed to defend against adversarial examples. Adversarial training BID1 augments the training data with adversarial examples. It has been shown that using stronger adversarial attacks in adversarial training can increase the robustness to stronger attacks, but at the cost of a decrease in clean accuracy (i.e. accuracy on samples that have not been adversarially perturbed) BID8 . Defensive distillation BID12 , feature squeezing BID22 , and Parseval training BID0 have also been shown to make models more robust against adversarial attacks.The goal of this work is to study the common properties of adversarial examples. We calculate the adversarial error, defined as the difference between clean accuracy and adversarial accuracy at a given size of adversarial perturbation ( ). Surprisingly, adversarial error has a similar dependence on small values of for all network models and datasets we studied, including linear, fully-connected, simple convolutional networks, Inception v3 BID19 , Inception-ResNet v2, Inception v4 BID20 , ResNet v1, ResNet v2 BID2 , NasNet-A BID24 BID25 , adversarially trained Inception v3 BID6 and Inception-ResNet v2 BID21 , and networks trained on randomly shuffled labels of MNIST. Adversarial error due to the Fast Gradient Sign Method (FGSM), its L2-norm variant, and Projected Gradient Descent (PGD) attack grows as a power-law like A B with B between 0.9 and 1.3. By contrast, we find that adversarial error caused by one-step least likely class method (step l.l.) also scales as a power-law where B is between 1.8 and 2.5 for small . This observed universality points to a mysterious commonality between these models and datasets, despite the different number of channels, pixels, and classes present. Adversarial error caused by FGSM on the training set of randomly shuffled labels of MNIST (LeCun & Cortes) also has the power-law form where B = 1.2, which implies that the universality is not a result of the specific content of these datasets nor the ability of the model to generalize.To discover the mechanism behind this universality we show how, at small , the success of an adversarial attack depends on the input-logit Jacobian of the model and on the logits of the network. We demonstrate that the susceptibility of a model to FGSM and PGD attacks is in large part dictated by the cumulative distribution of the difference between the most likely logit and the second most likely logit. We observe that this cumulative distribution has a universal form among all datasets and models studied, including randomly produced data. Together, we believe these results provide a compelling story regarding the susceptibility of machine learning models to adversarial examples at small .We show that training with single-step adversarial examples offers protection against large attacks (between 0.2 and 32), but does not help appreciably at defending against small attacks (below 0.2). At = 0.2, all ImageNet models we studied incur 10 to 25% adversarial error, and surprisingly, vanilla NASNet-A (best clean accuracy in our study) has a lower adversarial error than adversarially trained Inception-ResNet v2 or Inception v3 BID6 (Fig. 1(a ) ). In light of these results, we explore a different avenue to adversarial robustness through architecture selection. We perform neural architecture search (NAS) using reinforcement learning BID24 BID25 . These techniques allow us to find several architectures that are especially robust to adversarial perturbations. In addition , by analyzing the adversarial robustness of the tens-of-thousands of architectures constructed by NAS, we gain insights into the relationship between size of a model, its clean accuracy, and its adversarial robustness. In summary , the key contributions of our work are:• We study the functional form of adversarial error and logit differences across several models and datasets, which turn out to be universal. We analytically derive the commonality in the power-law tails of the logit differences, and show how it leads to the commonality in the form of adversarial error.• We observe that although the qualitative form of logit differences and adversarial error is universal, it can be quantitatively improved with entropy regularization and better network architectures.• We study the dependence of adversarial robustness on the network architecture via NAS.We show that while adversarial accuracy is strongly correlated with clean accuracy, it is only weakly correlated with model size. Our work leads to architectures that are more robust to white-box and black-box attacks on CIFAR10 BID4 ) than previous studies. In this paper we studied common properties of adversarial examples across different models and datasets. We theoretically derived a universality in logit differences and adversarial error of machine learning models. We showed that architecture plays an important role in adversarial robustness, which correlates strongly with clean accuracy. such that t β = 1 if β = γ for some γ and t β = 0 otherwise. We assume our network gets the answer correct so that h γ > h β for all β = γ. Then we apply the adversarial perturbation, DISPLAYFORM0 Note that we can write DISPLAYFORM1 Where we associate J αβ = ∂h β /∂x α with the input-to-logit Jacobian linking the inputs to the logits and δ = ∂L/∂h β the error of the outputs of the network. We can compute the change to the logits of the network due to this perturbation. We find, DISPLAYFORM2 DISPLAYFORM3 DISPLAYFORM4 where we have plugged in for eq. (11). Expressing the above equation in terms of the Jacobian, it follows that we can write the effect of the adversarial perturbation on the logits by, DISPLAYFORM5 as postulated. To make progress we will again make a mean field approximation and assume that each of the logits are i.i.d. with arbitrary distribution P (h). We denote the cumulative distribution F (h). While it is not obvious that the factorial approximation is valid here, we will see that the resulting distribution of P (∆ 1j ) shares many qualitative similarities with the distribution observed in real networks.We first change variables from the logits to a sorted version of the logits, r i . The ranked logits are defined such that r 1 = max({h i }), r 2 = max({h i }\{r 1 }), · · · . Our first result is to compute the resulting joint distribution between r 1 and r j , P j (r 1 , r j ) = A(N, j)F N −j (r j ) [F (r 1 ) − F (r j )] j−2 P (r j )P (r 1 )where A(N, j) = N (N − 1) N −2 j−2 is a combinatorial factor. Eq. (18) has a simple interpretation. F N −j (r j ) is the probability that there are N − j variables less than r j ; [F (r 1 ) − F (r j )] j−2 is the probability that j − 2 variables are between r j and r 1 ; P (r j )P (r 1 ) is the probability that there is one variable equal to each of r 1 and r j . The combinatorial factor can be understood since there are N ways of selecting r 1 , N − 1 ways of selecting r j , and N −2 j−2 ways of choosing j − 2 variables out of the remaining N − 2 to be between r j and r 1 .In terms of eq. FORMULA0 we can compute the distribution over ∆ 1j to be given by, P (∆ 1j ) = drP j (r + ∆ 1j , r)= A(N, j) drF N −j (r) [F (r + ∆ 1j ) − F (r)] j−2 P (r)P (r + ∆ 1j ).We can analyze this equation for small ∆ 1j . Expanding to lowest order in ∆ 1j , P (∆ 1j ) ≈ A(N, j) drF N −j (r) [F (r) + ∆ 1j P (r) − F (r)] j−2 P (r) P (r) + ∆ 1j dP (r) dr ( Since the term in the integral does not depend on ∆ 1j the result follows with, DISPLAYFORM6 6.2.3 ARCHITECTURES FIG3 : Left: Best architecture from Experiment 1. Right: Architecture of NAS Baseline. We note that the architecture from Experiment 1 is ""longer"" and ""narrower"" than previous architectures found by NAS for higher clean accuracy BID24 BID25 .","Adversarial error has similar power-law form for all datasets and models studied, and architecture matters.",ImageNet ; linear ; FGSM ; PGD ; Parseval ; the Fast Gradient Sign Method ; Projected Gradient Descent ; one ; MNIST (LeCun & Cortes ; Jacobian,ImageNet ; linear ; FGSM ; PGD ; Parseval ; the Fast Gradient Sign Method ; Projected Gradient Descent ; one ; MNIST (LeCun & Cortes ; Jacobian,"A growing number of machine learning classifiers are vulnerable to adversarial examples, resulting in distributions of logit differences having a universal functional form, independent of architecture, dataset, and training protocol. This results in adversarial error having a universal scaling, as a power-law, with respect to the size of the adversarial perturbation. Adversarial training BID1 augments training data with adversarial examples, while stronger adversarial attacks can increase the robustness to stronger attacks.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"Presently the most successful approaches to semi-supervised learning are based on consistency regularization, whereby a model is trained to be robust to small perturbations of its inputs and parameters. To understand consistency regularization, we conceptually explore how loss geometry interacts with training procedures. The consistency loss dramatically improves generalization performance over supervised-only training; however, we show that SGD struggles to converge on the consistency loss and continues to make large steps that lead to changes in predictions on the test data. Motivated by these observations, we propose to train consistency-based methods with Stochastic Weight Averaging (SWA), a recent approach which averages weights along the trajectory of SGD with a modified learning rate schedule. We also propose fast-SWA, which further accelerates convergence by averaging multiple points within each cycle of a cyclical learning rate schedule. With weight averaging, we achieve the best known semi-supervised results on CIFAR-10 and CIFAR-100, over many different quantities of labeled training data. For example, we achieve 5.0% error on CIFAR-10 with only 4000 labels, compared to the previous best result in the literature of 6.3%. Recent advances in deep unsupervised learning, such as generative adversarial networks (GANs) BID8 , have led to an explosion of interest in semi-supervised learning. Semisupervised methods make use of both unlabeled and labeled training data to improve performance over purely supervised methods. Semi-supervised learning is particularly valuable in applications such as medical imaging, where labeled data may be scarce and expensive BID23 .Currently the best semi-supervised results are obtained by consistency-enforcing approaches BID2 BID17 BID31 BID21 BID24 . These methods use unlabeled data to stabilize their predictions under input or weight perturbations. Consistency-enforcing methods can be used at scale with state-of-the-art architectures. For example, the recent Mean Teacher BID31 model has been used with the Shake-Shake BID7 architecture and has achieved the best semi-supervised performance on the consequential CIFAR benchmarks.This paper is about conceptually understanding and improving consistency-based semi-supervised learning methods. Our approach can be used as a guide for exploring how loss geometry interacts with training procedures in general. We provide several novel observations about the training objective and optimization trajectories of the popular ⇧ BID17 and Mean Teacher BID31 consistency-based models. Inspired by these findings , we propose to improve SGD solutions via stochastic weight averaging (SWA) BID12 , a recent method that averages weights of the networks corresponding to different training epochs to obtain a single model with improved generalization. On a thorough empirical study we show that this procedure achieves the best known semi-supervised results on consequential benchmarks. In particular:• We show in Section 3.1 that a simplified ⇧ model implicitly regularizes the norm of the Jacobian of the network outputs with respect to both its inputs and its weights, which in turn encourages flatter solutions. Both the reduced Jacobian norm and flatness of solutions have been related to generalization in the literature BID29 BID22 BID3 BID27 BID13 BID12 . Interpolating between the weights corresponding to different epochs of training we demonstrate that the solutions of ⇧ and Mean Teacher models are indeed flatter along these directions ( FIG0 ).• In Section 3.2, we compare the training trajectories of the ⇧, Mean Teacher, and supervised models and find that the distances between the weights corresponding to different epochs are much larger for the consistency based models. The error curves of consistency models are also wider ( FIG0 ), which can be explained by the flatness of the solutions discussed in section 3.1. Further we observe that the predictions of the SGD iterates can differ significantly between different iterations of SGD.• We observe that for consistency-based methods , SGD does not converge to a single point but continues to explore many solutions with high distances apart. Inspired by this observation, we propose to average the weights corresponding to SGD iterates, or ensemble the predictions of the models corresponding to these weights. Averaging weights of SGD iterates compensates for larger steps, stabilizes SGD trajectories and obtains a solution that is centered in a flat region of the loss (as a function of weights). Further, we show that the SGD iterates correspond to models with diverse predictions -using weight averaging or ensembling allows us to make use of the improved diversity and obtain a better solution compared to the SGD iterates. In Section 3.3 we demonstrate that both ensembling predictions and averaging weights of the networks corresponding to different training epochs significantly improve generalization performance and find that the improvement is much larger for the ⇧ and Mean Teacher models compared to supervised training. We find that averaging weights provides similar or improved accuracy compared to ensembling, while offering the computational benefits and convenience of working with a single model. Thus, we focus on weight averaging for the remainder of the paper.• Motivated by our observations in Section 3 we propose to apply Stochastic Weight Averaging (SWA) BID12 to the ⇧ and Mean Teacher models. Based on our results in Section 3.3 we propose several modifications to SWA in Section 4. In particular, we propose fast-SWA, which (1) uses a learning rate schedule with longer cycles to increase the distance between the weights that are averaged and the diversity of the corresponding predictions; and (2) averages weights of multiple networks within each cycle (while SWA only averages weights corresponding to the lowest values of the learning rate within each cycle). In Section 5, we show that fast-SWA converges to a good solution much faster than SWA.• Applying weight averaging to the ⇧ and Mean Teacher models we improve the best reported results on CIFAR-10 for 1k, 2k, 4k and 10k labeled examples, as well as on CIFAR-100 with 10k labeled examples. For example, we obtain 5.0% error on CIFAR-10 with only 4k labels, improving the best result reported in the literature BID31 ) by 1.3%. We also apply weight averaging to a state-of-the-art domain adaptation technique BID6 closely related to the Mean Teacher model and improve the best reported results on domain adaptation from CIFAR-10 to STL from 19.9% to 16.8% error.• We release our code at https://github.com/benathi/fastswa-semi-sup 2 BACKGROUND",Consistency-based models for semi-supervised learning do not converge to a single point but continue to explore a diverse set of plausible solutions on the perimeter of a flat region. Weight averaging helps improve generalization performance.,SGD ; Mean ; CIFAR ; Mean Teacher ; Jacobian ; STL ; error.•,SGD ; Mean ; CIFAR ; Mean Teacher ; Jacobian ; STL ; error.• ; Presently the most successful approaches ; semi-supervised learning ; consistency regularization,"Semi-supervised learning is based on consistency regularization, whereby a model is trained to be robust to small perturbations of its inputs and parameters. However, SGD struggles to converge on consistency loss and continues to make large steps that lead to changes in predictions on test data. Stochastic Weight Averaging (SWA) is a recent approach which averages weights along the trajectory of SGD with a modified learning rate schedule. Fast-SWA accelerates convergence by averaging multiple points within each cycle of a cyclical learning (CIFAR-100).",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"We note that common implementations of adaptive gradient algorithms, such as Adam, limit the potential benefit of weight decay regularization, because the weights do not decay multiplicatively (as would be expected for standard weight decay) but by an additive constant factor. 
 We propose a simple way to resolve this issue by decoupling weight decay and the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) 
decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam, and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter).
 We also demonstrate that longer optimization runs require smaller weight decay values for optimal results and introduce a normalized variant of weight decay to reduce this dependence. Finally, we propose a version of Adam with warm restarts (AdamWR) that has strong anytime performance while achieving state-of-the-art results on CIFAR-10 and ImageNet32x32. 
 Our source code will become available after the review process. Adaptive gradient methods, such as AdaGrad BID3 , RMSProp BID19 , and Adam BID12 have become a default method of choice for training feedforward and recurrent neural networks BID21 BID5 BID16 . Nevertheless, state-of-the-art results for popular image classification datasets, such as CIFAR-10 and CIFAR-100 BID13 , are still obtained by applying SGD with momentum BID7 BID4 BID15 BID4 . Furthermore, Wilson et al. (2017) suggested that adaptive gradient methods do not generalize as well as SGD with momentum when tested on a diverse set of deep learning tasks such as image classification, character-level language modeling and constituency parsing. Different hypotheses about the origins of this worse generalization have been investigated, such as the presence of sharp local minima BID11 BID2 and inherent problems of adaptive gradient methods BID20 . In this paper, we show that a major factor in the poor generalization of the most popular adaptive gradient method, Adam, lies in its dysfunctional implementation of weight decay; the issue we identify in Adam also pertains to other adaptive gradient methods.Specifically, our analysis of Adam given in this paper leads to the following observations:The standard way to implement L 2 regularization/weight decay in Adam is dysfunctional.One possible explanation why Adam and other adaptive gradient methods might be outperformed by SGD with momentum is that L 2 regularization/weight decay are implemented suboptimally in common deep learning libraries. Therefore, on tasks/datasets where the use of L 2 regularization is beneficial (e.g., on many popular image classification datasets), Adam leads to worse results than SGD with momentum (for which L 2 regularization behaves as expected). L 2 regularization and weight decay are not the same thing. Contrary to common belief, the two techniques are not equivalent. For SGD, they can be made equivalent by a reparameterization of the weight decay factor based on the learning rate; this is not the case for Adam. In particular, when combined with adaptive gradients, L 2 regularization leads to weights with large gradients being regularized less than they would be when using weight decay.Optimal weight decay is a function (among other things) of the total number of batch passes/weight updates. Our empirical analysis of Adam suggests that the longer the runtime/number of batch passes to be performed, the smaller the optimal weight decay. This effect tends to be neglected because hyperparameters are often tuned for a fixed or a comparable number of training epochs. As a result, the values of the weight decay found to perform best for short runs do not generalize to much longer runs.Our contributions are aimed at fixing the issues described above:Decoupling weight decay from the gradient-based update (Section 2). We suggest to decouple the gradient-based update from weight decay for both SGD and Adam. The resulting SGD version SGDW decouples optimal settings of the learning rate and the weight decay factor, and the resulting Adam version AdamW generalizes substantially better than Adam. Normalizing the values of weight decay (Section 3). We propose to parameterize the weight decay factor as a function of the total number of batch passes. This leads to a greater invariance of the hyperparameter settings in the sense that the values found to perform best for short runs also perform well for many times longer runs. Adam with warm restarts and normalized weight decay (Section 4). After we fix the weight decay in Adam and design AdamW, we introduce AdamWR to obtain strong anytime performance by performing warm restarts.The main motivation of this paper is to fix the weight decay in Adam to make it competitive w.r.t. SGD with momentum even for those problems where it did not use to be competitive. We hope that as a result, practitioners do not need to switch between Adam and SGD anymore, which in turn should help to reduce the common issue of selecting dataset/task-specific training algorithms and their hyperparameters. Following suggestions that adaptive gradient methods such as Adam might lead to worse generalization than SGD with momentum BID20 , we identified at least one possible explanation to this phenomenon: the dysfunctional use of L 2 regularization and weight decay. We proposed a simple fix to deal with this issue, yielding substantially better generalization performance in our AdamW variant. We also proposed normalized weight decay and warm restarts for Adam, showing that a more robust hyperparameteer selection and a better anytime performance can be achieved in our new AdamWR variant.Our preliminary results obtained with AdamW and AdamWR on image classification datasets must be verified on a wider range of tasks, especially the ones where the use of regularization is expected to be important. It would be interesting to integrate our findings on weight decay into other methods which attempt to improve Adam, e.g, normalized direction-preserving Adam BID22 . While we focussed our experimental analysis on Adam, we believe that similar results also hold for other adaptive gradient methods, such as AdaGrad BID3 and RMSProp BID19 .The results shown in FIG2 suggest that Adam and AdamW follow very similar curves most of the time until the third phase of the run where AdamW starts to branch out to outperform Adam. As pointed out by an anonymous reviewer, it would be interesting to investigate what causes this branching and whether the desired effects are observed at the bottom of the landscape. One could investigate this using the approach of BID9 to switch from Adam to AdamW at a given epoch index. Since it is quite possible that the effect of regularization is not that pronounced in the early stages of training, one could think of designing a version of Adam which exploits this by being fast in the early stages and well-regularized in the late stages of training. The latter might be achieved with a custom schedule of the weight decay factor.In this paper, we argue that the popular interpretation that weight decay = L 2 regularization is not precise. Instead, the difference between the two leads to the following important consequences. Two algorithms as different as SGD and Adam will exhibit different effective rates of weight decay even if the same regularization coefficient is used to include L 2 regularization in the objective function. Moreover, when decoupled weight decay is applied, two algorithms as different as SGDW and AdamW will optimize two effectively different objective functions even if the same weight decay factor is used. Our findings suggest that the original Adam algorithm with L 2 regularization affects effective rates of weight decay in a way that precludes effective regularization, and that effective regularization is achievable by decoupling the weight decay. BID0 analytically showed that in the limited data regime of deep networks the presence of eigenvalues that are zero forms a frozen subspace in which no learning occurs and thus smaller (e.g., zero) initial weight norms should be used to achieve best generalization results. Our future work shall consider adapting initial weight norms or weight norm constraints BID17 at each warm restart. BID10 proposed a family of regularization techniques which are specific to the current batch and its size. Similarly to L 2 regularization and weight decay, the latter techniques might be attempted to be transformed to act directly on weights.1 SUPPLEMENTARY MATERIAL",Fixing weight decay regularization in adaptive gradient methods such as Adam,Adam ; SGD ; AdaGrad ; Wilson ; One ; two ; decay ; SGDW ; AdamW ; at least one,Adam ; SGD ; AdaGrad ; Wilson ; One ; two ; decay ; SGDW ; AdamW ; at least one,"Adaptive gradient algorithms, such as Adam, limit the potential benefit of weight decay regularization because weights do not decay multiplicatively, but by an additive constant factor. We propose a simple way to resolve this issue by decoupling weight decay and optimization steps w.r.t. the loss function. Our proposed modification (i) decouples optimal choice of weight decay factor from setting the learning rate for both standard SGD and Adam, and (ii) significantly improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"We demonstrate a low effort method that unsupervisedly constructs task-optimized embeddings from existing word embeddings to gain performance on a supervised end-task. This avoids additional labeling or building more complex model architectures by instead providing specialized embeddings better fit for the end-task(s). Furthermore, the method can be used to roughly estimate whether a specific kind of end-task(s) can be learned form, or is represented in, a given unlabeled dataset, e.g. using publicly available probing tasks. We evaluate our method for diverse word embedding probing tasks and by size of embedding training corpus -- i.e. to explore its use in reduced (pretraining-resource) settings. Unsupervisedly pretrained word embeddings provide a low-effort, high pay-off way to improve the performance of a specific supervised end-task by exploiting Transfer learning from an unsupervised to the supervised task. Additionally, recent works indicate that universally best embeddings are not yet possible, and that instead embeddings need to be tuned to fit specific end-tasks using inductive bias -i.e. semantic supervision for the unsupervised embedding learning process BID1 BID13 . This way, embeddings can be tuned to fit a specific Single-task (ST) or Multi-task (MT: set of tasks) semantic BID16 BID7 . Hence the established notion, that in order to fine-tune embeddings for specific end-tasks, labels for those endtasks a required. However, in practice, especially in industry applications, labeled dataset are often either too small, not available or of low quality and creating or extending them is costly and slow.Instead, to lessen the need for complex supervised (Multi-task) fine-tuning, we explore using unsupervised fine-tuning of word embeddings for either a specific end-task (ST) or a set of desired end-tasks (MT). By taking pretrained word embeddings and unsupervisedly postprocessing (finetuning) them, we evaluate postprocessing performance changes on publicly available probing tasks developed by BID6 1 to demonstrate that widely used word embeddings like Fasttext and GloVe can either: (a) be unsupervisedly specialized to better fit a single supervised task or, (b) can generally improve embeddings for multiple supervised end-tasks -i.e. the method can optimize for single and Multi-task settings. As in standard methodology, optimal postprocessed embeddings can be selected using multiple proxy-tasks for overall improvement or using a single endtask's development split -e.g. on a fast baseline model for further time reduction. Since most embeddings are pretrained on large corpora, we also investigate whether our method -dubbed MORTY -benefits embeddings trained on smaller corpora to gauge usefulness for low-labeling-resource domains like biology or medicine. We demonstrate the method's application for Single-task, Multitask, small and large corpus-size setting in the evaluation section 3. Finally, MORTY (sec. 2), uses very little resources 2 , especially regarding recent approaches that exploit unsupervised pretaining to boost end-task performance by adding complex pretraining components like ELMo, BERT BID14 BID2 which may not yet be broadly usable due to their hardware and processing time requirements. As a result, we demonstrate a simple method, that allows further pretraining exploitation, while requiring minimum extra effort, time and compute resources. We demonstrated a low-effort method to unsupervisedly construct task-optimized word embeddings from existing ones to gain performance on a (set of) supervised end-task(s). Despite its simplicity, MORTY is able to produces significant performance improvements for Single and Multi-task supervision settings as well as for a variety of desirable word encoding properties -even on smaller corpus sizes -while forgoing additional labeling or building more complex model architectures.","Morty refits pretrained word embeddings to either: (a) improve overall embedding performance (for Multi-task settings) or improve Single-task performance, while requiring only minimal effort.",Transfer ; Fasttext ; GloVe ; sec ; ELMo ; Single,Transfer ; Fasttext ; GloVe ; sec ; ELMo ; Single ; We ; a low effort method ; that ; task-optimized embeddings,"Unsupervisedly constructed task-optimized embeddings from existing word embeddings to gain performance on a supervised end-task. This avoids additional labeling or building more complex model architectures by providing specialized embeddings better fit for the end-task(s). The method can be used to estimate whether a specific kind of end-task(s) can be learned form, or is represented in, a given unlabeled dataset. However, in practice, labeled datasets are often either too small, not available or of low quality.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"Deep networks have shown great performance in classification tasks. However, the parameters learned by the classifier networks usually discard stylistic information of the input, in favour of information strictly relevant to classification. We introduce a network that has the capacity to do both classification and reconstruction by adding a ""style memory"" to the output layer of the network. We also show how to train such a neural network as a deep multi-layer autoencoder, jointly minimizing both classification and reconstruction losses. The generative capacity of our network demonstrates that the combination of style-memory neurons with the classifier neurons yield good reconstructions of the inputs when the classification is correct. We further investigate the nature of the style memory, and how it relates to composing digits and letters. Deep neural networks now rival human performance in many complex classification tasks, such as image recognition. However, these classification networks are different from human brains in some basic ways. First of all, the mammalian cortex has many feed-back connections that project in the direction opposite the sensory stream BID1 . Moreover, these feed-back connections are implicated in the processing of sensory input, and seem to enable improved object/background contrast BID10 , and imagination BID11 . Feed-back connections are also hypothesized to be involved in generating predictions in the service of perceptual decision making BID14 .Humans (and presumably other mammals) are also less susceptible to being fooled by ambiguous or adversarial inputs. Deep neural networks have been shown to be vulnerable to adversarial examples BID15 BID3 . Slight modifications to an input can cause the neural network to misclassify it, sometimes with great confidence! Humans do not get fooled as easily, leading us to wonder if the feed-back, generative nature of real mammalian brains contributes to accurate classification.In pursuit of that research, we wish to augment classification networks so that they are capable of both recognition (in the feed-forward direction) and reconstruction (in the feed-back direction). We want to build networks that are both classifiers and generative.The nature of a classifier network is that it throws away most of the information, keeping only what is necessary to make accurate classifications. Simply adding feed-back connections to the network will not be enough to generate specific examples of the input -only a generic class archetype. But what if we combine the features of a classifier network and an autoencoder network by adding a ""style memory"" to the top layer of the network? The top layer would then consist of a classification component as well as a collection of neurons that are not constrained by any target classes.We hypothesized that adding a style memory to the top layer of a deep autoencoder would give us the best of both worlds, allowing the classification neurons to contribute the class of the input, while the style memory would record additional information about the encoded input -presumably information not encoded by the classification neurons. The objective of our network is to minimize both classification and reconstruction losses so that the network can perform both classification and reconstruction effectively. As a proof of concept, we report on a number of experiments with MNIST and EMNIST that investigate the properties of this style memory. Classification networks do not typically maintain enough information to reconstruct the input; they do not have to. Their goal is to map high-dimensional inputs to a small number of classes, typically using a lower-dimensional vector representation. In order for a classification network to be capable of generating samples, additional information needs to be maintained. In this paper, we proposed the addition of ""style memory"" to the top layer of a classification network. The top layer is trained using a multi-objective optimization, trying to simultaneously minimize classification error and reconstruction loss.(a ) (Figure 11: Image reconstruction with style memory interpolation between digits and letters shown in FIG7 and FIG0 , where λ was increasing from 0.1 to 1.0 with a step of 0.1 from top to bottom.Our experiments suggest that the style memory encodes information that is largely disjoint from the classification vector. For example, proximity in image space yields digits that employ an overlapping set of pixels. However , proximity in style-memory space yielded a different set of digits.For the style interpolation experiment, we generated images from a straight line in style-memory space. However , each position on this line generates a sample in image space -an image; it would be interesting to see what shape that 1-dimensional manifold takes in image space, and how it differs from straight-line interpolation in image space. However , the fact that we were able to interpolate digits and letters within the same class using novel style-memory activation patterns suggests that the style memory successfully encodes additional, abstract information about the encoded input.To our knowledge, existing defence mechanisms to combat adversarial inputs do not involve the generative capacity of a network. Motivated by the results in Sec. 4.1, preliminary experiments that we have done suggest that treating perception as a two-way process, including both classification and reconstruction, is effective for guarding against being fooled by adversarial or ambiguous inputs. Continuing in this vein is left for future work.Finally, we saw that the network has a property where the reconstruction generated was affected both by the classification neurons and style memory. Inspired by how human perception is influenced by expectation BID14 , we believe that this work opens up opportunities to create a classifier network that takes advantage of its generative capability to detect misclassifications. Moreover, predictive estimator networks might be a natural implementation for such feed-back networks BID17 BID14 BID9 . Perception and inference could be the result of running the network in feed-forward and feed-back directions simultaneously, like in the wake-sleep approach BID5 . These experiments are ongoing .",Augmenting the top layer of a classifier network with a style memory enables it to be generative.,First ; Sec ; two,First ; Sec ; two ; Deep networks ; great performance ; classification tasks ; the parameters ; the classifier networks ; stylistic information ; the input,"Deep neural networks can perform both classification and reconstruction by adding a ""style memory"" to the output layer of the network. The combination of style-memory neurons with the classifier neurons yields good reconstructions of the inputs when the classification is correct. The generative capacity of our network demonstrates that the combination of style-memory neurons with the classifier neurons yields good reconstructions when the classification is correct. The mammalian cortex has many feed-back connections that project in the opposite direction of the sensory stream BID1.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"Recent advances have made it possible to create deep complex-valued neural networks. Despite this progress, the potential power of fully complex intermediate computations and representations has not yet been explored for many challenging learning problems. Building on recent advances, we propose a novel mechanism for extracting signals in the frequency domain. As a case study, we perform audio source separation in the Fourier domain. Our extraction mechanism could be regarded as a local ensembling method that combines a complex-valued convolutional version of Feature-Wise Linear Modulation (FiLM) and a signal averaging operation. We also introduce a new explicit amplitude and phase-aware loss, which is scale and time invariant, taking into account the complex-valued components of the spectrogram. Using the Wall Street Journal Dataset, we compare our phase-aware loss to several others that operate both in the time and frequency domains and demonstrate the effectiveness of our proposed signal extraction method and proposed loss. When operating in the complex-valued frequency domain, our deep complex-valued network substantially outperforms its real-valued counterparts even with half the depth and a third of the parameters. Our proposed mechanism improves significantly deep complex-valued networks' performance and we demonstrate the usefulness of its regularizing effect. Complex-valued neural networks have been studied since long before the emergence of modern deep learning techniques (Georgiou & Koutsougeras, 1992; Zemel et al., 1995; Kim & Adalı, 2003; Hirose, 2003; Nitta, 2004) . Nevertheless, deep complex-valued models have only started to gain momentum (Reichert & Serre, 2014; Arjovsky et al., 2015; Danihelka et al., 2016; Trabelsi et al., 2017; Jose et al., 2017; Wolter & Yao, 2018b; Choi et al., 2019) , with the great majority of models in deep learning still relying on real-valued representations. The motivation for using complex-valued representations for deep learning is twofold: On the one hand, biological nervous systems actively make use of synchronization effects to gate signals between neurons -a mechanism that can be recreated in artificial systems by taking into account phase differences (Reichert & Serre, 2014) . On the other hand, complex-valued representations are better suited to certain types of data, particularly those that are naturally expressed in the frequency domain. Other benefits provided by working with complex-valued inputs in the spectral or frequency domain are computational. In particular, short-time Fourier transforms (STFTs) can be used to considerably reduce the temporal dimension of the representation for an underlying signal. This is a critical advantage, as training recurrent neural networks (RNNs) or convolutional neural networks (CNNs) on long sequences remains challenging due to unstable gradients and the computational requirements of backpropagation through time (BPTT) (Hochreiter, 1991; Bengio et al., 1994) . Applying the STFT on the raw signal, on the other hand, is computationally efficient, as in practice it is implemented with the fast Fourier transform (FFT) whose computational complexity is O(n log(n)). The aforementioned biological, representational and computational considerations provide compelling motivations for designing learning models for tasks where the complex-valued representation of the input and output data is more desirable than their real-counterpart. Recent work has provided building blocks for deep complex-valued neural networks (Trabelsi et al., 2017) . These building blocks have been shown, in many cases, to avoid numerical problems during training and, thereby, enable the use of complex-valued representations. These representations are well-suited for frequency domain signals, as they have the ability to explicitly encode frequency magnitude and phase components. This motivates us to design a new signal extraction mechanism operating in the frequency domain. In this work, our contributions are summarized as follows: 1. We present a new signal separation mechanism implementing a local ensembling procedure. More precisely, a complex-valued convolutional version of Feature-wise Linear Modulation (FiLM) (Perez et al., 2018 ) is used to create multiple separated candidates for each of the signals we aim to retrieve from a mixture of inputs. A signal averaging operation on the candidates is then performed in order to increase the robustness of the signal to noise and interference. Before the averaging procedure, a form of dropout is implemented on the signal candidates in order to reduce the amount of interference and noise correlation existing between the different candidates. 2. We propose and explore a new magnitude and phase-aware loss taking explicitly into account the magnitude and phase of signals. A key characteristic of our loss is that it is scale-and time-invariant. We test our proposed signal extraction mechanism in the audio source separation setting where we aim to retrieve distinct audio signals associated with each speaker in the input mix. Our experiments demonstrate the usefulness of our extraction method, and show its regularizing effect. In this work, we introduced a new complex-valued extraction mechanism for signal retrieval in the Fourier domain. As a case study, we considered audio source separation. We also proposed a new phase-aware loss taking, explicitly, into account the magnitude and phase of the reference and estimated signals. The amplitude and phase-aware loss improves over other frequency and time-domain losses. We believe that our proposed method could lead to new research directions where signal retrieval is needed. A APPENDIX",New Signal Extraction Method in the Fourier Domain,Fourier ; Feature-Wise Linear Modulation ; the Wall Street Journal Dataset ; half ; a third ; Georgiou & Koutsougeras ; Zemel ; al. ; Kim & Adalı ; Hirose,Fourier ; Feature-Wise Linear Modulation ; the Wall Street Journal Dataset ; half ; a third ; Georgiou & Koutsougeras ; Zemel ; al. ; Kim & Adalı ; Hirose,"Recent advances have made it possible to create deep complex-valued neural networks. However, the potential power of fully complex intermediate computations and representations has not yet been explored for many challenging learning problems. We propose a novel mechanism for extracting signals in the frequency domain. Our extraction mechanism could be regarded as a local ensembling method that combines a complex-valued convolutional version of FiLM and a signal averaging operation. We also introduce a new explicit amplitude and phase-aware loss, taking into account the complex-valued components of the spect",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"Deep latent variable models have become a popular model choice due to the scalable learning algorithms introduced by (Kingma & Welling 2013, Rezende et al. 2014). These approaches maximize a variational lower bound on the intractable log likelihood of the observed data. Burda et al. (2015) introduced a multi-sample variational bound, IWAE, that is at least as tight as the standard variational lower bound and becomes increasingly tight as the number of samples increases. Counterintuitively, the typical inference network gradient estimator for the IWAE bound performs poorly as the number of samples increases (Rainforth et al. 2018, Le et al. 2018). Roeder et a. (2017) propose an improved gradient estimator, however, are unable to show it is unbiased. We show that it is in fact biased and that the bias can be estimated efficiently with a second application of the reparameterization trick. The doubly reparameterized gradient (DReG) estimator does not suffer as the number of samples increases, resolving the previously raised issues. The same idea can be used to improve many recently introduced training techniques for latent variable models. In particular, we show that this estimator reduces the variance of the IWAE gradient, the reweighted wake-sleep update (RWS) (Bornschein & Bengio 2014), and the jackknife variational inference (JVI) gradient (Nowozin 2018). Finally, we show that this computationally efficient, drop-in estimator translates to improved performance for all three objectives on several modeling tasks. Following the influential work by BID20 BID30 , deep generative models with latent variables have been widely used to model data such as natural images BID29 BID14 , speech and music time-series BID8 BID11 BID22 , and video BID1 BID15 BID9 . The power of these models lies in combining learned nonlinear function approximators with a principled probabilistic approach, resulting in expressive models that can capture complex distributions. Unfortunately, the nonlinearities that empower these model also make marginalizing the latent variables intractable, rendering direct maximum likelihood training inapplicable. Instead of directly maximizing the marginal likelihood, a common approach is to maximize a tractable lower bound on the likelihood such as the variational evidence lower bound (ELBO) BID19 BID3 . The tightness of the bound is determined by the expressiveness of the variational family. For tractability, a factorized variational family is commonly used, which can cause the learned model to be overly simplistic. BID5 introduced a multi-sample bound, IWAE, that is at least as tight as the ELBO and becomes increasingly tight as the number of samples increases. Counterintuitively, although the bound is tighter, BID28 theoretically and empirically showed that the standard inference network gradient estimator for the IWAE bound performs poorly as the number of samples increases due to a diminishing signal-to-noise ratio (SNR). This motivates the search for novel gradient estimators. BID31 proposed a lower-variance estimator of the gradient of the IWAE bound. They speculated that their estimator was unbiased, however, were unable to prove the claim. We show that it is in fact biased, but that it is possible to construct an unbiased estimator with a second application of the reparameterization trick which we call the IWAE doubly reparameterized gradient (DReG) estimator. Our estimator is an unbiased, computationally efficient drop-in replacement, and does not suffer as the number of samples increases, resolving the counterintuitive behavior from previous work BID28 . Furthermore, our insight is applicable to alternative multisample training techniques for latent variable models: reweighted wake-sleep (RWS) BID4 and jackknife variational inference (JVI) BID27 .In this work, we derive DReG estimators for IWAE, RWS, and JVI and demonstrate improved scaling with the number of samples on a simple example. Then , we evaluate DReG estimators on MNIST generative modeling, Omniglot generative modeling, and MNIST structured prediction tasks. In all cases, we demonstrate substantial unbiased variance reduction, which translates to improved performance over the original estimators. In this work, we introduce doubly reparameterized estimators for the updates in IWAE, RWS, and JVI. We demonstrate that across tasks they provide unbiased variance reduction, which leads to improved performance. Furthermore, DReG estimators have the same computational cost as the original estimators. As a result, we recommend that DReG estimators be used instead of the typical gradient estimators.Variational Sequential Monte Carlo BID24 BID26 and Neural Adapative Sequential Monte Carlo BID13 extend IWAE and RWS to sequential latent variable models, respectively. It would be interesting to develop DReG estimators for these approaches as well.We found that a convex combination of IWAE-DReG and RWS-DReG performed best, however, the weighting was task dependent. In future work, we intend to apply ideas from BID2 to automatically adapt the weighting based on the data.Finally, the form of the IWAE-DReG estimator (Eq. 7) is surprisingly simple and suggests that there may be a more direct derivation that is applicable to general MCOs.",Doubly reparameterized gradient estimators provide unbiased variance reduction which leads to improved performance.,Kingma & Welling ; Le ; et a. ; second ; IWAE ; RWS ; three ; ELBO ; SNR ; JVI,Kingma & Welling ; Le ; et a. ; second ; IWAE ; RWS ; three ; ELBO ; SNR ; JVI,"Deep latent variable models have become popular due to the scalable learning algorithms introduced by (Kingma & Welling 2013). These approaches maximize a variational lower bound on the intractable log likelihood of the observed data. Burda et al. (2015) introduced a multi-sample variational bound, IWAE, that is at least as tight as the standard variational lower bound and becomes increasingly tight as the number of samples increases. Roeder et al. (2017) propose an improved gradient estim",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"Zeroth-order optimization is the process of minimizing an objective $f(x)$, given oracle access to evaluations at adaptively chosen inputs $x$. In this paper, we present two simple yet powerful GradientLess Descent (GLD) algorithms that do not rely on an underlying gradient estimate and are numerically stable. We analyze our algorithm from a novel geometric perspective and we show that for {\it any monotone transform} of a smooth and strongly convex objective with latent dimension $k \ge n$, we present a novel analysis that shows convergence within an $\epsilon$-ball of the optimum in $O(kQ\log(n)\log(R/\epsilon))$ evaluations, where the input dimension is $n$, $R$ is the diameter of the input space and $Q$ is the condition number. Our rates are the first of its kind to be both 1) poly-logarithmically dependent on dimensionality and 2) invariant under monotone transformations. We further leverage our geometric perspective to show that our analysis is optimal. Both monotone invariance and its ability to utilize a low latent dimensionality are key to the empirical success of our algorithms, as demonstrated on synthetic and MuJoCo benchmarks.
 We consider the problem of zeroth-order optimization (also known as gradient-free optimization, or bandit optimization), where our goal is to minimize an objective function f : R n → R with as few evaluations of f (x) as possible. For many practical and interesting objective functions, gradients are difficult to compute and there is still a need for zeroth-order optimization in applications such as reinforcement learning (Mania et al., 2018; Salimans et al., 2017; Choromanski et al., 2018) , attacking neural networks Papernot et al., 2017) , hyperparameter tuning of deep networks (Snoek et al., 2012) , and network control (Liu et al., 2017) . The standard approach to zeroth-order optimization is, ironically, to estimate the gradients from function values and apply a first-order optimization algorithm (Flaxman et al., 2005) . Nesterov & Spokoiny (2011) analyze this class of algorithms as gradient descent on a Gaussian smoothing of the objective and gives an accelerated O(n √ Q log((LR 2 + F )/ )) iteration complexity for an LLipschitz convex function with condition number Q and R = x 0 − x * and F = f (x 0 ) − f (x * ). They propose a two-point evaluation scheme that constructs gradient estimates from the difference between function values at two points that are close to each other. This scheme was extended by (Duchi et al., 2015) for stochastic settings, by (Ghadimi & Lan, 2013) for nonconvex settings, and by (Shamir, 2017) for non-smooth and non-Euclidean norm settings. Since then, first-order techniques such as variance reduction (Liu et al., 2018) , conditional gradients (Balasubramanian & Ghadimi, 2018) , and diagonal preconditioning (Mania et al., 2018) have been successfully adopted in this setting. This class of algorithms are also known as stochastic search, random search, or (natural) evolutionary strategies and have been augmented with a variety of heuristics, such as the popular CMA-ES (Auger & Hansen, 2005) . These algorithms, however, suffer from high variance due to non-robust local minima or highly non-smooth objectives, which are common in the fields of deep learning and reinforcement learn-ing. Mania et al. (2018) notes that gradient variance increases as training progresses due to higher variance in the objective functions, since often parameters must be tuned precisely to achieve reasonable models. Therefore, some attention has shifted into direct search algorithms that usually finds a descent direction u and moves to x + δu, where the step size is not scaled by the function difference. The first approaches for direct search were based on deterministic approaches with a positive spanning set and date back to the 1950s (Brooks, 1958) . Only recently have theoretical bounds surfaced, with Gratton et al. (2015) giving an iteration complexity that is a large polynomial of n and Dodangeh & Vicente (2016) giving an improved O(n 2 L 2 / ). Stochastic approaches tend to have better complexities: Stich et al. (2013) uses line search to give a O(nQ log(F/ )) iteration complexity for convex functions with condition number Q and most recently, Gorbunov et al. (2019) uses importance sampling to give a O(nQ log(F/ )) complexity for convex functions with average condition numberQ, assuming access to sampling probabilities. Stich et al. (2013) notes that direct search algorithms are invariant under monotone transforms of the objective, a property that might explain their robustness in high-variance settings. In general, zeroth order optimization suffers an at least linear dependence on input dimension n and recent works have tried to address this limitation when n is large but f (x) admits a low-dimensional structure. Some papers assume that f (x) depends only on k coordinates and Wang et al. (2017) applies Lasso to find the important set of coordinates, whereas Balasubramanian & Ghadimi (2018) simply change the step size to achieve an O(k(log(n)/ ) 2 ) iteration complexity. Other papers assume more generally that f (x) = g(P A x) only depends on a k-dimensional subspace given by the range of P A and Djolonga et al. (2013) apply low-rank approximation to find the low-dimensional subspace while Wang et al. (2013) use random embeddings. Hazan et al. (2017) assume that f (x) is a sparse collection of k-degree monomials on the Boolean hypercube and apply sparse recovery to achieve a O(n k ) runtime bound. We will show that under the case that f (x) = g(P A x), our algorithm will inherently pick up any low-dimensional structure in f (x) and achieve a convergence rate that depends on k log(n). This initial convergence rate survives, even if we perturb f (x) = g(P A x) + h(x), so long as h(x) is sufficiently small. We will not cover the whole variety of black-box optimization methods, such as Bayesian optimization or genetic algorithms. In general, these methods attempt to solve a broader problem (e.g. multiple optima), have weaker theoretical guarantees and may require substantial computation at each step: e.g. Bayesian optimization generally has theoretical iteration complexities that grow exponentially in dimension, and CMA-ES lacks provable complexity bounds beyond convex quadratic functions. In addition to the slow runtime and weaker guarantees, Bayesian optimization assumes the success of an inner optimization loop of the acquisition function. This inner optimization is often implemented with many iterations of a simpler zeroth-order methods, justifying the need to understand gradient-less descent algorithms within its own context.",Gradientless Descent is a provably efficient gradient-free algorithm that is monotone-invariant and fast for high-dimensional zero-th order optimization.,objective $ ; f(x)$ ; x$. ; two ; GradientLess Descent (GLD ; R$ ; first ; MuJoCo ; al. ; Salimans,objective $ ; f(x)$ ; x$. ; two ; GradientLess Descent (GLD ; R$ ; first ; MuJoCo ; al. ; Salimans,"Zeroth-order optimization is the process of minimizing an objective $f(x)$, given oracle access to evaluations at adaptively chosen inputs $x$. In this paper, we present two simple yet powerful GradientLess Descent (GLD) algorithms that do not rely on an underlying gradient estimate and are numerically stable. Our rates are both poly-logarithmically dependent on dimensionality and invariant under monotone transformations.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"In the context of multi-task learning, neural networks with branched architectures have often been employed to jointly tackle the tasks at hand. Such ramified networks typically start with a number of shared layers, after which different tasks branch out into their own sequence of layers. Understandably, as the number of possible network configurations is combinatorially large, deciding what layers to share and where to branch out becomes cumbersome. Prior works have either relied on ad hoc methods to determine the level of layer sharing, which is suboptimal, or utilized neural architecture search techniques to establish the network design, which is considerably expensive. In this paper, we go beyond these limitations and propose a principled approach to automatically construct branched multi-task networks, by leveraging the employed tasks' affinities. Given a specific budget, i.e. number of learnable parameters, the proposed approach generates architectures, in which shallow layers are task-agnostic, whereas deeper ones gradually grow more task-specific. Extensive experimental analysis across numerous, diverse multi-tasking datasets shows that, for a given budget, our method consistently yields networks with the highest performance, while for a certain performance threshold it requires the least amount of learnable parameters. Deep neural networks are usually trained to tackle different tasks in isolation. Humans, in contrast, are remarkably good at solving a multitude of tasks concurrently. Biological data processing appears to follow a multi-tasking strategy too; instead of separating tasks and solving them in isolation, different processes seem to share the same early processing layers in the brain -see e.g. V1 in macaques (Gur & Snodderly, 2007) . Drawing inspiration from such observations, deep learning researchers began to develop multi-task networks with branched architectures. As a whole, multi-task networks (Caruana, 1997) seek to improve generalization and processing efficiency through the joint learning of related tasks. Compared to the typical learning of separate deep neural networks for each of the individual tasks, multi-task networks come with several advantages. First, due to their inherent layer sharing (Kokkinos, 2017; Lu et al., 2017; Kendall et al., 2018; Guo et al., 2018; , the resulting memory footprint is typically substantially lower. Second, as features in the shared layers do not need to be calculated repeatedly for the different tasks, the overall inference speed is often higher (Neven et al., 2017; Lu et al., 2017) . Finally, multi-task networks may outperform their single-task counterparts (Kendall et al., 2018; Xu et al., 2018; Sener & Koltun, 2018; Maninis et al., 2019) . Evidently, there is merit in utilizing multi-task networks. When it comes to designing them, however, a significant challenge is to decide on the layers that need to be shared among tasks. Assuming a hard parameter sharing setting 1 , the number of possible network configurations grows quickly with the number of tasks. As a result, a trial-and-error procedure to define the optimal architecture becomes unwieldy. Resorting to neural architecture search (Elsken et al., 2019) techniques is not a viable option too, as in this case, the layer sharing has to be jointly optimized with the layers types, their connectivity, etc., rendering the problem considerably expensive. Instead, researchers have recently explored more viable alternatives, like routing (Rosenbaum et al., 2018) , stochastic filter grouping (Bragman et al., 2019) , and feature partitioning (Newell et al., 2019) , which are, however, closer to the soft parameter sharing setting. Previous works on hard parameter sharing opted for the simple strategy of sharing the initial layers in the network, after which all tasks branch out simultaneously. The point at which the branching occurs is usually determined ad hoc (Kendall et al., 2018; Guo et al., 2018; Sener & Koltun, 2018) . This situation hurts performance, as a suboptimal grouping of tasks can lead to the sharing of information between unrelated tasks, known as negative transfer . In this paper, we go beyond the aforementioned limitations and propose a novel approach to decide on the degree of layer sharing between tasks in order to eliminate the need for manual exploration. To this end, we base the layer sharing on measurable levels of task affinity or task relatedness: two tasks are strongly related, if their single task models rely on a similar set of features. Zamir et al. (2018) quantified this property by measuring the performance when solving a task using a variable sets of layers from a model pretrained on a different task. However, their approach is considerably expensive, as it scales quadratically with the number of tasks. Recently, Dwivedi & Roig (2019) proposed a more efficient alternative that uses representation similarity analysis (RSA) to obtain a measure of task affinity, by computing correlations between models pretrained on different tasks. Given a dataset and a number of tasks, our approach uses RSA to assess the task affinity at arbitrary locations in a neural network. The task affinity scores are then used to construct a branched multitask network in a fully automated manner. In particular, our task clustering algorithm groups similar tasks together in common branches, and separates dissimilar tasks by assigning them to different branches, thereby reducing the negative transfer between tasks. Additionally, our method allows to trade network complexity against task similarity. We provide extensive empirical evaluation of our method, showing its superiority in terms of multi-task performance vs computational resources. In this paper, we introduced a principled approach to automatically construct branched multi-task networks for a given computational budget. To this end, we leverage the employed tasks' affinities as a quantifiable measure for layer sharing. The proposed approach can be seen as an abstraction of NAS for MTL, where only layer sharing is optimized, without having to jointly optimize the layers types, their connectivity, etc., as done in traditional NAS, which would render the problem considerably expensive. Extensive experimental analysis shows that our method outperforms existing ones w.r.t. the important metric of multi-tasking performance vs number of parameters, while at the same time showing consistent results across a diverse set of multi-tasking scenarios and datasets. MTAN We tried re-implementing the MTAN model ) using a ResNet-50 backbone. The architecture was based on the Wide-ResNet architecture that is used in the original paper. After extensive hyperparameter tuning, we were unable to get a meaningful result on the Cityscapes dataset when trying to solve all three tasks jointly. Note that, the authors have only shown results in their paper when training semantic segmentation and monocular depth estimation.",A method for the automated construction of branched multi-task networks with strong experimental evaluation on diverse multi-tasking datasets.,Gur & Snodderly ; Caruana ; First ; Kokkinos ; Lu et al. ; Kendall ; al. ; Guo et al. ; Second ; Lu,Gur & Snodderly ; Caruana ; First ; Kokkinos ; Lu et al. ; Kendall ; al. ; Guo et al. ; Second ; Lu,"In multi-task learning, neural networks with branched architectures are often used to tackle tasks together. However, deciding which layers to share and where to branch out becomes cumbersome. In this paper, we propose a principled approach to automatically construct branched multi-task networks, by leveraging the employed tasks' affinities. Deep neural networks are usually trained to tackle different tasks in isolation, while humans are remarkably good at solving a multitude of tasks concurrently. Multi-task networks seek to improve generalization and processing efficiency through the joint learning of related tasks.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"In this paper we present the first freely available dataset for the development and evaluation of domain adaptation methods, for the sound event detection task. The dataset contains 40 log mel-band energies extracted from $100$ different synthetic sound event tracks, with additive noise from nine different acoustic scenes (from indoor, outdoor, and vehicle environments), mixed at six different sound-to-noise ratios, SNRs, (from -12 to -27 dB with a step of -3 dB), and totaling to 5400 (9 * 100 * 6) sound files and a total length of 30 564 minutes. We provide the dataset as is, the code to re-create the dataset and remix the sound event tracks and the acoustic scenes with different SNRs, and a baseline method that tests the adaptation performance with the proposed dataset and establishes some first results.",The very first freely available domain adaptation dataset for sound event detection.,first ; mel-band ; nine ; six ; acoustic,first ; mel-band ; nine ; six ; acoustic ; this paper ; we ; the first freely available dataset ; the development ; evaluation,"In this paper we present the first freely available dataset for the development and evaluation of domain adaptation methods, for the sound event detection task. The dataset contains 40 log mel-band energies extracted from $100$ different synthetic sound event tracks, with additive noise from nine different acoustic scenes, mixed at six different sound-to-noise ratios, SNRs, and totaling to 5400 (9 * 100 * 6) sound files and a total length of 30 564 minutes. We provide the dataset as is, the code to re-",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"Dialogue research tends to distinguish between chit-chat and goal-oriented tasks. While the former is arguably more naturalistic and has a wider use of language, the latter has clearer metrics and a more straightforward learning signal. Humans effortlessly combine the two, and engage in chit-chat for example with the goal of exchanging information or eliciting a specific response. Here, we bridge the divide between these two domains in the setting of a rich multi-player text-based fantasy environment where agents and humans engage in both actions and dialogue. Specifically, we train a goal-oriented model with reinforcement learning via self-play against an imitation-learned chit-chat model with two new approaches: the policy either learns to pick a topic or learns to pick an utterance given the top-k utterances. We show that both models outperform a strong inverse model baseline and can converse naturally with their dialogue partner in order to achieve goals. In the literature on artificial dialogue agents, a distinction is often made between ""goal-oriented"" dialogue, where an agent is tasked with filling slots or otherwise obtaining or disseminating specified information from the user to help complete a task, and ""chit-chat"", where an agent should imitate human small talk. Modeling goal-oriented dialogue can have advantages over chit-chat imitation as it gives clearer metrics of success and perhaps more meaningful learning signals; but goal-oriented dialogue data is often more specialized, covering only a narrow slice of natural language. Current goal-oriented datasets study setting like booking restaurants or airline tickets, or obtaining weather information, as standalone tasks (Raux et al., 2005; Henderson et al., 2014; Bordes et al., 2017; El Asri et al., 2017; Budzianowski et al., 2018) . Chit-chat agents, by contrast, might focus on coarse statistical regularities of dialogue data without accurately modeling the underlying ""meaning""; but the data often covers a much wider space of natural language. For example, Twitter or Reddit chitchat tasks (Li et al., 2016a; Yang et al., 2018; Mazaré et al., 2018 ) cover a huge spectrum of language and diverse topics. Chit-chat and goal-oriented dialogue are not mutually exclusive: when humans engage in chit-chat, their aim is to exchange information, or to elicit specific responses from their partners. Modeling such goals, however, is made difficult by the fact that it requires large amounts of world knowledge, and that goals in real life are implicit. In this work, we study goal-oriented dialogue agents in the setting of a multi-player text-based fantasy environment (Urbanek et al., 2019) . The environment is built on top of a game engine that grounds actions and reference objects, and thus codifies a body of world-knowledge. Although the interactions between objects and characters are simulated, the choice and types of interactions, the text used to describe them, and the dialogues between characters, are ""natural"" and wide-ranging, having been collected from human crowdworkers. We define the general task of, given a particular character in a particular scenario (location, set of objects and other characters to interact with) to conduct open-ended dialogue such that a given action is executed in the future by their dialogue partner. The given action could be an emote action (smile, laugh, ponder, . . . ), or a game action (wear chain mail, drink mead, put glass on table, . . . ). The richness of the environment means that there are a huge set of possible tasks and scenarios in which to achieve a wide range of actions. Thus, this task is ideally suited for bridging the divide between goal-oriented and chit-chat dialogue, combining clearer metrics and learning signals on the one hand, with the richness and complexity of situated but open-domain natural language on the other. Figure 1 : Example episode from the LIGHT dataset, consisting of an environment (location setting, characters with given personas, objects), utterances and game actions. There are 10,777 such humanhuman gameplay episodes, and a rich world of 663 locations, 1755 characters and 3462 objects. We train models to achieve these tasks using reinforcement learning (RL) and a type of self-play between two agents. The first agent, which we call the environment agent, is trained with imitation learning on human-human interactions (game actions, utterances and emotes) and subsequently kept fixed. The second agent, the RL agent, is trained to conduct dialogue given the goal, and the two agents interact within a given environment until the goal is either reached or a given number of turns has expired. At that point, rewards are given, and the RL agent is updated. We compare agents that have been trained to imitate human actions given a goal (an ""inverse model"") to two different RL approaches: optimizing actions with latent discrete variables (topics), or via rewarding actions sampled from the model (via the top-K outputs). We show that both types of RL agent are able to learn effectively, outperforming the inverse model approach or a vanilla chit-chat imitation baseline, and can converse naturally with their dialogue partner to achieve goals. In this paper, we investigate agents that can interact (speak or act) and can achieve goals in a rich world with diverse language, bridging the gap between chit-chat and goal-oriented dialogue. We achieve this by defining a task for an agent, where the goal is for the other player to execute a particular action. We explore two reinforcement learning based approaches to solve this task: the policy either learns to pick a topic or learns to pick an utterance given the top K utterances, and compare them against a strong baseline trained to imitate chit-chat. We show that these approaches effectively learn dialogue strategies that lead to successful completion of goals, while producing natural chat. Future work should explore further RL algorithms for agents that can act and speak in natural language at scale in our proposed rich task environment, and we expect further advancements.","Agents interact (speak, act) and can achieve goals in a rich world with diverse language, bridging the gap between chit-chat and goal-oriented dialogue.",two ; Raux et al. ; Henderson ; al. ; Bordes ; El Asri et al. ; Twitter ; Reddit ; Li ; Yang et al.,two ; Raux et al. ; Henderson ; al. ; Bordes ; El Asri et al. ; Twitter ; Reddit ; Li ; Yang et al.,Dialogue research distinguishes between chit-chat and goal-oriented tasks. The latter has clearer metrics and a more straightforward learning signal. Humans combine the two and engage in chit-chat with the goal of exchanging information or eliciting a specific response. We train a goal-oriented model with reinforcement learning via self-play against an imitation-learned chit-chat model with two new approaches. Both models outperform a strong inverse model baseline and can converse naturally with their dialogue partner in order to achieve goals.,/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"The Wasserstein probability metric has received much attention from the machine learning community. Unlike the Kullback-Leibler divergence, which strictly measures change in probability, the Wasserstein metric reflects the underlying geometry between outcomes. The value of being sensitive to this geometry has been demonstrated, among others, in ordinal regression and generative modelling, and most recently in reinforcement learning. In this paper we describe three natural properties of probability divergences that we believe reflect requirements from machine learning: sum invariance, scale sensitivity, and unbiased sample gradients. The Wasserstein metric possesses the first two properties but, unlike the Kullback-Leibler divergence, does not possess the third. We provide empirical evidence suggesting this is a serious issue in practice. Leveraging insights from probabilistic forecasting we propose an alternative to the Wasserstein metric, the Cramér distance. We show that the Cramér distance possesses all three desired properties, combining the best of the Wasserstein and Kullback-Leibler divergences. We give empirical results on a number of domains comparing these three divergences. To illustrate the practical relevance of the Cramér distance we design a new algorithm, the Cramér Generative Adversarial Network (GAN), and show that it has a number of desirable properties over the related Wasserstein GAN.
 In machine learning, the Kullback-Leibler (KL) divergence is perhaps the most common way of assessing how well a probabilistic model explains observed data. Among the reasons for its popularity is that it is directly related to maximum likelihood estimation and is easily optimized. However, the KL divergence suffers from a significant limitation: it does not take into account how close two outcomes might be, but only their relative probability. This closeness can matter a great deal: in image modelling, for example, perceptual similarity is key (Rubner et al., 2000; BID13 . Put another way, the KL divergence cannot reward a model that ""gets it almost right"".To address this limitation, researchers have turned to the Wasserstein metric, which does incorporate the underlying geometry between outcomes. The Wasserstein metric can be applied to distributions with non-overlapping supports, and has good out-of-sample performance BID11 . Yet , practical applications of the Wasserstein distance, especially in deep learning, remain tentative. In this paper we provide a clue as to why that might be: estimating the Wasserstein metric from samples yields biased gradients, and may actually lead to the wrong minimum. This precludes using stochastic gradient descent (SGD) and SGD-like methods, whose fundamental mode of operation is sample-based, when optimizing for this metric.As a replacement we propose the Cramér distance (Székely, 2002; Rizzo & Székely, 2016) , also known as the continuous ranked probability score in the probabilistic forecasting literature BID14 . The Cramér distance, like the Wasserstein metric, respects the underlying geometry but also has unbiased sample gradients. To underscore our theoretical findings, we demonstrate a significant quantitative difference between the two metrics when employed in typical machine learning scenarios: categorical distribution estimation, regression, and finally image generation. In the latter case, we use a multivariate generalization of the Cramér distance, the energy distance (Székely, 2002) , itself an instantiation of the MMD family of metrics BID16 . There are many situations in which the KL divergence, which is commonly used as a loss function in machine learning, is not suitable. The desirable alternatives, as we have explored, are the divergences that are ideal and allow for unbiased estimators: they allow geometric information to be incorporated into the optimization problem; because they are scale-sensitive and sum-invariant, they possess the convergence properties we require for efficient learning; and the correctness of their sample gradients means we can deploy them in large-scale optimization problems. Among open questions, we mention deriving an unbiased estimator that minimizes the Wasserstein distance, and variance analysis and reduction of the Cramér distance gradient estimate.","The Wasserstein distance is hard to minimize with stochastic gradient descent, while the Cramer distance can be optimized easily and works just as well.",Wasserstein ; Kullback-Leibler ; three ; first ; two ; third ; Cramér ; the Cramér Generative Adversarial Network ; GAN ; Wasserstein GAN,Wasserstein ; Kullback-Leibler ; three ; first ; two ; third ; Cramér ; the Cramér Generative Adversarial Network ; GAN ; Wasserstein GAN,"The Wasserstein probability metric, which measures change in probability, reflects the underlying geometry between outcomes. The value of being sensitive to this geometry has been demonstrated in ordinal regression and generative modelling, and most recently in reinforcement learning. In this paper, we describe three natural properties of probability divergences that we believe reflect requirements from machine learning: sum invariance, scale sensitivity, and unbiased sample gradients. The Cramér distance possesses all three desired properties, combining the best of the Wasserstein and Kullback-Leibler divergences. However, the KL diverg",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"Three-dimensional geometric data offer an excellent domain for studying representation learning and generative modeling. In this paper, we look at geometric data represented as point clouds. We introduce a deep autoencoder (AE) network with excellent reconstruction quality and generalization ability. The learned representations outperform the state of the art in 3D recognition tasks and enable basic shape editing applications via simple algebraic manipulations, such as semantic part editing, shape analogies and shape interpolation. We also perform a thorough study of different generative models including GANs operating on the raw point clouds, significantly improved GANs trained in the fixed latent space our AEs and, Gaussian mixture models (GMM). Interestingly, GMMs trained in the latent space of our AEs produce samples of the best fidelity and diversity.
 To perform our quantitative evaluation of generative models, we propose simple measures of fidelity and diversity based on optimally matching between sets point clouds. Three-dimensional (3D) representations of real-life objects are a core tool for vision, robotics, medicine, augmented and virtual reality applications. Recent encodings like view-based projections, volumetric grids and graphs, complement more traditional shape representations such as 3D meshes, level set functions, curve-based CAD models and constructive solid geometry BID1 . These encodings, while effective in their respective domains (e.g. acquisition or rendering), are often poor in semantics. For example, naïvely interpolating between two different cars in a view-based representation does not yield a representation of an ""intermediate"" car. Furthermore, these raw, high-dimensional representations are typically not well suited for the design of generative models via classic statistical methods. As such, editing and designing new objects with such representations frequently involves the construction and manipulation of complex, object-specific parametric models that link the semantics to the representation. This may require significant expertise and effort.Recent advances in deep learning bring the promise of a data-driven approach. In domains where data is plentiful, deep learning tools have eliminated the need for hand-crafting features and models. Deep learning architectures like autoencoders (AEs) BID22 Kingma & Welling, 2013) and Generative Adversarial Networks (GANs) BID10 BID20 Denton et al., 2015; BID6 are successful at learning complex data representations and generating realistic samples from complex underlying distributions. Recently, deep learning architectures for view-based projections BID30 BID14 , volumetric grids BID18 BID32 BID12 and graphs BID4 BID13 Defferrard et al., 2016; Yi et al., 2016b) have appeared in the 3D machine learning literature.In this paper we focus on point clouds, a relatively unexplored 3D modality. Point clouds provide a homogeneous, expressive and compact representation of surface geometry, easily amenable to geometric operations. These properties make them attractive from a learning point of view. In addition, they come up as the output of common range-scanning acquisition pipelines used in devices like the Kinect and iPhone's recent face identification feature. Only a handful of deep architectures for 3D point clouds exist in the literature: PointNet BID17 successfully tackled classification and segmentation tasks; BID14 used point-clouds as an intermediate step in their pipeline; Fan et al. (2016) used pointclouds as the underlying representation to extract 3D information from 2D images. We provide the first results that use deep architectures with the focus of learning representations and generative models for point clouds.Generative models have garnered increased attention recently in the deep learning community with the introduction of GANs BID10 ). An issue with GAN-based generative pipelines is that training them is notoriously hard and unstable BID24 . More importantly, there is no universally accepted way to evaluate generative models. In evaluating generative models one is interested in both fidelity, i.e. how much the generated points resemble the actual data, and coverage, i.e. what fraction of the data distribution a generated sample represents. The latter is especially important given the tendency of certain GANs to exhibit mode collapse. We provide simple methods to deal with both issues (training and evaluation) in our target domain. Our specific contributions are:• We design a new AE architecture-inspired by recent architectures used for classification BID17 -that is capable of learning compact representations of point clouds with excellent reconstruction quality even on unseen samples. The learned representations are (i) good for classification via simple methods (SVM), improving on the state of the art BID31 ; (ii) suitable for meaningful interpolations and semantic operations.• We create the first set of generative models which ( i) can generate point clouds measurably similar to the training data and held-out test data; (ii) provide good coverage of the training and test dataset. We argue that jointly learning the representation and training the GAN is unnecessary for our modality. We propose a workflow that first learns a representation by training an AE with a compact bottleneck layer, then trains a plain GAN in that fixed latent representation. Intuitively, training a GAN inside a compact, low-dimensional representation is easier. We point to theory BID0 that supports this idea, and verify it empirically. Latent GANs are much easier to train than monolithic (raw) GANs and achieve superior reconstruction with much better coverage. Somewhat surprisingly, GMMs trained in the latent space of fixed AEs achieve the best performance across the board.• We show that multi-class GANs work almost on par with dedicated GANs trained per-objectcategory, as long as they are trained in the latent space.• To support our qualitative evaluation, we perform a careful study of various old and new metrics, in terms of their applicability ( i) as objectives for learning good representations; (ii) for the evaluation of generated samples. We find that a commonly used point cloud metric, Chamfer distance, fails to discriminate certain pathological cases from good examples. We also propose fidelity and coverage metrics for our generative models, based on an optimal matching between two different samples, e.g. a set of point clouds generated by the model and a held-out test set.The rest of this paper is organized as follows: Section 2 outlines the necessary background and building blocks for our work and introduces our evaluation metrics. Section 3 introduces our models for latent representations and generation of point clouds. In Section 4, we evaluate all of our models both quantitatively and qualitatively, and analyze their behaviour. Further results and evaluation can be found in the appendix. The code for all our models is publicly available 1 .2 BACKGROUND DISPLAYFORM0 Autoencoders. Autoencoders (AE -inset) are deep architectures that aim to reproduce their input. They are especially useful, when they contain a narrow bottleneck layer between input and output. Upon successful training, the bottleneck layer corresponds to a lowdimensional representation, a code for the dataset. The Encoder (E) learns to compress a data point x into its latent representation, z. The Decoder (D) can then reproduce x from its encoded version z. DISPLAYFORM1 Generative Adversarial Networks. GANs are state-of-the-art generative models. The basic architecture (inset) is based on a adversarial game between a generator (G) and a discriminator (D). The generator aims to synthesize samples that look indistinguishable from real data (drawn from x ∼ p data ) by passing a randomly drawn sample z ∼ p z through the generator function G. The discriminator tries to tell synthesized from real samples. The most commonly used losses for the discriminator and generator networks are: DISPLAYFORM2 DISPLAYFORM3 where θ (D) , θ (G) are the parameters for the discriminator and the generator network respectively. In addition to the classical GAN formulation, we also use the improved Wasserstein GAN BID11 , which has shown improved stability during training.Challenges specific to point cloud geometry. Point clouds as an input modality present a unique set of challenges when building a network architecture. As an example, the convolution operator -now ubiquitous in image-processing pipelines -requires the signal (in our case, geometry) to be defined on top of an underlying grid-like structure. Such a structure is not available in raw point clouds, which renders them significantly more difficult to encode than e.g. images or voxel grids. Recent classification work on point clouds (PointNet -Qi et al. (2016a) ) bypasses this issue by circumventing 2D convolutions. Another issue with point clouds as a representation is that they are unordered -any permutation of a point set still describes the same shape. This complicates comparisons between two point sets, typically needed to define a loss function. This unorderedness of point clouds also creates the need for making the encoded feature permutation invariant.Point-set distances. Two permutation-invariant metrics for comparing unordered point sets have been proposed in the literature (Fan et al., 2016) . On the one hand, the Earth Mover's distance (EMD) BID21 is the solution of a transportation problem which attempts to transform one set to the other. For two equally sized subsets S 1 ⊆ R 3 , S 2 ⊆ R 3 , their EMD is defined by d EM D (S 1 , S 2 ) = min φ:S1→S2 x∈S1x − φ(x) 2 where φ is a bijection. Interpreted as a loss, EMD is differentiable almost everywhere. On the other hand, the Chamfer (pseudo)-distance (CD) measures the squared distance between each point in one set to its nearest neighbor in the other set: DISPLAYFORM4 It is still differentiable but more computationally efficient.Evaluation Metrics for representations and generative models. In the remainder of the paper, we will frequently need to compare a given set (distribution) of points clouds, whether reconstructed or synthesized, to its ground truth counterpart. For example, one might want to assess the quality of a representation model, in terms of how well it matches the training set or a held-out test set. Such a comparison might be done to evaluate the faithfulness and/or diversity of a generative model, and measure potential mode-collapse. To measure how well a point-cloud distribution A matches a ground truth distribution G, we use the following metrics:Coverage. For each point-cloud in A we find its closest neighbor in G; closeness can be computed using either CD or EMD, thus yielding two different metrics, COV-CD and COV-EMD. Coverage is measured as the fraction of the point-clouds in G that were matched to point-clouds in A. A high coverage score typically indicates that most of G is roughly represented within A. We presented a novel set of architectures for 3D point-cloud representation learning and generation. Our results show good generalization to unseen data and our representations encode meaningful semantics. In particular our generative models are able to produce faithful samples and cover most of the ground truth distribution without memorizing a few examples. Interestingly, we see that the best-performing generative model in our experiments is a GMM trained in the fixed latent space of an AE. While, this might not be a universal result, it suggests that simple classic tools should not be dismissed. A thorough investigation on the conditions under which simple latent GMMs are as powerful as adversarially trained models would be of significant interest.",Deep autoencoders to learn a good representation for geometric 3D point-cloud data; Generative models for point clouds.,Three ; Gaussian ; GMM ; fidelity ; CAD ; two ; Generative Adversarial Networks ; al. ; Yi et al. ; Kinect,Three ; Gaussian ; GMM ; fidelity ; CAD ; two ; Generative Adversarial Networks ; al. ; Yi et al. ; Kinect,"Three-dimensional geometric data offer an excellent domain for studying representation learning and generative modeling. In this paper, we introduce a deep autoencoder (AE) network with excellent reconstruction quality and generalization ability. The learned representations outperform the state of the art in 3D recognition tasks and enable basic shape editing applications via simple algebraic manipulations. We also perform a thorough study of different generative models including GANs operating on raw point clouds, significantly improved GANs trained in the fixed latent space our AEs and, Gaussian mixture models (GM).",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"Neural sequence-to-sequence models are a recently proposed family of approaches used in abstractive summarization of text documents, useful for producing condensed versions of source text narratives without being restricted to using only words from the original text. Despite the advances in abstractive summarization, custom generation of summaries (e.g. towards a user's preference) remains unexplored. In this paper, we present CATS, an abstractive neural summarization model, that summarizes content in a sequence-to-sequence fashion but also introduces a new mechanism to control the underlying latent topic distribution of the produced summaries. Our experimental results on the well-known CNN/DailyMail dataset show that our model achieves state-of-the-art performance. Automatic document summarization is defined as producing a shorter, yet semantically highly related, version of a source document. Solutions to this task are typically classified into two categories: Extractive summarization and abstractive summarization.Extractive summarization refers to methods that select sentences of a source text based on a scoring scheme, and eventually combine those exact sentences in order to produce a summary. Conversely, abstractive summarization aims at producing shortened versions of a source document by generating sentences that do not necessarily appear in the original text. Recent advances in neural sequence-to-sequence modeling have sparked interest in abstractive summarization due to its flexibility and broad range of applications.The majority of research on text summarization thus far has been focused on extractive summarization BID16 , due its simplicity compared to abstractive methods.Beyond providing a generic summary of a longer passage of text, a system which would allow selective summarization based on a user's preference of topic would be of great value in an array of domains. For example, in the field of information retrieval, it could be used to summarize the results of a user search based on the content of the query.Summarization is also extensively used in other domains such as concisely describing the gist of news articles and stories BID21 BID19 , supporting the minutetaking process BID20 in corporate meetings and in the electronic health record domain BID5 , to name a few.In this paper, we introduce CATS, a customizable abstractive topic-based sequence-to-sequence summarization model, which is not only capable of summarizing text documents with an improved performance as compared to the state of the art, but also allows to selectively focus on a range of desired topics of interest when generating summaries. Our experiments corroborate that our model can selectively add or remove certain topics from the summary. Furthermore, our experimental results on a publicly available dataset indicate that the proposed neural sequence-to-sequence model can effectively outperform state-of-the-art baselines in terms of ROUGE.The main contributions of this paper are: (1) We introduce a novel neural sequence-tosequence model based on an encoder-decoder architecture that outperforms the state-of-the-art baselines in the task of abstractive summarization on a benchmark dataset.(2 ) We show how the attention mechanism BID0 may be used for simultaneously identifying important topics as well as recognizing those parts of the encoder output that are vital to be focused on.The remainder of this paper is organized is as follows: Section 2 discusses related work on abstractive neural summarization. In Section 3, we introduce the CATS summarization model. In Section 4, we discuss our experimental setup and results comparing CATS to a broad range of competitive state-of-the-art baselines. Finally , in Section 5, we conclude this paper and present future directions of inquiry. In this paper we present CATS, an abstractive summarization model that makes use of latent topic information in a source document, and is thereby capable of controlling the topics appearing in an output summary of a source document. This can enable customization of generated texts based on user profiles or explicitly given topics, in order to present content tailored to a user's information needs.Our experimental results show that our CATS+coverage model achieves state-of-the-art performance in terms of standard evaluation metrics for summarization (i.e ROUGE) on an important benchmark dataset, while enabling customization in producing summaries.CATS can serve as a foundation for future work in the domain of automatic summarization. Based on the results of this paper, we believe the future work on summarization systems to be exciting, in that a generated summary could be customized to users' needs. We envision three ways of controlling the focus of output summaries using our models: First, as demonstrated in the experiment in Section 4.4.3, certain topics could be disabled in the output of the topic model and be consequently discarded from output summaries. Second, a reference document could be provided to the topic model, its topics could be extracted and subsequently direct the focus of generated summaries. This is useful when a user wants to see summaries/updates primarily or only regarding issues discussed in an existing reference document. Third, content extracted from user profiles (e.g. history of web pages of interest) could be provided to the topic model, their salient themes extracted by the model and then taken into account whenever presenting users with summaries. All three directions are interesting future works of this paper.",We present the first neural abstractive summarization model capable of customization of generated summaries.,CNN ; two ; CATS ; three ; First ; Second ; Third,CNN ; two ; CATS ; three ; First ; Second ; Third ; sequence ; a recently proposed family ; approaches,Neural sequence-to-sequence models are a recently proposed family of approaches used in abstractive summarization of text documents. They are useful for producing condensed versions of source text narratives without being restricted to using only words from the original text. Abstractive summarization aims at producing shortened versions of a source document by generating sentences that do not necessarily appear in the original text. A system which allows selective summarization based on a user's preference of topic would be of great value in an array of domains.,/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"Developing effective biologically plausible learning rules for deep neural networks is important for advancing connections between deep learning and neuroscience. To date, local synaptic learning rules like those employed by the brain have failed to match the performance of backpropagation in deep networks. In this work, we employ meta-learning to discover networks that learn using feedback connections and local, biologically motivated learning rules. Importantly, the feedback connections are not tied to the feedforward weights, avoiding any biologically implausible weight transport. It can be shown mathematically that this approach has sufficient expressivity to approximate any online learning algorithm. Our experiments show that the meta-trained networks effectively use feedback connections to perform online credit assignment in multi-layer architectures. Moreover, we demonstrate empirically that this model outperforms a state-of-the-art gradient-based meta-learning algorithm for continual learning on regression and classification benchmarks. This approach represents a step toward biologically plausible learning mechanisms that can not only match gradient descent-based learning, but also overcome its limitations. Deep learning has achieved impressive success in solving complex tasks, and in some cases its learned representations have been shown to match those in the brain [19, 10] . However, there is much debate over how well the learning algorithm commonly used in deep learning, backpropagation, resembles biological learning algorithms. Causes for skepticism include the facts that (1) backpropagation ignores the nonlinearities imposed by neurons in the backward pass and assumes instead that derivatives of the forward-pass nonlinearities can be applied, (2) in backpropagation, feedback path weights are exactly tied to feedforward weights, even as weights are updated with learning, and (3) backpropagation assumes alternating forward and backward passes [12] . The question of how so-called credit assignment -appropriate propagation of learning signals to non-output neurons -can be performed in biologically plausible fashion in deep neural networks remains open. We propose a new learning paradigm that aims to solve the credit assignment problem in more biologically plausible fashion. Our approach is as follows: (1) endow a deep neural network with feedback connections that propagate information about target outputs to neurons at all layers, (2) apply local plasticity rules (e.g. Hebbian or neuromodulated plasticity) to update feedforward synaptic weights following feedback projections, and (3) employ meta-learning to optimize for the initialization of feedforward weights, the setting of feedback weights, and synaptic plasticity levels. On a set of online regression and classification learning tasks, we find that meta-learned deep networks can successfully perform useful weight updates in early layers, and that feedback with local learning rules can in fact outperform gradient descent as an inner-loop learning algorithm on challenging few-shot and continual learning tasks. This work demonstrates that meta-learning procedures can optimize for neural networks that learn online using local plasticity rules and feedback connections. Several follow-up directions could be pursued. First, meta-learning of this kind is computationally expensive, as the meta-learner must backpropagate through the network's entire training procedure. In order to scale this approach, it will be important to find ways to meta-train networks that generalize to longer lifetimes than were used during meta-training, or to explore alternatives to backprop-based meta-training (e.g. evolutionary algorithms). The present work focused on the case of online learning, but the case of learning from repeated exposure to large datasets is also of interest, and scaling the method in this fashion will be crucial to exploring this regime. Future work could also increase the biological plausibility of the method. For instance, in the present implementation the feedforward and feedback + update passes occur sequentially. However, a natural extension would enable them to run in parallel. This requires ensuring (through appropriate meta-learning and/or a segregated dendrites model [6] ) that feedforward and feedback information do not interfere destructively. Third, the meta-learning procedure in this work optimizes for a precise feedforward and feedback weight initialization. Optimizing instead for a distribution of weight initializations or connectivity patterns would better reflect the stochasticity in synapse development. Another direction is to apply meta-learning to understand biological learning systems (see [9] for an example of such an effort). Well-constrained biological learning models meta-optimized in this manner might show emergence of learning circuits used in biology and even suggest new ones. [",Networks that learn with feedback connections and local plasticity rules can be optimized for using meta learning.,Hebbian ; First ; Third,Hebbian ; First ; Third ; effective biologically plausible learning rules ; deep neural networks ; connections ; deep learning ; neuroscience ; date ; local synaptic learning rules,"Developing effective biologically plausible learning rules for deep neural networks is important for advancing connections between deep learning and neuroscience. To date, local synaptic learning rules have failed to match the performance of backpropagation in deep networks. In this work, we employ meta-learning to discover networks that learn using feedback connections and local, biologically motivated learning rules. The feedback connections are not tied to feedforward weights, avoiding any biologically implausible weight transport. This approach has sufficient expressivity to approximate any online learning algorithm. The meta-trained networks effectively use feedback connections to perform online credit",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"To gain high rewards in muti-agent scenes, it is sometimes necessary to understand other agents and make corresponding optimal decisions. We can solve these tasks by first building models for other agents and then finding the optimal policy with these models. To get an accurate model, many observations are needed and this can be sample-inefficient. What's more, the learned model and policy can overfit to current agents and cannot generalize if the other agents are replaced by new agents. In many practical situations, each agent we face can be considered as a sample from a population with a fixed but unknown distribution. Thus we can treat the task against some specific agents as a task sampled from a task distribution. We apply meta-learning method to build models and learn policies. Therefore when new agents come, we can adapt to them efficiently. Experiments on grid games show that our method can quickly get high rewards. Applying Reinforcement Learning (RL) to multi-agent scenes requires carefully consideration about the influence of other agents. We cannot simply treat other agents as part of the environment and apply independent RL methods BID8 if the actions of them has impact on the payoff of the agent to be trained. For example, consider the two-person ultimatum bargaining game, where two players take part in. One player propose a deal to split a fixed amount of money for them two and the other player decides to accept it or not. If the second player accepts the proposal, they split the money, but if the proposal is refused, they both get zero. Experimental results BID4 show that in actual life, the second player makes the decision according to whether he or she judge the final result fair, rather than makes the obvious rational decision. Thus, the first player needs to predict how the second player will react so as to make the proposal acceptable.In order to exploit the other agents and find the corresponding optimal policy, we need to understand these agents. Here in this paper, we call all the other agents ""opponents"" to distinguish our agent from them, even if they may have cooperative relationship with our agent. For simplicity, we only consider tasks with only one opponent. Extension to tasks with more opponents is straightforward. A general way to exploit an opponent is to build a model for it from observations. This model can characterize any needed feature of the opponent, such as next action or the final goal. Such a model can make predictions for the opponent and thus turns the two-agent task into a simple-agent decision making problem. Then we can apply various RL methods to solve this problem.It is necessary that we need to have an accurate model for the opponent to help make decision. Previous works BID6 BID12 propose some methods to model the opponent. Generally, it requires many observations to get a precise model for the opponent. This may cost many iterations to act with the opponent. What's more, even if we can precisely model the opponent, there exists a main drawback of above process that the performance of the learned policy has no guarantee for any other opponent. Things are even worse if opponents have their private types which are unknown for us. New opponents with different types can have different policies or even different payoffs. Therefore, it seems that when a new opponent came, we have to learn a policy from the beginning. In some practical situations, the whole opponents follow a distributions over all these possible types. Let's come back to the ultimatum bargaining game. BID0 shows that people with different ethnicity may have different standards for fairness. Thus if we assume the type for player 2 to be its judgment for fairness, there can be a distribution for types dependent on the ethnic distribution. Given that opponents follows a distribution, it is possible that we can employ some given opponents to help us speed up the process of opponent modeling and policy improving for the current opponent.If we consider the policy learning against a specific opponent as a task, our goal can be considered as training a policy on various tasks so that it can efficiently adapt to a good policy on a new task with few training samples. This is exactly a meta-learning problem. We employ Model-Agnostic MetaLearning (MAML) BID1 to conduct meta-learning. BID11 applied meta-learning to understand opponents, but this work doesn't address the policy improvement for the agent to be trained. We apply meta-learning to opponent modeling and policy learning separately while training the two meta-learners jointly. Then we use the meta-learners to initialize the model and policy for the new opponent. Experimental results show that the agent can adapt to the new opponent with a small number of interactions with the opponent. In the face of other agents, it is beneficial to build models for opponents and find a corresponding good policy. This method can be sample-inefficient since it costs many observations to build models and learn a policy then. We propose a method that can employ the information learned from experiences with other opponents to speed up the learning process for the current opponents. This method is suitable for many practical situations where the opponent population has a relative stable distribution over their policies. We apply meta-learning to jointly train the opponent modeling and policy improving process. Experimental results show that our method can be sample-efficient.",Our work applies meta-learning to multi-agent Reinforcement Learning to help our agent efficiently adapted to new coming opponents.,first ; Applying Reinforcement Learning ; two ; One ; second ; zero ; only one ; Model-Agnostic MetaLearning ; MAML,first ; Applying Reinforcement Learning ; two ; One ; second ; zero ; only one ; Model-Agnostic MetaLearning ; MAML ; high rewards,"To gain high rewards in muti-agent scenes, it is sometimes necessary to understand other agents and make corresponding optimal decisions. To get an accurate model, many observations are needed and this can be sample-inefficient. The learned model and policy can overfit to current agents and cannot generalize if the other agents are replaced by new agents. Applying Reinforcement Learning (RL) to multi-agent scenes requires carefully consideration about the influence of other agents.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"We analyze the trade-off between quantization noise and clipping distortion in low precision networks. We identify the statistics of various tensors, and derive exact expressions for the mean-square-error degradation due to clipping. By optimizing these expressions, we show marked improvements over standard quantization schemes that normally avoid clipping. For example, just by choosing the accurate clipping values, more than 40\% accuracy improvement is obtained for the quantization of VGG-16 to 4-bits of precision. Our results have many applications for the quantization of neural networks at both training and inference time. 
 A significant drawback of deep learning models is their computational costs. Low precision is one of the key techniques being actively studied recently to conquer the problem. With hardware support, low precision training and inference can compute more operations per second, reduce memory bandwidth and power consumption, and allow bigger network to fit into a device.In general, a low-precision scheme involves a floating-point to integer conversion, which introduces quantization noise into the network. This quantization noise is strongly linked to the dynamic range, defined as the range between the largest and smallest values that need to quantized. For a given N -bit integer representation, a smaller dynamic range leads to a smaller spacing between the 2 N quantization levels, enabling improved resolution and smaller quantization noise. To reduce this quantization noise, the dynamic range can be limited by clipping the values in the tensor. This clipping process introduces an additional noise because of the loss of information that otherwise would be carried by the clipped portion of the tensor. Hence, a trade-off between clipping and quantization effects exist. To find the best clipping value we need to minimize the information loss.In this paper, we study the effect of clipping with the aim of improving overall quantization noise. To this end, we first study the distribution of values within these tensors. In all our measurements, the statistical distributions of weights and activations are observed to follow a bell-curve. This indicates that large values occur very rarely compared to small values, and suggests that the loss of information due to the clipping process might be compensated by improving the resolution of the more common smaller values.To optimize this process further, it is essential to understand the underlying distribution of tensor elements before applying the clipping. By running a few statistical tests, we were able to see on a variety of convolution models that activation tensors follow either a Gaussian or Laplacian distributions with a high degree of certainty (p-value < 0.01). This modeling of activation tensors enables a clear formulation of the quantization process and constitutes the first step for its optimization.We turn to consider the objective we aim to optimize. It is well known that when batch norm is applied after a convolution layer, the output is invariant to the norm of the output on the proceeding layer BID4 ] i.e., BN (C · W · x) = BN (W · x) for any given constant C. This quantity is often described geometrically as the norm of the activation tensor, and in the presence of this invariance, the only measure that needs to be preserved upon quantization is the directionality of the tensor. Therefore, quantization preserves tensor information if the angle between the highprecision tensor and its quantized version is small. Recently, BID0 has shown that this angle depends only on the quantization error power (L2 -norm) and the power of original tensor. Therefore, minimizing the power of the quantization error constitutes a plausible goal for the optimization of the quantized network in terms of accuracy.In Section 4, we provide a rigorous formulation to optimize the quantization effect of activation tensors using clipping by analyzing both the Gaussian and the Laplace priors. This formulation is henceforth refered to as Analytical Clipping for Integer Quantization (ACIQ).These analytical results have many applications for the quantization of neural networks at both training and inference time. For example , a straightforward quantization of the weights and activations to 8-bit fixed point representation has been shown to have a negligible effect on the model accuracy. Yet, in the majority of the applications, further reduction of precision quickly degrades performance, calling for an optimal clipping scheme to minimize information-loss during quantization. On a more general level, exploiting the statistics of the activation tensors to minimize their quantization toll is orthogonal to other techniques for network quantization. It can work in synergy with other schemes to achieve more than could have been achieved by each individually. Finally, it is easy to implement and requires only the adjustment of clipping value according to an analytical formula.We further demonstrate the applicability and usability of our analytic terms on the following challenging problem. Given a pre-trained network , we would like to quantize the weights to 8-bit of precision and most activations to 4-bits of precision without any further processing (e.g., re-training). This specific setting is of a particular interest due to quantization of activations to 4-bits, which alleviates a major bottleneck in terms of memory bandwidth. Prior attempts using standard techniques BID8 show severe degradation on accuracy. While several recent works were able to overcome this issue by additional re-training BID12 , this is not feasible in many practical settings, e.g., we often do not have the dataset on which the network is working on.We compare ACIQ against two methods: (i) the traditional method that avoids clipping (also known by gemmlowp BID6 ), where values are uniformly quantized between the largest and smallest tensor values; (ii) the iterative method suggested by NVIDIA to search for a good clipping threshold based on the Kullback-Leibler Divergence (KLD) measure BID13 . Results are summarized in TAB1 . While both ACIQ and gemmlowp are fast non-iterative methods, ACIQ significantly outperforms in terms of validation accuracy. On the hand, KLD is an exhaustive timeconsuming procedure, which iteratively evaluates the KLD measure on a large candidate set of clipping values, and then returns the clipping value for which best evaluation is attained. In our simulations ACIQ and gemmlowp require a single pass over tensor values, while KLD requires 4000 passes. Nonetheless, excluding ResNet-101, ACIQ outperforms KLD in terms of validation accuracy.The methods introduced in this work may be additionally useful to current and future applications, such as the attempts to fully train in a low precision setting BID0 We introduce ACIQ -an optimized clipping framework for improved quantization of neural networks. Optimized clipping is shown to have a drastic impact on quantization in a variety of models. The underlying reason lies in the statistical dispersion of activations, where large values occur very rarely. We show the bell-curve statistics of activations are best fit as either Laplace or Gaussian distributions, and formulate the clipping process as an optimization problem. The solution to this optimization problem constitutes a polynomial-exponential equation that can be calculated numerically for a variety of statistical parameters, and stored in a lookup table for fast retrieval. This scheme is very simple and easy to implement either in software or in hardware.While results are very encouraging, this work is only the first step on the ladder for successful deployment of clipping in neural networks. First, our main focus in this work is quantization of activations, while similar evaluation still needs to be done for weights. On a more general level, our framework is not restricted to the inference settings and can be extended to training. For example, our preliminary results show that quantization of gradients might benefit from the clipping of small values (i.e., sparsification). Establishing the correct threshold for gradients is yet another important direction for future work. While much work still needs to be done with regards to optimized clipping, we believe our work clearly demonstrates the major importance of this concept for the quantization of neural networks. A PIECE-WISE LINEAR APPROXIMATIONHere we provide a more accurate analysis related to the qunatization noise (i.e., the second term in Equation 3), measured as the expected mean-square-error when the range [−α, α] is quantized uniformly to 2 M discrete levels. To that end, we approximate the density function f by a construction of a piece-wise linear function g such that f (q i ) = g(q i ) for each i ∈ [0, 2 M − 1]. Since we consider only smooth probability density functions (e.g., Gaussian or Laplace), the resulting approximation error is small for sufficient resolution i.e., small quantization step size ∆. In figure 1 we provide an illustration for this construction.We turn to calculate the linear equation for each line segment of the piece-wise linear function g, falling in the range [−α + i · ∆, −α + (i + 1) · ∆]. To that end, we consider the slope (derivative) and the value of the density function at the midpoint q i . With these two values we can define for each segment i ∈ [0, 2 M − 1] the corresponding form of linear approximation: DISPLAYFORM0 We now turn to calculate the second term in Equation 3. By equation 14, and since q i is defined to be the midpoint between the integration limits, the following holds true","We analyze the trade-off between quantization noise and clipping distortion in low precision networks, and show marked improvements over standard quantization schemes that normally avoid clipping",more than ; second ; first ; Gaussian ; Laplacian ; W ; C. ; Laplace ; Analytical Clipping ; ACIQ).These,more than ; second ; first ; Gaussian ; Laplacian ; W ; C. ; Laplace ; Analytical Clipping ; ACIQ).These,"We analyze the trade-off between quantization noise and clipping distortion in low precision networks. We identify the statistics of various tensors, and derive exact expressions for the mean-square-error degradation due to clipping. Our results have many applications for the quantization of neural networks at both training and inference time. Low precision training and inference can reduce computational costs by reducing the dynamic range. To reduce this noise, we can limit the dynamic range by clipping the values in the tensor. The statistical distributions of weights and activations are observed to follow a bell-",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"Trading off exploration and exploitation in an unknown environment is key to maximising expected return during learning. A Bayes-optimal policy, which does so optimally, conditions its actions not only on the environment state but on the agent's uncertainty about the environment. Computing a Bayes-optimal policy is however intractable for all but the smallest tasks. In this paper, we introduce variational Bayes-Adaptive Deep RL (variBAD), a way to meta-learn to perform approximate inference in an unknown environment, and incorporate task uncertainty directly during action selection. In a grid-world domain, we illustrate how variBAD performs structured online exploration as a function of task uncertainty. We also evaluate variBAD on MuJoCo domains widely used in meta-RL and show that it achieves higher return during training than existing methods. Reinforcement learning (RL) is typically concerned with finding an optimal policy that maximises expected return for a given Markov decision process (MDP) with an unknown reward and transition function. If these were known, the optimal policy could in theory be computed without environment interactions. By contrast, learning in an unknown environment usually requires trading off exploration (learning about the environment) and exploitation (taking promising actions). Balancing this trade-off is key to maximising expected return during learning, which is desirable in many settings, particularly in high-stakes real-world applications like healthcare and education (Yauney & Shah, 2018; Liu et al., 2014) . A Bayes-optimal policy, which does this trade-off optimally, conditions actions not only on the environment state but on the agent's own uncertainty about the current MDP. In principle, a Bayes-optimal policy can be computed using the framework of Bayes-adaptive Markov decision processes (BAMDPs) (Martin, 1967; Duff & Barto, 2002) , in which the agent maintains a belief distribution over possible environments. Augmenting the state space of the underlying MDP with this belief yields a BAMDP, a special case of a belief MDP (Kaelbling et al., 1998) . A Bayes-optimal agent maximises expected return in the BAMDP by systematically seeking out the data needed to quickly reduce uncertainty, but only insofar as doing so helps maximise expected return. Its performance is bounded from above by the optimal policy for the given MDP, which does not need to take exploratory actions but requires prior knowledge about the MDP to compute. Unfortunately, planning in a BAMDP, i.e., computing a Bayes-optimal policy that conditions on the augmented state, is intractable for all but the smallest tasks. A common shortcut is to rely instead on posterior sampling (Thompson, 1933; Strens, 2000; Osband et al., 2013) . Here, the agent periodically samples a single hypothesis MDP (e.g., at the beginning of an episode) from its posterior, and the policy that is optimal for the sampled MDP is followed until the next sample is drawn. Planning is far more tractable since it is done on a regular MDP, not a BAMDP. However, posterior sampling's exploration can be highly inefficient and far from Bayes-optimal. Consider the example of a gridworld in Figure 1 , where the agent must navigate to an unknown goal located in the grey area (1a). To maintain a posterior, the agent can uniformly assign non-zero probability to cells where the goal could be, and zero to all other cells. A Bayes-optimal strategy strategically searches the set of goal positions that the posterior considers possible, until the goal is found (1b). Posterior sampling by contrast samples a possible goal position, takes the shortest route there, and then resamples a different goal position from the updated posterior (1c). Doing so is much less efficient since the agent's uncertainty is not reduced optimally (e.g., states are revisited). Average return over all possible environments, over six episodes with 15 steps each (after which the agent is reset to the starting position). The performance of any exploration strategy is bounded above by the optimal behaviour (of a policy with access to the true goal position). The Bayes-optimal agent matches this behaviour from the third episode, whereas posterior sampling needs six rollouts. VariBAD closely approximates Bayes-optimal behaviour in this environment. As this example illustrates, Bayes-optimal policies can explore much more efficiently than posterior sampling. Hence, a key challenge is to find ways to learn approximately Bayes-optimal policies while retaining the tractability of posterior sampling. In addition, many complex tasks pose another key challenge: the inference involved in maintaining a posterior belief, needed even for posterior sampling, may itself be intractable. In this paper, we combine ideas from Bayesian reinforcement learning, approximate variational inference, and meta-learning to tackle these challenges, and equip an agent with the ability to strategically explore unseen (but related) environments for a given distribution, in order to maximise its expected return. More specifically, we propose variational Bayes-Adaptive Deep RL (variBAD), a way to meta-learn to perform approximate inference on a new task 1 , and incorporate task uncertainty directly during action selection. We represent a single MDP using a learned, low-dimensional stochastic latent variable m. Given a set of tasks sampled from a distribution, we jointly meta-train: (1) a variational auto-encoder that can infer the posterior distribution over m in a new task while interacting with the environment, and (2) a policy that conditions on this posterior distribution over the MDP embeddings, and thus learns how to trade off exploration and exploitation when selecting actions under task uncertainty. Figure 1e shows the performance of our method versus the hard-coded optimal (i.e., given privileged goal information), Bayes-optimal, and posterior sampling exploration strategies. VariBAD's performance closely matches that of Bayes-optimal action selection, matching optimal performance from the third rollout. Previous approaches to BAMDPs are only tractable in environments with small action and state spaces. VariBAD offers a tractable and dramatically more flexible approach for learning Bayesadaptive policies tailored to the task distribution seen during training, with the only assumption that such a task distribution is available for meta-training. We evaluate our approach on the gridworld shown above and on MuJoCo domains that are widely used in meta-RL, and show that variBAD exhibits superior exploratory behaviour at test time compared to existing meta-learning methods, achieving higher returns during learning. As such, variBAD opens a path to tractable approximate Bayes-optimal exploration for deep reinforcement learning. We presented variBAD, a novel deep RL method to approximate Bayes-optimal behaviour, which uses meta-learning to utilise knowledge obtained in related tasks and perform approximate inference in unknown environments. In a didactic gridworld environment, our agent closely matches Bayesoptimal behaviour, and in more challenging MuJoCo tasks, variBAD outperforms existing methods in terms of achieved reward during a single episode. In summary, we believe variBAD opens a path to tractable approximate Bayes-optimal exploration for deep reinforcement learning. There are several interesting directions of future work based on variBAD. For example, we currently do not use the decoder at test time. One could instead use the decoder for model-predictive planning, or to get a sense for how wrong the predictions are (which might indicate we are out of distribution, and further training is necessary). Another exciting direction for future research is considering settings where the training and test distribution of environments are not the same. Generalising to out-of-distribution tasks poses additional challenges and in particular for variBAD two problems are likely to arise: the inference procedure will be wrong (the prior and/or posterior update) and the policy will not be able to interpret a changed posterior. In this case, further training of both the encoder/decoder might be necessary, together with updates to the policy and/or explicit planning.","VariBAD opens a path to tractable approximate Bayes-optimal exploration for deep RL using ideas from meta-learning, Bayesian RL, and approximate variational inference.",Bayes ; Bayes-Adaptive Deep RL ; MuJoCo ; Markov ; MDP ; Yauney & Shah ; Liu et al. ; Martin ; Duff & Barto ; BAMDP,Bayes ; Bayes-Adaptive Deep RL ; MuJoCo ; Markov ; MDP ; Yauney & Shah ; Liu et al. ; Martin ; Duff & Barto ; BAMDP,"Trading off exploration and exploitation in an unknown environment is key to maximising expected return during learning. A Bayes-optimal policy conditions its actions not only on the environment state but on the agent's uncertainty about the environment. However, a Bayes-optimal policy is intractable for all but the smallest tasks. In this paper, we introduce variational Bayes-Adaptive Deep RL (variBAD), a way to meta-learn to perform approximate inference in an unknown environment and incorporate task uncertainty directly during action selection. VariBAD achieves higher return during",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"To improve how neural networks function it is crucial to understand their learning process. The information bottleneck theory of deep learning proposes that neural networks achieve good generalization by compressing their representations to disregard information that is not relevant to the task. However, empirical evidence for this theory is conflicting, as compression was only observed when networks used saturating activation functions. In contrast, networks with non-saturating activation functions achieved comparable levels of task performance but did not show compression. In this paper we developed more robust mutual information estimation techniques, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions. Using these adaptive estimation techniques, we explored compression in networks with a range of different activation functions. With two improved methods of estimation, firstly, we show that saturation of the activation function is not required for compression, and the amount of compression varies between different activation functions. We also find that there is a large amount of variation in compression between different network initializations. Secondary, we see that L2 regularization leads to significantly increased compression, while preventing overfitting. Finally, we show that only compression of the last layer is positively correlated with generalization. Although deep learning (reviewed by BID15 ) has produced astonishing advances in machine learning BID17 , a rigorous statistical explanation for the outstanding performance of deep neural networks (DNNs) is still to be found.According to the information bottleneck (IB) theory of deep learning BID18 BID16 ) the ability of DNNs to generalize can be seen as a type of representation compression. The theory proposes that DNNs use compression to eliminate noisy and task-irrelevant information from the input, while retaining information about the relevant segments BID1 . The information bottleneck method BID19 quantifies the relevance of information by considering an intermediate representation T between the original signal X and the salient data Y . T is the most relevant representation of X, and is said to be an information bottleneck, when it maximally compresses the input, retaining only the most relevant information, while maximizing the information it shares with the target variable Y . Formally, the information bottleneck minimizes the Lagrangian: DISPLAYFORM0 where I(·) is mutual information. In this Lagrangian β is the Lagrange multiplier, determining the trade-off between compression and retention of information about the target. In the context of deep learning, T is a layer's hidden activity represented as a single variable, X is a data set and Y is the set of labels. Compression for a given layer is signified by a decrease in I(T, X) value, while I(T, Y ) is increasing during training. Fitting behaviour refers to both values increasing. BID16 visualized the dynamic of training a neural network by plotting the values of I(T, X) and I(T, Y ) against each other. This mapping was named the information plane. According to IB theory the learning trajectory should move the layer values to the top left of this plane. In fact what was observed was that a network with tanh activation function had two distinct phases: fitting and compression. The paper and the associated talks 1 show that the compression phase leads to layers stabilizing on the IB bound. When this study was replicated by BID14 with networks using ReLU BID12 activation function instead of tanh, the compression phase did not happen, and the information planes only showed fitting throughout the whole training process. This behaviour required more detailed study, as a constant increase in mutual information between the network and its input implies increasing memorization, an undesired trait that is linked to overfitting and poor generalization BID11 .Measuring differential mutual information in DNNs is an ill-defined task, as the training process is deterministic BID14 . Mutual information of hidden activity T with input X is: DISPLAYFORM1 If we consider the hidden activity variable T to be deterministic then entropy is: DISPLAYFORM2 However, if T is continuous then the entropy formula is: DISPLAYFORM3 In the case of deterministic DNNs, hidden activity T is a continuous variable and p(T |X) is distributed as the delta function. For the delta function : DISPLAYFORM4 Thus, the true mutual information value I(T, X) is in fact infinite. However, to observe the dynamics of training in terms of mutual information, finite values are needed. The simplest way to avoid trivial infinite mutual information values, is to add noise to hidden activity.Two ways of adding noise have been explored previously by BID16 and BID14 . One way is to add noise Z directly to T and get a noisy variableT = T + Z. Then H(T |X) = H(Z) and mutual information is I(T , X) = H(T ) + H(Z). When the additive noise is Gaussian, the mutual information can be approximated using kernel density estimation (KDE), with an assumption that the noisy variable is distributed as a Gaussian mixture BID9 . The second way to add noise is to discretize the continuous variables into bins. To estimate mutual information , BID16 and BID14 primarily relied on binning hidden activity. The noise comes embedded with the discretization that approximates the probability density function of a random variable. In context of neural networks , adding noise can be done by binning hidden activity and approximating H(T ) as a discrete variable. In this case H(T |X) = 0 since the mapping is deterministic and I(T, X) = H(T ).Generally, when considering mutual information in DNNs, the analyzed values are technically the result of the estimation process and, therefore, are highly sensitive to it. For this reason it is vital to maintain consistency when estimating mutual information. The problem is not as acute when working with DNNs implemented with saturating activation functions, since all hidden activity is bounded. However, with non-saturating functions, and the resulting unbounded hidden activity, the level of noise brought by the estimation procedure has to be proportional and consistent, adapting to the state of every layer of the network at a particular epoch.In the next section adaptive estimation schemes are presented, both for the binning and KDE estimators. It is shown that for networks with unbounded activation functions in their hidden layers, the estimates of information change drastically. Moreover, the adaptive estimators are better able to evaluate different activation functions in a way that allows them to be compared. This approach shows considerable variation in compression for different activation functions. It also shows that L2 regularization leads to more compression and clusters all layers to the same value of mutual information. When compression in hidden layers is quantified with a compression metric and compared with generalization, no significant correlation is observed. However, compression of the last softmax layer is correlated with generalization. In this paper we proposed adaptive approaches to estimating mutual information in the hidden layers of DNNs. These adaptive approaches allowed us to compare behaviour of different activation functions and to observe compression in DNNs with non-saturating activation functions. However, unlike saturating activation functions, compression is not always present and is sensitive to initialization. This may be due to the minimal size of the network architecture that was tested. Experiments with larger convolutional neural networks could be used to explore this possibility.Different non-saturating activation functions compress information at different rates. While saturation plays a role in compression rates, we show that its absence does not imply absence of compression. Even seemingly similar activation functions, such as softplus and centered softplus, gave different compression scores. Compression does not always happen in later stages of training, but can happen from initialization. Further work is needed to understand the other factors contributing to compression.We also found that DNNs implemented with L2 regularization strongly compress information, forcing layers to forget information about the input. The clustering of mutual information to a single point on the information plane has never been reported previously. This result could lay the ground for further research to optimize the regularization to stabilize the layers on the information bottleneck bound to achieve better generalization BID0 , as well as linking information compression to memorization in neural networks BID20 .There are a few limitations to the analysis presented here. Principally , for tractability, the networks we explored were much smaller and more straightforward than many state of the art networks used for practical applications. Furthermore , our methods for computing information, although adaptive for any distribution of network activity, were not rigorously derived. Finally, our compression metric is ad-hoc. However, overall we have three main observations: first, compression is not restricted to saturating activation functions, second, L2 regularization induces compression, and third, generalization accuracy is positively correlated with the degree of compression only in the last layer and is not significantly affected by compression of hidden layers.",We developed robust mutual information estimates for DNNs and used them to observe compression in networks with non-saturating activation functions,two ; firstly ; Secondary ; Lagrangian ; Lagrange ; I(T ; ReLU ; One ; H(T ; Gaussian,two ; firstly ; Secondary ; Lagrangian ; Lagrange ; I(T ; ReLU ; One ; H(T ; Gaussian,"The information bottleneck theory of deep learning proposes that neural networks achieve good generalization by compressing representations to disregard information that is not relevant to the task. However, empirical evidence for this theory is conflicting, as networks with non-saturating activation functions achieved comparable levels of task performance but did not show compression. In this paper, we developed more robust mutual information estimation techniques that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions. The amount of compression varies between different activation functions.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"We describe the use of an automated scheduling system for observation policy design and to schedule operations of the NASA (National Aeronautics and Space Administration) ECOSystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS). We describe the adaptation of the Compressed Large-scale Activity Scheduler and Planner (CLASP) scheduling system to the ECOSTRESS scheduling problem, highlighting multiple use cases for automated scheduling and several challenges for the scheduling technology: handling long-term campaigns with changing information, Mass Storage Unit Ring Buffer operations challenges, and orbit uncertainty. The described scheduling system has been used for operations of the ECOSTRESS instrument since its nominal operations start July 2018 and is expected to operate until mission end in Summer 2019. NASA's ECOSTRESS mission (NASA 2019) seeks to better understand how much water plants need and how they respond to stress. Two processes show how plants use water: transpiration and evaporation. Transpiration is the process of plants losing water through tiny pores in their leaves. Evaporation of water from the soil surrounding plants affects how much water the plants can use. ECOSTRESS measures the temperature of plants to understand combined evaporation and transpiration, known as evapotranspiration.ECOSTRESS launched on June 29, 2018 to the ISS (International Space Station) on a Space-X Falcon 9 rocket as part of a resupply mission. The instrument is attached to the Japanese Experiment Module Exposed Facility (JEM-EF) on the ISS and targets key biomes on the Earth's surface, as well as calibration/validation sites. Other science targets include cities and volcanoes. From the orbit of the Space Station FIG1 ), the instrument can see target regions at varying times throughout the day, rather than at a fixed time of day, allowing scientists to understand plant water use throughout the day.The instrument used for ECOSTRESS is a thermal infrared radiometer. A double-sided scan mirror, rotating at a constant 25.4 rpm, allows the telescope to view a 53°-wide nadir cross-track swath with one scan per 1.18 seconds. The nominal observation unit is a scene, made up of 44 scans, and takes roughly 52 seconds to acquire. For simplification of operations, we consider that ECOSTRESS scenes are 52 seconds long. About 1000 scenes may be acquired in a given week. FIG2 shows a set of planned observations over North America. Each square represents one 52-second long scene.CLASP BID4 was initially used prelaunch as a tool to analyze the addition of a new science campaign. CLASP was then used for operations to generate command sequences for the instrument. The command sequences are translated from the observation schedule generated by CLASP, and include other time and location dependent instrument actions besides observations, such as hardware power cycles through high radiation environments. Each mission comes with its own set of challenges, and there were three specifically that required adaptations to CLASP as follows.• ECOSTRESS has a long-term science campaign that we need to satisfy. From week to week, the orbital ephemeris can change, and thus the schedule needs to be updated each week. We need to be able to account for previously executed observations when scheduling for the future.• An issue with the instrument Mass Storage Unit (MSU) was discovered, and rather than performing an instrument firmware update, we proposed a ground-based solution that accounts for this additional complexity in the data modeling in the schedule.• The uncertainty in the orbital ephemeris (predictions of In the remainder of this paper, we describe these operational challenges and how we addressed them successfully. We also validate our methods used through computational analysis. This paper has described the use of an automated scheduling system in the analysis and operations for the ECOSTRESS mission. Changing orbital ephemeris and long-term campaign goals required adapting CLASP to consider past observations in scheduling for the future. The issue with the instrument ring buffer required scheduling with additional constraints, as well as scheduling another type of instrument activity. The uncertainty of the ISS orbital position required adapting how observations are scheduled. Through computational analysis we showed that our method for addressing the ring buffer approached the performance of schedules produced that did not have the added constraints, and that the second method of building observations up rather outperformed the method of adding a fixed amount of observational time to ensure no regions of interest were missed.",We describe the use of an automated scheduling system for observation policy design and to schedule operations of NASA's ECOSTRESS mission.,NASA ; National Aeronautics and Space Administration ; Activity Scheduler ; Planner ; ECOSTRESS ; Mass Storage Unit Ring Buffer ; Two ; Japanese ; ISS ; Earth,NASA ; National Aeronautics and Space Administration ; Activity Scheduler ; Planner ; ECOSTRESS ; Mass Storage Unit Ring Buffer ; Two ; Japanese ; ISS ; Earth,"NASA's ECOSystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) uses an automated scheduling system for observation policy design and to schedule operations. The proposed scheduling system is based on the Compressed Large-scale Activity Scheduler and Planner (CLASP) scheduling system, highlighting multiple use cases for automated scheduling and challenges for the scheduling technology. The instrument is attached to the Japanese Experiment Module Exposed Facility (JEM-EF) on the ISS and targets key biomes on the Earth's surface, as well as calibration/validation sites.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"Foveation is an important part of human vision, and a number of deep networks have also used foveation. However, there have been few systematic comparisons between foveating and non-foveating deep networks, and between different variable-resolution downsampling methods. Here we define several such methods, and compare their performance on ImageNet recognition with a Densenet-121 network. The best variable-resolution method slightly outperforms uniform downsampling. Thus in our experiments, foveation does not substantially help or hinder object recognition in deep networks.",We compare object recognition performance on images that are downsampled uniformly and with three different foveation schemes.,ImageNet,ImageNet ; Foveation ; an important part ; human vision ; a number ; deep networks ; few systematic comparisons ; foveating deep networks ; different variable-resolution downsampling methods ; we,"Foveation is an important part of human vision, and a number of deep networks have also used foveation. However, there have been few systematic comparisons between foveating and non-foveating deep networks, and between different variable-resolution downsampling methods. We define several such methods, and compare their performance on ImageNet recognition with a Densenet-121 network. The best variable-resolution method slightly outperforms uniform downsampling.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"We propose a framework to model the distribution of sequential data coming from
 a set of entities connected in a graph with a known topology. The method is
 based on a mixture of shared hidden Markov models (HMMs), which are trained
 in order to exploit the knowledge of the graph structure and in such a way that the
 obtained mixtures tend to be sparse. Experiments in different application domains
 demonstrate the effectiveness and versatility of the method. Hidden Markov models (HMMs) are a ubiquitous tool for modelling sequential data. They started by being applied to speech recognition systems and from there they have spread to almost any application one can think of, encompassing computational molecular biology, data compression, and computer vision. In the emerging field of cognitive radars BID12 , for the task of opportunistic usage of the spectrum, HMMs have been recently used to model the occupancy of the channels by primary users BID21 .When the expressiveness of an HMM is not enough, mixtures of HMM have been adopted. Roughly speaking, mixtures of HMMs can be interpreted as the result of the combination of a set of independent standard HMMs which are observed through a memoryless transformation BID5 BID8 BID22 BID13 .In many real-life settings one does not have a single data stream but an arbitrary number of network connected entities that share and interact in the same medium and generate data streams in real-time. The streams produced by each of these entities form a set of time series with both intra and inter relations between them. In neuroimaging studies, the brain can be regarded as a network: a connected system where nodes, or units, represent different specialized regions and links, or connections, represent communication pathways. From a functional perspective, communication is coded by temporal dependence between the activities of different brain areas BID6 . Also team sports intrinsically involve fast, complex and interdependent events among a set of entities (the players), which interact as a team BID24 BID23 . Thus, understanding a player's behavior implies understanding the behavior of his teammates and opponents over time.The extraction of knowledge from these streams to support the decision-making process is still challenging and the adaptation of HMM to this scenario is immature at best. BID9 proposed a hybrid approach combining the Self-Organizing Map (SOM) and the HMM with applications in clustering, dimensionality reduction and visualization of large-scale sequence spaces. Note that the model at each node is limited to a simple HMM. Wireless local area networks have also been modeled with Markov-based approaches. For instance, BID1 use HMMs for outlier detection in 802.11 wireless access points. However, the typical approaches include a common HMM model for all nodes (with strong limited flexibility) and a HMM model per node, independent of the others (not exploring the dependencies between nodes). BID4 built a sparse coupled hidden Markov model (SCHMM) framework to parameterize the temporal evolution of data acquired with functional magnetic resonance imaging (fMRI). The coupling is captured in the transition matrix, which is assumed to be a function of the activity levels of all the streams; the model per node is still restricted to a simple HMM.In general, in networked data streams, the stream observed in each sensor is often modeled by HMMs but the intercorrelations between sensors are seldom explored. The proper modeling of the intercorrelations has the potential to improve the learning process, acting as a regularizer in the learning process. In here we propose to tackle this void, by proposing as observation model at each node a sparse mixture of HMMs, where the dependencies between nodes are used to promote the sharing of HMM components between similar nodes. In this work we propose a method to model the generative distribution of sequential data coming from nodes connected in a graph with a known fixed topology. The method is based on a mixture of HMMs where its coefficients are regularized during the learning process in such a way that affine nodes will tend to have similar coefficients, exploiting the known graph structure. We also prove that the proposed regularizer promotes sparsity in the mixtures, which is achieved through a fully differentiable loss function (i.e. with no explicit L 0 penalty term). We evaluate the method's performance in two completely different tasks (anomaly detection in Wi-Fi networks and human motion forecasting), showing its effectiveness and versatility.For future work, we plan to extend/evaluate the usage of SpaMHMM for sequence clustering. This is an obvious extension that we did not explore thoroughly in this work, since its main focus was modeling the generative distribution of data. In this context, extending the idea behind SpaMHMM to mixtures of more powerful generative distributions is also in our plans. As is known, HMMs have limited expressiveness due to the strong independence assumptions they rely on. Thus, we plan to extend these ideas to develop an architecture based on more flexible generative models for sequence modeling, like those attained using deep recurrent architectures. After building the usual variational lower bound for the log-likelihood and performing the E-step, we get the following well-known objective:Jpθ, θ -q "" ÿ z,H ppX, z, H|y, θ -q log ppX, z, H|y, θq,which we want to maximize with respect to θ and where θ -are the model parameters that were kept fixed in the E-step. Some of the parameters in the model are constrained to represent valid probabilities, yielding the following Lagrangian: DISPLAYFORM0 Clearly, J r pθq´V r pθ, qq "" 1 Nˆl og ppX|y, θq´E z,H""q "" log ppX, z, H|y, θq qpz, Hq",A method to model the generative distribution of sequences coming from graph connected entities.,Markov ; Hidden Markov ; HMM ; the Self-Organizing Map ; SCHMM ; HMM.In ; two ; SpaMHMM ; H|y ; log ppX,Markov ; Hidden Markov ; HMM ; the Self-Organizing Map ; SCHMM ; HMM.In ; two ; SpaMHMM ; H|y ; log ppX,"The proposed framework is based on a mixture of shared hidden Markov models (HMMs), which are trained to exploit the knowledge of the graph structure and tend to be sparse. Experiments in different application domains demonstrate the effectiveness and versatility of the method. Hidden Markov models (HMMs) are a ubiquitous tool for modelling sequential data. They started with speech recognition systems and have spread to almost any application. In neuroimaging studies, the brain can be regarded as a network, where nodes represent different specialized regions and links represent communication pathways.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"It is challenging to disentangle an object into two orthogonal spaces of content and style since each can influence the visual observation in a different and unpredictable way. It is rare for one to have access to a large number of data to help separate the influences. In this paper, we present a novel framework to learn this disentangled representation in a completely unsupervised manner. We address this problem in a two-branch Autoencoder framework. For the structural content branch, we project the latent factor into a soft structured point tensor and constrain it with losses derived from prior knowledge. This encourages the branch to distill geometry information. Another branch learns the complementary style information. The two branches form an effective framework that can disentangle object's content-style representation without any human annotation. We evaluate our approach on four image datasets, on which we demonstrate the superior disentanglement and visual analogy quality both in synthesized and real-world data. We are able to generate photo-realistic images with 256x256 resolution that are clearly disentangled in content and style. Content and style are the two most inherent attributes that characterize an object visually. Computer vision researchers have devoted decades of efforts to understand object shape and extract features that are invariant to geometry change BID11 BID33 BID36 BID26 . Learning such disentangled deep representation for visual objects is an important topic in deep learning.The main objective of our work is to disentangle object's style and content in an unsupervised manner. Achieving this goal is non-trivial due to three reasons: 1) Without supervision, we can hardly guarantee the separation of different representations in the latent space. 2) Although some methods like InfoGAN are capable of learning several groups of independent attributes from objects, attributes from these unsupervised frameworks are uninterpretable since we cannot pinpoint which portion of the disentangled representation is related to the content and which to the style. 3) Learning structural content from a set of natural real-world images is difficult.To overcome the aforementioned challenges, we propose a novel two-branch Autoencoder framework, of which the structural content branch aims to discover semantically meaningful structural points (i.e., y in Fig 2) to represent the object geometry, while the other style branch learns the complementary style representation. The settings of these two branches are asymmetric. For the structural content branch, we add a layer-wise softmax operator to the last layer. We could regard this as a projection of a latent content to a soft structured point tensor space. Specifically designed prior losses are used to constrain the structured point tensors so that the discovered points have high repeatability across images yet distributed uniformly to cover different parts of the object. To encourage the framework to learn a disentangled yet complementary representation of both content and style, we further introduce a Kullback-Leibler (KL) divergence loss and skip-connections design to the framework. In FIG0 , we show the latent space walking results on cat face dataset, which demonstrates a reasonable coverage of the manifold and an effective disentanglement of the content and style space of our approach.Extensive experiments show the effectiveness of the proposed method in disentangling the content and style of natural images. We also conduct qualitative and quantitative experiments on MNISTColor, 3D synthesized data and several real-world datasets which demonstrate the superior performance of our method to state-of-the-art algorithms. We propose a novel model based on Autoencoder framework to disentangle object's representation by content and style. Our framework is able to mine structural content from a kind of objects and learn content-invariant style representation simultaneously, without any annotation. Our work may also reveal several potential topics for future research: 1) Instead of relying on supervision, using strong prior to restrict the latent variables seems to be a potential and effective tool for disentangling. 2) In this work we only experiment on near-rigid objects like chairs and faces, learning on deformable objects is still an opening problem.3) The content-invariant style representation may have some potentials on recognition tasks.",We present a novel framework to learn the disentangled representation of content and style in a completely unsupervised manner.,two ; four ; decades ; three ; Fig ; Kullback-Leibler ; KL ; MNISTColor,two ; four ; decades ; three ; Fig ; Kullback-Leibler ; KL ; MNISTColor ; It ; an object,"In order to disentangle an object into two orthogonal spaces of content and style, it is challenging to learn disentangled representations in unsupervised manner. In a two-branch Autoencoder framework, we project the latent factor into a soft structured point tensor and constrain it with losses derived from prior knowledge. The two branches form an effective framework that can disentangle object's content-style representation without any human annotation. We are able to generate photo-realistic images with 256x256 resolution that are clearly disentangled in content and style.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"Knowledge Bases (KBs) are becoming increasingly large, sparse and probabilistic. These KBs are typically used to perform query inferences and rule mining. But their efficacy is only as high as their completeness. Efficiently utilizing incomplete KBs remains a major challenge as the current KB completion techniques either do not take into account the inherent uncertainty associated with each KB tuple or do not scale to large KBs.

 Probabilistic rule learning not only considers the probability of every KB tuple but also tackles the problem of KB completion in an explainable way. For any given probabilistic KB, it learns probabilistic first-order rules from its relations to identify interesting patterns. But, the current probabilistic rule learning techniques perform grounding to do probabilistic inference for evaluation of candidate rules. It does not scale well to large KBs as the time complexity of inference using grounding is exponential over the size of the KB. In this paper, we present SafeLearner -- a scalable solution to probabilistic KB completion that performs probabilistic rule learning using lifted probabilistic inference -- as faster approach instead of grounding. 

 We compared SafeLearner to the state-of-the-art probabilistic rule learner ProbFOIL+ and to its deterministic contemporary AMIE+ on standard probabilistic KBs of NELL (Never-Ending Language Learner) and Yago. Our results demonstrate that SafeLearner scales as good as AMIE+ when learning simple rules and is also significantly faster than ProbFOIL+. There is an increasing tendency to construct knowledge bases and knowledge graphs by machine learning methods. As a result, knowledge bases are often incomplete and also uncertain. To cope with uncertainty, one often resorts to probabilistic databases and logics Suciu, 2017, De Raedt et al., 2016] , which take into account the probability of the tuples in the querying process. The most widely used probabilistic database semantics is based on the tuple-independent probabilistic databases model, which assumes that every tuple in every table of the database is independent of one another To cope with incomplete knowledge bases, various researchers have used machine learning techniques to learn a set of rules that can be used to infer new tuples from the existing ones, thereby completing the knowledge base BID2 . This traditional relational rule learning setting BID18 has been extended to probabilistic logics and databases by De Raedt et al. [2015] . However, the ProbFOIL approach of De Raedt et al. suffers from one key limitation: It does not scale well to large databases due to the grounding step, which results in an intractable probabilistic inference problem. The key contribution of this paper is the introduction of the SafeLearner system which performs two major tasks. 1) It uses lifted inference to avoid the grounding step and to improve scaling.2) It enhances a highly efficient rule generation system, AMIE+ BID11 ] to obtain deterministic candidate rules which are then made probabilistic using lifted inference.This paper is organized as follows. We introduce the background for this paper in Section 2. We define, in Section 3, the problem of learning a set of probabilistic rules. Sections 4 and 5 outline the idea behind the working of SafeLearner. Section 6 proposes the algorithm for SafeLearner. In Section 7, we present an experimental evaluation in the context of the NELL knowledge base BID2 ]. An overview of related work can be found in Section 8. Section 9 discusses future research directions and concludes. The work presented in this paper advances the works [De Raedt et al., 2015, Dylla and BID9 that also studied learning in the probabilistic database setting. 7 But compared with these previous works, we rely on lifted inference, which allows our approach to scale to much larger databases. Both of the previous approaches only use tuples from a given training set but do not take into account the behavior of the model on tuples not in the training set. This is problematic because, unless the training set is really large, these previous methods do not distinguish models that predict too many false positives (i.e. models that give too high probability to too many tuples outside the training set). This becomes an issue especially in sparse domains (and most real domains are indeed sparse). Our work is also closely related to the literature on learning from knowledge bases such as NELL within statistical relational learning (SRL), including works that use Markov logic networks BID20 , Bayesian logic programs BID19 and stochastic logic programs BID17 BID23 . A disadvantage of many of these methods is that the learned parameters of the models can not be interpreted easily, which is particularly an issue for Markov logic networks where the weight of a rule cannot be understood in isolation from the rest of the rules. In contrast, the learned weights of probabilistic rules in our work, and also in the other works relying on probabilistic databases [De Raedt et al., 2015, Dylla and BID9 , have a clear probabilistic interpretation.Parameter Learning with Different Losses Cross entropy is not the only loss function that may be considered for learning the parameters of probabilistic rules. Here, we discuss two additional loss functions that have already been used for the same or similar tasks in the literature: squared loss BID9 and a probabilistic extension of accuracy . Whereas cross entropy and squared loss belong among so-called proper scoring rules BID12 and, thus, reward estimates of probabilities that match the true probability, this is not the case for probabilistic accuracy. Moreover, each of these functions also relies on additional assumptions such as mutual independence of the examples' probabilities as well as mutual independence of the predictions, although this is not mentioned explicitly in the respective works BID9 Theobald, 2016, De Raedt et al., 2015] . Below, we briefly discuss squared loss and probabilistic accuracy.Squared Loss (Brier Score) As before, let p i denote the probability of the i-th example and q i the probability of the respective prediction. Then, the squared loss, which is a proper scoring rule, is: DISPLAYFORM0 2 which was among others used in BID9 for learning probabilities of tuples in PDBs. define probabilistic extension of accuracy and other measures of predictive quality such as precision and recall. Their version of probabilistic accuracy is Acc prob = 1− 1 |E| t i ,p i ∈E |p i − q i | . Unlike the other two discussed loss functions, probabilistic accuracy is not a proper scoring rule as the next example illus-7. Strictly speaking, the work was framed within the probabilistic logic programming setting. However, probabilistic logic programming systems, such as Problog BID10 , can be seen as generalizations of probabilistic databases. We proposed a probabilistic rule learning system, named SafeLearner, that supports lifted inference. It first performs structure learning by mining independent deterministic candidate rules using AMIE+ and later executes joint parameter learning over all the rule probabilities. SafeLearner extends ProbFOIL + by using lifted probabilistic inference (instead of using grounding). Therefore, it scales better than ProbFOIL + . In comparison with AMIE+, it is able to jointly learn probabilistic rules over a probabilistic KB unlike AMIE+ which only learns independent deterministic rules (with confidences) over a deterministic KB. We experimentally show that SafeLearner scales as good as AMIE+ when learning simple rules. Trying to learn complex rules leads to unsafe queries which are not suitable for lifted inference. But lifted inference helps SafeLearner in outperforming ProbFOIL + which does not scale to NELL Sports Database without the help of a declarative bias. A few limitations of SafeLearner are as follows: 1) It cannot learn complex rules that translate to an unsafe query. 2) It cannot use rules within the background theory. 3) It cannot learn rules on P DB with numeric data (without assuming them as discrete constants).The main contributions of SafeLearner are presented as follows. Firstly , it accomplishes probabilistic rule learning using a novel inference setting as it is the first approach that uses lifted inference for KB completion. Secondly , unlike ProbFOIL + , SafeLearner scales well on the full database of NELL with 233,000 tuples and 426 relations as well as on the standard subset of Yago 2.4 with 948,000 tuples and 33 relations. Thirdly , SafeLearner is faster than ProbFOIL + because of the following three factors: 1) it disintegrates longer complex queries to smaller simpler ones, 2) it caches the structure of queries before doing inference and 3) it uses lifted inference to infer on those simple queries. The first two factors of query disintegration and memoization are discussed in Appendix D in further detail.In future, this work could be advanced further to eliminate its shortcomings. In particular , a prominent direction of advancement would be to extend probabilistic rule learning to open-world setting of which the Lif t O R algorithm BID3 is capable.",Probabilistic Rule Learning system using Lifted Inference,KB ; first ; SafeLearner ; NELL ; Never-Ending Language Learner ; one ; De Raedt ; al. ; two ; Dylla,KB ; first ; SafeLearner ; NELL ; Never-Ending Language Learner ; one ; De Raedt ; al. ; two ; Dylla,"Knowledge bases (KBs) are becoming increasingly large, sparse and probabilistic. Their efficacy is only as high as their completeness. However, the current KB completion techniques do not take into account the inherent uncertainty associated with each KB tuple or do not scale well to large KBs. SafeLearner is a scalable solution to probabilistic KB completion that performs probabilistic rule learning using lifted probabilistic inference. The time complexity of inference using grounding is exponential over the size of the KB.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"Noisy labels are very common in real-world training data, which lead to poor generalization on test data because of overfitting to the noisy labels. In this paper, we claim that such overfitting can be avoided by ""early stopping"" training a deep neural network before the noisy labels are severely memorized. Then, we resume training the early stopped network using a ""maximal safe set,"" which maintains a collection of almost certainly true-labeled samples at each epoch since the early stop point. Putting them all together, our novel two-phase training method, called Prestopping, realizes noise-free training under any type of label noise for practical use. Extensive experiments using four image benchmark data sets verify that our method significantly outperforms four state-of-the-art methods in test error by 0.4–8.2 percent points under existence of real-world noise. By virtue of massive labeled data, deep neural networks (DNNs) have achieved a remarkable success in numerous machine learning tasks, such as image classification (Krizhevsky et al., 2012) and object detection (Redmon et al., 2016) . However, owing to their high capacity to memorize any label noise, the generalization performance of DNNs drastically falls down when noisy labels are contained in the training data (Jiang et al., 2018; Han et al., 2018; Song et al., 2019) . In particular, Zhang et al. (2017) have shown that a standard convolutional neural network (CNN) can easily fit the entire training data with any ratio of noisy labels and eventually leads to very poor generalization on the test data. Thus, it is challenging to train a DNN robustly even when noisy labels exist in the training data. A popular approach to dealing with noisy labels is ""sample selection"" that selects true-labeled samples from the noisy training data (Jiang et al., 2018; Ren et al., 2018; Han et al., 2018; Yu et al., 2019; Song et al., 2019) . Here, (1−τ )×100% of small-loss training samples are treated as true-labeled ones and then used to update a DNN robustly, where τ ∈ [0, 1] is a noise rate. This loss-based separation is well known to be justified by the memorization effect (Arpit et al., 2017) that DNNs tend to learn easy patterns first and then gradually memorize all samples. In practice, Han et al. (2018) empirically proved that training on such small-loss samples yields a much better generalization performance under artificial noise scenarios. Despite its great success, Song et al. (2019) have recently argued that the performance of the lossbased separation becomes considerably worse depending on the type of label noise. For instance, Figure 1: Loss distributions at the training accuracy of 50% using DenseNet (L=40, k=12): (a) and (b) show those on CIFAR-100 with two types of synthetic noises of 40%, where ""symmetric noise"" flips a true label into other labels with equal probability, and ""pair noise"" flips a true label into a specific false label; (c) shows those on FOOD-101N (Lee et al., 2018) with real-world noise of 18.4%. ) show how many true-labeled and false-labeled samples are memorized when training DenseNet (L=40, k=12) 1 on CIFAR-100 with two types of synthetic noises of 40%. ""Default"" is a standard training method, and ""Prestopping"" is our proposed one; (c) contrasts the convergence of test error between the two methods. in symmetric noise (Figure 1(a ) ), the loss-based approach well separates true-labeled samples from false-labeled ones because many true-labeled ones exhibit smaller loss than false-labeled ones. On the other hand, in pair and real-world noises (Figures 1(b) and 1(c)) , many false-labeled samples are misclassified as true-labeled ones because the two distributions overlap closely; this overlap confirms that the loss-based separation still accumulates severe label noise from many misclassified cases, especially in real-world noise or pair noise which is regarded as more realistic than symmetric noise (Ren et al., 2018; Yu et al., 2019) . This limitation definitely calls for a new approach that supports any type of label noise for practical use. In this regard, as shown in Figure 2 (a), we thoroughly investigated the memorization effect of a DNN on the two types of noises and found two interesting properties as follows: • A noise type affects the memorization rate for false-labeled samples: The memorization rate for false-labeled samples is faster with pair noise than with symmetric noise. That is, the red portion in Figure 2 (a) starts to appear earlier in pair noise than in symmetric noise. This observation supports the significant overlap of true-labeled and false-labeled samples in Figure 1(b ) . Thus , the loss-based separation performs well only if the false-labeled samples are scarcely learned at an early stage of training, as in symmetric noise. • There is a period where the network accumulates the label noise severely: Regardless of the noise type, the memorization of false-labeled samples significantly increases at a late stage of training. That is, the red portion in Figure 2 (a) increases rapidly after the dashed line, in which we call the error-prone period. (See Section 3.2.1 for the details of estimating the error-prone period). We note that the training in that period brings no benefit. The generalization performance of ""Default"" deteriorates sharply, as shown in Figure 2 (c). Based on these findings, we contend that eliminating this error-prone period should make a profound impact on robust optimization. In this paper, we propose a novel approach, called Prestopping, that achieves noise-free training based on the early stopping mechanism. Because there is no benefit from the error-prone period, Prestopping early stops training before that period begins. This early stopping effectively prevents a network from overfitting to false-labeled samples, and the samples memorized until that point are added to a maximal safe set because they are true-labeled (i.e., blue in Figure 2 (a)) with high precision. Then, Prestopping resumes training the early stopped network only using the maximal safe set in support of noise-free training. Notably, our proposed merger of ""early stopping"" and ""learning from the maximal safe set"" indeed eliminates the error-prone period from the training process, as shown in Figure 2 (b). As a result, the generalization performance of a DNN remarkably improves in both noise types, as shown in Figure 2 (c). To validate the superiority of Prestopping, DenseNet (Huang et al., 2017) and VGG-19 (Simonyan & Zisserman, 2015) were trained on both simulated and real-world noisy data sets, including CIFAR-10, CIFAR-100, ANIMAL-10N, and Food-101N. Compared with four state-of-the-art methods, Prestopping significantly improved test error by up to 18.1pp 2 in a wide range of noise rates. In this paper, we proposed a novel two-phase training strategy for the noisy training data, which we call Prestopping. The first phase, ""early stopping,"" retrieves an initial set of true-labeled samples as many as possible, and the second phase, ""learning from a maximal safe set,"" completes the rest training process only using the true-labeled samples with high precision. Prestopping can be easily applied to many real-world cases because it additionally requires only either a small clean validation set or a noise rate. Furthermore, we combined this novel strategy with sample refurbishment to develop Prestopping+. Through extensive experiments using various real-world and simulated noisy data sets, we verified that either Prestopping or Prestopping+ achieved the lowest test error among the seven compared methods, thus significantly improving the robustness to diverse types of label noise. Overall, we believe that our work of dividing the training process into two phases by early stopping is a new direction for robust training and can trigger a lot of subsequent studies. A Prestopping WITH NOISE-RATE HEURISTIC Algorithm 2 describes the overall procedure of Prestopping with the noise-rate heuristic, which is also self-explanatory. Compared with Algorithm 1, only the way of determining the best stop point in Lines 7-9 has changed. Algorithm 2 Prestopping with Noise-Rate Heuristic INPUT:D: data, epochs: total number of epochs, q: history length, τ : noise rate OUTPUT: θ t : network parameters 1: t ← 1; θ t ← Initialize the network parameter; 2: θ tstop ← ∅; /* The parameter of the stopped network */ 3: for i = 1 to epochs do /* Phase I: Learning from a noisy training data set */ 4: Draw a mini-batch B t fromD; 6: θ tstop ← θ t ; break; 10: t ← t + 1; 11: θ t ← θ tstop ; /* Load the network stopped at t stop */ 12: for i = stop_epoch to epochs do /* Phase II: Learning from a maximal safe set */ 13: Draw a mini-batch B t fromD;","We propose a novel two-phase training approach based on ""early stopping"" for robust training on noisy labels.",two ; four ; Krizhevsky et al. ; Redmon ; al. ; noisy labels ; Jiang et al. ; Han ; Song ; Zhang et al,two ; four ; Krizhevsky et al. ; Redmon ; al. ; noisy labels ; Jiang et al. ; Han ; Song ; Zhang et al,"Noisy labels are common in real-world training data, which lead to poor generalization on test data due to overfitting to the noisy labels. In this paper, we claim that overfitting can be avoided by ""early stopping"" training a deep neural network before the noisy labels are severely memorized. Then, we resume training the early stopped network using a ""maximal safe set,"" which maintains a collection of almost certainly true-labeled samples at each epoch since the early stop point.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"In this paper we study image captioning as a conditional GAN training, proposing both a context-aware LSTM captioner and co-attentive discriminator, which enforces semantic alignment between images and captions. We investigate the viability of two discrete GAN training methods: Self-critical Sequence Training (SCST) and Gumbel Straight-Through (ST) and demonstrate that SCST shows more stable gradient behavior and improved results over Gumbel ST. Significant progress has been made on the task of generating image descriptions using neural image captioning. Early systems were traditionally trained using cross-entropy (CE) loss minimization BID6 BID16 . Later, reinforcement learning techniques BID13 BID9 based on policy gradient methods were introduced to directly optimize metrics such as CIDEr or SPICE BID0 . Along a similar idea, BID14 introduced Self-critical Sequence Training (SCST), a light-weight variant of REINFORCE, which produced state of the art image captioning results using CIDEr as an optimization metric. To address the problem of sentence diversity and naturalness, image captioning has been explored in the framework of GANs. However, due to the discrete nature of text generation, GAN training remains challenging and has been generally tackled either with reinforcement learning techniques BID4 BID12 BID2 or by using Gumbel softmax relaxation BID5 , as in BID15 BID7 .Despite impressive advances, image captioning is far from being a solved task. It remains a challenge to satisfactorily bridge the semantic gap between image and captions to produce diverse, creative, and ""human-like"" captions. Although applying GANs to image captioning for promoting human-like captions is a very promising direction, the discrete nature of the text generation process makes it challenging to train such systems. The recent work of BID1 showed that the task of text generation for current discrete GAN models is difficult, often producing unsatisfactory results, and requires therefore new approaches and methods.In this paper, we propose a novel GAN-based framework for image captioning that enables better language composition, more accurate compositional alignment of image and text, and light-weight efficient training of discrete sequence GAN based on SCST. In summary, we demonstrated that SCST training for discrete GAN is a promissing new approach that outperforms the Gumbel relaxation in terms of training stability and the overall performance. Moreover, we showed that our context-aware attention gives larger gains as compared to the adaptive sentinel or the traditional visual attention. Finally, our co-attention model for discriminator compares favorably against the joint embedding architecture.","Image captioning as a conditional GAN training with novel architectures, also study two discrete GAN training methods.",GAN ; two ; Sequence Training ; Gumbel Straight-Through ; SCST ; Gumbel ST ; CE ; REINFORCE ; Gumbel,GAN ; two ; Sequence Training ; Gumbel Straight-Through ; SCST ; Gumbel ST ; CE ; REINFORCE ; Gumbel ; this paper,"In this paper, we study image captioning as a conditional GAN training, proposing both a context-aware LSTM captioner and co-attentive discriminator, which enforces semantic alignment between images and captions. We investigate the viability of two discrete GAN training methods: Self-critical Sequence Training (SCST) and Gumbel Straight-Through (ST), and demonstrate that SCST shows more stable gradient behavior and improved results over Gumbel ST. Image captioning has been explored in the framework of reinforcement learning techniques BID4 BID12 B",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"While Bayesian optimization (BO) has achieved great success in optimizing expensive-to-evaluate black-box functions, especially tuning hyperparameters of neural networks, methods such as random search (Li et al., 2016) and multi-fidelity BO (e.g. Klein et al. (2017)) that exploit cheap approximations, e.g. training on a smaller training data or with fewer iterations, can outperform standard BO approaches that use only full-fidelity observations. In this paper, we propose a novel Bayesian optimization algorithm, the continuous-fidelity knowledge gradient (cfKG) method, that can be used when fidelity is controlled by one or more continuous settings such as training data size and the number of training iterations. cfKG characterizes the value of the information gained by sampling a point at a given fidelity, choosing to sample at the point and fidelity with the largest value per unit cost. Furthermore, cfKG can be generalized, following Wu et al. (2017), to settings where derivatives are available in the optimization process, e.g. large-scale kernel learning, and where more than one point can be evaluated simultaneously. Numerical experiments show that cfKG outperforms state-of-art algorithms when optimizing synthetic functions, tuning convolutional neural networks (CNNs) on CIFAR-10 and SVHN, and in large-scale kernel learning. In hyperparameter tuning of machine learning models, we seek to find a set of hyperparameters x in some set A to minimize the validation error f (x), i.e., to solve min x∈A f (x) (1.1)Evaluating f (x) can take substantial time and computational power BID0 , and may not provide gradient evaluations. Thus, machine learning practitioners have turned to Bayesian optimization for solving (1.1) BID19 because it tends to find good solutions with few function evaluations BID6 .As the computational expense of training and testing a modern deep neural network for a single set of hyperparameters has grown as long as days or weeks, it has become natural to seek ways to solve (1.1) more quickly by supplanting some evaluations of f (x) with computationally inexpensive lowfidelity approximations. Indeed , when training a neural network or most other machine learning models, we can approximate f (x) by training on less than the full training data, or using fewer training iterations. Both of these controls on fidelity can be set to achieve either better accuracy or lower computational cost across a range of values reasonably modeled as continuous.In this paper, we consider optimization with evaluations of multiple fidelities and costs where the fidelity is controlled by one or more continuous parameters. We model these evaluations by a realvalued function g(x, s) where f (x) := g(x, 1 m ) and s ∈ [0, 1] m denotes the m fidelity-control parameters. g(x, s) can be evaluated, optionally with noise, at a cost that depends on x and s. In the context of hyperparameter tuning, we may take m = 2 and let g(x, s 1 , s 2 ) denote the loss on the validation set when training using hyperparameters x with a fraction s 1 of the training data and a fraction s 2 of some maximum allowed number of training iterations. We may also set m = 1 and let s index either training data or training iterations. We assume A is a compact connected uncountable set into which it is easy to project, such as a hyperrectangle.This problem setting also appears outside of hyperparameter tuning, in any application where the objective is expensive to evaluate and we may observe cheap low-fidelity approximations parameterized by a continuous vector. For example , when optimizing a system evaluated via a Monte Carlo simulator, we can evaluate a system configuration approximately by running with fewer replications. Also, when optimizing an engineering system modeled by a partial differential equation (PDE), we can evaluate a system configuration approximately by solving the PDE using a coarse grid.Given this problem setting, we use the knowledge gradient approach BID3 to design an algorithm to adaptively select the hyperparameter configuration and fidelity to evaluate, to best support solving (1.1). By generalizing a computational technique based on the envelope theorem first developed in Wu et al. (2017) , our algorithm supports parallel function evaluations, and also can take advantage of derivative observations when they are available. This algorithm chooses the point or set of points to evaluate next that maximizes the ratio of the value of information from evaluation against its cost.Unlike most existing work on discrete-and continuous-fidelity Bayesian optimization, our approach considers the impact of our measurement on the future posterior distribution over the full feasible domain, while existing expected-improvement-based approaches consider its impact at only the point evaluated. One exception is the entropy-search-based method [10] , which also considers the impact over the full posterior. Our approach differs from entropy search in that it chooses points to sample to directly minimize expected simple regret, while entropy search seeks to minimize the entropy of the location or value of the global optimizer, indirectly reducing simple regret.We summarize our contributions as follows. We propose a novel continuous-fidelity BO algorithm, cfKG, which generalizes naturally to batch and derivative settings. This algorithm can find good solutions to global optimization problems with less cost than state-of-art algorithms in applications including deep learning and kernel learning.",We propose a Bayes-optimal Bayesian optimization algorithm for hyperparameter tuning by exploiting cheap approximations.,Bayesian ; BO ; al. ; Klein ; fidelity ; one ; Wu et al. ; more than one ; SVHN ; x∈A,Bayesian ; BO ; al. ; Klein ; fidelity ; one ; Wu et al. ; more than one ; SVHN ; x∈A,"Bayesian optimization (BO) has achieved great success in optimizing expensive-to-evaluate black-box functions, especially tuning hyperparameters of neural networks, but methods such as random search and multi-fidelity BO exploit cheap approximations can outperform standard BO approaches that use only full-fidelity observations. The continuous-fidelity knowledge gradient (cfKG) method characterizes the value of information gained by sampling a point at a given fidelity, choosing to sample at the point and fidelity with the largest value per unit cost, and can be generalized to large-scale kernel learning.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"We introduce an approach for augmenting model-free deep reinforcement learning agents with a mechanism for relational reasoning over structured representations, which improves performance, learning efficiency, generalization, and interpretability. Our architecture encodes an image as a set of vectors, and applies an iterative message-passing procedure to discover and reason about relevant entities and relations in a scene. In six of seven StarCraft II Learning Environment mini-games, our agent achieved state-of-the-art performance, and surpassed human grandmaster-level on four. In a novel navigation and planning task, our agent's performance and learning efficiency far exceeded non-relational baselines, it was able to generalize to more complex scenes than it had experienced during training. Moreover, when we examined its learned internal representations, they reflected important structure about the problem and the agent's intentions. The main contribution of this work is to introduce techniques for representing and reasoning about states in model-free deep reinforcement learning agents via relational inductive biases. Our experiments show this approach can offer advantages in efficiency, generalization, and interpretability, and can scale up to meet some of the most challenging test environments in modern artificial intelligence. Recent deep reinforcement learning (RL) systems have achieved remarkable performance in very challenging problem domains (Mnih et al., 2015; BID22 , in large part because of their flexibility in how they learn and exploit the statistical structure underlying observations and reward signals. But the downsides to such flexibility often include low sample efficiency and poor transfer beyond the specifics of the training environment BID32 Lake et al., 2017; Kansky et al., 2017) . Various structured approaches to RL (e.g. BID9 ; BID8 ; BID7 ; BID12 ) have attempted to overcome these limitations by explicitly incorporating entity-based and symbolic representations, and specialized building blocks for solving the task at hand. Although these approaches are often highly efficient, they constrain the representations and admissible learning algorithms, they struggle to learn rich representations, and they are therefore confined to relatively simple tasks and data conditions.To strike favorable tradeoffs between flexibility and efficiency, a number of recent approaches have explored using relational inductive biases in deep learning, to reap the benefits of flexible statistical learning and more structured approaches. Methods such as ""graph networks"" BID20 Li et al., 2015; explicitly represent entities and their relations using using sets and graphs, and perform relational reasoning using learned message-passing BID13 and attention BID23 Hoshen, 2017; BID24 BID28 schemes. Because they are implemented using deep neural networks, they can learn transformations from input observations into task-relevant entities, as well as functions for computing rich interaction among these entities. This provides a powerful capacity for combinatorial generalization, where their learned building blocks can be composed to represent and reason about novel scenarios BID2 BID19 BID5 BID27 BID21 Kipf et al., 2018; .Drawing on several lines of work, we introduce an approach for incorporating relational inductive biases for entity-and relation-centric state representations, and iterated relational reasoning, into a deep RL agent. In contrast with prior work exploring relational inductive biases in deep RL (e.g., BID27 , our approach does not rely on a priori knowledge of the structure of the problem and is agnostic to the particular relations that need to be considered. To handle raw visual input data, our architecture used a convolutional front-end to compute embeddings of sets of entities, similar to previous work in visual question answering, physical prediction, and video understanding BID19 BID29 BID28 . To perform relational reasoning, we used a self-attention mechanism BID23 Hoshen, 2017; BID24 applied iteratively within each timestep, which can be viewed as learned message-passing (Li et al., 2015; BID13 . Our deep RL agent is based on an off-policy advantage actor-critic (A2C) method which is very effective across a range of standard RL environments BID10 .Our results show that this relational deep RL agent scale to very challenging tasks, achieving state-ofthe-art performance on six out of seven StarCraft II mini-games , surpassing grandmaster level on four mini-games. Additionally, we introduce a novel navigation and planning task, called ""Box-World"", which stresses the planning and reasoning components of the policy, factoring out other challenges like complex vision or large action spaces. Our agent reaches higher ceiling performance, more efficiently, than non-relational baseline, and is able to generalize to solve problems with more complex solutions than it had been trained on within this task. We also found that the intermediate representations involved in the relational computations were interpretable, and suggest that the agent has rich understanding of the underlying structure of the problem. By introducing structured perception and relational reasoning into deep RL architectures, our agents can learn interpretable representations, and exceed baseline agents in terms of sample complexity, ability to generalize, and overall performance. Behavioral analyses showed that the learned representations allowed for better generalization, which is characteristic of relational representations. Analysis of attention weights showed that the model's internal computations were interpretable, and congruent with the computations we would expect from a model computing task-relevant relations.One important future direction is to explore ways to scale our approach to larger inputs spaces, without suffering, as this and other approaches do (e.g., BID28 BID19 , from the quadratic complexity that results from considering all input pairs. Possible avenues involve using a distinct attentional mechanisms that scales linearly with the number of inputs (Hoshen, 2017) or filtering out unimportant relations BID3 . Other future directions include exploring perceiving complex scenes via more structured formats, such as scene graphs BID31 BID4 , which could be powerful additions to our approach's input module. More complex relational modules could be explored, such as richer graph network implementations , learned approaches for inducing compositional programs BID17 Parisotto et al., 2017; BID0 BID6 ) and reasoning about structured data (Neelakantan et al., 2015; Liang et al., 2016) , or even explicit logical reasoning over structured internal representations BID11 , drawing inspiration from more symbolic approaches in classic AI. Our approach may also interface well with approaches for hierarchical RL , planning BID14 , and structured behavior representation (Huang et al., 2018) , so that the structured internal representations and patterns of reasoning can translate into more structured behaviors.More speculatively, this work blurs the line between model-free agents, and those with a capacity for more abstract planning. An important feature of model-based approaches is making general knowledge of the environment available for decision-making. Here our inductive biases for entityand relation-centric representations and iterated reasoning reflect key knowledge about the structure of the world. While not a model in the technical sense, it is possible that the agent learns to exploit this relational architectural prior similarly to how an imagination-based agent's forward model operates BID15 . More generally, our work opens new directions for RL via a principled hybrid of flexible statistical learning and more structured approaches.",Relational inductive biases improve out-of-distribution generalization capacities in model-free reinforcement learning agents,six ; seven ; StarCraft II Learning ; four ; RL ; Mnih ; Lake et al. ; Kansky et al. ; al. ; Hoshen,six ; seven ; StarCraft II Learning ; four ; RL ; Mnih ; Lake et al. ; Kansky et al. ; al. ; Hoshen,"Our approach augments model-free deep reinforcement learning agents with a mechanism for relational reasoning over structured representations, which improves performance, learning efficiency, generalization, and interpretability. Our architecture encodes an image as a set of vectors, and applies an iterative message-passing procedure to discover and reason about relevant entities and relations in a scene. The main contribution of this work is to introduce techniques for representing and reasoning about states in model-free deep reinforcement learning agents via relational inductive biases, which can scale up to meet the most challenging test environments in modern artificial intelligence.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate mutual information (MI) between each hidden layer and the input/desired output, to construct the IP. For instance, hidden layers with many neurons require MI estimators with robustness towards the high dimensionality associated with such layers. MI estimators should also be able to naturally handle convolutional layers, while at the same time being computationally tractable to scale to large networks. None of the existing IP methods to date have been able to study truly deep Convolutional Neural Networks (CNNs), such as the e.g.\ VGG-16. In this paper, we propose an IP analysis using the new matrix--based R\'enyi's entropy coupled with tensor kernels over convolutional layers, leveraging the power of kernel methods to represent properties of the probability distribution independently of the dimensionality of the data. The obtained results shed new light on the previous literature concerning small-scale DNNs, however using a completely new approach. Importantly, the new framework enables us to provide the first comprehensive IP analysis of contemporary large-scale DNNs and CNNs, investigating the different training phases and providing new insights into the training dynamics of large-scale neural networks. Although Deep Neural Networks (DNNs) are at the core of most state-of-the art systems in computer vision, the theoretical understanding of such networks is still not at a satisfactory level (Shwartz-Ziv & Tishby, 2017) . In order to provide insight into the inner workings of DNNs, the prospect of utilizing the Mutual Information (MI), a measure of dependency between two random variables, has recently garnered a significant amount of attention (Cheng et al., 2018; Noshad et al., 2019; Saxe et al., 2018; Shwartz-Ziv & Tishby, 2017; Yu et al., 2018; . Given the input variable X and the desired output Y for a supervised learning task, a DNN is viewed as a transformation of X into a representation that is favorable for obtaining a good prediction of Y . By treating the output of each hidden layer as a random variable T , one can model the MI I(X; T ) between X and T . Likewise, the MI I(T ; Y ) between T and Y can be modeled. The quantities I(X; T ) and I(T ; Y ) span what is referred to as the Information Plane (IP). Several works have demonstrated that one may unveil interesting properties of the training dynamics by analyzing DNNs in the form of the IP Goldfeld et al., 2019; Noshad et al., 2019; Chelombiev et al., 2019) . Figure 1 , produced using our proposed estimator, illustrates one such insight that is similar to the observations of Shwartz-Ziv & Tishby (2017) , where training can be separated into two distinct phases, the fitting phase and the compression phase. This claim has been highly debated as subsequent research has linked the compression phase to saturation of neurons (Saxe et al., 2018) or clustering of the hidden representations (Goldfeld et al., 2019) . Contributions We propose a novel approach for estimating MI, wherein a kernel tensor-based estimator of Rényi's entropy allows us to provide the first analysis of large-scale DNNs as commonly found in state-of-the-art methods. We further highlight that the multivariate matrix-based approach, proposed by , can be viewed as a special case of our approach. However, our proposed method alleviates numerical instabilities associated with the multivariate matrixbased approach, which enables estimation of entropy for high-dimensional multivariate data. Further, using the proposed estimator, we investigate the claim of Cheng et al. (2018) that the entropy H(X) ≈ I(T ; X) and H(Y ) ≈ I(T ; Y ) in high dimensions (in which case MI-based analysis would be meaningless) and illustrate that this does not hold for our estimator. Finally, our results indicate that the compression phase is apparent mostly for the training data, particularly for more challenging datasets. By utilizing a technique such as early-stopping, a common technique to avoid overfitting, training tends to stop before the compression phase occurs (see Figure 1 ). This may indicate that the compression phase is linked to the overfitting phenomena. Figure 1 : IP obtained using our proposed estimator for a small DNN averaged over 5 training runs. The solid black line illustrates the fitting phase while the dotted black line illustrates the compression phase. The iterations at which early stopping would be performed assuming a given patience parameter are highlighted. Here, patience denotes the number of iterations that need to pass without progress on a validation set before training is stopped to avoid overfitting. It can be observed that for low patience values, training will stop before the compression phase. For the benefit of the reader, the bottom right corner displays a magnified version of the first four layers. In this work, we propose a novel framework for analyzing DNNs from a MI perspective using a tensor-based estimate of the Rényi's α-order entropy. Our experiments illustrate that the proposed approach scales to large DNNs, which allows us to provide insights into the training dynamics. We observe that the compression phase in neural network training tends to be more prominent when MI is estimated on the training set and that commonly used early-stopping criteria tend to stop training before or at the onset of the compression phase. This could imply that the compression phase is linked to overfitting. Furthermore, we showed that, for our tensor-based approach, the claim that H(X) ≈ I(T ; X) and H(Y ) ≈ I(T ; Y ) does not hold. We believe that our proposed approach can provide new insight and facilitate a more theoretical understanding of DNNs.",First comprehensive information plane analysis of large scale deep neural networks using matrix based entropy and tensor kernels.,IP ; Convolutional Neural Networks ; R\'enyi ; first ; Deep Neural Networks ; Shwartz-Ziv & Tishby ; the Mutual Information (MI ; two ; Cheng et al. ; Noshad,IP ; Convolutional Neural Networks ; R\'enyi ; first ; Deep Neural Networks ; Shwartz-Ziv & Tishby ; the Mutual Information (MI ; two ; Cheng et al. ; Noshad,"Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into their generalization ability. However, it is unclear how to estimate mutual information (MI) between each hidden layer and the input/desired output, to construct the IP. In this paper, we propose an IP analysis using the new matrix-based R'enyi's entropy coupled with tensor kernels over convolutional layers, leveraging the power of kernel methods to represent properties of the probability distribution independently of the dimensionality",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"Multimodal sentiment analysis is a core research area that studies speaker sentiment expressed from the language, visual, and acoustic modalities. The central challenge in multimodal learning involves inferring joint representations that can process and relate information from these modalities. However, existing work learns joint representations using multiple modalities as input and may be sensitive to noisy or missing modalities at test time. With the recent success of sequence to sequence models in machine translation, there is an opportunity to explore new ways of learning joint representations that may not require all input modalities at test time. In this paper, we propose a method to learn robust joint representations by translating between modalities. Our method is based on the key insight that translation from a source to a target modality provides a method of learning joint representations using only the source modality as input. We augment modality translations with a cycle consistency loss to ensure that our joint representations retain maximal information from all modalities. Once our translation model is trained with paired multimodal data, we only need data from the source modality at test-time for prediction. This ensures that our model remains robust from perturbations or missing target modalities. We train our model with a coupled translation-prediction objective and it achieves new state-of-the-art results on multimodal sentiment analysis datasets: CMU-MOSI, ICT-MMMO, and YouTube. Additional experiments show that our model learns increasingly discriminative joint representations with more input modalities while maintaining robustness to perturbations of all other modalities. Sentiment analysis, which involves identifying a speaker's opinion, is a core research problem in machine learning and natural language processing. However, language-based sentiment analysis through words, phrases, and their compositionality was found to be insufficient for inferring affective content from spoken opinions BID33 , which contain rich nonverbal behaviors in addition to verbal text. As a result, there has been a recent push towards using machine learning methods to learn joint representations from additional behavioral cues present in the visual and acoustic modalities. This research field has become known as multimodal sentiment analysis and extends the conventional textbased definition of sentiment analysis to a multimodal setup. For example, BID21 explore the additional acoustic modality while BID61 use the language, visual, and acoustic modalities present in monologue videos to predict sentiment. The abundance of multimodal data has led to the creation of multimodal datasets, such as CMU-MOSI BID66 and ICT-MMMO BID61 , as well as deep multimodal models that are highly effective at learning discriminative joint multimodal representations BID29 BID57 BID7 . Existing work learns joint representations using multiple modalities as input with neural networks BID28 , graphical models BID33 or geometric classifiers BID66 . However, this results in joint representations that are sensitive to noisy or missing modalities at test time. To address this problem, we draw inspirations from the recent success of sequence to sequence models for unsupervised representation learning BID55 . We propose the Multimodal Cyclic Translation Network model (MCTN) to learn robust joint multimodal representations by translating between modalities. FIG0 illustrates these translations between the language, visual and acoustic modalities. Our method is based on the key insight that translation from a source modality S to a target modality T results in an intermediate representation that captures joint information between modalities S and T . MCTN extends this insight using a cyclic translation loss involving both forward translations from source to target, and backward translations from the predicted target back to the source modality. Together, we call these multimodal cyclic translations to ensure that the learned joint representations capture maximal information from both modalities. We also propose a hierarchical MCTN to learn joint representations between a source modality and multiple target modalities. MCTN is trainable end-to-end with a coupled translation-prediction loss which consists of (1) the cyclic translation loss, and (2) a prediction loss to ensure that the learned joint representations are task-specific. Another advantage of MCTN is that once trained with paired multimodal data (S, T ), we only need data from the source modality S at test time to infer the joint representation and sentiment prediction. As a result, MCTN is completely robust to test-time perturbations on target modality T and missing modalities.Even though translation and generation of videos, audios, and text are difficult BID27 , our experiments show that the learned joint representations can help for discriminative tasks: MCTN achieves new state-of-the-art results on multimodal sentiment analysis using the CMU-MOSI, ICT-MMMO, and YouTube public datasets. Additional experiments show that MCTN learns increasingly discriminative joint representations with more input modalities while maintaining robustness to all target modalities. This section discusses several research questions and presents our experimental results. To conclude, this paper investigated learning joint representations via cyclic modality translations from source to target modalities. During testing, we only need the source modality for prediction which ensures that our model remains robust from noisy or missing target modalities. We demonstrate that cyclic translations and seq2seq models are especially useful for learning joint multimodal representations. In addition to achieving state-of-the-art results on three datasets, our model learns increasingly discriminative representations with more input modalities while maintaining robustness to all target modalities. Our approach presents several exciting areas for future work, such as: 1) combining our approach with the transformer architecture BID59 for modality translations, 2) exploring pretrained deep language models BID12 BID42 for translations, as well as 3) extending our translation model to work other multimodal tasks involving language and raw speech signals (prosody), videos with multiple speakers (diarization), and combinations of static and temporal data (i.e. image captioning).",We present a model that learns robust joint representations by performing hierarchical cyclic translations between multiple modalities.,CMU ; ICT ; YouTube ; the Multimodal Cyclic Translation Network ; MCTN ; three,CMU ; ICT ; YouTube ; the Multimodal Cyclic Translation Network ; MCTN ; three ; Multimodal sentiment analysis ; a core research area ; that ; speaker sentiment,"Multimodal sentiment analysis is a core research area that studies speaker sentiment expressed from the language, visual, and acoustic modalities. The central challenge in multimodal learning involves inferring joint representations that can process and relate information from these modalities. However, existing work learns joint representations using multiple modalities as input and may be sensitive to noisy or missing modalities at test time. With the recent success of sequence to sequence models in machine translation, there is an opportunity to explore new ways of learning joint representations that retain maximal information from all modalities.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"Deep neural networks are complex non-linear models used as predictive analytics tool and have demonstrated state-of-the-art performance on many classification tasks.   However, they have no inherent capability to recognize when their predictions might go wrong. There have been several efforts in the recent past to detect natural errors i.e.  misclassified inputs but these mechanisms pose additional energy requirements.   To address this issue, we present a novel post-hoc framework to detect natural errors in an energy efficient way.   We achieve this by appending relevant features based linear classifiers per class referred as Relevant features based Auxiliary Cells (RACs).    The proposed technique makes use of the consensus between RACs appended at few selected hidden layers to distinguish the correctly classified inputs from misclassified inputs. The combined confidence of RACs is utilized to determine if classification should terminate at an early stage. We demonstrate the effectiveness of our technique on various image classification datasets such as CIFAR10, CIFAR100 and Tiny-ImageNet. Our results show that for CIFAR100 dataset trained on VGG16 network, RACs can detect 46% of the misclassified examples along with 12% reduction in energy compared to the baseline network while 69% of the examples are correctly classified.
 Machine learning classifiers have achieved high performance on various classification tasks, e.g., object detection, speech recognition and image classification. Decisions made by these classifiers can be critical when employed in real-world tasks such as medical diagnosis, self-driving cars, security etc. Hence, identifying incorrect predictions i.e. detecting abnormal inputs and having a wellcalibrated predictive uncertainty is of great importance to AI safety. Note that abnormal samples include natural errors, adversarial inputs and out-of-distribution (OOD) examples. Natural errors are samples in the test data which are misclassified by the final classifier in a given network. Various techniques have been proposed in literature to address the issue of distinguishing abnormal samples. A baseline method for detecting natural errors and out-of-distribution examples utilizing threshold based technique on maximal softmax response was suggested by Hendrycks & Gimpel (2017) . A simple unified framework to detect adversarial and out-of-distribution samples was proposed by Lee et al. (2018) . They use activations of hidden layers along with a generative classifier to compute Mahalanobis distance (Mahalanobis, 1936) based confidence score. However, they do not deal with detection of natural errors. Bahat et al. (2019) ; Hendrycks & Gimpel (2017) ; Mandelbaum & Weinshall (2017) focus on detecting natural errors. Mandelbaum & Weinshall (2017) use distance based confidence method to detect natural errors based on measuring the point density in the effective embedding space of the network. More recently, Bahat et al. (2019) showed that KL-divergence between the outputs of the classifier under image transformations can be used to distinguish correctly classified examples from adversarial and natural errors. To enhance natural error detection, they further incorporate Multi Layer Perceptron (MLP) at the final layer which is trained to detect misclassifications. Most prior works on the line of error detection do not consider the latency and energy overheads that incur because of the detector or detection mechanism. It is known that deeper networks expend higher energy and latency during feed-forward inference. Adding a detector or detection mechanism on top of this will give rise to additional energy requirements. The increase in energy may make these networks less feasible to employ on edge devices where reduced latency and energy with the ability to identify abnormal inputs is significant. Many recent efforts toward energy efficient deep neural networks (DNNs) have explored early exit techniques. Here, the main idea is to bypass (or turn off) computations of latter layers if the network yields high confidence prediction at early layers. Some of these techniques include the adaptive neural networks (Stamoulis et al., 2018) , the edge-host partitioned neural network Ko et al. (2018) , the distributed neural network (Teerapittayanon et al., 2017) , the cascading neural network (Leroux et al., 2017) , the conditional deep learning classifier (Panda et al., 2016) and the scalable-effort classifier (Venkataramani et al., 2015) . So far, there has been no unified technique that enables energy efficient inference in DNNs while improving their robustness towards detecting abnormal samples. In this work, we target energy efficient detection of natural errors, which can be extended and applied to detecting OOD examples and adversarial data. We propose adding binary linear classifiers at two or more intermediate (or hidden) layers of an already trained DNN and utilize the consensus between the outputs of the classifiers to perform early classification and error detection. This idea is motivated from the following two observations: • If an input instance can be classified at early layers Panda et al. (2016) then processing the input further by latter layers can lead to incorrect classification due to over-fitting. This can be avoided by making early exit which also has energy benefits. • We have observed that on an average, the examples which are misclassified do not have consistent hidden representations compared to correctly classified examples. The additional linear classifiers and their consensus enables identifying this inconsistent behaviour to detect misclassified examples or natural errors. The training and construction of the linear classifiers is instrumental towards the accurate and efficient error detection with our approach. We find that at a given hidden layer, the error detection capability (detecting natural errors) is higher if we use class-specific binary classifiers trained on the corresponding relevant feature maps from the layer. In fact, using a fully connected classifier trained on all feature maps (conventionally used in early exit techniques of Panda et al. (2016) ) does not improve error detection capability. Training these binary classifiers on relevant features can be considered as encoding prior knowledge on the learned hidden feature maps, thereby, yielding better detection capability. Besides improved error detection, a key advantage of using class wise binary linear classifiers trained on only relevant features is that they incur less overhead in terms of total number of parameters, as compared to a fully connected classifier trained on all feature maps. We use ""relevant features"" and class specific classifiers instead of using all the feature maps and one fully connected classifier at a hidden layer for the following reasons: (a) We have observed that at a given hidden layer, the detection capability (detecting natural errors) is higher if we use relevant feature maps with class-specific binary classifiers than using a fully connected classifier trained on all feature maps. Training these binary classifiers on relevant features can be considered as encoding prior knowledge on the learned hidden features maps and hence is expected to have better detection capability. (b) The class wise binary linear classifiers trained on relevant features have less number of parameters compared to a fully connected classifier trained on all the feature maps. In the proposed framework, class-specific binary linear classifiers are appended at few selected hidden layers which have maximal information. These hidden layers are referred to as validation layers. The set of all binary linear classifiers at a validation layer constitute a Relevant feature based Auxiliary Cell (RAC). We use the consensus of RACs to detect natural errors which improves the robustness of the network. The combined confidence of RACs is used to perform early classification that yields energy efficiency. Deep neural networks are crucial for many classification tasks and require robust and energy efficient implementations for critical applications. In this work, we device a novel post-hoc technique for energy efficient detection of natural errors. In essence, our main idea is to append class-specific binary linear classifiers at few selected hidden layers referred as Relevant features based Auxiliary Cells (RACs) which enables energy efficient detection of natural errors. With explainable techniques such as Layerwise Relevance Propagation (LRP), we determine relevant hidden features corresponding to a particular class which are fed to the RACs. The consensus of RACs (and final classifier if there is no early termination) is used to detect natural errors and the confidence of RACs is utilized to decide on early classification. We also evaluate robustness of DNN with RACs towards adversarial inputs and out-of-distribution samples. Beyond the immediate application to increase robustness towards natural errors and reduce energy requirement, the success of our framework suggests further study of energy efficient error detection mechanisms using hidden representations.",Improve the robustness and energy efficiency of a deep neural network using the hidden representations.,linear ; Auxiliary Cells ; Tiny-ImageNet ; AI ; Hendrycks & Gimpel ; Lee et al ; Mandelbaum & Weinshall ; Bahat et al ; Multi Layer Perceptron ; Stamoulis et al.,linear ; Auxiliary Cells ; Tiny-ImageNet ; AI ; Hendrycks & Gimpel ; Lee et al ; Mandelbaum & Weinshall ; Bahat et al ; Multi Layer Perceptron ; Stamoulis et al.,"Deep neural networks are complex non-linear models used as predictive analytics tool but lack the capability to recognize when their predictions might go wrong. To address this issue, we present a novel post-hoc framework to detect natural errors in an energy efficient way by appending relevant features based linear classifiers per class referred to as Relevant features based Auxiliary Cells (RACs). The combined confidence of RACs is utilized to determine if classification should terminate at an early stage. Machine learning classifiers have achieved high performance on various classification tasks such as medical diagnosis, self-driving cars",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"We consider off-policy policy evaluation when the trajectory data are generated by multiple behavior policies. Recent work has shown the key role played by the state or state-action stationary distribution corrections in the infinite horizon context for off-policy policy evaluation. We propose estimated mixture policy (EMP), a novel class of partially policy-agnostic methods to accurately estimate those quantities. With careful analysis, we show that EMP gives rise to estimates with reduced variance for estimating the state stationary distribution correction while it also offers a useful induction bias for estimating the state-action stationary distribution correction. In extensive experiments with both continuous and discrete environments, we demonstrate that our algorithm offers significantly improved accuracy compared to the state-of-the-art methods. In many real-world decision-making scenarios, evaluating a novel policy by directly executing it in the environment is generally costly and can even be downright risky. Examples include evaluating a recommendation policy (Swaminathan et al., 2017; Zheng et al., 2018) , a treatment policy (Hirano et al., 2003; Murphy et al., 2001) , and a traffic light control policy ( Van der Pol & Oliehoek, 2016) . Off-policy policy evaluation methods (OPPE) utilize a set of previously-collected trajectories (for example, website interaction logs, patient trajectories, or robot trajectories) to estimate the value of a novel decision-making policy without interacting with the environment (Precup et al., 2001; Dudík et al., 2011) . For many reinforcement learning applications, the value of the decision is defined in a long-or infinite-horizon, which makes OPPE more challenging. The state-of-the-art methods for infinite-horizon off-policy policy evaluation rely on learning (discounted) state stationary distribution corrections or ratios. In particular, for each state in the environments, these methods estimate the likelihood ratio of the long-term probability measure for the state to be visited in a trajectory generated by the target policy, normalized by the probability measure generated by the behavior policy. This approach can effectively avoid the exponentially high variance compared to the more classic importance sampling (IS) estimation methods (pre; Dudík et al., 2011; Hirano et al., 2003; Wang et al., 2017; Murphy et al., 2001) , especially for infinite-horizon policy evaluation (Liu et al., 2018; Nachum et al., 2019; Hallak & Mannor, 2017) . However, learning state stationary distribution requires detailed information on distributions of the behavior policy, and we call them policy-aware methods. As a consequence, policy-aware methods are difficult to apply when off-policy data are pre-generated by multiple behavior policies or when the behavior policy's form is unknown. To address this issue, Nachum et al. (2019) proposes a policy-agnostic method, DualDice, which learns the joint state-action stationary distribution correction that is much higher dimension, and therefore needs more model parameters than the state stationary distribution. Besides, there is no theoretic comparison between policy-aware and policy-agnostic methods. In this paper, we propose a OPPE method with behavior policy learning, EMP (estimated mixture policy) for infinite-horizon off-policy policy evaluation with multiple known or unknown behavior policies. We call EMP a partially policy-agnostic method in the sense that, EMP does not require any information on each""physical"" behavior policy, instead, it utilizes some aggregated information of the behavior policies learned from data. In detail, EMP includes a pre-estimation step using certain parametric model to learn a ""virtual"" policy (we call it the mixture policy and formally define it in Section 4). Hence, its performance depends on the accuracy of mixture policy estimation. Like the method in Liu et al. (2018) , EMP obtain OPPE also via learning the state stationary distribution correction, so it remains computationally cheap and is scalable in terms of the number of behavior policies. Besides, inspired by Hanna et al. (2019) , we provide a theoretic guarantee that EMP yields smaller mean square error (MSE) than the policy-aware methods in stationary distribution corrections learning, even in the single-behavior policy setting. On the other hand, compared to DualDice, EMP learns the state stationary distribution correction of smaller dimension, more importantly the estimation of the mixture policy can be considered as an inductive bias as far as the stationary distribution correction is concerned, and hence could achieve better performance when the pre-estimation is not expensive. In addition, we propose an ad-hoc improvement of EMP, whose theoretical analysis is left for future studies. EMP is compared with both policy-aware and policy-agnostic methods in a set of continuous and discrete control tasks and shows significant improvement. In this paper, we advocate the viewpoint of partial policy-awareness and the benefits of estimating a ""virtual"" mixture policy for off-policy policy evaluation. The theoretical results of reduced variance coupled with experimental results illustrate the power of this class of methods. One key question that still remains is the following: if we are willing to estimate the individual behavior policies, can we further improve EMP by developing an efficient algorithm to compute the optimal weights? The preliminary experiment results suggest that the answer would be yes, and we will leave this for future study.",A new partially policy-agnostic method for infinite-horizon off-policy policy evalution with multiple known or unknown behavior policies.,EMP ; Swaminathan ; Zheng et al. ; Hirano ; Murphy ; al. ; Pol & Oliehoek ; OPPE ; Precup ; Dudík,EMP ; Swaminathan ; Zheng et al. ; Hirano ; Murphy ; al. ; Pol & Oliehoek ; OPPE ; Precup ; Dudík,"Off-policy policy evaluation (OPPE) considers trajectory data generated by multiple behavior policies. Recent work has shown the key role played by the state or state-action stationary distribution corrections in the infinite horizon context for off-policy policy evaluation. We propose estimated mixture policy (EMP), a novel class of partially policy-agnostic methods to accurately estimate these quantities. EMP gives rise to estimates with reduced variance for estimating the state stationary distribution correction while it also offers a useful induction bias for estimating the state-action stationary distribution correction.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"Mixed precision training (MPT) is becoming a practical technique to improve the speed and energy efficiency of training deep neural networks by leveraging the fast hardware support for IEEE half-precision floating point that is available in existing GPUs. MPT is typically used in combination with a technique called loss scaling, that works by scaling up the loss value up before the start of backpropagation in order to minimize the impact of numerical underflow on training. Unfortunately, existing methods make this loss scale value a hyperparameter that needs to be tuned per-model, and a single scale cannot be adapted to different layers at different training stages. We introduce a loss scaling-based training method called adaptive loss scaling that makes MPT easier and more practical to use, by removing the need to tune a model-specific loss scale hyperparameter. We achieve this by introducing layer-wise loss scale values which are automatically computed during training to deal with underflow more effectively than existing methods. We present experimental results on a variety of networks and tasks that show our approach can shorten the time to convergence and improve accuracy, compared with using the existing state-of-the-art MPT and single-precision floating point. Training deep neural networks (DNNs) is well-known to be time and energy consuming, motivating the development of new methods and hardware to make training more efficient. One way to improve training efficiency is to use numerical representations that are more hardware-friendly. This is the reason that the IEEE 754 32-bit single-precision floating point format (FP32) is more widely used for training DNNs than the more precise double precision format (FP64), which is commonly used in other areas of high-performance computing. In an effort to further improve hardware efficiency, there has been increasing interest in using data types with even lower precision than FP32 for training Wang et al., 2018; Kalamkar et al., 2019; Sakr et al., 2019) . Of these, the IEEE half-precision floating-point (FP16) format is already well supported by modern GPU vendors (Choquette et al., 2018) . Using FP16 for training can reduce the memory footprint by half compared to FP32 and significantly improve the runtime performance and power efficiency. Nevertheless, numerical issues like overflow, underflow, and rounding errors frequently occur when training in low precision only. Percentage of underflow (%) Underflow rate among activation gradients accross all layers iter=10000 iter=50000 iter=80000 iter=110000 (a) Underflow rate is calculated by counting the absolute gradients below 2 −24 , the smallest positive FP16 number. Loss scale expected by each layer iter=10000 iter=50000 iter=80000 iter=110000 (b) Expected loss scale of each layer is calculated by 1 over the (0.01N )-th smallest absolute gradient, where N is the size of each gradient and 0.01 is the largest underflow rate permitted. Figure 1: Statistics of activation gradients collected from training SSD by FP32. Data are collected from different training iterations (120k in total). Layer ID are assigned in the topological order of backpropagation computation. Layers with higher ID are closer to the input. start of the backward pass so that the computed (scaled) gradients can then be properly represented in FP16 without significant underflow. For an appropriate choice of α, loss scaling can achieve state of the art results that are competitive with regular FP32 training. Unfortunately, there is no single value of α that will work in arbitrary models, and so it often needs to be tuned per model. Its value must be chosen large enough to prevent underflow issues from affecting training accuracy. However, if α is chosen too large, it could amplify rounding errors caused by swamping (Higham, 1993) or even result in overflow. This observed sensitivity to the particular choice of loss scale is also reported by , who find that different values can lead to very different ResNet-50 MPT convergence behavior. Furthermore, the data distribution of gradients can vary both between layers and between iterations (Figure 1 ), which implies that a single scale is insufficient. For instance, gradients closer to the input require a higher loss scale that may cause overflow or severe rounding errors if the same value were used in layers closer to the output. Including the time spent tuning α, the total training time of MPT can even exceed regular FP32 training. We introduce a loss scaling-based training method called adaptive loss scaling that makes MPT easier and more practical to use. We hope that this will help to utilize better existing hardware with support for fast FP16 operations. Our method improves the usability of MPT compared to existing methods by removing the need to tune a model-specific loss scale hyperparameter, while retaining (and in some cases surpassing) the accuracy of regular FP32 training. We achieve this by introducing layer-wise loss scale values which are automatically computed and dynamically updated during training to deal with underflow more effectively than existing methods. Experimental results on several examples show that MPT with adaptive loss scaling can achieve the best model accuracy and the shortest overall training time, especially when training deep models on large datasets. This paper presents adaptive loss scaling, a method that calculates layer-wise loss scale during runtime, to improve the performance and usability of MPT. Empirically we find it works better than plain MPT, existing loss scaling methods, and even FP32 in some cases, regarding model accuracy and the time taken to converge. Future work includes evaluating adaptive loss scaling on other tasks and models, especially those for Natural Language Processing; and trying to find a tighter upper bound of loss scale for each layer, e.g., based on the variance analysis in (Sakr et al., 2019) , such that each layer can be scaled more effectively; extending it to FP8 is also intriguing to try. A DETAILED ANALYSIS ON CIFAR RESULTS Table 1 shows that adaptive loss scaling is beneficial for training ResNet-110, while less advantageous for ResNet-20 and ResNet-56. We hypothesize the reason behind is that underflow causes more numerical problems when the model is deeper. For shallower models, the difference between the oracle gradient values and the underflowing ones is moderate and can even be viewed as a form of regularization. This argument is supported by the fact that the training accuracy of ResNet models on CIFAR can always reach 100%. In this way, even though adaptive loss scaling can improve the accuracy of the computed gradients, this does not necessarily always translate to improved test accuracy. 19% 93.19% 93.19% We dive deeper into this argument by reviewing Table 4 , which shows the test accuracy of the two shallower ResNet models on CIFAR-10. For both models, the test accuracy first increases to a maxima at 16, then there is a sudden drop at 128, and finally it climbs up to a plateau. Our hypothetical interpretation is as follows: 1. Initially the test accuracy is low. Here the underflow rate is expected to be at its highest, and it is the major cause for the low test accuracy. 2. The test accuracy then increases with loss scale, mainly due to the mitigation of underflow by loss scaling. However, as the gradients become more accurate, the regularizing effect from underflow is also reduced and the test accuracy will drop, until the loss scale reaches around 128. 3. If the loss scale continues to increase, the high rounding error and swamping problem caused by large scales will arise. It adds another kind of regularization, which is relatively more harmful than what underflow may cause, and the test accuracy cannot improve much. Even though this interpretation is hypothetical, this empirical evaluation in Table 4 shows that the relationship between the goodness of a loss scaling scheme and test accuracy is complicated when the model tends to overfit.",We devise adaptive loss scaling to improve mixed precision training that surpass the state-of-the-art results.,half ; MPT ; One ; IEEE ; Wang ; al. ; Kalamkar et al. ; Sakr et al. ; GPU ; Choquette et al.,half ; MPT ; One ; IEEE ; Wang ; al. ; Kalamkar et al. ; Sakr et al. ; GPU ; Choquette et al.,"Mixed precision training (MPT) is becoming a practical technique to improve speed and energy efficiency of training deep neural networks by leveraging the fast hardware support for IEEE half-precision floating-point available in existing GPUs. MPT is typically used in combination with a technique called loss scaling, which works by scaling up the loss value before the start of backpropagation in order to minimize the impact of numerical underflow on training. We introduce a loss scaling-based training method called adaptive loss scaling, which makes MPT easier and more practical by removing the need",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"This paper presents the ballistic graph neural network. Ballistic graph neural network tackles the weight distribution from a transportation perspective and has many different properties comparing to the traditional graph neural network pipeline. The ballistic graph neural network does not require to calculate any eigenvalue. The filters propagate exponentially faster($\sigma^2 \sim T^2$) comparing to traditional graph neural network($\sigma^2 \sim T$). We use a perturbed coin operator to perturb and optimize the diffusion rate. Our results show that by selecting the diffusion speed, the network can reach a similar accuracy with fewer parameters. We also show the perturbed filters act as better representations comparing to pure ballistic ones. We provide a new perspective of training graph neural network, by adjusting the diffusion rate, the neural network's performance can be improved. How to collect the nodes' correlation on graphs fast and precisely? Inspired by convolutional neural networks(CNNs), graph convolutional networks(GCNs) can be applied to many graph-based structures like images, chemical molecules and learning systems. Kipf & Welling (2016) Similar to neural networks, GCNs rely on random walk diffusion based feature engineering to extract and exploit the useful features of the input data. Recent works show random walk based methods can represent graph-structured data on the spatial vertex domain. For example, Li et al. (2017) use bidirectional random walks on the graph to capture the spatial dependency and Perozzi et al. (2014) present a scalable learning algorithm for latent representations of vertices in a network using random walks. Except for the spatial domain, many researchers focus on approximating filters using spectral graph theory method, for example, Bruna et al. (2013) construct a convolutional architecture based on the spectrum of the graph Laplacian; Defferrard et al. (2016) use high order polynomials of Laplacian matrix to learn the graphs in a NN structure model. The ballistic walk algorithm consists of two parts, a walker in the position space H spatial and a coin in the coin space H c . Thus the walker is described using states in Hibert space H spatial ⌦ H c . Let the walker initially be at the state | i 0 = |i, ji p ⌦ s 0 , where s 0 is normally symmetric state in H c . In analogy to the classical random walk, the next state of the walker can be expressed by In this paper, we consider the ballistic walk on a regular two-dimensional graph. The coin space H c consists of four states: |#i, |""i, | i, |!i , represents move up, down, left and right for the next step. The spatial space H spatial consists N states representing the walker's position, where N is the number of nodes. The notation |ni denotes an orthonormal basis for H spatial and hn| is the Hermitian conjugate of the state. For a finite-dimensional vector space, the inner product hn 0 |ni is nn 0 and the outer product |n 0 i hn| equals to a matrix in R N ⇥N . The probability stay on the node |i, ji is Pseudo-code of our method is given in Algorithm 1. Algorithm 1: Ballistic walk on 2D regular graph Result: The walker's state after K steps start from (i, j) We want our filters have a diffusion distance in a reasonable region(a < Distance < b). However, the ballistic filters' distances increase with steps. The filters are not capable to dense sampling some specific regions. By selecting different randomness and steps, we can generate filters localized in a bounded area. The noisy Hadamard can be written as Table 2 : Diffusion rate with different randomness Table 3 shows the accuracy with different perturbed filters(↵ = 0, 0.05, 0.10, 0.15, 0.20). = 2 ⇥ R ⇥ ⇡↵ denotes the randomness in the coin space, R is a random number between 0 and 1. The corresponding transportation speed is shown in table 2 and Figure 12 . As the ↵ increases to 0.20, the speed drops to 0.323. ↵ is a controller of the diffusion speed, as ↵ becomes larger, the ballistic tranportation will finally evolve to the classical diffusive couterpart. In this paper, we introduced a generalization of graph neural network: ballistic graph neural network. We started from the speed problem of the traditional diffusive kernel and tackle this problem from the perspective of transportation behaviour. We showed the linear transportation behaviour of ballistic filters and introduced the de-coherence scheme to adjust the filters' speed. Compared with diffusive filters, the ballistic filters achieve similar accuracy using fewer of the parameters. Besides, we showed that the efficiency of the ballistic filters could be improved by controlling transportation behaviour. Compared to the random walk method, we used two operators: the coin operator and shift operator to control the walker, and thus controlled the information the walker gathers. Our pipeline provides a new perspective for efficient extracting the graphical information using diffusion-related models. Future work can investigate these two directions: The Network Structure. In this paper, we use simplified architecture to demonstrate the concept of the ballistic walk, the layers are limited to 5 layers, and we use traditional average pooling. More layers can be added to improve particular accuracy, and more sophisticated pooling methods can be introducedDefferrard et al. (2016) . Other techniques like dropout can also be employed to improve accuracy. The Ballistic Filter. De-coherence can also be introduced into the shift operator. In other words, we can use perturbed shifted operator, and thus we introduce randomness in the spatial domain. We can also try different unitary operators in the coin space or change the initial state of the walker. The extension to general graphs can be generalized by adding self-loops to the nodes and thus make the graph regular. The ballistic filters are inspired by two-dimensional quantum walk. The quantum coherence effect guarantees fast ballistic transportation. The different states in the coin space can be regarded as the independent state from spatial behaviour, for example, the spin of fermions or the polarization of light. More information about the quantum walk can be found at Childs et al. (2003) . Why introducing ballistic filters results in better performance? We here offer a conjecture from the perspective of signal processing using one-dimensional condition. The classical diffusion in the one-dimensional case has the shape of: and the frequency part can be written as:ĝ The g(x) can be regarded as a gaussian low pass filter. For a gaussian high-pass filter, the spatial distribution is: Makandar & Halalli (2015) hg The long time probability distribution of ballistic walk is: Luo & Xue (2015) P (x) = P 0 + ae (12) Figure 15 shows the distribution of gaussian high pass filter and the cumulative distribution of 24th and 25step of ballistic diffusion. These two distributions have a similar shape while the ballistic distribution has steeper edges resulted from fast transportation. The ballistic filters' capability to collect the long-time probability means it can act as a high-pass filter with different sizes. The size of the filters depends on the walking steps. Figure 16 shows the ballistic diffusion with a pulse signal from t = ⌧ to t = ⌧ . The orange dashed line is an approximated shape of ballistic transportation of the leftmost signal(t = ⌧ ), and the blue dashed line corresponds to t = ⌧ . The width of the approximated shape is related to the walking steps. For random walk based diffusive transportation after certain steps of diffusion, the region from t = ⌧ to t = ⌧ have a gaussian shape since it is sum of gaussian distribution with centers range from t = ⌧ to t = ⌧ . The classical diffusion acts like a blur filter(low pass filter). For ballistic diffusion, the shape of the pulse signal from t = ⌧ to t = ⌧ evolves to a 'valley' shape and thus, the ballistic diffusion is similar to a high pass filter.",A new perspective on how to collect the correlation between nodes based on diffusion properties.,"Kipf & Welling ; Li ; Bruna ; Laplacian ; Defferrard et al ; two ; Hibert ; four ; |""i ; |!i","Kipf & Welling ; Li ; Bruna ; Laplacian ; Defferrard et al ; two ; Hibert ; four ; |""i ; |!i","The ballistic graph neural network tackles the weight distribution from a transportation perspective and has many different properties compared to the traditional graph neural network pipeline. The filter propagates exponentially faster($sigma2 sim T2$), using a perturbed coin operator to perturb and optimize the diffusion rate. GCNs rely on random walk diffusion based feature engineering to extract and exploit useful features of input data. Recent work shows random walk based methods can represent graph-structured data on the spatial vertex domain.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"Graph-based dependency parsing consists of two steps: first, an encoder produces a feature representation for each parsing substructure of the input sentence, which is then used to compute a score for the substructure; and second, a decoder} finds the parse tree whose substructures have the largest total score. Over the past few years, powerful neural techniques have been introduced into the encoding step which substantially increases parsing accuracies. However, advanced decoding techniques, in particular high-order decoding, have seen a decline in usage. It is widely believed that contextualized features produced by neural encoders can help capture high-order decoding information and hence diminish the need for a high-order decoder. In this paper, we empirically evaluate the combinations of different neural and non-neural encoders with first- and second-order decoders and provide a comprehensive analysis about the effectiveness of these combinations with varied training data sizes. We find that: first, when there is large training data, a strong neural encoder with first-order decoding is sufficient to achieve high parsing accuracy and only slightly lags behind the combination of neural encoding and second-order decoding; second, with small training data, a non-neural encoder with a second-order decoder outperforms the other combinations in most cases.    Dependency parsing (Kübler et al., 2009) is an important task in natural language processing (NLP) and a large number of methods have been proposed, most of which can be divided into two categories: graph-based methods (Dozat & Manning, 2017; Shi & Lee, 2018) and transition-based methods (Weiss et al., 2015; Andor et al., 2016; Ma et al., 2018) . In this paper, we focus on graphbased dependency parsing, which traditionally has higher parsing accuracy. A typical graph-based dependency parser consists of two parts: first, an encoder that produces a feature representation for each parsing substructure of the input sentence and computes a score for the substructure based on its feature representation; and second, a decoder that finds the parse tree whose substructures have the largest total score. Over the past few years, powerful neural techniques have been introduced into the encoding step that represent contextual features as continuous vectors. The introduction of neural methods leads to substantial increase in parsing accuracy (Kiperwasser & Goldberg, 2016) . High-order decoding techniques, on the other hand, have seen a decline in usage. The common belief is that high-order information has already been captured by neural encoders in the contextual representations and thus the need for high-order decoding is diminished (Falenska & Kuhn, 2019) . In this paper, we empirically evaluate different combinations of neural and non-neural encoders with first-and second-order decoders to thoroughly examine their effect on parsing performance. From the experimental results we make the following observations: First, powerful neural encoders indeed diminish to some extend the necessity of high-order decoding when sufficient training data is provided. Second, with smaller training data, the advantage of a neural encoder with a second-order decoder begins to decrease, and its performance surpassed by other combinations in some treebanks. Finally, if we further limit the training data size to a few hundred sentences, the combination of a simple non-neural encoder and a high-order decoder emerges as the preferred choice for its robustness and relatively higher performance. We empirically evaluate the combinations of neural and non-neural encoders with first-and secondorder decoders on six treebanks with varied data sizes. The results suggest that with sufficiently large training data (a few tens of thousands of sentences), one should use a neural encoder and perhaps a high-order decoder to achieve the best parsing accuracy; but with small training data (a few hundred sentences), one should use a traditional non-neural encoder plus a high-order decoder. Possible future work includes experimenting with second-order sibling decoding, third-order decoding, neural encoders with biaffine and triaffine score computation, and finally transition-based dependency parsers.",An empirical study that examines the effectiveness of different encoder-decoder combinations for the task of dependency parsing,two ; first ; second ; the past few years ; Kübler et al. ; NLP ; Dozat & Manning ; Shi & Lee ; Weiss ; al.,two ; first ; second ; the past few years ; Kübler et al. ; NLP ; Dozat & Manning ; Shi & Lee ; Weiss ; al.,"Graph-based dependency parsing consists of two steps: first, an encoder produces a feature representation for each parsing substructure of the input sentence, which is then used to compute a score for the substructure; and second, a decoder finds the parse tree whose substructures have the largest total score. Over the past few years, powerful neural techniques have been introduced into the encoding step, which substantially increases parsing accuracies. However, advanced decoding techniques, in particular high-order decoders, have seen a decline",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
"The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent. Recently, the Vision-and-Language (VLN) navigation task BID3 , which requires the agent to follow natural language instructions to navigate through a photo-realistic unknown environment, has received significant attention BID46 BID19 ). In the VLN task, an agent is placed in an unknown realistic environment and is required to follow natural language instructions to navigate from its starting location to a target location. In contrast to some existing navigation tasks BID24 BID53 BID33 , we address the class of tasks where the agent does not have an explicit representation of the target (e.g., location in a map or image representation of the goal) to know if the goal has been reached or not BID31 BID22 BID18 BID6 . Instead, the agent needs to be aware of its navigation status through the association between the sequence of observed visual inputs to instructions.Consider an example as shown in FIG0 , given the instruction ""Exit the bedroom and go towards the table. Go to the stairs on the left of the couch. Wait on the third step."", the agent first needs to locate which instruction is needed for the next movement, which in turn requires the agent to be aware of (i.e., to explicitly represent or have an attentional focus on) which instructions were completed or ongoing in the previous steps. For instance, the action ""Go to the stairs"" should be carried out once the agent has exited the room and moved towards the table. However, there exists inherent ambiguity for ""go towards the table"". Intuitively, the agent is expected to ""Go to the stairs"" after completing ""go towards the table"". But, it is not clear what defines the completeness of ""Go towards the table"". The completeness of an ongoing action often depends on the availability of the next action. Since the transition between past and next part of the instructions is a soft boundary, in order to determine when to transit and to follow the instruction correctly the agent is required to keep track of both grounded instructions. On the other hand, assessing the progress made towards the goal has indeed been shown to be important for goal-directed tasks in humans decision-making BID8 BID12 BID9 . While a number of approaches have been proposed for VLN BID3 BID46 BID19 , previous approaches generally are not aware of which instruction is next nor progress towards the goal; indeed, we qualitatively show that even the attentional mechanism of the baseline does not successfully track this information through time.In this paper, we propose an agent endowed with the following abilities: (1) identify which direction to go by finding the part of the instruction that corresponds to the observed images-visual grounding, (2) identify which part of the instruction has been completed or ongoing and which part is potentially needed for the next action selection-textual grounding, and (3) ensure that the grounded instruction can correctly be used to estimate the progress made towards the goal, and apply regularization to ensure this -progress monitoring. Therefore, we introduce the self-monitoring agent consisting of two complementary modules: visual-textual co-grounding and progress monitor.More specifically, we achieve both visual and textual grounding simultaneously by incorporating the full history of grounded instruction, observed images, and selected actions into the agent. We leverage the structural bias between the words in instructions used for action selection and progress made towards the goal and propose a new objective function for the agent to measure how well it can estimate the completeness of instruction-following. We then demonstrate that by conditioning on the positions and weights of grounded instruction as input, the agent can be self-monitoring of its progress and further ensure that the textual grounding accurately reflects the progress made.Overall, we propose a novel self-monitoring agent for VLN and make the following contributions: (1) We introduce the visual-textual co-grounding module, which performs grounding interdependently across both visual and textual modalities. We show that it can outperform the baseline method by a large margin. (2) We propose to equip the self-monitoring agent with a progress monitor, and for navigation tasks involving instructions instantiate this by introducing a new objective function for training. We demonstrate that, unlike the baseline method, the position of grounded instruction can follow both past and future instructions, thereby tracking progress to the goal. (3) With the proposed self-monitoring agent, we set the new state-of-the-art performance on both seen and unseen environments on the standard benchmark. With 8% absolute improvement in success rate on the unseen test set, we are ranked #1 on the challenge leaderboard. We introduce a self-monitoring agent which consists of two complementary modules: visual-textual co-grounding module and progress monitor. The visual-textual co-grounding module locates the instruction completed in the past, the instruction needed in the next action, and the moving direction from surrounding images. The progress monitor regularizes and ensures the grounded instruction correctly reflects the progress towards the goal by explicitly estimating the completeness of instruction-following. This estimation is conditioned on the positions and weights of grounded instruction. Our approach sets a new state-of-the-art performance on the standard Room-to-Room dataset on both seen and unseen environments. While we present one instantiation of self-monitoring for a decision-making agent, we believe that this concept can be applied to other domains as well. BID46 , and Speaker-Follower BID19 . *: with data augmentation. TAB4 . We can see that our proposed method outperformed existing approaches with a large margin on both validation unseen and test sets. Our method with greedy decoding for action selection improved the SR by 9% and 8% on validation unseen and test set. When using progress inference for action selection, the performance on the test set significantly improved by 5% compared to using greedy decoding, yielding 13% improvement over the best existing approach. Network architecture. The embedding dimension for encoding the navigation instruction is 256. We use a dropout layer with ratio 0.5 after the embedding layer. We then encode the instruction using a regular LSTM, and the hidden state is 512 dimensional. The MLP g used for projecting the raw image feature is BN − → F C − → BN − → Dropout − → ReLU . The FC layer projects the 2176-d input vector to a 1024-d vector, and the dropout ratio is set to be 0.5. The hidden state of the LSTM used for carrying the textual and visual information through time in Eq. 1 is 512. We set the maximum length of instruction to be 80, thus the dimension of the attention weights of textual grounding α t is also 80. The dimension of the learnable matrices from Eq. 2 to 5 are: DISPLAYFORM0 DISPLAYFORM1 closest previous trajectory, so that when a single agent traverses through all recorded trajectories, the overhead for switching from one trajectory to another can be reduced significantly. The final selected trajectory from beam search is then lastly logged to the trajectory. This therefore yields exactly the same success rate and navigation error, as the metrics are computed according to the last viewpoint from a trajectory.",We propose a self-monitoring agent for the Vision-and-Language Navigation task.,VLN ; two ; Code ; third ; first ; SR ; FC ; Eq,VLN ; two ; Code ; third ; first ; SR ; FC ; Eq ; (VLN ; an agent,"The Vision-and-Language Navigation (VLN) task involves an agent following navigational instruction in photo-realistic unknown environments. This task requires the agent to be aware of which instruction was completed, which instruction is needed next, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate instruction completed in the past, the instruction required for the next action, and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/t5-base_cs_KWplus,kwplus
