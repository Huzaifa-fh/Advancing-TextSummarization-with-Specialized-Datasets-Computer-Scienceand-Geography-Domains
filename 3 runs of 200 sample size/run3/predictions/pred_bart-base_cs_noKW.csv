Text,Abstractive,topics_kw,topics_kwplus,topics_prefix,prediction,model_dir,kw_variant
"This paper is concerned with the robustness of VAEs to adversarial attacks. We highlight that conventional VAEs are brittle under attack but that methods recently introduced for disentanglement such as β-TCVAE (Chen et al., 2018) improve robustness, as demonstrated through a variety of previously proposed adversarial attacks (Tabacof et al. (2016); Gondim-Ribeiro et al. (2018); Kos et al.(2018)). This motivated us to develop Seatbelt-VAE, a new hierarchical disentangled VAE that is designed to be significantly more robust to adversarial attacks than existing approaches, while retaining high quality reconstructions. Unsupervised learning of disentangled latent variables in generative models remains an open research problem, as is an exact mathematical definition of disentangling (Higgins et al., 2018) . Intuitively, a disentangled generative model has a one-to-one correspondence between each input dimension of the generator and some interpretable aspect of the data generated. For VAE-derived models (Kingma & Welling, 2013; Rezende et al., 2014) this is often based around rewarding independence between latent variables. Factor VAE (Kim & Mnih, 2018) , β-TCVAE (Chen et al., 2018) and HFVAE (Esmaeili et al., 2019) have shown that the evidence lower bound can be decomposed to obtain a term capturing the degree of independence between latent variables of the model, the total correlation. By up-weighting this term, we can obtain better disentangled representations under various metrics compared to β-VAEs (Higgins et al., 2017a) . Disentangled representations, much like PCA or factor analysis, are not only human-interpretable but also offer more informative and robust latent space representations. In addition, information theoretic interpretations of deep learning show that having a disentangled hidden layer within a discriminative deep learning model increases robustness to adversarial attack (Alemi et al., 2017) . Adversarial attacks on deep generative models, more difficult than those on discriminative models (Tabacof et al., 2016; Gondim-Ribeiro et al., 2018; Kos et al., 2018) , attempt to fool a model into reconstructing a chosen target image by adding distortions to the original input image. Generally, the most effective attack mode involves making the latent-space representation of the distorted input match that of the target image (Gondim-Ribeiro et al., 2018; Kos et al., 2018) . This kind of attack is particularly relevant to applications where the encoder's output is used downstream. Projections of data from VAEs, disentangled or not, are used for tasks such as: text classification (Xu et al., 2017) ; discrete optimisation (Kusner et al., 2017) ; image compression (Theis et al., 2017; Townsend et al., 2019) ; and as the perceptual part of a reinforcement learning algorithm (Ha & Schmidhuber, 2018; Higgins et al., 2017b) , the latter of which uses a disentangled VAE's encoder to improve the robustness of the agent to domain shift. Here we demonstrate that β-TCVAEs are significantly more robust to 'latent-space' attack than standard VAEs, and are generally more robust to attacks that act to maximise the evidence lower bound for the adversarial input. The robustness of these disentangled models is highly relevant because of the use-cases for VAEs highlighted above. However, imposing additional disentangling constraints on a VAE training objective degrades the quality of resulting drawn or reconstructed images (Higgins et al., 2017a; Chen et al., 2018) . We sought whether more powerful, expressive models, can help ameliorate this and in doing so built Figure 1: Latent-space adversarial attacks on Chairs, 3D Faces and CelebA for different models, including our proposed Seatbelt-VAE. β = 10 for β-TCVAE, β-TCDLGM and Seatbelt-VAE. L is the number of stochastic layers. Clockwise within each plot we show the initial input, its reconstruction, the adversarial input, the adversarial distortion added to make it (shown normalised), the adversarial input's reconstruction, and the target image. Following Tabacof et al. (2016) ; Gondim-Ribeiro et al. (2018) we attack with different degrees of penalisation on the magnitude of the adversarial distortion; in choosing the distortion to show, we pick the one with the penalisation that resulted in the value of the attack objective just better than the mean. See Section 5 for more details. a hierarchical disentangled VAE, Seatbelt-VAE, drawing on works like Ladder VAEs (Sønderby et al., 2016) and BIVA (Maaløe et al., 2019) . We demonstrate that Seatbelt-VAEs are more robust to adversarial attacks than β-TCVAEs and β-TCDLGMs (the latter a simple generalisation we make of β-TC penalisation to hierarchical VAEs). See Figure 1 for a demonstration. Rather than being concerned with human-interpretable controlled generation by our models, which has been the focus of much research into disentangling, instead we are interested in the robustness afforded by disentangled representations. Thus our key contributions are: • A demonstration that β-TCVAEs are significantly more robust to adversarial attacks via their latents than vanilla VAEs. • The introduction of Seatbelt-VAE, a hierarchical version of the β-TCVAE, designed to further increase robustness to various types of adversarial attack, while also giving better perceptual quality of reconstructions even when regularised. We have presented the increases in robustness to adversarial attack afforded by β-TCVAEs. This increase in robustness is strongest for attacks via the latent space. While disentangled models are often motivated by their ability to provide interpretable conditional generation, many use cases for VAEs centre on the learnt latent representation of data. Given the use of these representations as inputs for other tasks, the latent attack mode is the most important to protect against. Recent work by Shamir et al. (2019) gives a constructive proof for the existence of adversarial inputs for deep neural network classifiers with small Hamming distances. The proof holds with deterministic defence procedures that work as additional deterministic layers of the networks, and in the presence of adversarial training (Szegedy et al., 2014; Ganin et al., 2016; Tramèr et al., 2018; Shaham et al., 2018) . Shamir et al. (2019) thus give a theoretical grounding for using stochastic methods to defend against adversarial inputs. As VAEs are already used to defend deep net classifiers (Schott et al., 2019; Ghosh et al., 2019) , more robust VAEs, like β-TCVAEs, could find use in this area. We introduce Seatbelt-VAE, a particular hierarchical VAE disentangled on the top-most layer with skip connections down to the decoder. This model further increases robustness to adversarial attacks, while also increasing the quality of reconstructions. The performance of our model under adversarial attack to robustness is mirrored in robustness to uncorrelated noise: these models are effective denoising autoencoders as well. We hope this work stimulates further interest in defending and attacking VAEs. (5) cf. Eq (7) in the main paper. The likelihood is conditioned on all z layers: Now we have: Apply βTC decomposition to T as in Chen et al. (2018) . j indexes over units in z L . (A.14) for our chosen generative model. As in Chen et al. (2018), we choose to weight T b , the total correlation for q φ (z L ), by a prefactor β. Giving us the ELBO for Seatbelt-VAEs, Eq (10).","We show that disentangled VAEs are more robust than vanilla VAEs to adversarial attacks that aim to trick them into decoding the adversarial input to a chosen target. We then develop an even more robust hierarchical disentangled VAE, Seatbelt-VAE.",Kim & Mnih ; Shaham ; Alemi ; Ganin ; Rezende ; Chairs ; Kusner ; L ; Ha & Schmidhuber ; Xu et al.,Higgins et al ; a VAE training objective degrades ; discrete optimisation ; the one ; various metrics ; The likelihood ; deep net classifiers ; This paper ; all z layers ; a discriminative deep learning model,Kim & Mnih ; Shaham ; Alemi ; Ganin ; Rezende ; Chairs ; Kusner ; L ; Ha & Schmidhuber ; Xu et al.,"The robustness of VAEs to adversarial attacks has been enhanced by the introduction of new disentanglement methods, such as β-TCVAE (Chen et al., 2018) and HFVAE. This led to Seatbelt-VAE, a hierarchical disentangled VAE designed to be significantly more robust to the adversarial attack, while retaining high quality reconstructions. However, unsupervised learning of latent variables in generative models remains an open research problem, as is an exact mathematical definition of disentangling (Higgins et al, 2018). In addition, information theoretic interpretations of deep",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"In an explanation generation problem, an agent needs to identify and explain the reasons for its decisions to another agent. Existing work in this area is mostly confined to planning-based systems that use automated planning approaches to solve the problem. In this paper, we approach this problem from a new perspective, where we propose a general logic-based framework for explanation generation. In particular, given a knowledge base $KB_1$ that entails a formula $\phi$ and a second knowledge base $KB_2$ that does not entail $\phi$, we seek to identify an explanation $\epsilon$ that is a subset of $KB_1$ such that the union of $KB_2$ and $\epsilon$ entails $\phi$. We define two types of explanations, model- and proof-theoretic explanations, and use cost functions to reflect preferences between explanations. Further, we present our algorithm implemented for propositional logic that compute such explanations and empirically evaluate it in random knowledge bases and a planning domain. With increasing proliferation and integration of AI systems in our daily life, there is a surge of interest in explainable AI, which includes the development of AI systems whose actions can be easily understood by humans. Driven by this goal, machine learning (ML) researchers have begun to classify commonly used ML algorithms according to different dimensions of explainability (Guidotti et al. 2018) ; improved the explainability of existing ML algorithms BID3 BID0 BID3 ; as well as proposed new ML algorithms that trade off accuracy for increasing explainability (Dong et al. 2017; BID1 . 1 While the term interpretability is more commonly used in the ML literature and can be used interchangeably with explainability, we use the latter term as it is more commonly used broadly across different subareas of AI.In contrast, researchers in the automated planning community have mostly taken a complementary approach. While there is some work on adapting planning algorithms to find easily explainable plans 2 (i.e., plans that are easily understood and accepted by a human user) BID0 , most work has focused on the explanation generation problem (i.e., the problem of identifying explanations of plans found by planning agents that when presented to users, will allow them to understand and accept the proposed plan) (Langley 2016; Kambhampati 1990) . Within this context, researchers have tackled the problem where the model of the human user may be (1) inconsistent with the model of the planning agent (Chakraborti et al. 2017b) ; (2) must be learned BID0 ; and (3) a different form or abstraction than that of the planning agent BID0 Tian et al. 2016) . However, a common thread across most of these works is that they, not surprisingly, employ mostly automated planning approaches. For example, they often assume that the models of both the agent and human are encoded in PDDL format.In this paper, we approach the explanation generation problem from a different perspective -one based on knowledge representation and reasoning (KR). We propose a general logic-based framework for explanation generation, where given a knowledge base KB 1 (of an agent) that entails a formula φ and a knowledge base KB 2 (of a human user) that does not entail φ, the goal is to identify an explanation ⊆ KB 1 such that KB 2 ∪ entails φ. We define two types of explanations, model-and proof-theoretic explanations, and use cost functions to reflect preferences between explanations. Further, we present an algorithm, implemented for propositional logic, that computes such explanations and evaluate its performance experimentally in random knowledge bases as well as in a planning domain.In addition to providing an alternative approach to solve the same explanation generation problem tackled thus far by the automated planning community, our approach has the merit of being more generalizable to other problems beyond planning problems as long as they can be modeled using a logical KR language. There is a very large body of work related to the very broad area of explainable AI. We have briefly discussed some of them from the ML literature in Section . We refer readers to surveys by BID0 and (Dosilovic et al. 2018 ) for more in-depth discussions of this area. We focus below on related work from the KR and planning literature only since we employ KR techniques to solve explainable planning problems in this paper.Related Work from the KR Literature: We note that the notion of an explanation proposed in this paper might appear similar to the notion of a diagnosis that has been studied extensively in the last several decades (e.g., (Reiter 1987)) as both aim at explaining something to an agent. Diagnosis focuses on identifying the reason for the inconsistency of a theory whereas an mor p-explanation aims at identifying the support for a formula. The difference lies in that a diagnosis is made with respect to the same theory and m-or p-explanation is sought for the second theory.Another earlier research direction that is closely related to the proposed notion of explanation is that of developing explanation capabilities of knowledge-based systems and decision support systems, which resulted in different notions of explanation such as trace, strategic, deep, or reasoning explanations (see review by BID3 for a discussion of these notions). All of these types of explanations focus on answering why certain rules in a knowledge base are used and how a conclusion is derived. This is not our focus in this paper. The present development differs from earlier proposals in that m-or p-explanations are identified with the aim of explaining a given formula to a second theory. Furthermore, the notion of an optimal explanation with respect to the second theory is proposed.There have been attempts to using argumentation for explanation (Cyras et al. 2017; Cyras et al. 2019) because of the close relation between argumentation and explanation. For example, argumentation was used by (Cyras et al. 2019) to answer questions such as why a schedule does (does not) satisfy a criteria (e.g., feasibility, efficiency, etc.); the approach was to develop for each type of inquiry, an abstract argumentation framework (AF) that helps explain the situation by extracting the attacks (non-attacks) from the corresponding AF. Our work differs from these works in that it is more general and does not focus on a specific question.It is worth to pointing out that the problem of computing a most preferred explanation for ϕ from KB 1 to KB 2 might look similar to the problem of computing a weakest sufficient condition of ϕ on KB 1 under KB 2 as described by BID3 . As it turns out, the two notions are quite different. Given that KB 1 = {p, q} and KB 2 = {p}. It is easy to see that q is the unique explanation for q from KB 1 to KB 2 . On the other hand, the weakest sufficient condition of q on KB 1 under KB 2 is ⊥ (Proposition 8, BID3 ).Related Work from the Planning Literature: In human-aware planning, the (planning) agent must have knowledge of the human model in order to be able to contemplate the goals of the humans as well as foresee how its plan will be perceived by them. This is of the highest importance in the context of explainable planning since an explanation of a plan cannot be onesided (i.e., it must incorporate the human's beliefs of the planner). In a plan generation process, a planner performs argumentation over a set of different models (Chakraborti et al. 2017a ); these models usually are the model of the agent incorporating the planner, the model of the human in the loop, the model the agent thinks the human has, the model the human thinks the agent has, and the agent's approximation of the latter.Therefore, the necessity for plan explanations arises when the model of the agent and the model the human thinks the agent has diverge so that the optimal plans in the agent's model are inexplicable to the human. During a collaborative activity, an explainable planning agent BID1 ) must be able to account for such model differences and maintain an explanatory dialogue with the human so that both of them agree on the same plan. This forms the nucleus of explanation generation of an explainable planning agent, and is referred to as model reconciliation (Chakraborti et al. 2017b) . In this approach , the agent computes the optimal plan in terms of his model and provides an explanation of that plan in terms of model differences. Essentially, these explanations can be viewed as the agent's attempt to move the human's model to be in agreement with its own. Further, for computing explanations using this approach the following four requirements are considered:• Completeness -No better solution exists. This is achieved by enforcing that the plan being explained is optimal in the updated human model.• Conciseness -Explanations should be easily understandable to the human.• Monotonicity -The remaining model differences cannot change the completeness of an explanation.• Computability -Explanations should be easy to compute (from the agent's perspective).As our work is motivated by these ideas , we now identify some similarities and connections with our proposed approach. First, it is easy to see that we implicitly enforce the first three requirements when computing an explanation -the notions of completeness and conciseness are captured through the use of our cost functions. We do not claim to satisfy the computability requirement as it is more subjective and is more domain dependent.In a nutshell, the model reconciliation approach works by providing a model update such that the optimal plan is feasible and optimal in the updated model of the human. This is similar to our definition of the explanation generation problem where we want to identify an explanation ⊆ KB 1 (i.e., a set of formulae) such that KB 2 ∪ |= φ. In addition, the ⊆-minimal support in Definition 1 is equivalent to minimally complete explanations (MCEs) (the shortest explanation). The -general support can be viewed as similar to the minimally monotonic explanations (MMEs) (the shortest explanation such that no further model updates invalidate it), with the only difference being that in the general support scenario, the explanations are such that all subsuming are also valid supports.In contrast, model patch explanations (MPEs) (includes all the model updates) are trivial explanations and are equivalent to our definition that KB 1 itself serves as an m-explanation for KB 2 . Note that, in our approach, we do not allow for explanations on ""mistaken"" expectations in the human model, as it can be inferred from Proposition 1 (monotonic language L). From the model reconciliation perspective, such restriction is relaxed and allowed. However, a similar property can be seen if the mental model is not known and, therefore, by taking an ""empty"" model as starting point explanations can only add to the human's understanding but not mend mistaken ones. Explanation generation is an important problem within the larger explainable AI thrust. Existing work on this problem has been done in the context of automated planning domains, where researchers have primarily employed, unsurprisingly, automated planning approaches. In this paper, we approach the problem from the perspective of KR, where we propose a general logic-based framework for explanation generation. We further define two types of explanations, model-and proof-theoretic explanations, and use cost functions to reflect preferences between explanations. Our empirical results with algorithms implemented for propositional logic on both random knowledge bases as well as a planning domain demonstrate the generality of our approach beyond planning problems. Future work includes investigating more complex scenarios, such as one where an agent needs to persuade another that its knowledge base is incorrect.",A general framework for explanation generation using Logic.,Guidotti ; ML ; model- ; Cyras ; p ; the KR Literature ; four ; Langley ; the last several decades ; First,Section ; connections ; the ML literature ; the human's understanding ; the minimally monotonic explanations ; both random knowledge bases ; PDDL format ; these ideas ; an explainable planning agent ; a human user,Guidotti ; ML ; model- ; Cyras ; p ; the KR Literature ; four ; Langley ; the last several decades ; First,"In an explanation generation problem, an agent needs to identify and explain the reasons for its decisions to another agent. However, this approach is limited to planning-based systems that use automated planning approaches to solve the problem. In this paper, we define two types of explanations, model- and proof-theoretic explanations, and use cost functions to reflect preferences between explanations. In contrast, researchers in the automated planning community have adopted a complementary approach. For example, when a knowledge base $KB_1$ entails a formula $\phi$ and a second knowledge base (KB_2$ entails $\epsilon) with a union",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Across numerous applications, forecasting relies on numerical solvers for partial differential equations (PDEs). Although the use of deep-learning techniques has been proposed, the uses have been restricted by the fact the training data are obtained using PDE solvers. Thereby, the uses were limited to domains, where the PDE solver was applicable, but no further. 

 We present methods for training on small domains, while applying the trained models on larger domains, with consistency constraints ensuring the solutions are physically meaningful even at the boundary of the small domains. We demonstrate the results on an air-pollution forecasting model for Dublin, Ireland. Solving partial differential equations (PDEs) underlies much of applied mathematics and engineering, ranging from computer graphics and financial pricing, to civil engineering and weather prediction. Conventional approaches to prediction in PDE models rely on numerical solvers and require substantial computing resources in the model-application phase. While in some application domains, such as structural engineering, the longer run-times may be acceptable, in domains with rapid decay of value of the prediction, such as weather forecasting, the run-time of the solver is of paramount importance.In many such applications, the ability to generate large volumes of data facilitates the use of surrogate or reduced-order models BID3 obtained using deep artificial neural networks BID11 . Although the observation that artificial neural networks could be applied to physical models is not new BID14 BID15 BID16 BID25 BID9 BID20 BID26 BID16 BID27 , and indeed, it is seen as one of the key trends BID2 BID13 BID31 on the interface of applied mathematics, data science, and deep learning, their applications did not reach the level of success observed in the field of the image classification, speech recognition, machine translation, and other problems processing unstructured high-dimensional data, yet. A key issue faced by applications of deep-learning techniques to physical models is their scalability.Even very recent research on deep-learning for physical models BID32 BID12 BID34 ) uses a solver for PDEs to obtain hundreds of thousands of outputs. The deep learning can then be seen as means of non-linear regression between the inputs and outputs. For example, BID12 have recently observed a factor of 12,000 computational speedup compared to that of a leading solver for the PDE, on the largest domain they were able to work with. Considering the PDE solver is used to generate the outputs to train the deep-learning model on, however, the deep-learning model is limited to the domain and application that it is trained on.We present methods for training Deep Neural Networks (DNNs) on small domains, while applying the trained models on larger domains, with consistency constraints ensuring the solutions are physically meaningful even at the boundaries of the small domains. Our contributions are as follows:• definition of the consistency constraints, wherein the output for one (tile of a) mesh is used to constrain the output for another (tile of a) mesh.• methods for applying the consistency constraints within the training of a DNN, which allows for an increase in the extent of the spatial domain by concatenating the outputs of several PDE-based models by considering boundary conditions and state at the boundary.• a numerical study of the approach on a pollution-forecasting problem, wherein we lose accuracy from 1 to 7 per cent compared to the unconstrained model, but remove boundary artefacts.We note that the methods can be applied both in terms of ""patching"" multiple (tiles of a) meshes, and in terms of ""zooming"" in multi-resolution approaches, where lower-resolution (e.g., city-, countryscale) component constrains higher-resolution components (e.g., district-, city-scale), which in turn impose consistency constraints on the former. We have presented consistency constraints, which make it possible to train DNN on small domains and apply the trained models to larger domains while allowing incorporation of information external to the domain. The consistency constraints will ensure the solutions are physically meaningful even at the boundary of the small domains in the output of the DNN. We have demonstrated promising results on an air-pollution forecasting model for Dublin, Ireland.The work is a first that makes possible numerous extensions. First, one could consider further applications of the consistency constraints, e.g., in energy conservation, or in consider merging the outputs of a number of PDE models within multi-physics applications. Second, in some applications, in may be useful to explore other network topologies. Following BID34 , one could use long short-term memory (LSTM) units. Further, over-fitting control could be based on an improved stacked auto-encoder architecture BID35 . In interpretation of the trained model, the approach of BID7 may be applicable.Our work could also be seen as an example of Geometric Deep Learning BID5 , especially in conjunction with the use of mesh-free methods BID28 , such as the 3D point clouds BID23 , non-uniform meshing, or non-uniform choice of receptors within the meshes. Especially for applications, where the grids are in 3D or higher dimensions, the need for such techniques is clear. More generally, one could explore links to isogeometric analysis of BID6 , which integrates solving PDEs with geometric modelling.Finally, one could generalise our methods in a number of directions of the multi-fidelity modelling, e.g., by combining the reduced-order and full-order models using adaptation, fusion, or filtering. Overall, the scaling up of deep learning for PDE-based models seems to be a particular fruitful area for further research.Within the domain of our example application, recent surveys BID2 suggest that ours is the first use of deep learning in the forecasting of air pollution levels. Following the copious literature on PDE-based models of air pollution, one could consider further pollutants such as ground-level ozone concentrations BID18 , and ensemble BID17 or multi-fidelity methods. One may also consider a joint model, allowing for traffic forecasting, weather forecasting, and air pollution forecasting, within the same network, possibly using LSTM units BID7 , at the same time.","We present RNNs for training surrogate models of PDEs, wherein consistency constraints ensure the solutions are physically meaningful, even when the training uses much smaller domains than the trained model is applied to.",PDE ; Dublin ; Ireland ; obtain hundreds of thousands ; Deep Neural Networks ; city- ; first ; Second ; Geometric Deep Learning ; One,the meshes ; multi-physics applications ; PDE models ; links ; surrogate or reduced-order models ; the level ; DNN ; the approach ; turn ; the longer run-times,PDE ; Dublin ; Ireland ; obtain hundreds of thousands ; Deep Neural Networks ; city- ; first ; Second ; Geometric Deep Learning ; One,"The use of deep-learning techniques for partial differential equations (PDEs) has been restricted by the fact the training data are obtained using PDE solvers. These methods are limited to domains where the PDE Solver was applicable, with consistency constraints ensuring the solutions are physically meaningful even at the boundary of the small domains. The results on an air-pollution forecasting model for Dublin, Ireland illustrate the importance of numerical solvers in PDE models. In many applications, the ability to generate large volumes of data facilitates the use of surrogate or reduced-order models BID3 obtained using deep artificial neural networks BID11.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Electronic Health Records (EHR) comprise of longitudinal clinical observations portrayed with sparsity, irregularity, and high-dimensionality which become the major obstacles in drawing reliable downstream outcome. Despite greatly numbers of imputation methods are being proposed to tackle these issues, most of the existing methods ignore correlated features or temporal dynamics and entirely put aside the uncertainty. In particular, since the missing values estimates have the risk of being imprecise, it motivates us to pay attention to reliable and less certain information differently. In this work, we propose a novel variational-recurrent imputation network (V-RIN), which unified imputation and prediction network, by taking into account the correlated features, temporal dynamics, and further utilizing the uncertainty to alleviate the risk of biased missing values estimates. Specifically, we leverage the deep generative model to estimate the missing values based on the distribution among variables and a recurrent imputation network to exploit the temporal relations in conjunction with utilization of the uncertainty. We validated the effectiveness of our proposed model with publicly available real-world EHR dataset, PhysioNet Challenge 2012, and compared the results with other state-of-the-art competing methods in the literature. Electronic Health Records (EHR) store longitudinal data comprising of patient's clinical observations in the intensive care unit (ICU). Despite the surge of interest in clinical research on EHR, it still holds diverse challenging issues to be tackled with, such as high-dimensionality, temporality, sparsity, irregularity, and bias (Cheng et al., 2016; Lipton et al., 2016; Yadav et al., 2018; Shukla & Marlin, 2019) . Specifically, sequences of the medical events are recorded irregularly in terms of variables and time, due to various reasons such as lack of collection or documentation, or even recording fault (Wells et al., 2013; Cheng et al., 2016) . In fact, since it carries essential information regarding the patient's health status, improper handling of missing values might draw an unintentional bias (Wells et al., 2013; Beaulieu-Jones et al., 2017) yielding unreliable downstream analysis and verdict. Complete-case analysis is one approach to draw the clinical outcome by disregarding the missing values and relying only on the observed values. However, excluding the missing data shows poor performance at high missing rates and also requires modeling separately for different dataset. In fact, the missing values and their patterns are correlated with the target labels (Che et al., 2018) . Thus, we resort to the imputation approach to improve clinical outcomes prediction as the downstream task. There exist numerous proposed strategies in imputing missing values in the literature. Brick & Kalton (1996) classified the imputation methods of being deterministic or stochastic in terms of the utilization of the randomness. While deterministic methods such as mean (Little & Rubin, 1987) and median filling (Acuña & Rodriguez, 2004) produced only one possible value, it is desirable to generate samples by considering the data distribution, thus leading to stochastic-based imputation methods. Moreover, since we are dealing with multivariate time series, an adequate imputation model should reflect several properties altogether, namely, 1) temporal relations, 2) correlations across variables, and additionally 3) offering a probabilistic interpretation for uncertainty estimation (Fortuin et al., 2019) . Recently, the rise of the deep learning models offers potential solutions in accommodating aforementioned conditions. Variational autoencoders (VAEs) (Kingma & Welling, 2014) and generative adversarial networks (GANs) (Goodfellow et al., 2014) exploited the latent distribution of highdimensional incomplete data and generated comparable data points as the approximation estimates for the missing or corrupted values (Nazabal et al., 2018; Luo et al., 2018; Jun et al., 2019) . However, even though these models employed the stochastic approach in inferring and generating samples, they scarcely utilized the uncertainty. In addition, such deep generative models are insufficient in estimating the missing values of multivariate time series, due to their nature of ignoring temporal relations between a span of time points. Hence, it requires additional approaches to model the temporal dynamics, such as Gaussian process (Fortuin et al., 2019) or recurrent neural network (RNNs) (Luo et al., 2018; Jun et al., 2019) . On the other hand, by the virtue of RNNs which have proved a remarkable performance in modeling the sequential data, we can estimate the complete data by taking into account the temporal characteristics. GRU-D (Che et al., 2018) proposed a modified gated-recurrent unit (GRU) cell to model missing patterns in the form of masking vector and temporal delay. Likewise, BRITS (Cao et al., 2018) modeled the temporal relations by bi-directional dynamics, and also considered features correlation by regression layers in estimating the missing values. However, they didn't take into account the uncertainty in estimating the missing values. That is, since the imputation estimates are not thoroughly accurate, we may introduce their fidelity score denoted by the uncertainty, which enhances the task performance by emphasizing the reliable or less uncertain information and vice versa (He, 2010; Gemmeke et al., 2010; Jun et al., 2019) . In this work, we define our primary task as prediction of in-hospital mortality on EHR data. However, since the data are characterized by sparse and irregularly-sampled, we devise an effective imputation model as the secondary problem but major concern in this work. We propose a novel variational-recurrent imputation network (V-RIN), which unified imputation and prediction network for multivariate time series EHR data, governing both correlations among variables and temporal relations. Specifically, given the sparse data, an inference network of VAE is employed to capture data distribution in the latent space. From this, we employ a generative network to obtain the reconstructed data as the imputation estimates for the missing values as well as the uncertainty indicating the imputation fidelity score. Then, we integrate the temporal and feature correlations into a combined vector and feed it into a novel uncertainty-aware GRU in the recurrent imputation network. Finally, we obtain the mortality prediction as a clinical verdict from the complete imputed data. In general, our main contributions in this paper are as follows: • We estimate the missing values by utilizing deep generative model combined with recurrent imputation network to capture both features correlations and the temporal dynamics jointly, yielding the uncertainty. • We effectively incorporate the uncertainty with the imputation estimates in our novel uncertainty-aware GRU cell for better prediction result. • We evaluated the effectiveness of the proposed models by training the imputation and prediction networks jointly using the end-to-end manner, achieving the superior performance among other state-of-the-art competing methods on real-world multivariate time series EHR data. In this paper, we proposed a novel unified framework comprising of imputation and prediction network for sparse high-dimensional multivariate time series. It combined deep generative model with recurrent model to capture features correlations and temporal relations in estimating the missing values and yielding uncertainty. We utilized the uncertainties as the fidelity of our estimation and incorporated them for clinical outcome prediction. We evaluated the effectiveness of proposed model with PhysioNet 2012 Challenge dataset as the real-world EHR multivariate time series data, proving the superiority of our model in the in-mortality prediction task, compared to other state-of-the-art comparative models in the literature.","Our variational-recurrent imputation network (V-RIN) takes into account the correlated features, temporal dynamics, and further utilizes the uncertainty to alleviate the risk of biased missing values estimates.",Luo et al. ; Goodfellow et al. ; Yadav ; Gemmeke ; Wells et al. ; Cheng ; Kingma & Welling ; only one ; Fortuin et al. ; secondary,the deep learning models ; uncertainty estimation ; the downstream task ; they ; et al ; uncertainty ; a clinical verdict ; numbers ; the uncertainties ; documentation,Luo et al. ; Goodfellow et al. ; Yadav ; Gemmeke ; Wells et al. ; Cheng ; Kingma & Welling ; only one ; Fortuin et al. ; secondary,"Electronic Health Records (EHR) comprise longitudinal clinical observations portrayed with sparsity, irregularity, and high-dimensionality which become the major obstacles in drawing reliable downstream outcome. However, most imputation methods ignore correlated features or temporal dynamics and focus on the missing values estimates. In this work, we propose a novel variational-recurrent imputation network (V-RIN), which unified imputation and prediction networks, by taking into account correlated features, temporal dynamics, and further utilizing the uncertainty to alleviate the risk of biased missing values. The deep generative model, which utilizes the temporal relations and recurrent imputation networks",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"In this paper, we study the learned iterative shrinkage thresholding algorithm (LISTA) for solving sparse coding problems.   Following assumptions made by prior works, we first discover that the code components in its estimations may be lower than expected, i.e., require gains, and to address this problem, a gated mechanism amenable to theoretical analysis is then introduced. Specific design of the gates is inspired by convergence analyses of the mechanism and hence its effectiveness can be formally guaranteed. In addition to the gain gates, we further introduce overshoot gates for compensating insufficient step size in LISTA. Extensive empirical results confirm our theoretical findings and verify the effectiveness of our method. Sparse coding serves as the foundation of many machine learning applications, e.g., the direction-ofarrival estimation (Xu et al., 2012) , signal denoising (Elad & Aharon, 2006) , and super resolution imaging (Yang et al., 2010) . In general, it aims to recover an inherently sparse vector x s ∈ R n from an observation y ∈ R m corrupted by a noise vector ε ∈ R m . That is, in which A ∈ R m×n is an over-complete basis matrix. The problem of recovering x s , however, is a challenging task, in which the main difficulties are to incorporate the sparse constraint which is nonconvex and to further determine the indices of its non-zero elements, i.e., the support of the vector. A reasonable solution to the problem is to use convex functions as surrogates to relax the constraint of sparsity, among which the most classical one probably is the l 1 -norm penalty. Such a problem is carefully studied in Lasso (Tibshirani, 1996) , and it can be solved via least angle regression (Efron et al., 2004) , the iterative shrinkage and thresholding algorithm (ISTA) (Daubechies et al., 2004) , etc. Despite the simplicity, these conventional solvers suffer from critical shortcomings. Taking ISTA as an example, we know that 1) it converges very slowly with only a sublinear rate (Beck & Teboulle, 2009) , 2) the correlation between each of the two columns of A should be relatively low. In recent years, deep learning (LeCun et al., 2015) methods have achieved remarkable successes. Deep neural networks (DNNs) have been proven both effective and efficient in dealing with many tasks, including image classification (He et al., 2016) , object detection (Girshick, 2015) , speech recognition (Hinton et al., 2012) , and also sparse coding (Gregor & LeCun, 2010; Borgerding et al., 2017; He et al., 2017; Zhang & Ghanem, 2018; Chen et al., 2018; Liu et al., 2019; Sulam et al., 2019) . The core idea behind deep learning-based sparse coding is to train DNNs to approximate the optimal sparse code. For instance, an initial work of Gregor and LeCun's (2010) takes the inspiration from ISTA and develops an approximator named learned ISTA (LISTA), which is structurally similar to a recurrent neural network (RNN). It has been demonstrated both empirically and theoretically that LISTA is superior to ISTA Moreau & Bruna, 2017; Giryes et al., 2018; Chen et al., 2018) . Nevertheless, it is also uncontroversial that there exists much room for further enhancing it. In this paper, we delve deeply into the foundation of (L)ISTA and discover possible weaknesses of LISTA. First and foremost, we know from prior arts (Chen et al., 2018; Liu et al., 2019) that LISTA tends to learn large enough biases to achieve no ""false positive"" in the support of generated codes and further ensure linear convergence, and we prove that this tendency, however, also makes the magnitude of the code components being lower than that of the ground-truth. That said, there probably exists a requirement of gains in the code estimations. Second, regarding the optimization procedure of ISTA as to minimize an upper bound of its objective function at each step, we conjecture that the element-wise update of (L)ISTA normally ""lags behind"" the optimal solution, which suggests that it requires overshoots to reach the optimum, just like what has been suggested in fast ISTA (FISTA) (Beck & Teboulle, 2009 ) and learned FISTA (LFISTA) (Moreau & Bruna, 2017) . In this paper, our main contributions are summarized as follows: • We discover weaknesses of LISTA by theoretically analyzing its optimization procedure, for mitigating which we introduce gain gates and overshoot gates, akin to update gate and reset gate mechanisms in the gated recurrent unit (GRU) Cho et al. (2014) . • We provide convergence analyses for LISTA (with or without gates), which further give rise to conditions on which the performance of our method with gain gates can be guaranteed. A practical case is considered, where the assumption of no ""false positive"" is relaxed. • Insightful expressions for the gates are presented. In comparison with state-of-the-art sparse coding networks (not limited to previous extensions to LISTA), our method achieves superior performance. It also applies to variants of LISTA, e.g., LFSITA (Moreau & Bruna, 2017) and ALISTA (Liu et al., 2019) . Notations: In this paper, unless otherwise clarified, vectors and matrices are denoted by lowercase and uppercase characters, respectively. For vectors/matrices originally introduced without any subscript, adding a subscript (e.g., i) indicates its element/column at the corresponding position. For instance, for x ∈ R n , x i represents the i-th element of the vector, and W :,i and W i,: denote the i-th column and row of a matrix W respectively. While for vectors introduced with subscripts already, e.g., x s , we use (x s ) i to denote its i-th element. The operator is used to indicate element-wise multiplication of two vectors. The support of a vector is denoted as supp(x) := {i|x i = 0}. We use sup xs as the simplified form of sup xs∈X (B,s,0) , see Assumption 1 for the definition of X (B, s, 0). In this paper, we study LISTA for solving sparse coding problems. We discover its potential weaknesses and introduce gated mechanisms to address them accordingly. In particular, we theoretically prove that LISTA with gain gates can achieve faster convergence than the standard LISTA. We also discover that LISTA (with or without gates) can obtain lower reconstruction errors under a weaker assumption of ""false positive"" in its code estimations. It helps us improve the convergence analyses to achieve more solid theoretical results, which have been perfectly confirmed in simulation experiments. The effectiveness of our introduced gates is verified in a variety of sparse coding experiments and the state-of-the-art performance is achieved. In the future, we aim to extend the method to convolutional neural networks to deal with more complex tasks. Before we delve deeply into the proof, we first give some importance notations. We define S as the support of the vector x s , i.e. S = supp(x s ), and let |S| denote the number of elements in the set S. For a vector that shares the same size with x s , say z, we denote by z S ∈ R |S| a vector that keeps the elements with indices of z in S and removes the others. If the vectors have been introduced with subscripts already, e.g. x s , we use (x s ) S to denote vectors obtained in such a manner. For a square matrix with the same number of row and column as the size of x s , say M , M (S, S) is its principal minor with the index set formed by removing rows and columns whose indices are not in S. Assume a vector x with no zero elements, sign(·) is defined as (sign(x)) i = x i /|x i |, i.e. (sign(x)) i = 1 when x i > 0, and (sign(x)) i = −1 when x i < 0.","We propose gated mechanisms to enhance learned ISTA for sparse coding, with theoretical guarantees on the superiority of the method.",Chen ; Gregor ; Lasso ; Zhang & Ghanem ; Efron et al. ; Cho et al ; ALISTA ; al. ; Liu et al. ; RNN,"sign ; generated codes ; such a manner ; i.e., the support ; denote vectors ; experiments ; large enough biases ; our main contributions ; Efron et al ; Hinton et al",Chen ; Gregor ; Lasso ; Zhang & Ghanem ; Efron et al. ; Cho et al ; ALISTA ; al. ; Liu et al. ; RNN,"The learned iterative shrinkage thresholding algorithm (LISTA) for solving sparse coding problems is based on assumptions made by prior works. To address this problem, a gated mechanism amenable to theoretical analysis is introduced. It is inspired by convergence analyses of the mechanism and hence its effectiveness can be formally guaranteed. In addition to the gain gates, we introduce overshoot gates for compensating insufficient step size in LISTA. Sparse coding serves as the foundation of many machine learning applications, e.g., direction-ofarrival estimation (Xu et al., 2012), signal denoising (Elad &",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"In this work we introduce a new framework for performing temporal predictions
 in the presence of uncertainty. It is based on a simple idea of disentangling com-
 ponents of the future state which are predictable from those which are inherently
 unpredictable, and encoding the unpredictable components into a low-dimensional
 latent variable which is fed into the forward model. Our method uses a simple su-
 pervised training objective which is fast and easy to train. We evaluate it in the
 context of video prediction on multiple datasets and show that it is able to consi-
 tently generate diverse predictions without the need for alternating minimization
 over a latent space or adversarial training. Learning forward models in time series is a central task in artificial intelligence, with applications in unsupervised learning, planning and compression. A major challenge in this task is how to handle the multi-modal nature of many time series. When there are multiple valid ways in which a time series can evolve, training a model using classical 1 or 2 losses produces predictions which are the average or median of the different outcomes across each dimension, which is itself often not a valid prediction.In recent years, Generative Adversarial Networks BID9 have been introduced, a general framework where the prediction problem is formulated as a minimax game between the predictor function and a trainable discriminator network representing the loss. By using a trainable loss function, it is in theory possible to handle multiple output modes since a generator which covers each of the output modes will fool the discriminator leading to convergence. However, a generator which covers a single mode can also fool the discriminator and converge, and this behavior of mode collapse has been widely observed in practice. Some workarounds have been introduced to resolve or partially reduce mode-collapsing, such as minibatch discrimination, adding parameter noise BID23 , backpropagating through the unrolled discriminator BID18 and using multiple GANs to cover different modes BID27 . However, many of these techniques can bring additional challenges such as added complexity of implementation and increased computational cost. The mode collapsing problem becomes even more pronounced in the conditional generation setting when the output is highly dependent on the context, such as video prediction BID12 .In this work, we introduce a novel architecture that allows for robust multimodal conditional predictions in time series data. It is based on a simple intuition of separating the future state into a deterministic component, which can be predicted from the current state, and a stochastic (or difficult to predict) component which accounts for the uncertainty regarding the future mode. By training a deterministic network, we can obtain this factorization in the form of the network's prediction together with the prediction error with respect to the true state. This error can be encoded as a lowdimensional latent variable which is fed into a second network which is trained to accurately correct the determinisic prediction by incorporating this additional information. We call this model the Error Encoding Network (EEN). In a nutshell , this framework contains three function mappings at each timestep: (i) a mapping from the current state to the future state, which separates the future state into deterministic and non-deterministic components; (ii) a mapping from the non-deterministic component of the future state to a low-dimensional latent vector; (iii) a mapping from the current state to the future state conditioned on the latent vector, which encodes the mode information of the future state. While the training procedure involves all these mappings, the inference phase involves only (iii).Both networks are trained end-to-end using a supervised learning objective and latent variables are computed using a learned parametric function, leading to easy and fast training. We apply this method to video datasets from games, robotic manipulation and simulated driving, and show that the method is able to consistently produce multimodal predictions of future video frames for all of them. Although we focus on video in this work, the method itself is general and can in principle be applied to any continuous-valued time series. In this work, we have introduced a new framework for performing temporal prediction in the presence of uncertainty by disentangling predictable and non-predictable components of the future state. It is fast, simple to implement and easy to train without the need for an adverserial network or al- ternating minimization, and does not require additional tuning to prevent mode collapse. We have provided one instantiation in the context of video prediction using convolutional networks, but it is in principle applicable to different data types and architectures. There are several directions for future work. Here, we have adopted a simple strategy of sampling uniformly from the z distribution without considering their possible dependence on the state x, and there are likely better methods. In addition, one advantage of our model is that it can extract latent variables from unseen data very quickly, since it simply requires a forward pass through a network. If latent variables encode information about actions in a manner that is easy to disentangle, this could be used to extract actions from large unlabeled datasets and perform imitation learning. Another interesting application would be using this model for planning and having it unroll different possible futures.",A simple and easy to train method for multimodal prediction in time series.,recent years ; fed ; second ; the Error Encoding Network ; three ; one,a forward pass ; convolutional networks ; the multi-modal nature ; this ; a trainable loss function ; practice ; all ; the inference phase ; future video frames ; predictions,recent years ; fed ; second ; the Error Encoding Network ; three ; one,"In this work, we introduce a new framework for performing temporal predictions in the presence of uncertainty. The framework is based on a simple idea of disentangling com-ponents of the future state which are predictable from those which are inherently unpredictable, and encoding the unpredictable components into a low-dimensional latent variable which is fed into the forward model. This approach is fast and easy to train, and it is able to consi-tently generate diverse predictions without the need for alternating minimization or adversarial training.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Deep networks were recently suggested to face the odds between accuracy (on clean natural images) and robustness (on adversarially perturbed images) (Tsipras et al., 2019). Such a dilemma is shown to be rooted in the inherently higher sample complexity (Schmidt et al., 2018) and/or model capacity (Nakkiran, 2019), for learning a high-accuracy and robust classifier. In view of that, give a classification task, growing the model capacity appears to help draw a win-win between accuracy and robustness, yet at the expense of model size and latency, therefore posing challenges for resource-constrained applications. Is it possible to co-design model accuracy, robustness and efficiency to achieve their triple wins? This paper studies multi-exit networks associated with input-adaptive efficient inference, showing their strong promise in achieving a “sweet point"" in co-optimizing model accuracy, robustness, and efficiency. Our proposed solution, dubbed Robust Dynamic Inference Networks (RDI-Nets), allows for each input (either clean or adversarial) to adaptively choose one of the multiple output layers (early branches or the final one) to output its prediction. That multi-loss adaptivity adds new variations and flexibility to adversarial attacks and defenses, on which we present a systematical investigation. We show experimentally that by equipping existing backbones with such robust adaptive inference, the resulting RDI-Nets can achieve better accuracy and robustness, yet with over 30% computational savings, compared to the defended original models.
 Deep networks, despite their high predictive accuracy, are notoriously vulnerable to adversarial attacks (Goodfellow et al., 2015; Biggio et al., 2013; Szegedy et al., 2014; Papernot et al., 2016) . While many defense methods have been proposed to increase a model's robustness to adversarial examples, they were typically observed to hamper its accuracy on original clean images. Tsipras et al. (2019) first pointed out the inherent tension between the goals of adversarial robustness and standard accuracy in deep networks, whose provable existence was shown in a simplified setting. theoretically quantified the accuracy-robustness trade-off, in terms of the gap between the risk for adversarial examples versus the risk for non-adversarial examples. It is intriguing to consider whether and why the model accuracy and robustness have to be at odds. demonstrated that the number of samples needed to achieve adversarially robust generalization is polynomially larger than that needed for standard generalization, under the adversarial training setting. A similar conclusion was concurred by Sun et al. (2019) in the standard training setting. Tsipras et al. (2019) considered the accuracy-robustness trade-off as an inherent trait of the data distribution itself, indicating that this phenomenon persists even in the limit of infinite data. Nakkiran (2019) argued from a different perspective, that the complexity (e.g. capacity) of a robust classifier must be higher than that of a standard classifier. Therefore, replacing a largercapacity classifier might effectively alleviate the trade-off. Overall, those existing works appear to suggest that, while accuracy and robustness are likely to trade off for a fixed classification model and on a given dataset, such trade-off might be effectively alleviated (""win-win""), if supplying more training data and/or replacing a larger-capacity classifier. On a separate note, deep networks also face the pressing challenge to be deployed on resourceconstrained platforms due to the prosperity of smart Internet-of-Things (IoT) devices. Many IoT applications naturally demand security and trustworthiness, e.g., , biometrics and identity verification, but can only afford limited latency, memory and energy budget. Hereby we extend the question: can we achieve a triple-win, i.e., , an accurate and robust classfier while keeping it efficient? This paper makes an attempt in providing a positive answer to the above question. Rather than proposing a specific design of robust light-weight models, we reduce the average computation loads by input-adaptive routing to achieve triple-win. To this end, we introduce the input-adaptive dynamic inference (Teerapittayanon et al., 2017; , an emerging efficient inference scheme in contrast to the (non-adaptive) model compression, to the adversarial defense field for the first time. Given any deep network backbone (e.g., , ResNet, MobileNet), we first follow (Teerapittayanon et al., 2017) to augment it with multiple early-branch output layers in addition to the original final output. Each input, regardless of clean or adversarial samples, adaptively chooses which output layer to take for its own prediction. Therefore, a large portion of input inferences can be terminated early when the samples can already be inferred with high confidence. Up to our best knowledge, no existing work studied adversarial attacks and defenses for an adaptive multi-output model, as the multiple sources of losses provide much larger flexibility to compose attacks (and therefore defenses), compared to the typical single-loss backbone. We present a systematical exploration on how to (white-box) attack and defense our proposed multi-output network with adaptive inference, demonstrating that the composition of multiple-loss information is critical in making the attack/defense strong. Fig. 1 illustrates our proposed Robust Dynamic Inference Networks (RDI-Nets). We show experimentally that the input-adaptive inference and multi-loss flexibility can be our friend in achieving the desired ""triple wins"". With our best defended RDI-Nets, we achieve better accuracy and robustness, yet with over 30% inference computational savings, compared to the defended original models as well as existing solutions co-designing robustness and efficiency (Gui et al., 2019; Guo et al., 2018) . The codes will be publicly released upon acceptance. Our proposed RDI-Net framework, a defended multi-output network enabling dynamic inference. Each image, being it clean or adversarially perturbed, adaptively picks one branch to exit. Intuition: Multi-Output Networks as Special Ensembles Our intuition on defending multioutput networks arises from the success of ensemble defense in improving both accuracy and robustness (Tramèr et al., 2018; Strauss et al., 2017) , which also aligns with the model capacity hypothesis (Nakkiran, 2019) . A general multi-output network could be decomposed by an ensemble of single-output models, with weight re-using enforced among them. It is thus more compact than an ensemble of independent models, and the extent of sharing weight calibrates ensemble diversity versus efficiency. Therefore, we expect a defended multi-output network to (mostly) inherit the strong accuracy/robustness of ensemble defense, while keeping the inference cost lower. Do ""Triple Wins"" Go Against the Model Capacity Needs? We point out that our seemingly ""free"" efficiency gains (e.g., not sacrificing TA/ATA) do not go against the current belief that a more accurate and robust classifier relies on a larger model capacity (Nakkiran, 2019) . From the visualization, there remains to be a portion of clean/adversarial examples that have to utilize the full inference to predict well. In other words, the full model capacity is still necessary to achieve our current TAs/ATAs. Meanwhile, just like in standard classification , not all adversarial examples are born equally. Many of them can be predicted using fewer inference costs (taking earlier exits). Therefore, RDI-Nets reduces the ""effective model capacity"" averaged on all testing samples for overall higher inference efficiency, while not altering the full model capacity. This paper targets to simultaneously achieve high accuracy and robustness and meanwhile keeping inference costs lower. We introduce the multi-output network and input-adaptive dynamic inference, as a strong tool to the adversarial defense field for the first time. Our RDI-Nets achieve the ""triple wins"" of better accuracy, stronger robustness, and around 30% inference computational savings. Our future work will extend RDI-Nets to more dynamic inference mechanisms, e.g., .","Is it possible to co-design model accuracy, robustness and efficiency to achieve their triple wins? Yes!","Gui et al. ; Papernot ; Goodfellow et al. ; Teerapittayanon et al. ; Strauss et al. ; Triple Wins"" Go Against the Model Capacity Needs ; Szegedy et al. ; Guo et al. ; Sun ; Multi-Output Networks as Special Ensembles Our","the input-adaptive dynamic inference ; it ; new variations ; a separate note ; the defended original models ; them ; ""Triple Wins ; input inferences ; other words ; an adaptive multi-output model","Gui et al. ; Papernot ; Goodfellow et al. ; Teerapittayanon et al. ; Strauss et al. ; Triple Wins"" Go Against the Model Capacity Needs ; Szegedy et al. ; Guo et al. ; Sun ; Multi-Output Networks as Special Ensembles Our","Deep networks face the challenges between accuracy and robustness due to their inherently higher sample complexity and model capacity. However, growing the model capacity is crucial to achieve a win-win between accuracy, robustness, and efficiency. The complexity of the dataset and the high-accuracy and robust classifier are crucial factors in the success of multi-exit networks associated with input-adaptive efficient inference. The resulting RDI-Nets can achieve better accuracy and efficiency, with over 30% computational savings, compared to defended original models.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"We explore the role of multiplicative interaction as a unifying framework to describe a range of classical and modern neural network architectural motifs, such as gating, attention layers, hypernetworks, and dynamic convolutions amongst others.
 Multiplicative interaction layers as primitive operations have a long-established presence in the literature, though this often not emphasized and thus under-appreciated. We begin by showing that such layers strictly enrich the representable function classes of neural networks. We conjecture that multiplicative interactions offer a particularly powerful inductive bias when fusing multiple streams of information or when conditional computation is required. We therefore argue that they should be considered in many situation where multiple compute or information paths need to be combined, in place of the simple and oft-used concatenation operation. Finally, we back up our claims and demonstrate the potential of multiplicative interactions by applying them in large-scale complex RL and sequence modelling tasks, where their use allows us to deliver state-of-the-art results, and thereby provides new evidence in support of multiplicative interactions playing a more prominent role when designing new neural network architectures. Much attention has recently turned toward the design of custom neural network architectures and components in order to increase efficiency, maximise performance, or otherwise introduce desirable inductive biases. While there have been a plethora of newer, intricate architectures proposed, in this work we train our sights instead on an older staple of the deep learning toolkit: multiplicative interactions. Although the term itself has fallen somewhat out of favour, multiplicative interactions have reappeared in a range of modern architectural designs. We start this work by considering multiplicative interactions as an object of study in their own right. We describe various formulations and how they relate to each other as well as connect more recent architectural developments (e.g. hypernetworks Ha et al. (2017) , dynamic convolutions Wu et al. (2019) ) to the rich and longer-standing literature on multiplicative interactions. We hypothesise that multiplicative interactions are suitable for representing certain meaningful classes of functions needed to build algorithmic operations such as conditional statements or similarity metrics, and more generally as an effective way of integrating contextual information in a network in a way that generalizes effectively. We show this empirically in controlled synthetic scenarios, and also demonstrate significant performance improvement on a variety of challenging, large-scale reinforcement learning (RL) and sequence modelling tasks when a conceptually simple multiplicative interaction module is incorporated. Such improvements are consistent with our hypothesis that the use of appropriately applied multiplicative interactions can provide a more suitable inductive bias over function classes leading to more data-efficient learning, better generalization, and stronger performance. We argue that these operations should feature more widely in neural networks in and of themselves, especially in the increasingly important setting of integrating multiple streams of information (including endogenously created streams e.g. in branching architectures). Our contributions are thus: (i) to re-explore multiplicative interactions and their design principles; (ii) to aid the community's understanding of other models (hypernetworks, gating, multiplicative RNNs) through them; (iii) to show their efficacy at representing certain solutions; and (iv) to empirically apply them to large scale sequence modeling and reinforcement learning problems, where we demonstrate state-of-the-art results. In this work we considered multiplicative interactions and various formulations thereof, connecting them to a variety of architectures, both older and modern, such as Hypernetworks, multplicative LSTMs or gating methods. We hypothesise that the ability of such networks to better represent a broader range of algorithmic primitives (e.g. conditional-statements or inner products) allows them to better integrate contextual or task-conditional information to fuse multiple stream of data. We first tested empirically this hypothesis in two controlled settings, in order to minimize the effect of confounding factors. We further show that we could match state-of-the-art methods on multiple domains with only LSTMs and multiplicative units. While we do not necessarily advocate for a specific instance of the above methods, we hope that this work leads to a broader understanding and consideration of such methods by practitioners, and in some cases replacing the standard practice of concatenation when using conditioning, contextual inputs, or additional sources of information. We believe there are many ways to explore this space of ideas more broadly, for instance looking at: the role of various approximations to these methods; ways to make their implementations more efficient; and their application to newer domains. Finally, while attention models use some of these multiplicative interactions, we hope that applying some of the lessons from this work (such as higher order interactions) will allow even greater integration of information in attention systems. Proof. Inclusion comes directly from the fact that if we split input into arbitrary parts [x; z] we get: which proves that H mlp ⊂ H mu . Thus, the only aspect of the theorm left to prove is that the inclusion is strict. Let us consider a 1D function x → x 2 , and for simplicity let x = z (a domain where context equals input). A single layer MLP with a single multiplicative unit can represent this function exactly, by using A = 0 and W = I, as then we obtain x T W x = x T x = x 2 . Since our function is positive, it is not affecting the multiplicative network output. For a regular MLP, let us first notice that we need at least one hidden layer, as otherwise MLP is a linear function and f is not. Lets denote by V, c and w, b weights and biases of second, and first layers respectively. Then we have to satisfy where g is transformation represented by all higher layers of the MLP (in particular if there are just 2 layers, then g(x) = x). Note that RHS is differentiable everywhere, while LHS is differentiable iff for each i and for each x we have w i x + b i = 0 (or f is independent from x, which x 2 does not satisfy). However, this is impossible, as if w i = 0, then we can always pick x = −b i /w i , and if all 2 , leading to a contradiction. Proof. Inclusion comes directly from the fact that only some activations are replaced, and in particular we can always replace none, thus leading to equality of hypotheses classes. To show that the inclusion is strict lets consider a Weierstrass function itself f (x) = σ w (x). We definitely have f ∈ H w as we can define 1 hidden layer network, with one hidden neuron and all the weights set to 1, and all biases to 0. Now, relu networks are piece-wise linear while the Weierstrass function is nowhere differentiable Weierstrass (1895) and thus not piece-wise linear. Similarly, network with an activation that is differentiable everywhere (e.g. sigmoid or tanh) is everywhere differentiable wrt. inputs, while Weierstrass function -nowhere Weierstrass (1895).","We explore the role of multiplicative interaction as a unifying framework to describe a range of classical and modern neural network architectural motifs, such as gating, attention layers, hypernetworks, and dynamic convolutions amongst others.",LHS ; RHS ; Wu et al. ; z ; Hypernetworks ; two ; second ; at least one ; first ; Ha et al,multiple domains ; concatenation ; the potential ; an object ; desirable inductive biases ; all the weights ; methods ; thus not piece-wise linear ; some ; attention systems,LHS ; RHS ; Wu et al. ; z ; Hypernetworks ; two ; second ; at least one ; first ; Ha et al,"The role of multiplicative interaction as a unifying framework to describe classical and modern neural network architectural motifs, such as gating, attention layers, hypernetworks, dynamic convolutions, etc., is under-appreciated. Multiplicative interaction layers as primitive operations have a long-established presence in the literature, but are often under-discussed and under-valued. However, multiplicative interactions can be used in large-scale complex RL and sequence modelling tasks, where their use enables us to deliver state-of-the-art results. These improvements are consistent with our hypothesis that the use of appropriately applied multipl",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Momentum based stochastic gradient methods such as heavy ball (HB) and Nesterov's accelerated gradient descent (NAG) method are widely used in practice for training deep networks and other supervised learning models, as they often provide significant improvements over stochastic gradient descent (SGD). Rigorously speaking, fast gradient methods have provable improvements over gradient descent only for the deterministic case, where the gradients are exact. In the stochastic case, the popular explanations for their wide applicability is that when these fast gradient methods are applied in the stochastic case, they partially mimic their exact gradient counterparts, resulting in some practical gain. This work provides a counterpoint to this belief by proving that there exist simple problem instances where these methods cannot outperform SGD despite the best setting of its parameters. These negative problem instances are, in an informal sense, generic; they do not look like carefully constructed pathological instances. These results suggest (along with empirical evidence) that HB or NAG's practical performance gains are a by-product of minibatching.

 Furthermore, this work provides a viable (and provable) alternative, which, on the same set of problem instances, significantly improves over HB, NAG, and SGD's performance. This algorithm, referred to as Accelerated Stochastic Gradient Descent (ASGD), is a simple to implement stochastic algorithm, based on a relatively less popular variant of Nesterov's Acceleration. Extensive empirical results in this paper show that ASGD has performance gains over HB, NAG, and SGD. The code for implementing the ASGD Algorithm can be found at https://github.com/rahulkidambi/AccSGD.
 First order optimization methods, which access a function (to be optimized) through its gradient or an unbiased approximation of its gradient, are the workhorses for modern large scale optimization problems, which include training the current state-of-the-art deep neural networks. Gradient descent (Cauchy, 1847) is the simplest first order method that is used heavily in practice. However, it is known that for the class of smooth convex functions as well as some simple non-smooth problems (Nesterov, 2012a) ), gradient descent is suboptimal BID1 and there exists a class of algorithms called fast gradient/momentum based methods which achieve optimal convergence guarantees. The heavy ball method BID5 ) and Nesterov's accelerated gradient descent (Nesterov, 1983 ) are two of the most popular methods in this category.On the other hand, training deep neural networks on large scale datasets have been possible through the use of Stochastic Gradient Descent (SGD) BID10 , which samples a random subset of training data to compute gradient estimates that are then used to optimize the objective function. The advantages of SGD for large scale optimization and the related issues of tradeoffs between computational and statistical efficiency was highlighted in Bottou & Bousquet (2007) .The above mentioned theoretical advantages of fast gradient methods BID5 Nesterov, 1983) (albeit for smooth convex problems) coupled with cheap to compute stochastic gradient estimates led to the influential work of BID16 , which demonstrated the empirical advantages possessed by SGD when augmented with the momentum machinery. This work has led to widespread adoption of momentum methods for training deep neural nets; so much so that, in the context of neural network training, gradient descent often refers to momentum methods.But, there is a subtle difference between classical momentum methods and their implementation in practice -classical momentum methods work in the exact first order oracle model BID1 , i.e., they employ exact gradients (computed on the full training dataset), while in practice BID16 , they are implemented with stochastic gradients (estimated from a randomly sampled mini-batch of training data). This leads to a natural question:""Are momentum methods optimal even in the stochastic first order oracle (SFO) model, where we access stochastic gradients computed on a small constant sized minibatches (or a batchsize of 1?)""Even disregarding the question of optimality of momentum methods in the SFO model, it is not even known if momentum methods (say, BID5 ; Nesterov (1983) ) provide any provable improvement over SGD in this model. While these are open questions, a recent effort of Jain et al. (2017) showed that improving upon SGD (in the stochastic first order oracle) is rather subtle as there exists problem instances in SFO model where it is not possible to improve upon SGD, even information theoretically. Jain et al. (2017) studied a variant of Nesterov's accelerated gradient updates BID2 for stochastic linear regression and show that their method improves upon SGD wherever it is information theoretically admissible. Through out this paper, we refer to the algorithm of Jain et al. (2017) as Accelerated Stochastic Gradient Method (ASGD) while we refer to a stochastic version of the most widespread form of Nesterov's method (Nesterov, 1983) as NAG; HB denotes a stochastic version of the heavy ball method BID5 . Critically , while Jain et al. (2017) shows that ASGD improves on SGD in any information-theoretically admissible regime, it is still not known whether HB and NAG can achieve a similar performance gain.A key contribution of this work is to show that HB does not provide similar performance gains over SGD even when it is informationally-theoretically admissible. That is, we provide a problem instance where it is indeed possible to improve upon SGD (and ASGD achieves this improvement), but HB cannot achieve any improvement over SGD. We validate this claim empirically as well. In fact, we provide empirical evidence to the claim that NAG also do not achieve any improvement over SGD for several problems where ASGD can still achieve better rates of convergence.This raises a question about why HB and NAG provide better performance than SGD in practice BID16 , especially for training deep networks. Our conclusion (that is well supported by our theoretical result) is that HB and NAG's improved performance is attributed to mini-batching and hence, these methods will often struggle to improve over SGD with small constant batch sizes. This is in stark contrast to methods like ASGD, which is designed to improve over SGD across both small or large mini-batch sizes. In fact, based on our experiments, we observe that on the task of training deep residual networks (He et al., 2016a) on the cifar-10 dataset, we note that ASGD offers noticeable improvements by achieving 5 − 7% better test error over HB and NAG even with commonly used batch sizes like 128 during the initial stages of the optimization. In this paper, we show that the performance gain of HB over SGD in stochastic setting is attributed to mini-batching rather than the algorithm's ability to accelerate with stochastic gradients. Concretely, we provide a formal proof that for several easy problem instances, HB does not outperform SGD despite large condition number of the problem; we observe this trend for NAG in our experiments. In contrast, ASGD (Jain et al., 2017) provides significant improvement over SGD for these problem instances. We observe similar trends when training a resnet on cifar-10 and an autoencoder on mnist. This work motivates several directions such as understanding the behavior of ASGD on domains such as NLP, and developing automatic momentum tuning schemes BID18 .A SUBOPTIMALITY OF HB: PROOF OF PROPOSITION 3Before proceeding to the proof, we introduce some additional notation. Let θ DISPLAYFORM0 t+1 denote the concatenated and centered estimates in the j th direction for j = 1, 2. DISPLAYFORM1 , j = 1, 2.Since the distribution over x is such that the coordinates are decoupled, we see that θ (j) t+1 can be written in terms of θ (j) t as: DISPLAYFORM2 t+1 denote the covariance matrix of θ DISPLAYFORM3 with, B (j) defined as DISPLAYFORM4 We prove Proposition 3 by showing that for any choice of stepsize and momentum, either of the two holds:• B (1) has an eigenvalue larger than 1, or,• the largest eigenvalue of B (2) is greater than 1 − 500 κ . This is formalized in the following two lemmas.Lemma 4. If the stepsize δ is such that δσ DISPLAYFORM5 (1) has an eigenvalue ≥ 1.Lemma 5. If the stepsize δ is such that δσ DISPLAYFORM6 (2) has an eigenvalue of magnitude DISPLAYFORM7 Given this notation, we can now consider the j th dimension without the superscripts; when needed, they will be made clear in the exposition. Denoting x def = δσ 2 and t def = 1 + α − x, we have: DISPLAYFORM8 The analysis goes via computation of the characteristic polynomial of B and evaluating it at different values to obtain bounds on its roots.Lemma 6. The characteristic polynomial of B is: DISPLAYFORM9 Proof. We first begin by writing out the expression for the determinant: DISPLAYFORM10 expanding along the first column, we have: DISPLAYFORM11 Expanding the terms yields the expression in the lemma.The next corollary follows by some simple arithmetic manipulations. Corollary 7. Substituting z = 1 − τ in the characteristic equation of Lemma 6, we have: DISPLAYFORM12 Proof of Lemma 4. The first observation necessary to prove the lemma is that the characteristic polynomial D(z) approaches ∞ as z → ∞, i.e., lim z→∞ D(z) = +∞.Next, we evaluate the characteristic polynomial at 1, i.e. compute D(1). This follows in a straightforward manner from corollary (7) by substituting τ = 0 in equation (2), and this yields, DISPLAYFORM13 As α < 1, x = δσ 2 > 0, we have the following by setting D(1) ≤ 0 and solving for x: DISPLAYFORM14 Since D(1) ≤ 0 and D(z) ≥ 0 as z → ∞, there exists a root of D(·) which is ≥ 1.Remark 8. The above characterization is striking in the sense that for any c > 1, increasing the momentum parameter α naturally requires the reduction in the step size δ to permit the convergence of the algorithm, which is not observed when fast gradient methods are employed in deterministic optimization. For instance, in the case of deterministic optimization, setting c = 1 yields δσ 2 1 < 2(1 + α). On the other hand, when employing the stochastic heavy ball method with x (j) = 2σ 2 j , we have the condition that c = 2, and this implies, δσ DISPLAYFORM15 We now prove Lemma 5. We first consider the large momentum setting. Lemma 9. When the momentum parameter α is set such that 1 − 450/κ ≤ α ≤ 1, B has an eigenvalue of magnitude ≥ 1 − 450 κ .Proof. This follows easily from the fact that det(B ) DISPLAYFORM16 Remark 10. Note that the above lemma holds for any value of the learning rate δ, and holds for every eigen direction of H. Thus, for ""large"" values of momentum, the behavior of stochastic heavy ball does degenerate to the behavior of stochastic gradient descent.We now consider the setting where momentum is bounded away from 1.Corollary 11. Consider B (2) , by substituting τ = l/κ, x = δλ min = c(δσ 2 1 )/κ in equation (2) and accumulating terms in varying powers of 1/κ, we obtain: DISPLAYFORM17 Substituting the value of l in equation (3) , the coefficient of DISPLAYFORM18 We will bound this term along with (3 DISPLAYFORM19 2 to obtain: DISPLAYFORM20 where, we use the fact that α < 1, l ≤ 9. The natural implication of this bound is that the terms that are lower order, such as O(1/κ 4 ) and O(1/κ 5 ) will be negative owing to the large constant above. Let us verify that this is indeed the case by considering the terms having powers of O(1/κ 4 ) and O(1/κ 5 ) from equation (3) : DISPLAYFORM21 κ 4 The expression above evaluates to ≤ 0 given an upperbound on the value of c. The expression above follows from the fact that l ≤ 9, κ ≥ 1.","Existing momentum/acceleration schemes such as heavy ball method and Nesterov's acceleration employed with stochastic gradients do not improve over vanilla stochastic gradient descent, especially when employed with small batch sizes.",Jain et al. ; two ; Nesterov's Acceleration ; Bottou & Bousquet ; z→∞ ; det(B ; First ; NLP ; Stochastic Gradient Descent ; ≥,several problems ; the characteristic polynomial D(z ; optimal convergence guarantees ; the above lemma ; training data ; The first observation ; Jain ; practice -classical momentum methods ; the learning rate δ ; the reduction,Jain et al. ; two ; Nesterov's Acceleration ; Bottou & Bousquet ; z→∞ ; det(B ; First ; NLP ; Stochastic Gradient Descent ; ≥,"Momentum based stochastic gradient methods such as heavy ball (HB) and Nesterov's accelerated gradient descent (NAG) are widely used in training deep networks and supervised learning models. These fast gradient methods have provable improvements over gradient descent only for the deterministic case, where the gradients are exact. However, the popular explanations for their wide applicability are that they partially mimic their exact gradient counterparts, resulting in practical gains. This work provides a counterpoint to this belief by proving that there exist simple problem instances where these methods cannot outperform SGD despite the best setting of its parameters. These negative problem",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"We propose a solution for evaluation of mathematical expression. However, instead of designing a single end-to-end model we propose a Lego bricks style architecture. In this architecture instead of training a complex end-to-end neural network, many small networks can be trained independently each accomplishing one specific operation and acting a single lego brick. More difficult or complex task can then be solved using a combination of these smaller network. In this work we first identify 8 fundamental operations that are commonly used to solve arithmetic operations (such as 1 digit multiplication, addition, subtraction, sign calculator etc). These fundamental operations are then learned using simple feed forward neural networks. We then shows that different operations can be designed simply by reusing these smaller networks. As an example we reuse these smaller networks to develop larger and a more complex network to solve n-digit multiplication, n-digit division, and cross product. This bottom-up strategy not only introduces reusability, we also show that it allows to generalize for computations involving n-digits and we show results for up to 7 digit numbers. Unlike existing methods, our solution also generalizes for both positive as well as negative numbers. The success of feed-forward Artificial Neural Network (ANN) lies in their ability to learn that allow an arbitrarily connected network to develop an internal structure appropriate for a particular task. This learning is dependent on the data provided to the network during the training process. It has been commonly observed that almost all ANNs lack generalization and their performance drastically degrades on unseen data. This includes degradation of performance on data containing the seen categories but acquired under from a different setup (location, lighting, view point, size, ranges etc) . Although there are techniques such as Domain Adaptation to address these generalization issues, however this behaviour indicates that the learning process in neural network is primarily based on memorization and they lack understanding of inherent rules. Thus the decision making process in ANN is lacking quantitative reasoning, numerical extrapolation or systematic abstraction. However when we observe other living species, numerical extrapolation and quantitative reasoning is their fundamental capability what makes them intelligent beings. For e.g. if we observe the learning process among children, they can memorize single digit arithmetic operation and then extrapolate it to higher digits. More specifically our ability to +, −, × and ÷ higher digit number is based on understanding how to reuse the examples that we have memorized for single digits. This indicates that the key to generalization is in understanding to reuse what has been memorized. Furthermore, complex operations are usually combination of several simple function. Thus complex numerical extrapolation and quantitative reasoning among ANNs can be developed by identifying and learning the fundamental operations that can be reused to develop complex functions. Inspired from the methodology of learning adopted by humans, in this work we first identify several fundamental operations that are commonly used to solve arithmetic operations (such as 1 digit multiplication, addition, subtraction, merging of two number based on their place value, learning to merge sign +/− etc). These fundamental operations are then learned using simple feed forward neural networks. We then reuse these smaller networks to develop larger and a more complex network to solve various problems like n-digit multiplication, n-digit division, cross product etc. To the best of our knowledge this is the first work that proposed a generalized solution for these arith-metic operations. Furthermore, unlike exiting methods ( Hornik et al. (1989) ; Siu & Roychowdhury (1992); Peddada (2015) ; Sharma (2013) ; Trask et al. (2018) ) ours is also the first solution that works for both positive as well as negative numbers. In this paper we show that many complex tasks can be divided into smaller sub-tasks, furthermore many complex task share similar sub-tasks. Thus instead of training a complex end-to-end neural network, many small networks can be trained independently each accomplishing one specific operation. More difficult or complex task can then be solved using a combination of these smaller network. In this work we first identify several fundamental operations that are commonly used to solve arithmetic operations (such as 1 digit multiplication, addition, subtraction, place value shifter etc). These fundamental operations are then learned using simple feed forward neural networks. We then reuse these smaller networks to develop larger and a more complex network to solve various problems like n-digit multiplication and n-digit division. One of the limitation of the proposed work is the use of float operation in the tokenizer which limits the end-to-end training of complex networks. However, since we are only using pre-trained smaller network representing fundamental operations, this does not creates any hinderance in our current work. However, we aim to resolve this issue in future. We have also designed a cross product network using the same strategy and we are currently testing its accuracy. As a future work we aim to develop a point cloud segmentation algorithm by using a larger number of identical smaller network (i.e. cross product) that can compute a normal vector using 3 3D points as input.","We train many small networks each for a specific operation, these are then combined to perform complex operations","Peddada ; ANN ; first ; Siu & Roychowdhury ; Sharma ; +, −, × ; Artificial Neural Network ; two ; one ; Trask",Trask et al ; lighting ; it ; float operation ; the same strategy ; the tokenizer ; almost all ANNs ; inherent rules ; our solution ; tasks,"Peddada ; ANN ; first ; Siu & Roychowdhury ; Sharma ; +, −, × ; Artificial Neural Network ; two ; one ; Trask","In this approach, many small networks can be trained independently each accomplishing one specific operation and acting a single lego brick. More difficult or complex tasks can then be solved using a combination of these smaller networks. These fundamental operations are then learned using simple feed forward neural networks. The success of feed-forward Artificial Neural Network (ANN) lies in their ability to learn that allow an arbitrarily connected network to develop an internal structure appropriate for a particular task. This learning is dependent on the data provided to the network during the training process. In order to generalize computations involving n-digits and higher digits, we use simple feed",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Bayesian inference promises to ground and improve the performance of deep neural networks. It promises to be robust to overfitting, to simplify the training procedure and the space of hyperparameters, and to provide a calibrated measure of uncertainty that can enhance decision making, agent exploration and prediction fairness.
 Markov Chain Monte Carlo (MCMC) methods enable Bayesian inference by generating samples from the posterior distribution over model parameters.
 Despite the theoretical advantages of Bayesian inference and the similarity between MCMC and optimization methods, the performance of sampling methods has so far lagged behind  optimization methods for large scale deep learning tasks.
 We aim to fill this gap and introduce ATMC, an adaptive noise MCMC algorithm that estimates and is able to sample from the posterior of a neural network.
 ATMC dynamically adjusts the amount of momentum and noise applied to each parameter update in order to compensate for the use of stochastic gradients.
 We use a ResNet architecture without batch normalization to test ATMC on the Cifar10 benchmark and the large scale ImageNet benchmark and show that, despite the  absence of batch normalization, ATMC outperforms a strong optimization baseline in terms of both classification accuracy and test log-likelihood. We show that ATMC is intrinsically robust to overfitting on the training data and that ATMC provides a better calibrated measure of uncertainty compared to the optimization baseline. In contrast to optimization approaches in machine learning that derive a single estimate for the weights of a neural network, Bayesian inference aims at deriving a posterior distribution over the weights of the network. This makes it possible to sample model instances from the distribution over the weights and offers unique advantages. Multiple model instances can be aggregated to obtain robust uncertainty estimates over the network's predictions; uncertainty estimates are crucial in domains such as medical diagnosis and autonomous driving where following a model's incorrect predictions can result in catastrophe (Kendall & Gal, 2017) . Sampling a distribution, as opposed to optimizing a loss, is less prone to overfitting and more training doesn't decrease test performance. Bayesian inference can also be applied to differential privacy, where each individual sample has increased privacy guarantees (Wang et al., 2015) , and to reinforcement learning, where one can leverage model uncertainty to balance between exploration and exploitation (Osband & Van Roy, 2017) . Traditional Markov Chain Monte Carlo (MCMC) methods like HMC (Neal et al., 2011) are a standard class of methods for generating samples from the posterior distribution over model parameters. These methods are seldom applied in deep learning because they have traditionally failed to scale well with large datasets and many parameters (Rajaratnam & Sparks, 2015) . Stochastic Gradient MCMC (SG-MCMC) methods have fared somewhat better in scaling to large datasets due to their close relationship to stochastic optimization methods. For example the SGLD sampler (Welling & Teh, 2011) amounts to performing stochastic gradient descent while adding Gaussian noise to each parameter update. Despite these improvements, samplers like SGLD are only guaranteed to converge to the correct G t ← minibatch gradient(θ t ) 6: distribution when the step size is annealed to zero; additional control variates have been developed to mitigate this to some extent (Ahn et al., 2012; Ding et al., 2014) . The objective of this work is to make Bayesian inference practical for deep learning by making SG-MCMC methods scale to large models and datasets. The contributions described in this work fall in three categories. We first propose the Adaptive Thermostat Monte Carlo (ATMC) sampler that offers improved convergence and stability. ATMC dynamically adjusts the amount of momentum and noise applied to each model parameter. Secondly, we improve an existing second order numerical integration method that is needed for the ATMC sampler. Third, since ATMC, like other SG-MCMC samplers, is not directly compatible with stochastic regularization methods such as batch normalization (BatchNorm) and Dropout (see Sect. 4), we construct the ResNet++ network by taking the original ResNet architecture (He et al., 2016) , removing BatchNorm and introducing SELUs (Klambauer et al., 2017) , Fixup initialization (Zhang et al., 2019a) and weight normalization (Salimans & Kingma, 2016) . We design ResNet++ so that its parameters are easy to sample from and the gradients are well-behaved even in the absence of BatchNorm. We show that the ATMC sampler is able to outperform optimization methods in terms of accuracy, log-likelihood and uncertainty calibration in the following settings. First, when using the ResNet++ architecture for both the ATMC sampler and the optimization baseline, the ATMC sampler significantly outperforms the optimization baseline on both Cifar-10 and ImageNet. Secondly, when using the standard ResNet for the optimization baseline and the ResNet++ for the ATMC sampler, multiple samples of the ATMC that approximate the predictive posterior of the model are still able to outperform the optimization baseline on ImageNet. Using the ResNet++ architecture, the ATMC sampler reduces the need for hyper-parameter tuning since it does not require early stopping, does not use stochastic regularization, is not prone to over-fitting on the training data and avoids a carefully tuned learning rate decay schedule. The empirical results show it is possible to sample the posterior distribution of neural networks on large scale image classification problems like ImageNet. A major obstacle for sampling the posterior of ResNets in particular is the lack of compatibility with BatchNorm. Using recent advances in initialization and the SELU activation function we are able to stabilize and speed up training of ResNets without resorting to BatchNorm. Nonetheless, we observe that BatchNorm still offers a unique advantage in terms of generalization per-formance. We hope that future work will allow the implicit inductive bias that BatchNorm has to be transferred into an explicit prior that is compatible with sampling methods. Multiple posterior samples provide a much more accurate estimate of the posterior predictive, and consequently much better accuracy and uncertainty estimates. For inference, making predictions using a large ensemble of models sampled from the posterior can be costly. Variational Inference methods can be used to quickly characterize a local mode of the posterior (Blundell et al., 2015) . More recent work shows that a running estimate of the mean and variance of the parameters during training can also be used to approximate a mode of the posterior (Maddox et al., 2019) . Methods like distillation could potentially be used to compress a high-quality ensemble into a single network with a limited computational budget (Balan et al., 2015) . Although the form in (4) is very general, alternative methods for dealing with stochastic gradients have been proposed in the literature. One approach is to estimate the covariance of the stochastic gradient noise B explicitly and use it correct and pre-condition the sampling dynamics (Ahn et al., 2012; Li et al., 2016) . Other sampling methods are not based on an SDE that converges to the target distribution. Under some conditions stochastic optimization methods can be interpreted as such a biased sampling method (Mandt et al., 2017) . Predictions based on multiple samples from the trajectory of SGD have been used successfully for obtaining uncertainty estimates in large scale Deep Learning (Maddox et al., 2019) . However, these methods rely on tuning hyperparameters in such a way that just the right amount of noise is inserted.",We scale Bayesian Inference to ImageNet classification and achieve competitive results accuracy and uncertainty calibration.,BatchNorm ; Ding ; al. ; Rajaratnam & Sparks ; Li ; Variational Inference ; Gaussian ; the Adaptive Thermostat Monte Carlo ; first ; Maddox,additional control variates ; It ; MCMC and optimization methods ; Van Roy ; noise ; generalization ; the posterior predictive ; the ATMC sampler ; a running estimate ; the ResNet++ network,BatchNorm ; Ding ; al. ; Rajaratnam & Sparks ; Li ; Variational Inference ; Gaussian ; the Adaptive Thermostat Monte Carlo ; first ; Maddox,"Bayesian inference promises to ground and improve the performance of deep neural networks. It promises to simplify training procedure and the space of hyperparameters, and to provide a calibrated measure of uncertainty that can enhance decision making, agent exploration and prediction fairness. The performance of sampling methods has lagged behind  optimization methods for large scale deep learning tasks. In order to fill this gap, we introduce ATMC, an adaptive noise MCMC algorithm that estimates and is able to sample from the posterior of a neural network. ATMC dynamically adjusts the amount of momentum and noise applied to each parameter update in order to compensate for stochastic gradients",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"We introduce an approach for augmenting model-free deep reinforcement learning agents with a mechanism for relational reasoning over structured representations, which improves performance, learning efficiency, generalization, and interpretability. Our architecture encodes an image as a set of vectors, and applies an iterative message-passing procedure to discover and reason about relevant entities and relations in a scene. In six of seven StarCraft II Learning Environment mini-games, our agent achieved state-of-the-art performance, and surpassed human grandmaster-level on four. In a novel navigation and planning task, our agent's performance and learning efficiency far exceeded non-relational baselines, it was able to generalize to more complex scenes than it had experienced during training. Moreover, when we examined its learned internal representations, they reflected important structure about the problem and the agent's intentions. The main contribution of this work is to introduce techniques for representing and reasoning about states in model-free deep reinforcement learning agents via relational inductive biases. Our experiments show this approach can offer advantages in efficiency, generalization, and interpretability, and can scale up to meet some of the most challenging test environments in modern artificial intelligence. Recent deep reinforcement learning (RL) systems have achieved remarkable performance in very challenging problem domains (Mnih et al., 2015; BID22 , in large part because of their flexibility in how they learn and exploit the statistical structure underlying observations and reward signals. But the downsides to such flexibility often include low sample efficiency and poor transfer beyond the specifics of the training environment BID32 Lake et al., 2017; Kansky et al., 2017) . Various structured approaches to RL (e.g. BID9 ; BID8 ; BID7 ; BID12 ) have attempted to overcome these limitations by explicitly incorporating entity-based and symbolic representations, and specialized building blocks for solving the task at hand. Although these approaches are often highly efficient, they constrain the representations and admissible learning algorithms, they struggle to learn rich representations, and they are therefore confined to relatively simple tasks and data conditions.To strike favorable tradeoffs between flexibility and efficiency, a number of recent approaches have explored using relational inductive biases in deep learning, to reap the benefits of flexible statistical learning and more structured approaches. Methods such as ""graph networks"" BID20 Li et al., 2015; explicitly represent entities and their relations using using sets and graphs, and perform relational reasoning using learned message-passing BID13 and attention BID23 Hoshen, 2017; BID24 BID28 schemes. Because they are implemented using deep neural networks, they can learn transformations from input observations into task-relevant entities, as well as functions for computing rich interaction among these entities. This provides a powerful capacity for combinatorial generalization, where their learned building blocks can be composed to represent and reason about novel scenarios BID2 BID19 BID5 BID27 BID21 Kipf et al., 2018; .Drawing on several lines of work, we introduce an approach for incorporating relational inductive biases for entity-and relation-centric state representations, and iterated relational reasoning, into a deep RL agent. In contrast with prior work exploring relational inductive biases in deep RL (e.g., BID27 , our approach does not rely on a priori knowledge of the structure of the problem and is agnostic to the particular relations that need to be considered. To handle raw visual input data, our architecture used a convolutional front-end to compute embeddings of sets of entities, similar to previous work in visual question answering, physical prediction, and video understanding BID19 BID29 BID28 . To perform relational reasoning, we used a self-attention mechanism BID23 Hoshen, 2017; BID24 applied iteratively within each timestep, which can be viewed as learned message-passing (Li et al., 2015; BID13 . Our deep RL agent is based on an off-policy advantage actor-critic (A2C) method which is very effective across a range of standard RL environments BID10 .Our results show that this relational deep RL agent scale to very challenging tasks, achieving state-ofthe-art performance on six out of seven StarCraft II mini-games , surpassing grandmaster level on four mini-games. Additionally, we introduce a novel navigation and planning task, called ""Box-World"", which stresses the planning and reasoning components of the policy, factoring out other challenges like complex vision or large action spaces. Our agent reaches higher ceiling performance, more efficiently, than non-relational baseline, and is able to generalize to solve problems with more complex solutions than it had been trained on within this task. We also found that the intermediate representations involved in the relational computations were interpretable, and suggest that the agent has rich understanding of the underlying structure of the problem. By introducing structured perception and relational reasoning into deep RL architectures, our agents can learn interpretable representations, and exceed baseline agents in terms of sample complexity, ability to generalize, and overall performance. Behavioral analyses showed that the learned representations allowed for better generalization, which is characteristic of relational representations. Analysis of attention weights showed that the model's internal computations were interpretable, and congruent with the computations we would expect from a model computing task-relevant relations.One important future direction is to explore ways to scale our approach to larger inputs spaces, without suffering, as this and other approaches do (e.g., BID28 BID19 , from the quadratic complexity that results from considering all input pairs. Possible avenues involve using a distinct attentional mechanisms that scales linearly with the number of inputs (Hoshen, 2017) or filtering out unimportant relations BID3 . Other future directions include exploring perceiving complex scenes via more structured formats, such as scene graphs BID31 BID4 , which could be powerful additions to our approach's input module. More complex relational modules could be explored, such as richer graph network implementations , learned approaches for inducing compositional programs BID17 Parisotto et al., 2017; BID0 BID6 ) and reasoning about structured data (Neelakantan et al., 2015; Liang et al., 2016) , or even explicit logical reasoning over structured internal representations BID11 , drawing inspiration from more symbolic approaches in classic AI. Our approach may also interface well with approaches for hierarchical RL , planning BID14 , and structured behavior representation (Huang et al., 2018) , so that the structured internal representations and patterns of reasoning can translate into more structured behaviors.More speculatively, this work blurs the line between model-free agents, and those with a capacity for more abstract planning. An important feature of model-based approaches is making general knowledge of the environment available for decision-making. Here our inductive biases for entityand relation-centric representations and iterated reasoning reflect key knowledge about the structure of the world. While not a model in the technical sense, it is possible that the agent learns to exploit this relational architectural prior similarly to how an imagination-based agent's forward model operates BID15 . More generally, our work opens new directions for RL via a principled hybrid of flexible statistical learning and more structured approaches.",Relational inductive biases improve out-of-distribution generalization capacities in model-free reinforcement learning agents,Kansky et al. ; AI ; Mnih ; Neelakantan ; al. ; Parisotto ; Behavioral ; Li ; StarCraft II Learning ; One,standard RL environments ; the-art ; relational inductive biases ; work ; large part ; Liang et al ; hierarchical RL ; more complex solutions ; An important feature ; rich understanding,Kansky et al. ; AI ; Mnih ; Neelakantan ; al. ; Parisotto ; Behavioral ; Li ; StarCraft II Learning ; One,"We introduce an approach for augmenting model-free deep reinforcement learning agents with relational reasoning over structured representations, which improves performance, learning efficiency, generalization, and interpretability. Our architecture encodes an image as a set of vectors, and applies an iterative message-passing procedure to discover and reason about relevant entities and relations in a scene. In six of seven StarCraft II Learning Environment mini-games, our agent achieved state-of-the-art performance and surpassed human grandmaster-level on four. In a novel navigation and planning task, the agent's performance and learning efficiency far exceeded non-relational baselines",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Reinforcement learning (RL) has led to increasingly complex looking behavior in recent years. However, such complexity can be misleading and hides over-fitting. We find that visual representations may be a useful metric of complexity, and both correlates well objective optimization and causally effects reward optimization. We then propose curious representation learning (CRL) which allows us to use better visual representation learning algorithms to correspondingly increase visual representation in policy through an intrinsic objective on both simulated environments and transfer to real images. Finally, we show better visual representations induced by CRL allows us to obtain better performance on Atari without any reward than other curiosity objectives. In recent years, reinforcement learning(RL) has lead to increasingly complex behavior from simulated environments (Silver et al., 2016; OpenAI, 2018; Mnih et al., 2013; Andrychowicz et al., 2018 ). Yet despite this, there lacks a quantitative measure of intelligence in these agents. Qualitative measures can be deceptive. Consider agent Alice and Bob in Minecraft. Alice is capable of a constructing a house while Bob appears to only be able to navigate around the world. While at face value it may then appear that Alice is more complex, upon closer inspection we may find that Alice has simply memorized a set of actions to construct a house in that particular environment! How can we be certain that our agents are not simply not memorizing a set of moves? One hypothesis is that the more intelligent an agent is, the more likely the inner representations in its policy will exhibit disentangled properties of the world. Towards this end, we investigate the emergent visual representations that occur in RL policies. We investigate on various objectives and environment conditions, and find that the quality of visual representation learning correlates well with progress in reward optimization. Similarily, we find improved visual representations help agents perform better reward optimization. Thus, another natural question to ask is, how can we enable our agents to have better visual representations? While there are ways to hardcode reward functions to enable agents perform well, can we come up with a generic objective that our agents can optimize that will directly lead them to have good representations? One idea towards this is to use recent work in curiosity. In curiosity, agents are typically given rewards corresponding to surprisal of state. But another view of curiosity is that of a minimax game where a curious agent is seeking to maximize the surprisal of an uncertainty model, while the uncertainty model seeks become less surprised about new states. Thus, to enable a policy to learn good visual representations, we can treat the uncertainty model as a representation learning model. We then seek a policy that wants to lower the loss of the representation learning objective, while the model itself tries to optimize this loss. Under this objective, a policy must learn good visual representations, so that it is able to find visually surprising inputs for the vision model. We call this overall objective, Curious Representation Learning (CRL). By coupling policy learning with representation learning, we find that CRL allows us to get better policy visual representations simply by applying better visual representation learning algorithms to the model. As a result, we find that CRL obtains consistently good representations in policies across environment size and type, often beating many hard-coded domain specific objectives. As an added bonus, we find that CRL is also able to achieve better visual representation learning than other data collection methods, as it actively sees diverse inputs that surprise it. In this paper, we have shown visual representations correspond and help reward optimization. Motivated by this insight, we propose a new method, CRL, that allows us to get improved visual representations in policies through better visual representations in model. We further illustrate that these better visual representation can provide incentives to explore more in no reward scenarios. We hope that our results will inspire further exploration on both better visual representation learning models/policies and better reward optimization. We further show nearest neighbor images on VizDoom in Figure 8 . The leftmost column is the query image while the other 4 columns are the 4 nearest neighbors in embedding space. Training through CRL allows clustering of various doom objects.",We present a formulation of curiosity as a visual representation learning problem and show that it allows good visual representations in agents.,OpenAI ; Bob ; Atari ; hardcode ; Minecraft ; One ; al. ; Curious Representation Learning ; Alice ; Mnih,good representations ; a new method ; al ; space ; Qualitative measures ; this insight ; the quality ; One hypothesis ; Training ; our results,OpenAI ; Bob ; Atari ; hardcode ; Minecraft ; One ; al. ; Curious Representation Learning ; Alice ; Mnih,"Reinforcement learning (RL) has led to increasingly complex behavior in recent years. However, it can be misleading and hides over-fitting. We propose curious representation learning (CRL) which enables agents to correspondingly increase visual representation in policy through an intrinsic objective on both simulated environments and transfer to real images. In addition, we show better visual representations induced by CRL, enabling agents to obtain better performance on Atari without any reward than other curiosity objectives. In recent years, reinforcement learning(RL) led to more complex behavior from simulated environments, resulting in increasingly complex behaviors from agents. The quality of visual representations in agents'",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"While Bayesian optimization (BO) has achieved great success in optimizing expensive-to-evaluate black-box functions, especially tuning hyperparameters of neural networks, methods such as random search (Li et al., 2016) and multi-fidelity BO (e.g. Klein et al. (2017)) that exploit cheap approximations, e.g. training on a smaller training data or with fewer iterations, can outperform standard BO approaches that use only full-fidelity observations. In this paper, we propose a novel Bayesian optimization algorithm, the continuous-fidelity knowledge gradient (cfKG) method, that can be used when fidelity is controlled by one or more continuous settings such as training data size and the number of training iterations. cfKG characterizes the value of the information gained by sampling a point at a given fidelity, choosing to sample at the point and fidelity with the largest value per unit cost. Furthermore, cfKG can be generalized, following Wu et al. (2017), to settings where derivatives are available in the optimization process, e.g. large-scale kernel learning, and where more than one point can be evaluated simultaneously. Numerical experiments show that cfKG outperforms state-of-art algorithms when optimizing synthetic functions, tuning convolutional neural networks (CNNs) on CIFAR-10 and SVHN, and in large-scale kernel learning. In hyperparameter tuning of machine learning models, we seek to find a set of hyperparameters x in some set A to minimize the validation error f (x), i.e., to solve min x∈A f (x) (1.1)Evaluating f (x) can take substantial time and computational power BID0 , and may not provide gradient evaluations. Thus, machine learning practitioners have turned to Bayesian optimization for solving (1.1) BID19 because it tends to find good solutions with few function evaluations BID6 .As the computational expense of training and testing a modern deep neural network for a single set of hyperparameters has grown as long as days or weeks, it has become natural to seek ways to solve (1.1) more quickly by supplanting some evaluations of f (x) with computationally inexpensive lowfidelity approximations. Indeed , when training a neural network or most other machine learning models, we can approximate f (x) by training on less than the full training data, or using fewer training iterations. Both of these controls on fidelity can be set to achieve either better accuracy or lower computational cost across a range of values reasonably modeled as continuous.In this paper, we consider optimization with evaluations of multiple fidelities and costs where the fidelity is controlled by one or more continuous parameters. We model these evaluations by a realvalued function g(x, s) where f (x) := g(x, 1 m ) and s ∈ [0, 1] m denotes the m fidelity-control parameters. g(x, s) can be evaluated, optionally with noise, at a cost that depends on x and s. In the context of hyperparameter tuning, we may take m = 2 and let g(x, s 1 , s 2 ) denote the loss on the validation set when training using hyperparameters x with a fraction s 1 of the training data and a fraction s 2 of some maximum allowed number of training iterations. We may also set m = 1 and let s index either training data or training iterations. We assume A is a compact connected uncountable set into which it is easy to project, such as a hyperrectangle.This problem setting also appears outside of hyperparameter tuning, in any application where the objective is expensive to evaluate and we may observe cheap low-fidelity approximations parameterized by a continuous vector. For example , when optimizing a system evaluated via a Monte Carlo simulator, we can evaluate a system configuration approximately by running with fewer replications. Also, when optimizing an engineering system modeled by a partial differential equation (PDE), we can evaluate a system configuration approximately by solving the PDE using a coarse grid.Given this problem setting, we use the knowledge gradient approach BID3 to design an algorithm to adaptively select the hyperparameter configuration and fidelity to evaluate, to best support solving (1.1). By generalizing a computational technique based on the envelope theorem first developed in Wu et al. (2017) , our algorithm supports parallel function evaluations, and also can take advantage of derivative observations when they are available. This algorithm chooses the point or set of points to evaluate next that maximizes the ratio of the value of information from evaluation against its cost.Unlike most existing work on discrete-and continuous-fidelity Bayesian optimization, our approach considers the impact of our measurement on the future posterior distribution over the full feasible domain, while existing expected-improvement-based approaches consider its impact at only the point evaluated. One exception is the entropy-search-based method [10] , which also considers the impact over the full posterior. Our approach differs from entropy search in that it chooses points to sample to directly minimize expected simple regret, while entropy search seeks to minimize the entropy of the location or value of the global optimizer, indirectly reducing simple regret.We summarize our contributions as follows. We propose a novel continuous-fidelity BO algorithm, cfKG, which generalizes naturally to batch and derivative settings. This algorithm can find good solutions to global optimization problems with less cost than state-of-art algorithms in applications including deep learning and kernel learning.",We propose a Bayes-optimal Bayesian optimization algorithm for hyperparameter tuning by exploiting cheap approximations.,Monte Carlo ; one ; more than one ; as long as days ; PDE ; fidelity ; SVHN ; first ; x∈A ; al.,expected simple regret ; its cost ; training iterations ; simple regret ; convolutional neural networks ; computational power ; parallel function evaluations ; a system ; our algorithm ; the point,Monte Carlo ; one ; more than one ; as long as days ; PDE ; fidelity ; SVHN ; first ; x∈A ; al.,"Bayesian optimization (BO) has achieved success in optimizing expensive-to-evaluate black-box functions, especially tuning hyperparameters of neural networks, and multi-fidelity BO (e.g. Klein et al. (2017)) that exploit cheap approximations, e.g., training on smaller training data or with fewer iterations, outperform standard BO approaches that use only full-Fidelity observations. In hyperparameter tuning of machine learning models, we propose a novel Bayesian optimization algorithm, the continuous-fibidelity knowledge gradient (cfKG) that can be used when fidelity is controlled by one or",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Recent visual analytics systems make use of multiple machine learning models to better fit the data as opposed to traditional single, pre-defined model systems. However, while multi-model visual analytic systems can be effective, their added complexity poses usability concerns, as users are required to interact with the parameters of multiple models. Further, the advent of various model algorithms and associated hyperparameters creates an exhaustive model space to sample models from. This poses complexity to navigate this model space to find the right model for the data and the task. In this paper, we present Gaggle, a multi-model visual analytic system that enables users to interactively navigate the model space. Further translating user interactions into inferences, Gaggle simplifies working with multiple models by automatically finding the best model from the high-dimensional model space to support various user tasks. Through a qualitative user study, we show how our approach helps users to find a best model for a classification and ranking task. The study results confirm that Gaggle is intuitive and easy to use, supporting interactive model space navigation and automated model selection without requiring any technical expertise from users. Visual analytic (VA) techniques continue to leverage machine learning (ML) to provide people effective systems for gaining insights into data [27] . Systems such as Interaxis [36] help domain experts combine their knowledge and reasoning skills about a dataset or domain with the computational prowess of machine learning. These systems are traditionally designed with a pre-defined single ML model that has a carefully chosen learning algorithm and hyperparameter settings. Various combination of learning algorithms and hyperparameters give rise to a vast number of different model types (see Table 1 ). These different models constitute an exhaustive model space from which unique models can be sampled using a distinct combination of a learning algorithm and associated hyperparameters. For example, support vector machine (SVM) models have many options for kernel functions (i.e., linear, poly or radial) and hyperparameters (i.e., C (regularization parameter), γ (kernel coefficient), etc. ). When a model is correctly chosen for the phenomena, task, data distribution, or question users try to answer, existing VA techniques can effectively support users in exploration and analysis. However, in cases where the right model to use for a problem is not known a priori, one needs to navigate this model space to find a fitting model for the task or the problem. To combat this, recent VA systems use multiple ML models to support a diverse set of user tasks (e.g., Regression, Clustering , etc. [15, 21, 17, 63] ). For example, the VA system Clustervision [39] allows users to inspect multiple clustering models and select one based on quality and preference. Similarly, Snowcat [16] allows inspecting multiple ML models across a diverse set of tasks, such as classification, regression, time-series forecasting, etc. However, these multimodel systems are often more complex to use compared to single-model alternatives (e.g, in Clustervision users need to be well-versed with cluster model metrics and shown models.) We refer to this complex combination of parameter and hyperparameter settings as model space, as there are a large number of models that can be instantiated in this hyperdimensional space. Further, the interactive exploration of different parameter and hyperparameter combinations can be referred to as model space navigation. Our definition of model space is related to the work by Brown et al. [14] where they presented a tool called ModelSpace to analyze how the model parameters have changed over time during data exploration. In this paper we present Gaggle, a visual analytic tool that provides the user experience of a single-model system, yet leverages multiple models to support data exploration. Gaggle constructs multiple classification and ranking models, and then using a bayesian optimization hyperparameter selection technique, automatically finds a classification and ranking model for users to inspect, thus simplifying the search for an optimal model. Furthermore, our technique utilises simple user interactions for model space navigation to find the right model for the task. For example, users can drag data items into specific classes to record classification task's user preferences. Similarly, users can demonstrate that specific data items should be higher or lower in rank within a class by dragging them on top of each other. Gaggle uses ML to help users in data exploration or data structuring tasks, e.g, grouping data in self-defined categories, and ranking the members of the group based on their representativeness to the class. For example, a professor may want to use a tool to help categorize new student applications in different sets, and then rank the students in each set. Similarly, a salesman may want to cluster and rank potential clients in various groups. These problems fall under classification tasks in ML; however, unlike a conventional classification problem, our use case specifically supports interactive data exploration or data structuring, the models constructed are not meant to predict labels for unseen data items in future. Using this workflow, we expect our technique guards against possible model overfitting incurred due to adjusting the models confirm to specified user preferences. Furthermore, Gaggle addresses a common problem of datasets that either lack adequate ground truth, or do not have it [61, 72, 49] . To resolve this problem, Gaggle allows users to iteratively define classes and add labels. On each iteration, users add labels to data items and then build a classifier. We conducted a qualitative user study of Gaggle to collect user feedback on the system design and usability. The results of our study indicate that users found the workflow in Gaggle intuitive, and they were able to perform classification and ranking tasks effectively. Further, users confirmed that Gaggle incorporated their feedback into the interactive model space navigation technique to find the right model for the task. Overall, the contributions of this paper include: • A model space navigation technique facilitated by a Bayesian optimization hyperparameter tuning and automated model selection approach. • A VA tool Gaggle, that allows interactive model space navigation supporting classification and ranking tasks using simple demonstration-based user interactions. • The results of a user study testing Gaggle's effectiveness. Large Model Search Space: Searching models by combining different learning algorithms and hyperparameters leads to an extremely large search space. As a result, a small set of constraints on the search process would not sufficiently reduce the space, leading to a large number of sub-constrained and ill-defined solutions. Thus, how many interactions are considered optimal for a given model space? In this work, we approached this challenge by using Bayesian optimization for ranking models. However, larger search spaces may pose scalability issues while too many user constraints may ""over-constrain"" models leading to poor results. In this paper, we present an interactive model space navigation approach for helping people perform classification and ranking tasks. Current VA techniques rely on a pre-selected model for a designated task or problem. However, these systems may fail if the selected model does not suit the task or the user's goals. As a solution, our technique helps users find a model suited to their goals by interactively navigating the high-dimensional model space. Using this approach, we prototyped Gaggle, a VA system to facilitate classification and ranking of data items. Further, with a qualitative user study, we collected and analyzed user feedback to understand the usability and effectiveness of Gaggle. The study results show that users agree that Gaggle is easy to use, intuitive, and helps them interactively navigate the model space to find an optimal classification and ranking model.","Gaggle, an interactive visual analytic system to help users interactively navigate model space for classification and ranking tasks.",linear ; e.g ; SVM ; ModelSpace ; VA ; Brown ; Snowcat ; one ; ML ; Bayesian,data items ; the high-dimensional model space ; Systems ; models ; a carefully chosen learning algorithm ; an extremely large search space ; sub-constrained and ill-defined solutions ; a professor ; poor results ; a salesman,linear ; e.g ; SVM ; ModelSpace ; VA ; Brown ; Snowcat ; one ; ML ; Bayesian,"Multi-model visual analytic systems are used to better fit the data as opposed to traditional single, pre-defined model systems. The advent of various model algorithms and associated hyperparameters creates an exhaustive model space to sample models from. In this paper, Gaggle (Gaggle) enables users to interactively navigate the model space by automatically finding the best model from the high-dimensional model space. This simplifies working with multiple models and helps domain experts combine their knowledge and reasoning skills to find a best model for a classification and ranking task. The results of a qualitative user study confirm that GaggLE is intuitive and easy",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"This paper proposes a new model for document embedding. Existing approaches either require complex inference or use recurrent neural networks that are difficult to parallelize. We take a different route and use recent advances in language modeling to develop a convolutional neural network embedding model. This allows us to train deeper architectures that are fully parallelizable. Stacking layers together increases the receptive filed allowing each successive layer to model increasingly longer range semantic dependences within the document. Empirically we demonstrate superior results on two publicly available benchmarks. Full code will be released with the final version of this paper. A typical approach is to develop a document embedding model which produces fixed length vector representations that preserve relevant semantic information. These models are trained in unsupervised fashion on unlabeled text, and the resulting embeddings can be used as input for a variety of NLP tasks such as sentiment analysis and information retrieval BID2 BID21 BID19 . Despite significant research effort in this area the most commonly used methods are still based on the bag-of-words (n-grams) representations.However, recent work has shown that remarkably accurate embedding models can be learned using distributed representations of words BID27 . Within this category two popular approaches are doc2vec BID21 and skip-thought BID19 . doc2vec extends the distributed word model word2vec BID27 by attaching document-specific vectors to word2vec and learning them jointly with word representations. While accurate, this model requires iterative optimization to be conducted for each new document making it challenging to deploy in high volume production environments. Furthermore, doc2vec is trained using localized contexts of very small size (typically 5 to 10 words) and never sees the whole document. This makes it difficult to capture long range semantic relationships within the document.Skip-thought uses a recurrent neural network (RNN) to sequentially ingest the document one word at a time. Last layer activations after the last word are then taken as document embedding. RNN models have been gaining popularity and a number of other approaches have been proposed BID10 BID22 . Recurrent architecture addresses both problems of the doc2vec approach. During inference only a forward pass through the network is required to produce an embedding that is based on the entire content of the document. However, the sequential nature of the RNN makes it difficult to leverage the full benefits of modern hardware such as GPUs that offer highly scalable parallel execution. This can significantly slow down both training and inference. Consequently most RNN models including skip-thought are relatively shallow with only a few hidden layers. Moreover, many of the commonly used RNN achitectures such as LSTM BID11 and GRU BID3 , gate information form already seen input at each recurrence step. Repeated gating has an effect where more weight is placed on latter words and the network can ""forget"" earlier parts of the document BID20 . This is not ideal for document embedding where long range relationships that can occur anywhere in the document need to modeled.In this work we propose an embedding model that addresses the aforementioned problems. We show that there is a direct connection between language and embedding models. We then use recent advances in language modeling to derive a convolutional neural network (CNN) embedding model. Similarly to skip-thought, inference in our model is done via a forward pass through the CNN. However, the CNN architecture allows to process the entire document in parallel significantly accelerating both learning and inference. We show that the variable length input problem can be effectively dealt with using either padding or global pooling in the last convolutional layer. Moreover significant gains can be achieved using deeper architectures where each successive layer captures increasingly longer range dependencies in the document. We presented a CNN model for document embedding. In this approach successive layers of convolutions are applied to distributed word representations to model increasingly longer range semantic relationships within the document. We further proposed a stochastic forward prediction learning algorithm where the model is trained to predict the successive words for randomly chosen subsequences within the document. This learning procedure has few hyper parameters to tune and is straightforward to implement. Our model is able to take full advantage of parallel execution, and achieves better performance while also being significantly faster than current state-of-the-art RNN models.",Convolutional neural network model for unsupervised document embedding.,two ; NLP ; RNN ; CNN,full advantage ; Recurrent architecture ; the successive words ; the most commonly used methods ; the model ; range dependencies ; a number ; GPUs ; this category ; very small size,two ; NLP ; RNN ; CNN,"We use recent advances in language modeling to develop a convolutional neural network embedding model, which enables deeper architectures that are fully parallelizable. Stacking layers together increases the receptive filed allowing each successive layer to model increasingly longer range semantic dependences within the document. The final version of this paper will be released with the final version. The most popular embedding models are doc2vec BID21 and skip-thought BID19. These models are trained in unsupervised fashion on unlabeled text, and the resulting embeddings can be used as input for a variety of NLP tasks such as sentiment analysis and",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"We introduce MultiGrain, a neural network architecture that generates compact image embedding vectors that solve multiple tasks of different granularity: class, instance, and copy recognition. MultiGrain is trained jointly for classification by optimizing the cross-entropy loss and for instance/copy recognition by optimizing a self-supervised ranking loss. The self-supervised loss only uses data augmentation and thus does not require additional labels. Remarkably, the unified embeddings are not only much more compact than using several specialized embeddings, but they also have the same or better accuracy. When fed to a linear classifier, MultiGrain using ResNet-50 achieves 79.4% top-1 accuracy on ImageNet, a +1.8% absolute improvement over the the current state-of-the-art AutoAugment method. The same embeddings perform on par with state-of-the-art instance retrieval with images of moderate resolution. An ablation study shows that our approach benefits from the self-supervision, the pooling method and the mini-batches with repeated augmentations of the same image.
 Image recognition is central to computer vision, with dozens of new approaches being proposed every year, each optimized for particular aspects of the problem. From coarse to fine, we may distinguish the recognition of (a) classes, where one looks for a certain type of object regardless of intra-class variations, (b) instances, where one looks for a particular object despite changes in the viewing conditions, and (c) copies, where one looks for a copy of a specific image despite edits. While these problems are in many ways similar, the standard practice is to use specialized, and thus incompatible, image representations for each case. Consider for example image retrieval, where the goal is to match a query image to a large database of other images, whose applications include detection of copyrighted images and exemplar-based recognition of unseen objects. Often one would like to search the same collection with multiple granularities, by matching the query by class, instance, or copy. Adopting multiple image embeddings, narrowly optimized for each granularity, means multiplying the resource usage. Using a single embedding relevant to all these tasks reduces both the computing time and the storage space. However, this might come at the cost of a reduced accuracy. In this paper we introduce MultiGrain, a compact embedding that, as illustrated in fig. 1 , can solve recognition tasks of different granularities while maintaining or surpassing the accuracy of specialized embeddings. MultiGrain is obtained by training a Convolutional Neural Network (CNN) jointly on the different tasks. CNNs trained for image classification are known to be good universal features extractors. However, authors (Babenko & Lempitsky, 2015) have noted that the intermediate layers of such CNNs are generally better for low-level tasks such as instance and copy recognition. In contrast, our work extracts a single global embedding at the top of the network. The key is to optimize this embedding simultaneously for classification and instance retrieval. In this manner, the same representation integrates different degrees of invariance. Indeed, by definition, copies of the same image contain the same instance, and images that contain the same instance also contain the same class. Figure 1 : Top: Our goal is to extract an image descriptor incorporating different levels of granularity, so that we can solve, classification and particular object recognition tasks: The descriptor is either fed to a linear classifier, or directly compared with cosine similarity. Right: The MultiGrain architecture. As an additional contribution, we show that MultiGrain can be learned using only class-level labels via self-supervised learning (Caron et al., 2018) . The instance recognition is learned for free, without labels specific to instance recognition: we use the identity of arbitrary images as labels, and data augmentation to generate different versions of each image. We also find that, unexpectedly, forming batches with multiple augmentations of the same image, improves the classifier performance, even for models trained only for classification. This contradicts the common knowledge that training batches should maximize diversity. Finally, we incorporate in MultiGrain a pooling layer inspired by image retrieval that boosts the classification accuracy for high-resolution images. Overall, MultiGrain offers compelling performance both for classification and image retrieval, including outperforming the SoTA classification accuracy on ImageNet for ResNet-50. MultiGrain is a unified embedding for image classification and instance retrieval. It relies on a classical CNN trunk, with a GeM pooling layer, topped with two heads at training time. We have discovered that this pooling layer allows us to increase the resolution of images used at inference time, while maintaining a small resolution at training time. We have shown that MultiGrain embeddings can perform well on classification and retrieval. Interestingly, MultiGrain also sets a new state of the art on pure classification compared to all results obtained with the same convolutional trunk. Our approach will be open-sourced. We report a few details and additional experiments that did not fit in the main paper. Appendix A outlines the repeated augmentation sampling algorithm. Appendix B illustrates the effect of GeM pooling on activation maps. Appendix C studies the effect of the loss weighting parameter. Appendix D shows the effect of data-augmented batches when training a simple toy model. Appendix E lists the values of a few hyper-parameters used in our method. Appendix F gives a some more ablation results in the retrieval setting. Finally, Appendix G shows how to use the ingredients of MultiGrain to improve the accuracy of an off-the-shelf pre-trained ConvNet at almost no additional training cost. It obtains what appear to be the best reported classification results on imagenet-2012 for a convnet with publicly available weights.","Combining classification and image retrieval in a neural network architecture, we obtain an improvement for both tasks.",CNN ; SoTA ; one ; Babenko & Lempitsky ; dozens ; fig ; Appendix G ; linear ; Convolutional Neural Network ; Appendix F,a unified embedding ; different versions ; images ; two heads ; (a) classes ; these problems ; cosine similarity ; the same representation ; The descriptor ; the pooling method,CNN ; SoTA ; one ; Babenko & Lempitsky ; dozens ; fig ; Appendix G ; linear ; Convolutional Neural Network ; Appendix F,"MultiGrain is a neural network architecture that generates compact image embedding vectors that solve multiple tasks of different granularity: class, instance, copy recognition, and self-supervised ranking loss. The unified embeddings achieve 79.4% top-1 accuracy on ImageNet, a +1.8% absolute improvement over the current state-of-the-art AutoAugment method. The same embedding can be used to search the same collection with multiple granularities, reducing the resource usage and reducing the computing time and storage space. In this paper, MultiGrain, a compact embedding, can solve",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"We present a framework for automatically ordering image patches that enables in-depth analysis of dataset relationship to learnability of a classification task using convolutional neural network. An image patch is a group of pixels residing in a continuous area contained in the sample. Our preliminary experimental results show that an informed smart shuffling of patches at a sample level can expedite training by exposing important features at early stages of training. In addition, we conduct systematic experiments and provide evidence that CNN’s generalization capabilities do not correlate with human recognizable features present in training samples. We utilized the framework not only to show that spatial locality of features within samples do not correlate with generalization, but also to expedite convergence while achieving similar generalization performance. Using multiple network architectures and datasets, we show that ordering image regions using mutual information measure between adjacent patches, enables CNNs to converge in a third of the total steps required to train the same network without patch ordering. Adva nc e s in Deep Lear nin g (DL) and Conv olu tio na l Neura l Netw or ks (CNN ) have dram atic a l l y impro ve d the state-of-the -ar t in compu te r vision tasks. Many of these brea kth ro ug h s are attribute d to the succe ssiv e featu re extrac tion and an increa sin g abstr a ct repre se nta tion of the underly ing training dat a using multi-stag e simple oper ation s such as conv olutio n. These opera tion s posse ss seve ra l mod e l para m ete r s such as conv olution filter whic h are traine d to amplif y and refine infor m a tio n that are relev a n t to the classific a tio n, and to suppr e ss irrele v an t infor m atio n (Ian BID8 . The traini n g proce d u re uses backp ro p a ga tio n algorithm with super vision . This algorith m comb ine d with Stocha s t i c Gradie nt Desc e nt (SGD ), attem pts to minim iz e the over all erro r or devia tio n from true label by compu ti n g the error grad ien t of each para m e te r and by perfo rm in g small upda te s in the opposite directio n. Desp i t e their succ e ss, theore tic a l char ac te riz ation of deep learnin g and CNN s is still at its infanc y and valua b l e corre latio ns such as numbe r of layer s need ed to achie ve a certain perfo rm a n c e are not well under sto o d . However, the success of deep learning has spawned many research avenues in order to explain deep network's exceptional generalization performance BID19 BID14 BID16 Tishby and Zaslavsky, 2015) . One promising theoretical characterization of deep learning that supports an intuition that motivated this work is the characterization that uses an information theoretic view of feature extraction. In particular it is based on the information bottleneck (IB) method which is concerned with the problem of how one extracts an efficient representation of relevant information contained in a large set of features BID21 . BID19 proposes to study deep learning through the lens of information theory using the IB principle. In this characterization, deep learning is modeled as a representation learning. Each layer of a deep neural network can be seen as a set of summary statistics which contain some of the information present in the training set, while retaining as much information about the target output as possible BID19 . In this context a relevant information, of a cat vs dog classification task for instance, is the information pattern present in all the cat samples useful for predicting any picture of a cat. With this view, the amount of information relating the training set and the labels encoded in the hidden layers can be measured over the course of training (Tishby and Zaslavsky, 2015) . Inspired by this view, we use information theoretic measures of entropy extended to measure image characteristics, to develop preprocessing techniques that enable rob ust features extraction during training. One relev a nt insigh t prese nte d in these pape r s is that the goal of DL is to captu re and efficie n tly repr e se nt the relev a nt inform a tion in the input varia b le that desc rib e the outp u t variab le. This is equiv ale nt to the IB meth od whose goal is to find maxim a lly comp re sse d mappin g of the input while prese rvin g as much relev a nt inform ation of the output as possible . This chara cte riz a ti o n leads us to ask the questio n: In superv ise d learnin g, we are intere sted in good featu re repre se nta tio n s of the input patte rn that ena b l e good predictio n of the label BID9 . As a result, a training set for ima g e classific a tio n tasks that employ superv ise d learnin g, is constr uc te d with the help of huma n labele r . For instan ce , for a cat vs dog classifica tion proble m , the huma n labele r must cate go riz e each sample into eithe r one of the classe s. Durin g this proce ss, the labele r must recog niz e and classify each input usin g their own expe rie nc e and distin guis hing capa b ilitie s. Considering this, a natural question we first must answer before addressing the question above is: We proposed several automated patch ordering techniques to assess their impact on training and assess the relationship between dataset characteristics and training and generalization performances . Our methods rank, and reorder patches of every sample based on a standalone meas ure and based on similarit y between patches. We used traditional image similarity measures as well as information theory -based content measures of images to reconstruct training samples. We started off with theoretical foundations for measures used and highlighted the intuition regarding ordering and classification performance. We tested the proposed methods using several architectures, each effectively designed to achieve high accuracy on image classification tasks. The empirical evidence and our analysis using multiple datasets and Inception network architecture, suggest that training a convolutional neural network by supplying inputs that have some ordering, at patch level, according to some measure, are effective in allowing a gradient step to be taken in a direction that minimizes cost at every iteration. Specifically, our experiment s and CIFAR100 (right) datasets. Total training loss (top) and regularization loss (bottom) for Unmodified dataset, and datasets modified by applying Algorithm 1 using the MI metric and patch sizes 4x4, 8x8 and 16x16). The overall size of each sample is 32 by 32.show that supplying training sample such that the mutual information between adjacent patches is minimum, reduces the loss faster than all other techniques when optimizing a non-convex loss function. In addition, using these systematic approaches, we have shown that image characteristics and human recognizable features contained within training samples are uncorrelated with network performance. In other words, the view that CNNs learn combination of features in increasing abstraction does not explain their ability to fit images that have no recognizable features for the human eyes. Such a view also discounts the ability of the networks to fit random noise during training . Instead further investig a t i o n using theore tic a l chara cte riz a tio n s such as the IB metho d are nece ssa ry to form ally char a cte riz e learn ab il i t y of a given trainin g set using CNN .",Develop new techniques that rely on patch reordering to enable detailed analysis of data-set relationship to training and generalization performances.,prese ; IB ; ra l mod e l para ; first ; ga ; algorith m comb ine ; DL ; Zaslavsky ; One ; Unmodified,similarit ; an intuition ; the underly ing training ; every iteration ; their ability ; (right) datasets ; Desp ; the information ; a tio ; image classification tasks,prese ; IB ; ra l mod e l para ; first ; ga ; algorith m comb ine ; DL ; Zaslavsky ; One ; Unmodified,"We present a framework for automatically ordering image patches that enables in-depth analysis of dataset relationship to learnability of classification task using convolutional neural network. An image patch is a group of pixels residing in a continuous area contained in the sample. An informed smart shuffling of patches at a sample level can expedite training by exposing important features at early stages of training. In addition, we conduct systematic experiments and provide evidence that CNN's generalization capabilities do not correlate with human recognizable features present in training samples. We utilized the framework not only to show that spatial locality of features within samples, but also to expedite convergence while achieving",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Artificial neural networks revolutionized many areas of computer science in recent years since they provide solutions to a number of previously unsolved problems.
 On the other hand, for many problems, classic algorithms exist, which typically exceed the accuracy and stability of neural networks.
 To combine these two concepts, we present a new kind of neural networks—algorithmic neural networks (AlgoNets).
 These networks integrate smooth versions of classic algorithms into the topology of neural networks.
 Our novel reconstructive adversarial network (RAN) enables solving inverse problems without or with only weak supervision. Artificial Neural Networks are employed to solve numerous problems, not only in computer science but also in all other natural sciences. Yet, the reasoning for the topologies of neural networks seldom reaches beyond empirically-based decisions. In this work, we present a novel approach to designing neural networks-algorithmic neural networks (short: AlgoNet). Such networks integrate algorithms as algorithmic layers into the topology of neural networks. However, propagating gradients through such algorithms is problematic, because crisp decisions (conditions, maximum, etc.) introduce discontinuities into the loss function. If one passes from one side of a crisp decision to the other, the loss function may change in a non-smooth fashion-it may ""jump."" That is, the loss function suddenly improves (or worsens, depending on the direction) without these changes being locally noticeable anywhere but exactly at these ""jumps."" Hence, a gradient descent based training, regardless of the concrete optimizer, cannot approach these ""jumps"" in a systematic fashion, since neither the loss function nor the gradient provides any information about these ""jumps"" in any place other than exactly the location at which they occur. Therefore, a smoothing is necessary, such that information about the direction of improvement becomes exploitable by gradient descent also in the area surrounding the ""jump."" That is, by smoothing, e.g., an if, one can smoothly, by gradient descent, undergo a transition between the two crisp cases using only local gradient information. Generally, for end-to-end trainable neural network systems, all components should at least be C 0 smooth, i.e., continuous, to avoid ""jumps."" However, having C k smooth, i.e., k times differentiable and then still continuous components with k ≥ 1 is favorable. This property of higher smoothness allows for higher-order derivatives and thus prevents unexpected behavior of the gradients. Hence, we designed smooth approximations to basic algorithms where the functions representing the algorithms are ideally C ∞ smooth. That is, we designed pre-programmed neural networks (restricted to smooth components) with the structure of given algorithms. Related work [1] - [3] in neural networks focused on dealing with crisp decisions by passing through gradients for the alternatives of the decisions. There is no smooth transition between the alternatives, which introduces discontinuities in the loss function that hinder learning, which of the alternatives should be chosen. TensorFlow contains a sorting layer (tf.sort) as well as a while loop construct (tf.while_loop). Since the sorting layer only performs a crisp relocation of the gradients and the while loop has a crisp exit condition, there is no gradient with respect to the conditions in these layers. Concurrently, we developed a smooth sorting layer and a smooth while loop. Theoretical work by DeMillo et al. [4] proved that any program could be modeled by a smooth function. Consecutive works [5] - [7] provided approaches for smoothing programs using, i.a., Gaussian smoothing [6] , [7] . We presented AlgoNets as a new kind of layers for neural networks and RANs as a novel technique for solving ill-posed inverse problems. Concurrent with their benefits, AlgoNets, such as the aforementioned rendering layer, can get computationally very expensive. On the other hand, the rendering layer is very powerful since it allows training a 3D reconstruction without 3D supervision using the RAN. Since the RAN is a very complex architecture that requires a very specific training paradigm, it can also take relatively long to train it. To accommodate this issue, we found that by increasing some loss weights and introducing a probability of whether the computation is executed, the training time can be reduced by a factor of two or more. The AlgoNet can also be used in such a way that algorithmic layers solve sub-problems of a given problem to assist a neural network in solving a larger problem. This principle could also be used in the realm of explainable artificial intelligence [13] by adding residual algorithmic layers into neural networks and then analyzing the neurons of the trained AlgoNet. For that, network activation and/or network sensitivity can indicate the relevance of the residual algorithmic layer. To compute the network sensitivity of an algorithmic layer, the gradient with respect to additional weights (constant equal to one) in the algorithmic layer could be computed. By that, similarities between classic algorithms and the behavior of neural networks could be inferred. An alternative approach would be to gradually replace parts of trained neural networks with algorithmic layers and analyzing the effect on the new model accuracy. In the future, we will develop a high-level smooth programming language to improve smooth representations of higher-level programming concepts. Adding trainable weights to the algorithmic layers to improve the accuracy of smooth algorithms and/or allow the rest of the network to influence the behavior of the algorithmic layer is subject to future research. Another future objective is the exploration of neural networks not with a fixed but instead a smooth topology.",Solving inverse problems by using smooth approximations of the forward algorithms to train the inverse models.,i.a. ; AlgoNet ; AlgoNets ; DeMillo ; Gaussian ; one ; Artificial Neural Networks ; TensorFlow ; two ; RAN,parts ; explainable artificial intelligence ; Such networks ; a high-level smooth programming language ; a gradient descent based training ; gradient descent ; a factor ; RANs ; the training time ; crisp decisions,i.a. ; AlgoNet ; AlgoNets ; DeMillo ; Gaussian ; one ; Artificial Neural Networks ; TensorFlow ; two ; RAN,"Artificial neural networks have revolutionized computer science in recent years. However, they often exceed the accuracy and stability of neural networks. Algorithmic neural networks (AlgoNets) integrate smooth versions of classic algorithms into the topology, enabling inverse problems without or with only weak supervision. Artificial Neural Networks are employed to solve numerous problems, not only in computer science but also in all other natural sciences. The reasoning for neural networks seldom reaches beyond empirically-based decisions. In this work, we present a novel approach to designing neural networks-algorithmic networks (short: AlgoNet). Such networks integrate algorithms as algorith",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Through many recent advances in graph representation learning, performance achieved on tasks involving graph-structured data has substantially increased in recent years---mostly on tasks involving node-level predictions. The setup of prediction tasks over entire graphs (such as property prediction for a molecule, or side-effect prediction for a drug), however, proves to be more challenging, as the algorithm must combine evidence about several structurally relevant patches of the graph into a single prediction.
 Most prior work attempts to predict these graph-level properties while considering only one graph at a time---not allowing the learner to directly leverage structural similarities and motifs across graphs. Here we propose a setup in which a graph neural network receives pairs of graphs at once, and extend it with a co-attentional layer that allows node representations to easily exchange structural information across them. We first show that such a setup provides natural benefits on a pairwise graph classification task (drug-drug interaction prediction), and then expand to a more generic graph regression setup: enhancing predictions over QM9, a standard molecular prediction benchmark. Our setup is flexible, powerful and makes no assumptions about the underlying dataset properties, beyond anticipating the existence of multiple training graphs. We study the task of graph-level representation learning: i.e., computing representations of entire input graphs, for the purposes of downstream tasks (such as graph classification or regression). This is typically a step-up in complexity compared to node classification or link prediction, given that the learning algorithm must aggregate useful structural information across the graph into a single prediction-relying on only this global supervision signal (as opposed to having feedback from every node/edge of the graph). Perhaps the highest challenge this kind of architecture must face is inductivity and generalisation across structures. Specifically, an inductive model must be readily applicable across several graph structures-including ones unseen during training. Additionally, the model is tasked with discovering interesting structural ""motifs"" across the entire dataset of graphs, whose presence or absence may help determine the overall predictions. However, even enabling inductivity is not a traditionally simple task in graph representation learning, as many prior approaches (Bruna et al., 2013; Perozzi et al., 2014; Defferrard et al., 2016) are not inductive by design. Furthermore, even the models that are currently used for graph-level representation learning; e.g. Gilmer et al. (2017) ; ; Xu et al. (2018) ; Lu et al. (2019) , operate over only a single graph at a time-making it challenging for them to reason about common substructures across graphs from a graph-level supervision signal alone. In this manuscript, we propose the approach of paired training-i.e., learning representations over pairs of input graphs at once. Intuitively, as long as we allow for dataflow between the representations of the two graphs within a pair, this allows the graph neural network to directly observe related (sub)structures from other inputs, to solidify its decision making. We note that in the context of graph-structured inputs this may be particularly useful as, unlike simpler inputs such as images or text, there are no guarantees that different graphs within a dataset will have equal or even similar overall structure. To facilitate this dataflow, we propose the usage of graph co-attention for exchanging representations of nodes across the two graphs. Intuitively, this operator performs attention (Bahdanau et al., 2014; Vaswani et al., 2017) over the fully-connected bipartite graph, with one part corresponding to all nodes in one graph. This allows every node of the first graph to detect and reuse useful patch representations in the second graph (in a form of hierarchical graph matching), and vice-versa. Initially, we validate our model performance on a pairwise graph classification task-classifying drug pairs for side effects caused by drug-drug interactions (DDI) (Jin et al., 2017; Zitnik et al., 2018) . In this setting, a pairwise approach is natural, as we inherently have to classify pairs of graphs. We demonstrate that learning a joint representation using graph co-attention provides substantial benefits to predictive power, setting the state-of-the-art result on this task. From there, we demonstrate the applicability of our approach to arbitrary multi-graph datasets; for this, we leverage the QM9 dataset for predicting quantum chemistry properties of small molecules (Ramakrishnan et al., 2014) . As such, it represents a challenging graph regression problem. We propose using paired training to perform regression on two molecules at once, demonstrating clear benefits to doing so. In a similar vein, we execute variants of our model on standard graph kernel classification benchmarks (Kersting et al., 2016) , showing advantages to generic graph classification. Our approach paves the way to a promising direction for graph-level prediction tasks, that is in principle applicable to any kind of multi-graph dataset, especially under availability of large quantities of labelled examples. Our model builds up on a large existing body of work in graph convolutional networks (Bruna et al., 2013; Defferrard et al., 2016; Kipf & Welling, 2016a; Gilmer et al., 2017; , that have substantially advanced the state-of-the-art in many tasks requiring graph-structured input processing (such as the chemical representation (Gilmer et al., 2017; De Cao & Kipf, 2018; of the drugs leveraged here). Furthermore, we build up on work proposing co-attention (Lu et al., 2016; Deac et al., 2018) as a mechanism to allow for individual set-structured datasets (such as nodes in multimodal graphs) to interact. Specifically, such mechanisms have already been used for explicit matching of graph structure motifs (Li et al., 2019) , and therefore represent a natural methodology for our purposes. Overall, these (and related) techniques lie within the domain of graph representation learning, one of the latest major challenges of machine learning (Bronstein et al., 2017; Hamilton et al., 2017; Battaglia et al., 2018) , with transformative potential across a wide spectrum of potential applications, extending outside the biochemical domain. We have presented a novel way of training graph neural networks on multi-graph datasets, relying on making predictions jointly, in pairs of graphs-the paired training approach. Additionally, we allowed for arbitrary representation exchange between these graphs by way of co-attentive mechanisms. The two combined allow for extraction of stronger and more robust representations as opposed to single-graph learning, which is a claim we verified across several established molecular prediction tasks: polypharmacy side effect prediction (where we set a state-of-the-art result), quantum chemistry properties prediction and graph classification. As a flexible and generic approach which doesn't rely on dataset properties in any way, so long as it consists of multiple graphs, we believe it to be a useful direction to explore for graph representation learning as a whole.",We use graph co-attention in a paired graph training system for graph classification and regression.,Li ; Lu et al. ; Defferrard et al. ; De Cao & Kipf ; Xu et al ; Lu et al ; Kipf & Welling ; Ramakrishnan et al. ; Battaglia ; Vaswani,every node ; Most prior work attempts ; structural information ; a pair ; the underlying dataset properties ; these ; Hamilton et al ; predictive power ; these graphs ; arbitrary multi-graph datasets,Li ; Lu et al. ; Defferrard et al. ; De Cao & Kipf ; Xu et al ; Lu et al ; Kipf & Welling ; Ramakrishnan et al. ; Battaglia ; Vaswani,"In graph representation learning, performance achieved on tasks involving graph-structured data has significantly increased in recent years. However, the setup of prediction tasks over entire graphs (such as property prediction for a molecule, side-effect prediction, drug-drug interaction prediction) is more challenging, as the learning algorithm must combine evidence about several structurally relevant patches of the graph into a single prediction. In contrast to node classification or link prediction, inductivity and generalisation across structures are not inductive by design. The task of graph-level representation learning involves computing representations of entire input graphs, and the model is tasked with discovering interesting structural """,/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"In this paper, we consider the problem of training neural networks (NN). To promote a NN with specific structures, we explicitly take into consideration the nonsmooth regularization (such as L1-norm) and constraints (such as interval constraint). This is formulated as a constrained nonsmooth nonconvex optimization problem, and we propose a convergent proximal-type stochastic gradient descent (Prox-SGD) algorithm. We show that under properly selected learning rates, momentum eventually resembles the unknown real gradient and thus is crucial in analyzing the convergence. We establish that with probability 1, every limit point of the sequence generated by the proposed Prox-SGD is a stationary point. Then the Prox-SGD is tailored to train a sparse neural network and a binary neural network, and the theoretical analysis is also supported by extensive numerical tests. In this paper, we consider the problem of training neural networks (NN) under constraints and regularization. It is formulated as an optimization problem where x is the parameter vector to optimize, y i is the i-th training example which consists of the training input and desired output, and m is the number of training examples. The training loss f is assumed to be smooth (but nonconvex) with respect to x, the regularization r is assumed to be convex (but nonsmooth), proper and lower semicontinuous, and the constraint set X is convex and compact (closed and bounded). When r(x) = 0 and X = R n , stochastic gradient descent (SGD) has been used to solve the optimization problem (1). At each iteration, a minibatch of the m training examples are drawn randomly, and the obtained gradient is an unbiased estimate of the true gradient. Therefore SGD generally moves along the descent direction, see Bertsekas & Tsitsiklis (2000) . SGD can be accelerated by replacing the instantaneous gradient estimates by a momentum aggregating all gradient in past iterations. Despite the success and popularity of SGD with momentum, its convergence had been an open problem. Assuming f is convex, analyzing the convergence was first attempted in Kingma & Ba (2015) and later concluded in Reddi et al. (2018) . The proof for a nonconvex f was later given in Chen et al. (2019) ; Lei et al. (2019) . In machine learning, the regularization function r is typically used to promote a certain structure in the optimal solution, for example sparsity as in, e.g., feature selection and compressed sensing, or a zero-mean-Gaussian prior on the parameters (Bach et al., 2011; Boyd et al., 2010) . It can be interpreted as a penalty function since at the optimal point x of problem (1), the value r(x ) will be small. One nominant example is the Tikhonov regularization r(x) = µ x 2 2 for some predefined constant µ, and it can be used to alleviate the ill-conditioning and ensure that the magnitude of the weights will not become exceedingly large. Another commonly used regularization, the 1 -norm where r(x) = µ x 1 = µ n j=1 |x j | (the convex surrogate of the 0 -norm), would encourage a sparse solution. In the context of NN, it is used to (i) promote a sparse neural network (SNN) to alleviate overfitting and to allow a better generalization, (ii) accelerate the training process, and (iii) prune the network to reduce its complexity, see Louizos et al. (2018) and Gale et al. (2019) . Technically, it is difficult to analyze the regularizations as some commonly used convex regularizers are nonsmooth, for example, 1 -norm. In current implementations of Tensorflow, the gradient of |x| is simply set to 0 when x = 0. This amounts to the stochastic subgradient descent method and usually exhibits slow convergence. Other techniques to promote a SNN includes magnitude pruning and variational dropout, see Gale et al. (2019) . Although regularization can be interpreted as a constraint from the duality theory, sometimes it may still be more desirable to use explicit constraints, for example, x 2 j ≤ α, where the summation is over the weights on the same layer. This is useful when we already know how to choose α. Another example is the lower and upper bound on the weights, that is, l ≤ w ≤ u for some predefined l and u. Compared with regularization, constraints do not encourage the weights to stay in a small neighborhood of the initial weight, see Chapter 7.2 of Goodfellow et al. (2016) for more details. The set X models such explicit constraints, but it poses an additional challenge for stochastic gradient algorithms as the new weight obtained from the SGD method (with or without momentum) must be projected back to the set X to maintain its feasibility. However, projection is a nonlinear operator, so the unbiasedness of the random gradient would be lost. Therefore the convergence analysis for constrained problems is much more involved than unconstrained problems. In this paper, we propose a convergent proximal-type stochastic gradient algorithm (Prox-SGD) to train neural networks under nonsmooth regularization and constraints. It turns out momentum plays a central role in the convergence analysis. We establish that with probability (w.p.) 1, every limit point of the sequence generated by Prox-SGD is a stationary point of the nonsmooth nonconvex problem (1). This is in sharp contrast to unconstrained optimization, where the convergence of the vanilla SGD method has long been well understood while the convergence of the SGD method with momentum was only settled recently. Nevertheless, the convergence rate of Prox-SGD is not derived in the current work and is worth further investigating. To test the proposed algorithm, we consider two applications. The first application is to train a SNN, and we leverage 1 -regularization, that is, The second application is to train a binary neural network (BNN) where the weights (and activations) are either 1 or -1 (see Courbariaux et al. (2015; ; Hou et al. (2017) ; Yin et al. (2018) ; Bai et al. (2019) for more details). To achieve this, we augment the loss function with a term that penalizes the weights if they are not +1 or -1: where µ is a given penalty parameter. The binary variable a j can be interpreted as a switch for weight x j : when a j = 0, (1 − a j )(x j − 1) 2 is activated, and there is a strong incentive for x j to be 1 (the analysis for a j = 1 is similar). Since integer variables are difficult to optimize, we relax a j to be a continuous variable between 0 and 1. To summarize, a BNN can be obtained by solving the following regularized optimization problem under constraints with respect to x and a If µ is properly selected (or sufficiently large), the optimal a j will be exactly or close to 0 or 1. Consequently, regularization and constraints offer interpretability and flexibility, which allows us to use more accurate models to promote structures in the neural networks, and the proposed convergent Prox-SGD algorithm ensures efficient training of such models.",We propose a convergent proximal-type stochastic gradient descent algorithm for constrained nonsmooth nonconvex optimization problems,≤ w ≤ ; |x| ; Reddi ; Hou et al ; Lei et al ; zero ; Tensorflow ; Louizos et al. ; Gale ; Yin et al,example ; the duality theory ; algorithm ; the regularization r ; an optimization problem ; more accurate models ; the convex surrogate ; such models ; stochastic gradient algorithms ; projection,≤ w ≤ ; |x| ; Reddi ; Hou et al ; Lei et al ; zero ; Tensorflow ; Louizos et al. ; Gale ; Yin et al,"In this paper, we consider training neural networks (NN) under constraints and regularization. This is formulated as a constrained nonsmooth nonconvex optimization problem, and a convergent proximal-type stochastic gradient descent (Prox-SGD) algorithm. The convergence of SGD under properly selected learning rates, momentum eventually resembles the unknown real gradient and is crucial in analyzing the convergence. In machine learning, the regularization function r is typically used to promote a certain structure in the optimal. The training loss f is assumed to be smooth, proper and compact, and the constraint set X is convex and",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Particle-based inference algorithm is a promising method to efficiently generate samples for an intractable target distribution by iteratively updating a set of particles. As a noticeable example, Stein variational gradient descent (SVGD) provides a deterministic and computationally efficient update, but it is known to underestimate the variance in high dimensions, the mechanism of which is poorly understood. In this work we explore a connection between SVGD and MMD-based inference algorithm via Stein's lemma. By comparing the two update rules, we identify the source of bias in SVGD as a combination of high variance and deterministic bias, and empirically demonstrate that the removal of either factors leads to accurate estimation of the variance. In addition, for learning high-dimensional Gaussian target, we analytically derive the converged variance for both algorithms, and confirm that only SVGD suffers from the ""curse of dimensionality"". The Stein Variational Gradient Descent (SVGD) (Liu and Wang, 2016 ) is a deterministic particle-based inference algorithm that iteratively transports the particles by the functional gradient in the reproducing kernel Hilbert space (RKHS) of KL-divergence, which takes the form of a kernelized Stein's operator. In contrast to the empirical successes (Liu et al., 2017; Haarnoja et al., 2017; Kim et al., 2018) , very few convergence guarantees have been established for SVGD except for the mean-field regime (Liu and Wang, 2018; Lu et al., 2019) . Moreover, it has been observed that the variance estimated by SVGD scales inversely with the dimensionality of the problem. This is a highly undesirable property for two reasons: 1) underestimating the variance leads to failures of explaining the uncertainty of model predictions; 2) modern inference problems are usually high-dimensional. For example, Bayesian neural networks (MacKay, 1992) could be more than millions of dimensions. We study the algorithmic bias of SVGD that leads to the variance underestimation in high dimensions. We construct another kernel-based inference algorithm termed MMDdescent, which closely resembles SVGD but estimate the variance accurately. By comparing their updates, we identify the cause of variance collapse in SVGD as a combination of high variance due to Stein's lemma, and deterministic bias, i.e. the inability to resample particles. We empirically verify that removing either of these two factors, while computationally expensive, leads to accurate variance estimation. Then, under mild assumptions, we derive the equilibrium variance of SVGD and MMD-descent in matching high-dimensional Gaussians, and confirm that variance estimated by SVGD scales inversely with the dimensionality.",Analyze the underlying mechanisms of variance collapse of SVGD in high dimensions.,Lu et al. ; Wang ; Gaussian ; RKHS ; more than millions ; MMDdescent ; Liu ; Bayesian ; Hilbert ; MacKay,failures ; the mean-field regime ; Particle-based inference algorithm ; example ; a deterministic and computationally efficient update ; this work ; Liu et al ; the reproducing kernel ; the empirical successes ; samples,Lu et al. ; Wang ; Gaussian ; RKHS ; more than millions ; MMDdescent ; Liu ; Bayesian ; Hilbert ; MacKay,"The Stein variational gradient descent (SVGD) is a deterministic and computationally efficient update, but it is known to underestimate the variance in high dimensions due to Stein's lemma and deterministic bias. The MMD-based inference algorithm, which closely resembles SVGD, estimates the variance accurately, and removes either of these factors, while computationally expensive, leads to accurate estimation of the variance. For learning high-dimensional Gaussian target, we analyze the converged variance for both algorithms, and confirm that only SVGD suffers from the ""curse of dimensionality"". The Stein Variational Gradient Descent (",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"We first pose the Unsupervised Continual Learning (UCL) problem: learning salient representations from a non-stationary stream of unlabeled data in which the number of object classes varies with time. Given limited labeled data just before inference, those representations can also be associated with specific object types to perform classification. To solve the UCL problem, we propose an architecture that involves a single module, called Self-Taught Associative Memory (STAM), which loosely models the function of a cortical column in the mammalian brain. Hierarchies of STAM modules learn based on a combination of Hebbian learning, online clustering, detection of novel patterns and forgetting outliers, and top-down predictions. We illustrate the operation of STAMs in the context of learning handwritten digits in a continual manner with only 3-12 labeled examples per class. STAMs suggest a promising direction to solve the UCL problem without catastrophic forgetting.",We introduce unsupervised continual learning (UCL) and a neuro-inspired architecture that solves the UCL problem.,first ; the Unsupervised Continual Learning ; UCL ; Self-Taught Associative Memory ; Hebbian ; STAMs,specific object types ; outliers ; which ; detection ; the operation ; class ; limited labeled data ; novel patterns ; the number ; Hebbian learning,first ; the Unsupervised Continual Learning ; UCL ; Self-Taught Associative Memory ; Hebbian ; STAMs,"The Unsupervised Continual Learning (UCL) problem involves learning salient representations from a non-stationary stream of unlabeled data in which the number of object classes varies with time. These representations can also be associated with specific object types to perform classification. The self-taught Associative Memory (STAM) architecture combines Hebbian learning, online clustering, and top-down predictions. STAMs offer a promising direction to solve the UCL problem without catastrophic forgetting.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Most approaches to learning action planning models heavily rely on a significantly large volume of training samples or plan observations. In this paper, we adopt a different approach based on deductive learning from domain-specific knowledge, specifically from logic formulae that specify constraints about the possible states of a given domain. The minimal input observability required by our approach is a single example composed of a full initial state and a partial goal state. We will show that exploiting specific domain knowledge enables to constrain the space of possible action models as well as to complete partial observations, both of which turn out helpful to learn good-quality action models. The learning of action models in planning has been typically addressed with inductive learning data-intensive approaches. From the pioneer learning system ARMS BID13 ) to more recent ones BID8 Zhuo and Kambhampati 2013; Kucera and Barták 2018) , all of them require thousands of plan observations or training samples, i.e., sequences of actions as evidence of the execution of an observed agent, to obtain and validate an action model. These approaches return the statistically significant model that best explains the plan observations by minimizing some error metric. A model explains an observation if a plan containing the observed actions is computable with the model and the states induced by this plan also include the possibly partially observed states. The limitation of posing model learning and validation as optimization tasks over a set of observations is that it neither guarantees completeness (the model may not explain all the observations) nor correctness (the states induced by the execution of the plan generated with the model may contain contradictory information).Differently , other approaches rely on symbolic-via learning. The Simultaneous Learning and Filtering (SLAF) approach BID2 exploits logical inference and builds a complete explanation through a CNF formula that represents the initial belief state, and a plan observation. The formula is updated with every action and state of Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. the observation, thus representing all possible transition relations consistent with it. SLAF extracts all satisfying models of the learned formula with a SAT solver although the algorithm cannot effectively learn the preconditions of actions. A more recent approach addresses the learning of action models from plan observations as a planning task which searches the space of all possible action models BID0 . A plan here is conceived as a series of steps that determine the preconditions and effects of the action models plus other steps that validate the formed actions in the observations. The advantage of this approach is that it only requires input samples of about a total of 50 actions.This paper studies the impact of using mixed input data, i.e, automatically-collected plan observations and humanencoded domain-specific knowledge, in the learning of action models. Particularly, we aim to stress the extreme case of having a single observation sample and answer the question to whether the lack of training samples can be overcome with the supply of domain knowledge. The question is motivated by (a) the assumption that obtaining enough training observations is often difficult and costly, if not impossible in some domains (Zhuo 2015); (b) the fact that although the physics of the real-world domain being modeled are unknown, the user may know certain pieces of knowledge about the domain; and (c) the desire for correct action models that are usable beyond their fitness to a set of testing observations. To this end, we opted for checking our hypothesis in the framework proposed in BID0 since this planning-based satisfiability approach allows us to configure additional constraints in the compilation scheme, it is able to work under a minimal set of observations and uses an off-the-shelf planner 1 . Ultimately, we aim to compare the informational power of domain observations (information quantity) with the representational power of domainspecific knowledge (information quality). Complementarily, we restrict our attention to solely observations over fluents as in many applications the actual actions of an agent may not be observable BID11 .Next section summarizes basic planning concepts and outlines the baseline learning approach BID0 ). Then we formalize our one-shot learning task with domain knowledge and subsequently we explain the task-solving process. Section 5 presents the experimental evaluation and last section concludes. We present an approach to learn action models that builds upon a former compilation-to-planning learning system BID0 . Our proposal studies the gains of using domain-specific knowledge when the availability (amount and observability) of learning examples is very limited. Introducing domain knowledge encoded as schematic mutexes allows to narrow down the search space of the learning task and improve overall the performance of the learning system to the point that it offsets the lack of learning examples. In a theoretical work that analyzes the relation between the number of observed trajectory plans and the guarantee for a learned action model to achieve the goal BID12 , authors conclude that the number of trajectories needed scales gracefully and the guarantee grows linearly with the number of predicates and quasi-linearly with the number of actions. This evidences that learning accurate models is heavily dependent on the number and quality (observability) of the learning examples. In this sense, our proposal comes to alleviate this dependency by relying on easily deducible domain knowledge. It is not only capable of learning from a single non-fully observable learning example but also proves that learning from a 30%-observable example with domain-specific knowledge is comparable to learning from a complete plan observation.",Hybrid approach to model acquisition that compensates a lack of available data with domain specific knowledge provided by experts,Filtering ; i.e ; The Simultaneous Learning ; SAT ; SLAF ; Association for the Advancement of Artificial Intelligence ; Copyright ; CNF ; thousands ; Zhuo,"the experimental evaluation ; information quantity ; more recent ones ; the availability ; a plan ; observations ; our hypothesis ; enough training observations ; this end ; i.e., sequences",Filtering ; i.e ; The Simultaneous Learning ; SAT ; SLAF ; Association for the Advancement of Artificial Intelligence ; Copyright ; CNF ; thousands ; Zhuo,"The learning of action planning models relies on a significantly large volume of training samples or plan observations. In this paper, we explore deductive learning from domain-specific knowledge and logic formulae that specify constraints about the possible states of a given domain. The minimal input observability required by our approach is due to exploiting specific domain knowledge, which enables constrain the space of possible action models as well as to complete partial observations, both of which turn out to be useful for learning good-quality action models. The learning of actions models in planning has been typically addressed with inductive learning data-intensive approaches. From the pioneer learning system",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Over the last few years, the phenomenon of adversarial examples --- maliciously constructed inputs that fool trained machine learning models --- has captured the attention of the research community, especially when the adversary is restricted to making small modifications of a correctly handled input. At the same time, less surprisingly, image classifiers lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this work, we show that these are two manifestations of the same underlying phenomenon. We establish this connection in several ways. First, we find that adversarial examples exist at the same distance scales we would expect from a linear model with the same performance on corrupted images. Next, we show that Gaussian data augmentation during training improves robustness to small adversarial perturbations and that adversarial training improves robustness to several types of image corruptions. Finally, we present a model-independent upper bound on the distance from a corrupted image to its nearest error given test performance and show that in practice we already come close to achieving the bound, so that improving robustness further for the corrupted image distribution requires significantly reducing test error. All of this suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. This yields a computationally tractable evaluation metric for defenses to consider: test error in noisy image distributions. State-of-the-art computer vision models can achieve superhuman performance on many image classification tasks. Despite this, these same models still lack the robustness of the human visual system to various forms of image corruptions. For example, they are distinctly subhuman when classifying images distorted with additive Gaussian noise BID12 , they lack robustness to different types of blur, pixelation, and changes in brightness BID17 , lack robustness to random translations of the input BID2 , and even make errors when foreign objects are inserted into the field of view BID25 . At the same time, they also are sensitive to small, worst-case perturbations of the input, so-called ""adversarial examples"" BID28 . This latter phenomenon has struck many in the machine learning community as surprising and has attracted a great deal of research interest, while the former seems to inspire less surprise and has received considerably less attention.Our classification models make errors on two different sorts of inputs: those found by randomly sampling from some predetermined distribution, and those found by an adversary deliberately searching for the closest error to a given point. In this work, we ask what, if anything, is the difference between these two types of error. Given that our classifiers make errors in these corrupted image distributions, there must be a closest such error; do we find that this closest error appears at the distance we would expect from the model's performance in noise, or is it in fact ""surprisingly"" close?The answer to this question has strong implications for the way we approach the task of eliminating these two types of errors. An assumption underlying most of the work on adversarial examples is that solving it requires a different set of methods than the ones being developed to improve model generalization. The adversarial defense literature focuses primarily on improving robustness to small perturbations of the input and rarely reports improved generalization in any distribution.We claim that, on the contrary, adversarial examples are found at the same distance scales that one should expect given the performance on noise that we see in practice. We explore the connection between small perturbation adversarial examples and test error in noise in two different ways.First, in Sections 4 and 5, we provide empirical evidence of a close relationship between test performance in Gaussian noise and adversarial perturbations. We show that the errors we find close to the clean image and the errors we sample under Gaussian noise are part of the same large set and show some visualizations that illustrate this relationship. (This analysis builds upon prior work which makes smoothness assumptions on the decision boundary to relate these two quantities.) This suggests that training procedures designed to improve adversarial robustness might reduce test error in noise and vice versa. We provide results from experiments which show that this is indeed the case: for every model we examined, either both quantities improved or neither did. In particular , a model trained on Gaussian noise shows significant improvements in adversarial robustness, comparable to (but not quite as strong as) a model trained on adversarial examples. We also found that an adversarially trained model on CIFAR-10 shows improved robustness to random image corruptions.Finally, in Section 6, we establish a relationship between the error rate of an image classification model in the presence of Gaussian noise and the existence of adversarial examples for noisy versions of test set images. In this setting we can actually prove a rigorous, model-independent bound relating these two quantities that is achieved when the error set is a half space, and we see that the models we tested are already quite close to this optimum. Therefore, for these noisy image distributions, our models are already almost as adversarially robust as they can be given the error rates we see, so the only way to defend against adversarial examples is to reduce test error.In this work we will investigate several different models trained on the MNIST, CIFAR-10 and ImageNet datasets. For MNIST and CIFAR-10 we look at the naturally trained and adversarially trained models which have been open-sourced by BID22 . We also trained the same model on CIFAR-10 with Gaussian data augmentation. For ImageNet, we investigate Wide ResNet-50 trai]ned with Gaussian data augmentation. We were unable to study the effects of adversarial training on ImageNet because no robust open sourced model exists (we considered the models released in BID29 but found that they only minimally improve robustness to the white box PGD adversaries we consider here). Additional training details can be found in Appendix A. We proved a fundamental relationship between generalization in noisy image distributions and the existence of small adversarial perturbations. By appealing to the Gaussian isoperimetric inequality, we formalized the notion of what it means for a decision boundary to be badly behaved. We showed that, for noisy images, there is very little room to improve robustness without also decreasing the volume of the error set, and we provided evidence that small perturbations of clean images can also be explained in a similar way. These results show that small-perturbation adversarial robustness is closely related to generalization in the presence of noise and that future defense efforts can measure progress by measuring test error in different noise distributions.Indeed, several such noise distributions have already been proposed, and other researchers have developed methods which improve generalization in these distributions BID17 BID12 a; BID30 BID35 . Our work suggests that adversarial defense and improving generalization in noise involve attacking the same set of errors in two different ways -the first community tries to remove the errors on the boundary of the error set while the second community tries to reduce the volume of the error set. The isoperimetric inequality connects these two perspectives, and suggests that improvements in adversarial robustness should result in improved generalization in noise and vice versa. Adversarial training on small perturbations on CIFAR-10 also improved generalization in noise, and training on noise improved robustness to small perturbations.In the introduction we referred to a question from BID28 about why we find errors so close to our test points while the test error itself is so low. We can now suggest an answer: despite what our low-dimensional visual intuition may lead us to believe, these errors are not in fact unnaturally close given the error rates we observe in noise. There is a sense, then, in which we simply haven't reduced the test error enough to expect to have removed most nearby errors.While we focused on the Gaussian distribution, similar conclusions can be made about other distributions. In general, in high dimensions, the -boundary measure of a typical set is large even when its volume is small, and this observation does not depend on anything specific about the Gaussian distribution. The Gaussian distribution is a special case in that we can easily prove that all sets will have large -boundary measure. BID23 proved a similar theorem for a larger class of distributions. For other data distributions not every set has large -boundary measure, but under some additional assumptions it still holds that most sets do. An investigation of this relationship on the MNIST distribution can be found in Gilmer et al. (2018b, Appendix G) .We believe it would be beneficial for the adversarial defense literature to start reporting generalization in noisy image distributions, such as the common corruption benchmark introduced in BID17 , rather than the current practice of only reporting empirical estimates of adversarial robustness. There are several reasons for this recommendation.1. Measuring test error in noise is significantly easier than measuring adversarial robustnesscomputing adversarial robustness perfectly requires solving an NP-hard problem for every point in the test set BID19 . Since BID28 , hundreds of adversarial defense papers have been published. To our knowledge , only one BID22 has reported robustness numbers which were confirmed by a third party. We believe the difficulty of measuring robustness under the usual definition has contributed to this unproductive situation. 2. Measuring test error in noise would also allow us to determine whether or not these methods improve robustness in a trivial way, such as how the robust MNIST model learned to threshold the input, or whether they have actually succeeded in improving generalization outside the natural data distribution. 3. All of the failed defense strategies we examined failed to improve generalization in noise.For this reason, we should be highly skeptical of defense strategies that only claim improved l p -robustness but do not demonstrate robustness in more general settings. 4. Finally, if the goal is improving the security of our models in adversarial settings, errors in the presence of noise are already indicative that our models are not secure. Until our models are perfectly robust in the presence of average-case corruptions, they will not be robust in worst-case settings. The usefulness of l p -robustness in realistic threat models is limited when attackers are not constrained to making small modifications.The interest in measuring l p robustness arose from a sense of surprise that errors could be found so close to correctly classified points. But from the perspective described in this paper, the phenomenon is less surprising. Statistical classifiers make a large number of errors outside the data on which they were trained, and small adversarial perturbations are simply the nearest ones. Table 3 : The models from Section 1 trained and tested on ImageNet with Gaussian noise with standard deviation σ; the column labeled 0 refers to a model trained only on clean images.",Small adversarial perturbations should be expected given observed error rates of models outside the natural data distribution.,half ; hundreds ; two ; PGD ; Gilmer ; Appendix G ; First ; ImageNet ; third ; Appendix A. We,"other data distributions ; the clean image ; small, worst-case perturbations ; the natural data distribution ; the data ; this reason ; An assumption ; the error rate ; inputs ; an NP-hard problem",half ; hundreds ; two ; PGD ; Gilmer ; Appendix G ; First ; ImageNet ; third ; Appendix A. We,"In the last few years, adversarial examples have captured the attention of the research community, especially when the adversary is restricted to small modifications of a correctly handled input. However, image classifiers lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this work, Gaussian data augmentation improves robustness to small adversarial perturbations and adversarial training improves performance to several types of image corruptions. In addition, we present a model-independent upper bound on the distance from a corrupted image to its nearest error given test performance and show that in practice, improving robustness further for",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"In this paper, we propose the use of in-training matrix factorization to reduce the model size for neural machine translation. Using in-training matrix factorization, parameter matrices may be decomposed into the products of smaller matrices, which can compress large machine translation architectures by vastly reducing the number of learnable parameters. We apply in-training matrix factorization to different layers of standard neural architectures and show that in-training factorization is capable of reducing nearly 50% of learnable parameters without any associated loss in BLEU score. Further, we find that in-training matrix factorization is especially powerful on embedding layers, providing a simple and effective method to curtail the number of parameters with minimal impact on model performance, and, at times, an increase in performance. While neural models for machine translation have realized considerable breakthroughs in recent years and are now state-of-the-art in many contexts, they are frequently expensive in resources, owing to their large number of parameters. In a context of democratization of deep learning tools, having smaller models that can be learned and/or applied offline on small devices would have immediate and important applications, for example for privacy reasons. However, it is often necessary to leverage deep networks with large layers in order to capture abstract and complex patterns and interactions over the data. We posit that the need for such large parameter matrices is not because they are essential to the representational power of the network. Rather, our hypothesis is that having many parameters can help neural architectures overcome limitations introduced with approximate optimization methods, noisy data supervision, and sub-optimal model architectures. Moreover, recent work has suggested many parameters in large neural architectures are largely superfluous, serving solely to accommodate the stochastic nature of modern machine learning optimization algorithms (Frankle & Carbin, 2019) . Motivated by those hypotheses, we study the application of in-training matrix factorization to neural machine translation. Traditional matrix factorization methods are a simple but powerful technique that has been used after training in other deep learning systems, such as Sainath et al. (2013) for speech recognition and Kim et al. (2015) for computer vision. In contrast to traditional matrix factorization techniques, in-training matrix factorization reduces the number of learnable parameters at training time, lessening the need for computational resources. The main contributions of this work are: 1. We formally define in-training matrix factorization and present a technique to utilize matrix factorization during training time. 2. We conduct sets of experiments on two standard neural machine translation architectures: the LSTM encoder-decoder and the transformer network. 3. We show that in-training factorization can decrease a model's size by half with no impact on performance, and at times, can improve model performance Figure 1 : A diagram of in-training factorization. A weight matrix is replaced by the product of two weight matrices, followed by the bias and activation. We have introduced a method to use matrix factorization at training time to reduce the parameter footprint of neural machine translation models. We compare in-training factorization to existing post-hoc parameter reduction methods, including parameter pruning and post-training factorization. We find that using factorization comes with significant gains in both final performance and number of required parameters. Lastly, we demonstrate the effectiveness of our in-training factorization technique to learn a model with fewer parameters, improved accuracy, and decreased training time.","This paper proposes using matrix factorization at training time for neural machine translation, which can reduce model size and decrease training time without harming performance.",BLEU ; recent years ; noisy data supervision ; Frankle & Carbin ; Sainath et al. ; Kim et al ; two ; half,Frankle ; different layers ; parameters ; the representational power ; machine translation ; activation ; neural machine translation models ; Sainath et al ; significant gains ; contrast,BLEU ; recent years ; noisy data supervision ; Frankle & Carbin ; Sainath et al. ; Kim et al ; two ; half,"In-training matrix factorization can reduce the model size for neural machine translation by decompressing parameter matrices into smaller matrices, which can compress large machine translation architectures by vastly reducing the number of learnable parameters. In-training factorization is especially powerful on embedding layers, providing a simple and effective method to curtail the number number of parameters with minimal impact on model performance, and, at times, an increase in performance. However, it is often necessary to leverage deep networks with large layers in order to capture abstract and complex patterns and interactions over the data. The need for such large parameters is not because they are essential to",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Informed and robust decision making in the face of uncertainty is critical for robots that perform physical tasks alongside people. We formulate this as a Bayesian Reinforcement Learning problem over latent Markov Decision Processes (MDPs). While Bayes-optimality is theoretically the gold standard, existing algorithms do not scale well to continuous state and action spaces. We propose a scalable solution that builds on the following insight: in the absence of uncertainty, each latent MDP is easier to solve. We split the challenge into two simpler components. First, we obtain an ensemble of clairvoyant experts and fuse their advice to compute a baseline policy. Second, we train a Bayesian residual policy to improve upon the ensemble's recommendation and learn to reduce uncertainty. Our algorithm, Bayesian Residual Policy Optimization (BRPO), imports the scalability of policy gradient methods as well as the initialization from prior models. BRPO significantly improves the ensemble of experts and drastically outperforms existing adaptive RL methods. Robots operating in the real world must resolve uncertainty on a daily basis. Often times, a robot is uncertain about how the world around it evolves. For example, a self-driving car must drive safely around unpredictable actors like pedestrians and bicyclists. A robot arm must reason about occluded objects when reaching into a cluttered shelf. On other occasions, a robot is uncertain about the task it needs to perform. An assistive home robot must infer a human's intended goal by interacting with them. Both examples of uncertainty require simultaneous inference and decision making, which can be framed as Bayesian reinforcement learning (RL) over latent Markov Decision Processes (MDPs). Agents do not know which latent MDP they are interacting with, preventing them from acting optimally with respect to that MDP. Instead, Bayes optimality only requires that agents be optimal with respect to their current uncertainty over latent MDPs. The Bayesian RL problem can be viewed as solving a large continuous belief MDP, which is computationally infeasible to solve directly (Ghavamzadeh et al., 2015) . We build upon a simple yet recurring observation (Osband et al., 2013; Kahn et al., 2017; Choudhury et al., 2018) : while solving the belief MDP may be hard, solving individual latent MDPs is much more tractable. Given exact predictions for all actors, the self-driving car can invoke a motion planner to find a collision-free path. The robot arm can employ an optimal controller to dexterously retrieve an object given exact knowledge of all objects. Once the human's intended goal is discovered, the robot can provide assistance. Hence, the overall challenge boils down to solving two (perhaps) simpler sub-challenges: solving the latent MDPs and combining these solutions to solve the belief MDP. Let's assume we can approximately solve the latent MDPs to create an ensemble of policies as shown in Figure 1 . We can think of these policies as clairvoyant experts, i.e., experts that think they know the latent MDP and offer advice accordingly. A reasonable strategy is to weigh these policy proposals by the belief and combine them into a single recommendation to the agent. While this recommendation is good for some regimes, it can be misleading when uncertainty is high. The onus then is on the agent to disregard the recommendation and explore the space effectively to collapse uncertainty. This leads to our key insight. Learning Bayesian corrections on top of clairvoyant experts is a scalable strategy for solving complex reinforcement learning problems. While learning corrections echoes the philosophy of boosting (Freund & Schapire, 1999) , our agent goes one step beyond: it learns to take uncertainty-reducing actions that highlight which expert to boost. Our algorithm, Bayesian Residual Policy Optimization (BRPO), augments a belief-space batch policy optimization algorithm (Lee et al., 2019) with clairvoyant experts (Figure 1 ). The agent observes the experts' recommendation, belief over the latent MDPs, and state. It returns a correction over the expert proposal, including uncertainty-reducing sensing actions that experts never need to take. Our key contribution is the following: • We propose a scalable Bayesian RL algorithm to solve problems with complex latent rewards and dynamics. • We experimentally demonstrate that BRPO outperforms both the ensemble of experts and existing adaptive RL algorithms. In the real world, robots must deal with uncertainty, either due to complex latent dynamics or task specifics. Because uncertainty is an inherent part of these tasks, we can at best aim for optimality under uncertainty, i.e., Bayes optimality. Existing BRL algorithms or POMDP solvers do not scale well to problems with complex latent MDPs or a large (continuous) set of MDPs. We decompose BRL problems into two parts: solving each latent MDP and being Bayesian over the solutions. Our algorithm, Bayesian Residual Policy Optimization, operates on the residual belief-MDP space given an ensemble of experts. BRPO focuses on learning to explore, relying on the experts for exploitation. BRPO is capable of solving complex problems, outperforming existing BRL algorithms and improving on the original ensemble of experts. Although out of scope for this work, a few key challenges remain. First is an efficient construction of an ensemble of experts, which becomes particularly important for continuous latent spaces with infinitely many MDPs. Infinitely many MDPs do not necessarily require infinite experts, as many may converge to similar policies. An important future direction is subdividing the latent space and computing a qualitatively diverse set of policies (Liu et al., 2016) . Another challenge is developing an efficient Bayes filter, which is an active research area. In certain occasions, the dynamics of the latent MDPs may not be accessible, which would require a learned Bayes filter. Combined with a tractable, efficient Bayes filter and an efficiently computed set of experts, we believe that BRPO will provide an even more scalable solution for BRL problems. As discussed in Section 3.1, Bayesian reinforcement learning and posterior sampling address quite different problems. We present a toy problem to highlight the distinction between them. Consider a deterministic tree-like MDP ( Figure 6 ). Reward is received only at the terminal leaf states: one leaf contains a pot of gold (R = 100) and all others contain a dangerous tiger (R = −10). All non-leaf states have two actions, go left (L) and go right (R). The start state additionally has a sense action (S), which is costly (R = −0.1) but reveals the exact location of the pot of gold. Both algorithms are initialized with a uniform prior over the N = 2 d possible MDPs (one for each possible location of the pot of gold). To contrast the performance of the Bayes-optimal policy and posterior sampling, we consider the multi-episode setting where the agent repeatedly interacts with the same MDP. The MDP is sampled once from the uniform prior, and agents interact with it for T episodes. This is the setting typically considered by posterior sampling (PSRL) (Osband et al., 2013) . Before each episode, PSRL samples an MDP from its posterior over MDPs, computes the optimal policy, and executes it. After each episode, it updates the posterior and repeats. Sampling from the posterior determinizes the underlying latent parameter. As a result, PSRL will never produce sensing actions to reduce uncertainty about that parameter because the sampled MDP has no uncertainty. More concretely, the optimal policy for each tree MDP is to navigate directly to the gold without sensing; PSRL will never take the sense action. Thus, PSRL makes an average of N −1 2 mistakes before sampling the correct pot of gold location and the cumulative reward over T episodes is In the first episode, the Bayes-optimal first action is to sense. All subsequent actions in this first episode navigate toward the pot of gold, for an episode reward of −0.1 + 100. In the subsequent T − 1 episodes, the Bayes-optimal policy navigates directly toward the goal without needing to sense, for a cumulative reward of 100T − 0.1. The performance gap between the Bayes-optimal policy and posterior sampling grows exponentially with depth of the tree d. Practically, a naïve policy gradient algorithm (like BPO) would struggle to learn the Bayes-optimal policy: it would need to learn to both sense and navigate the tree to the sensed goal. BRPO can take advantage of the set of experts, which each navigate to their designated leaf. During training, the BRPO agent only needs to learn to balance sensing with navigation. As mentioned in Section 3.1, PSRL is an online learning algorithm and is designed to address domains where the posterior naturally updates as a result of multiple episodes of interactions with the latent MDP. PSRL is more focused on improving the performance over episodes, which is different from the average performance or zero-shot performance that we consider in this work.",We propose a scalable Bayesian Reinforcement Learning algorithm that learns a Bayesian correction over an ensemble of clairvoyant experts to solve problems with complex latent rewards and dynamics.,Liu et al. ; Lee et al. ; First ; one ; Bayesian ; Bayesian Residual Policy Optimization ; Freund & Schapire ; Learning Bayesian ; BPO ; daily,Existing BRL algorithms ; an average ; Bayes optimality ; navigation ; exact predictions ; the sense action ; a belief-space batch policy optimization algorithm ; advice ; domains ; The performance gap,Liu et al. ; Lee et al. ; First ; one ; Bayesian ; Bayesian Residual Policy Optimization ; Freund & Schapire ; Learning Bayesian ; BPO ; daily,"Informed and robust decision making in the face of uncertainty is critical for robots that perform physical tasks alongside people. Bayesian Reinforcement Learning (RL) over latent Markov Decision Processes (MDPs) is the gold standard, but existing algorithms do not scale well to continuous state and action spaces. The Bayesian Residual Policy Optimization (BRPO) combines the scalability of policy gradient methods as well as the initialization from prior models. BRPO significantly improves the ensemble of experts and drastically outperforms existing adaptive RL methods. Robots operating in the real world must resolve uncertainty on a daily basis. Often times, a",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Automatic melody generation for pop music has been a long-time aspiration for
 both AI researchers and musicians. However, learning to generate euphonious
 melody has turned out to be highly challenging due to a number of factors. Representation
 of multivariate property of notes has been one of the primary challenges.
 It is also difficult to remain in the permissible spectrum of musical variety, outside
 of which would be perceived as a plain random play without auditory pleasantness.
 Observing the conventional structure of pop music poses further challenges.
 In this paper, we propose to represent each note and its properties as a unique
 ‘word,’ thus lessening the prospect of misalignments between the properties, as
 well as reducing the complexity of learning. We also enforce regularization policies
 on the range of notes, thus encouraging the generated melody to stay close
 to what humans would find easy to follow. Furthermore, we generate melody
 conditioned on song part information, thus replicating the overall structure of a
 full song. Experimental results demonstrate that our model can generate auditorily
 pleasant songs that are more indistinguishable from human-written ones than
 previous models. Recent explosion of deep learning techniques has opened up new potentials for various fields of multimedia. Vision and language have been its primary beneficiary, particularly with rising interest in generation task. Considerable amount of recent works on vision and language have hinged beyond mere generation onto artistic aspects, often producing works that are indistinguishable from human works BID2 ; BID14 ; BID13 ). On the other hand, it is only recently that deep learning techniques began to be applied to music, and the quality of the results are yet far behind those in other domains, as there are few works that demonstrate both euphonious sound and structural integrity that characterize the human-made musical contents. This unfortunate status holds true for both music in its physical audio format and its abstraction as notes or MIDI (Musical Instrument Digital Interface).Such lagging of deep learning-enabled music generation, particularly in music as abstraction, can be attributed to a number of factors. First , a note in a musical work contains various properties, such as its position, pitch, length, and intensity. The overall tendency of each property and the correlation among them can significantly vary depending on the type of music, which makes it difficult to model. Second , the boundary between musical creativity and plain clumsiness is highly indefinite and difficult to quantify, yet exists. As much as musical creativity cannot be limited, there is yet a certain aspect about it that makes it sound like (or not sound like) human-written music. Finally , music is not merely a series of notes, but entails an overall structure of its own. Classical music pieces are well-known for their high structural complexity, and much of pop music follows the general convention of verse -pre-chorus -chorus structure. This structure inevitably necessitates different modeling of musical components; for example, notes in the chorus part generally tend to be more high-pitched. It goes without saying that these structure-oriented variations further complicate the modeling of music generation.In this paper, we propose a new model for music generation, specifically symbolic generation of melodies for pop music in MIDI format. The term ""pop music "" can have different meanings depending on the context, but we use the term in this paper to refer to its musical characteristics as conventionally accepted. Specifically, it refers to the songs of relatively short lengths, mostly around 3 minutes, with simple and memorable melodies that have relatively low structural complexity, especially in comparison to classical music. Music in MIDI format (or , equivalently, in notes) can be considered a discrete abstraction of musical sound, analogous to the relationship between text and speech. Just as understanding text is not only essential in its own merit, but provides critical clues to speech and language in general, understanding music at its abstraction can provide an ample amount of insights as to music and sound as a physical format, while being fun and significant per se.We address each of the challenges described above in our proposed model. First, we propose to treat a note and its varying properties as a unique 'word,' as opposed to many previous approaches that took each property into consideration separately, by implementing different layers for generation. In our model, it suffices to train only one model for generation, as each 'word' is an incarnation of all of its properties, thus forming a melody as a 'sentence' consisting of those notes and the properties. This approach was inspired by recent successes in image captioning task BID9 ; BID19 ; BID20 ), in which a descriptive sentence is generated with one word at a time in a recurrent manner, while being conditioned on the image features. Likewise, we generate the melody with one note at a time in a recurrent manner. The difference is that, instead of image features obtained via convolutional neural networks (CNN), we condition the generation process on simple two-hot vectors that contain information on chords sequences and the part within the song. Chord sequences and part annotations are automatically generated using multinomial hidden markov model (HMM) whose state transition probabilities are computed from our own dataset. Combining Bayesian graphical models with deep neural netweorks (DNN) has become a recent research interest BID1 ), but our model differs in that HMM is purely used for feature input generation that is processed by neural networks.Second, we enforce regularization policy on the range of notes. Training with a large amount of data can lead to learning of excessively wide range of pitches, which may lead to generation of melodies that are not easy to sing along. We alleviate these problem by assigning a loss function for the range of notes. Finally, we train our system with part annotation, so that more appropriate melody for the corresponding part can be generated, even when the given chord sequences are identical with other parts of the song. Apart from the main model proposed, we also perform additional experiments with generative adversarial networks BID2 ) and with multi-track songs.Our main contributions can be summarized as following:• proposal of a model to generate euphonious melody for pop music by treating each note and its properties as single unique ""word"", which alleviates the complexity of learning • implementation of supplementary models, such as chord sequence generation and regularization, that refine the melody generation • construction of dataset with chord and part annotation that enables efficient learning and is publicly available. Being able to automatically describe the content of an image using properly formed English sentences is a very challenging task, but it could have great impact, for instance by helping visually impaired people better understand the content of images on the web. This task is significantly harder, for example, than the well-studied image classification or object recognition tasks, which have been a main focus in the computer vision community [27] . Indeed, a description must capture not only the objects contained in an image, but it also must express how these objects relate to each other as well as their attributes and the activities they are involved in. Moreover, the above semantic knowledge has to be expressed in a natural language like English, which means that a language model is needed in addition to visual understanding.Most previous attempts have proposed to stitch together FIG0 . NIC, our model, is based end-to-end on a neural network consisting of a vision CNN followed by a language generating RNN. It generates complete sentences in natural language from an input image, as shown on the example above.existing solutions of the above sub-problems, in order to go from an image to its description [6, 16] . In contrast, we would like to present in this work a single joint model that takes an image I as input, and is trained to maximize the likelihood p(S|I) of producing a target sequence of words S = {S1, S2, . . .} where each word St comes from a given dictionary, that describes the image adequately.The main inspiration of our work comes from recent advances in machine translation, where the task is to transform a sentence S written in a source language, into its translation T in the target language, by maximizing p(T |S). For many years, machine translation was also achieved by a series of separate tasks (translating words individually, aligning words, reordering, etc), but recent work has shown that translation can be done in a much simpler way using Recurrent Neural Networks (RNNs) [3, 2, 30] and still reach state-of-the-art performance. An ""encoder"" RNN reads the source sentence and transforms it into a rich fixed-length vector representation, which in turn in used as the initial hidden state of a ""decoder"" RNN that generates the target sentence.Here, we propose to follow this elegant recipe, replacing the encoder RNN by a deep convolution neural network (CNN). Over the last few years it has been convincingly shown that CNNs can produce a rich representation of the input image by embedding it to a fixed-length vector, such that this representation can be used for a variety of vision Section 4.3, our model can generate simultaneous notes for a single instrument. BID5 also take a similar approach of applying Gibbs sampling to generate Bach-like chorale music, but mostly share the same drawbacks that make a contrast to our model. BID8 proposed RL Tuner to supplement recurrent neural networks with reinforcement learning by imposing cross-entropy reward function along with off-policy methods from KL control. Note RNN trained on MIDI files is implemented to assign rewards based on the log probability of a note given a melody. They defined a number of music-theory based rules to set up the reward function. Our model, on the other hand, does not require any pre-set rules, and the outcome can be easily controlled with simple regularizations. BID0 proposed a hierarchical recurrent neural network model to produce multi-track songs, where the bottom layers generate the melody and the higher levels generate the drums and chords. They built separate layers for pitch and duration that generate an output at each time step, whereas our model needs only one layer for pitch and duration and does not have to be aware of time step. They also conditioned their model on scale types, whereas we condition our model on chord sequence and part information.While generating music as physical audio format is out of scope of this paper, we briefly discuss one of the recent works that demonstrated promising results. Originally designed for text-to-speech conversion, WaveNet (van den Oord et al. (2016) ) models waveform as a series of audio sample x t conditioned on all previous timesteps, whose dependence is regulated by causal convolutional layers that prevent the violations in ordering. When applied to music, it was able to reconstruct the overall characteristics of corresponding music datasets. While only for a few seconds with frequent inconsistency, it was able to generate samples that often sound harmonic and pleasant.3 GENERATION MODEL Although our model was inspired by the model used in image captioning task, its task objective has a fundamental difference from that of image captioning. In image captioning task, more resemblance to human-written descriptions reflects better performance. In fact, matching human-written descriptions is usually the evaluation scheme for the task. However, in melody generation, resembling human-written melody beyond certain extent becomes plagiarism. Thus, while we need sufficient amount of training to learn the patterns, we also want to avoid overfitting to training data at the same time. This poses questions about how long to train, or essentially how to design the loss function. We examined generations with parameters learned at different epochs. Generated songs started to stay in tune roughly after 5 epochs. However, after 20 epochs and on, we could frequently observe the same melodies as in the training data, implying overfitting (check our demo). So there seems to exist a 'safe zone' in which it learns enough from the data but not exceedingly to copy it. Previous approaches like BID8 have dealt with this dilemma by rewarding for following the pre-determined rules, but encouraging off-policy at the same time. Since we aim for learning without pre-determined rules, alternative would be to design a loss function where matching the melody in training data over n consecutive notes of threshold is given penalty. Designing a more appropriate loss function remains as our future work. On the other hand, generating songs with parameters obtained at different stages within the 'safe zone' of training leads to diversity of melodies, even when the input vectors are identical. This property nicely complements our relatively low-dimensional input representation. In this paper, we proposed a novel model to generate melody for pop music. We generate melody with word representation of notes and their properties, instead of training multiple layers for each property, thereby reducing the complexity of learning. We also proposed a regularization model to control the outcome. Finally, we implemented part-dependent melody generation which helps the generated song preserve the overall structure, along with a publicly available dataset. Experimental results demonstrate that our model can generate songs whose melody sounds more like human-written ones, and is more well-structured than previous models. Moreover, people found it more difficult to distinguish the songs from our model from human-written songs than songs from previous models. On the other hand, examining other styles such as music of minor scale, or incorporating further properties of notes, such as intensity or vibrato, has not been examined yet, and remains as future work. As discussed in Section 4, learning to model the correlations among different instruments also remains to be done, and designing an appropriate loss function for the task is one of the most critical tasks to be done. We plan to constantly update our dataset and repository, addressing the future works.","We propose a novel model to represent notes and their properties, which can enhance the automatic melody generation.",Chord ; Recurrent Neural Networks ; one ; a few seconds ; the last few years ; Bach ; two ; many years ; Oord et al ; St,the above sub ; a source language ; the other hand ; the term ; human-written music ; our relatively low-dimensional input representation ; images ; other styles ; the same drawbacks ; music-theory based rules,Chord ; Recurrent Neural Networks ; one ; a few seconds ; the last few years ; Bach ; two ; many years ; Oord et al ; St,"Automatic melody generation for pop music has been a long-time aspiration for both AI researchers and musicians. However, learning to generate euphonious melodies with multivariate property of notes has been challenging due to a number of factors. For instance, the conventional structure of pop music poses further challenges. In this paper, we propose to represent each note and its properties as a unique 'word', reducing the possibility of misalignments between properties, as well as reducing the complexity of learning. Furthermore, we enforce regularization policies on the range of notes, encouraging the generated melody to stay close to what humans would find easy to follow.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Some of the most successful applications of deep reinforcement learning to challenging domains in discrete and continuous control have used policy gradient methods in the on-policy setting. However, policy gradients can suffer from large variance that may limit performance, and in practice require carefully tuned entropy regularization to prevent policy collapse. As an alternative to policy gradient algorithms, we introduce V-MPO, an on-policy adaptation of Maximum a Posteriori Policy Optimization (MPO) that performs policy iteration based on a learned state-value function. We show that V-MPO surpasses previously reported scores for both the Atari-57 and DMLab-30 benchmark suites in the multi-task setting, and does so reliably without importance weighting, entropy regularization, or population-based tuning of hyperparameters. On individual DMLab and Atari levels, the proposed algorithm can achieve scores that are substantially higher than has previously been reported. V-MPO is also applicable to problems with high-dimensional, continuous action spaces, which we demonstrate in the context of learning to control simulated humanoids with 22 degrees of freedom from full state observations and 56 degrees of freedom from pixel observations, as well as example OpenAI Gym tasks where V-MPO achieves substantially higher asymptotic scores than previously reported. Deep reinforcement learning (RL) with neural network function approximators has achieved superhuman performance in several challenging domains (Mnih et al., 2015; . Some of the most successful recent applications of deep RL to difficult environments such as Dota 2 (OpenAI, 2018a), Capture the Flag , Starcraft II (Vinyals et al., 2019) , and dexterous object manipulation (OpenAI, 2018b) have used policy gradient-based methods such as Proximal Policy Optimization (PPO) (Schulman et al., 2017) and the Importance-Weighted Actor-Learner Architecture (IMPALA) , both in the approximately on-policy setting. Policy gradients, however, can suffer from large variance that may limit performance, especially for high-dimensional action spaces (Wu et al., 2018) . In practice, moreover, policy gradient methods typically employ carefully tuned entropy regularization in order to prevent policy collapse. As an alternative to policy gradient-based algorithms, in this work we introduce an approximate policy iteration algorithm that adapts Maximum a Posteriori Policy Optimization (MPO) (Abdolmaleki et al., 2018a; b) to the on-policy setting. The modified algorithm, V-MPO, relies on a learned state-value function V (s) instead of the state-action value function used in MPO. Like MPO, rather than directly updating the parameters in the direction of the policy gradient, V-MPO first constructs a target distribution for the policy update subject to a sample-based KL constraint, then calculates the gradient that partially moves the parameters toward that target, again subject to a KL constraint. As we are particularly interested in scalable RL algorithms that can be applied to multi-task settings where a single agent must perform a wide variety of tasks, we show for the case of discrete actions that the proposed algorithm surpasses previously reported performance in the multi-task setting for both the Atari-57 (Bellemare et al., 2012) and DMLab-30 (Beattie et al., 2016) benchmark suites, and does so reliably without population-based tuning of hyperparameters (Jaderberg et al., 2017a) . For a few individual levels in DMLab and Atari we also show that V-MPO can achieve scores that are substantially higher than has previously been reported in the single-task setting, especially in the challenging Ms. Pacman. V-MPO is also applicable to problems with high-dimensional, continuous action spaces. We demonstrate this in the context of learning to control both a 22-dimensional simulated humanoid from full state observations-where V-MPO reliably achieves higher asymptotic performance than previous algorithms-and a 56-dimensional simulated humanoid from pixel observations (Tassa et al., 2018; Merel et al., 2019) . In addition, for several OpenAI Gym tasks (Brockman et al., 2016) we show that V-MPO achieves higher asymptotic performance than has previously been reported. In this work we have introduced a scalable on-policy deep reinforcement learning algorithm, V-MPO, that is applicable to both discrete and continuous control domains. For the results presented in this work neither importance weighting nor entropy regularization was used; moreover, since the size of neural network parameter updates is limited by KL constraints, we were also able to use the same learning rate for all experiments. This suggests that a scalable, performant RL algorithm may not require some of the tricks that have been developed over the past several years. Interestingly, both the original MPO algorithm for replay-based off-policy learning (Abdolmaleki et al., 2018a; b) and V-MPO for on-policy learning are derived from similar principles, providing evidence for the benefits of this approach as an alternative to popular policy gradient-based methods.",A state-value function-based version of MPO that achieves good results in a wide range of tasks in discrete and continuous control.,first ; Merel et al. ; OpenAI Gym ; the past several years ; Jaderberg ; OpenAI ; Proximal Policy Optimization ; al. ; PPO ; KL,popular policy gradient-based methods ; policy gradient-based methods ; replay-based off-policy learning ; a learned state-value function ; both the original MPO algorithm ; the results ; Starcraft II ; The modified algorithm ; population-based tuning ; example OpenAI Gym tasks,first ; Merel et al. ; OpenAI Gym ; the past several years ; Jaderberg ; OpenAI ; Proximal Policy Optimization ; al. ; PPO ; KL,"The most successful applications of deep reinforcement learning to challenging domains in discrete and continuous control have used policy gradient methods in the on-policy setting. However, policy gradients can suffer from large variance that may limit performance, requiring carefully tuned entropy regularization to prevent policy collapse. In this work, we introduce a policy iteration algorithm that adapts Maximum a Posteriori Policy Optimization (MPO) (Abdolmaleki et al., 2018a; b) to the multi-task setting. V-MPO surpasses previously reported scores for both the Atari-57 and DMLab-30 benchmark suites in the multi",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"This work addresses the long-standing problem of robust event localization in the presence of temporally of misaligned labels in the training data. We propose a novel versatile loss function that generalizes a number of training regimes from standard fully-supervised cross-entropy to count-based weakly-supervised learning. Unlike classical models which are constrained to strictly fit the annotations during training, our soft localization learning approach relaxes the reliance on the exact position of labels instead. Training with this new loss function exhibits strong robustness to temporal misalignment of labels, thus alleviating the burden of precise annotation of temporal sequences. We demonstrate state-of-the-art performance against standard benchmarks in a number of challenging experiments and further show that robustness to label noise is not achieved at the expense of raw performance. Figure 1: Temporal localization under label misalignment. Models are trained with noisy labels that differ from the actual ground-truth, while the final inference objective is the precise localization of events. The surge of deep neural networks Schmidhuber, 2015) has accentuated the evergrowing need for large corpora of data (Banko & Brill, 2001; Halevy et al., 2009) . The main bottleneck for the efficient creation of datasets remains the annotation process. Over the years, while new labeling paradigms have emerged to alleviate this issue (e.g., crowdsourcing (Deng et al., 2009) or external information sources (Abu-El-Haija et al., 2016) ), these methods have also highlighted, and emphasized, the prevalence of label noise. Deep neural networks are unfortunately not immune to these perturbations as their intrinsic ability to memorize and learn label noise (Zhang et al., 2017) can be the cause of training robustness issues and poor generalization performance. In this context, the development of models robust to label noise is essential. This work tackles the problem of precise temporal localization of events (i.e., determining when and which events occur) in sequential data (e.g. time series, video or audio sequences) despite only having access to poorly aligned annotations for training (see Figure 1 ). This task is characterized by the discrepency between the precision required of the predictions during inference and the noisiness of the training labels. Indeed, while models are trained on inaccurate data, they are evaluated on their ability to predict event occurences as precisely as possible with respect to the ground-truth. In such a setting, effective models have to infer event locations more accurately than the labels they relied on for training. This requirement is particularly challenging for most classical approaches that are designed to learn localization by strictly mimicking the provided annotations. Indeed, as the training labels themselves do not accurately reflect the event location, focusing on replicating these unreliable patterns is incompatible with the overall objective of learning the actual ground-truth. These challenges highlight the need for more relaxed learning approaches that are less dependent on the exact location of labels for training. The presence of temporal noise in localization tasks is ubiquitous given the continuous nature of the perturbation, in contrast to classification noise where only a fraction of the samples are misclassified. Temporal labeling is further characterized by an inevitable trade-off between annotation precision and time investment. For instance, while a coarse manual transcription of a minute of complex piano music might be achieved within a moderate time frame, a millisecond precision requirement -a common assumption for deep learning models -significantly increases the annotation burden. In this respect, models alleviating the need for costly annotations are key for a wide and efficient deployment of deep learning models in temporal localization applications. This work introduces a novel model-agnostic loss function that relaxes the reliance of the learning process on the exact temporal location of the annotations. This softer learning approach inherently makes the model more robust to temporally misaligned labels. Contributions This work: a) proposes a novel loss function for robust temporal localization under label misalignment, b) presents a succinct analysis of the loss' properties, c) evaluates the robustness of state-of-the-art localization models to label misalignment, and d) demonstrates the effectiveness of the proposed approach in various experiments. In this work, we have shown how relaxing annotation requirements (i.e., weakening the model's reliance on the exact location of events) not only has the practical benefit of alleviating annotation efforts but, more importantly, leads to a model that is robust to temporal noise without compromising performance on clean training data. This contrasts with traditional approaches which attempt to strictly mimic the annotations, leading to poor predictions when training with noisy labels. We have demonstrated these claims on a number of classical challenging tasks, in which our SoftLoc loss exhibits state-of-the-art performance. The proposed loss function is agnostic to the underlying network and hence can be used as a loss replacement in almost any recurrent architecture. The versatility of the model can find applications in a wide array of tasks, even beyond temporal localization.",This work introduces a novel loss function for the robust training of temporal localization DNN in the presence of misaligned labels.,Schmidhuber ; Banko & Brill ; Halevy ; al. ; the years ; Deng et al. ; Abu-El-Haija ; Zhang et al. ; transcription of a minute ; SoftLoc,clean training data ; a novel loss function ; the expense ; classical challenging tasks ; Brill ; their intrinsic ability ; this respect ; the samples ; the annotation burden ; Models,Schmidhuber ; Banko & Brill ; Halevy ; al. ; the years ; Deng et al. ; Abu-El-Haija ; Zhang et al. ; transcription of a minute ; SoftLoc,"This work addresses the long-standing problem of robust event localization in the presence of temporally misaligned labels in the training data. The soft localization learning approach relaxes the reliance on the exact position of labels instead of the precise annotation of temporal sequences. The performance against standard benchmarks in a number of challenging experiments demonstrates that robustness to label noise is not achieved at the expense of raw performance. Figure 1: Temporal localization under label misalignment. Models are trained with noisy labels that differ from the actual ground-truth, while the final inference objective is precise localization of events. The surge of deep neural networks (Schmidhu",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Learning high-quality word embeddings is of significant importance in achieving better performance in many down-stream learning tasks. On one hand, traditional word embeddings are trained on a large scale corpus for general-purpose tasks, which are often sub-optimal for many domain-specific tasks. On the other hand, many domain-specific tasks do not have a large enough domain corpus to obtain high-quality embeddings. We observe that domains are not isolated and a small domain corpus can leverage the learned knowledge from many past domains to augment that corpus in order to generate high-quality embeddings. In this paper, we formulate the learning of word embeddings as a lifelong learning process. Given knowledge learned from many previous domains and a small new domain corpus, the proposed method can effectively generate new domain embeddings by leveraging a simple but effective algorithm and a meta-learner, where the meta-learner is able to provide word context similarity information at the domain-level. Experimental results demonstrate that the proposed method can effectively learn new domain embeddings from a small corpus and past domain knowledges\footnote{We will release the code after final revisions.}. We also demonstrate that general-purpose embeddings trained from a large scale corpus are sub-optimal in domain-specific tasks. Learning word embeddings BID18 ; BID29 ; BID14 BID17 c) ; BID22 ) has received a significant amount of attention due to its high performance on many down-stream learning tasks. Word embeddings have been shown effective in NLP tasks such as named entity recognition BID26 ), sentiment analysis BID13 ) and syntactic parsing BID6 ). Such embeddings are shown to effectively capture syntactic and semantic level information associated with a given word BID14 ).The ""secret sauce"" of training word embedding is to turn a large scale in-domain corpus into billions of training examples. There are two common assumptions for training word embeddings: 1) the training corpus is largely available and bigger than the training data of the potential downstream learning tasks; and 2) the topic of the training corpus is closely related to the topic of the down-stream learning tasks. However , real-world learning tasks often do not meet one of these assumptions. For example , a domain-specific corpus that is closely related to a down-stream learning task may often be of limited size. If we lump different domain corpora together and train general-purpose embeddings over a large scale corpus (e.g., GloVe embeddings BID22 ) are trained from the corpus Common Crawl, which covers almost any topic on the web), the performance of such embeddings on many domain-specific tasks is sub-optimal (we show this in Section 6). A possible explanation is that although many domain words share similar meanings with the same out-of-domain words, with no in-domain awareness, dumping many out-of-domain co-occurrences as training examples may bias in-domain embeddings. (e.g., if the domain is about food, then an out-of-domain ""python"" as a programming language can bias ""java"", while the indomain word ""chocolate"" is more likely to help).To solve the problem of the limited domain corpus, one possible solution is to use transfer learning BID20 ) for training domain-specific embeddings BID2 ; BID31 ). However, these methods just manage to leverage out-of-domain embeddings trained from a large scale corpus to help limited in-domain corpus. The very in-domain corpus is never expanded. Also, one common assumption of these works is that a pair of similar source domain and target domain is manually identified in advance. In reality, given many domains , manually catching useful information in so many domains are very hard. In contrast, we humans learn the meaning of a word more smartly. We accumulate different domain contexts for the same word. When a new learning task comes , we may quickly identify the new domain contexts and borrow the word meanings from existing domain contexts. This is where lifelong learning comes to the rescue. Lifelong machine learning (LML) is a continual learning paradigm that retains the knowledge learned in past tasks 1, . . . , n, and uses it to help learning the new task n + 1 BID28 ; BID27 ; BID4 ). In the setting of word embedding : we assume that the learning system has seen n domain corpora: (D 1 , . . . , D n ), when a new domain corpus D n+1 comes by demands from that domain's potential down-stream learning tasks, the learning system can automatically generate word embeddings for the n + 1-th domain by effectively leveraging useful past domain knowledge.The main challenges of this task are 2 fold. 1) How to identify useful past domain knowledge to train the embeddings for the new domain. 2) How to automatically identify such kind of information, without help from human beings. To tackle these challenges, the system has to learn how to identify similar words in other domains for a given word in a new domain. This, in general, belongs to metalearning BID30 ; BID21 ). Here we do not focus on specific embedding learning but focus on learning how to characterize corpora of different domains for embedding purpose.The main contributions of this paper can be summarized as follows: 1) we propose the problem of lifelong word embedding, which may benefit many down-stream learning tasks. We are not aware of any existing work on word embedding using lifelong learning 2) we propose a lifelong embedding learning method , which leverages meta-learning to aggregate useful knowledge from past domain corpora to generate embeddings for the new domain. In this paper, we formulate a lifelong word embedding learning process. Given many previous domains and a small new domain corpus, the proposed method can effectively generate new domain embeddings by leveraging a simple but effective algorithm and a meta-learner. The meta-learner is able to provide word context similarity information on domain-level. Such information can help to accumulate new domain-specific training corpus in order to get better embedding. Experimental results show that the proposed method is effective in learning new domain embeddings from a small corpus and past domain knowledge.",learning better domain embeddings via lifelong learning and meta-learning,NLP ; billions ; two ; one ; GloVe ; Common Crawl ; LML,these assumptions ; this task ; a small corpus ; past domain corpora ; other domains ; domain-level ; training word ; The main challenges ; a continual learning paradigm ; domain-specific tasks,NLP ; billions ; two ; one ; GloVe ; Common Crawl ; LML,"Learning high-quality word embeddings is crucial in achieving better performance in many down-stream learning tasks. However, many domain-specific tasks do not have a large enough domain corpus to achieve high-level embedding. A simple but effective algorithm and a meta-learner can augment the corpus in order to generate high-value embedding in the domain-level. In this paper, we formulate the learning of word embedding as a lifelong learning process. Given knowledge learned from many previous domains and a small new domain corpus, the proposed method can effectively learn new domain embedding from a small corpus and a large domain corpus",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Exploration in environments with sparse rewards is a key challenge for reinforcement learning. How do we design agents with generic inductive biases so that they can explore in a consistent manner instead of just using local exploration schemes like epsilon-greedy? We propose an unsupervised reinforcement learning agent which learns a discrete pixel grouping model that preserves spatial geometry of the sensors and implicitly of the environment as well. We use this representation to derive geometric intrinsic reward functions, like centroid coordinates and area, and learn policies to control each one of them with off-policy learning. These policies form a basis set of behaviors (options) which allows us explore in a consistent way and use them in a hierarchical reinforcement learning setup to solve for extrinsically defined rewards. We show that our approach can scale to a variety of domains with competitive performance, including navigation in 3D environments and Atari games with sparse rewards. Exploration in environments with sparse feedback is a key challenge for deep reinforcement learning (DRL) research. In DRL, agents typically explore with local exploration strategies like epsilon greedy or entropy based schemes. We are interested in learning structured exploration algorithms, grounded in spatio-temporal visual abstractions given raw pixels. In human perception and its developmental trajectory, spatio-temporal pixel groupings is one of the first visual abstractions to emerge, which is also used for intrinsically motivated goal-driven behaviors BID16 . Inspired by this insight, we develop a new agent architecture and loss functions to autonomously learn visual abstractions and ground temporally extended behaviors in them.Our approach and key contributions can be broken down into two parts: (1) an information theoretic loss function and a neural network architecture to learn visual groupings (abstractions) given raw pixels and actions, (2) a hierarchical action-value function agent which explores in the space of options grounded in the learned visual abstractions, instead of low level actions.In the first step, we pass images through an encoder which outputs spatial discrete vector-quantized (VQ') grids, with 1 of E discrete components. We train this encoder to maximize the mutual information between VQ layers at different time steps, in order to obtain a temporally consistent representation, that preserves controllability and appearance information. We extract segmentation masks from the VQ layers for the second step, referred to as visual entities. We compute affine geometric measurements for each entity, namely centroid and area of the corresponding segment. We use off-policy learning to train action-value function to minimize or maximize these measurements, referred collectively as the options bank. Controlling these measurements enable higher levels of behaviors such as approaching an object (maximizing area), avoiding objects (minimize area or minimize/maximize centroid coordinates), moving an object away towards the left (minimize centroid x coordinate), controlling the avatars position on the screen etc. Finally, given a task reward, we use off-policy learning to train a meta action-value function that takes actions at fixed intervals and selects either one of the policies in the options bank or low-level actions. So effectively, this hierarchical action-value function setup solves a semi markov decision process as in BID18 BID9 .We demonstrate that our approach can scale to two different domains -navigation in a 3D environment and challenging Atari games -given raw pixels. Although much work remains in improving the visual and temporal abstraction discovery models, our results indicate that it is possible to learn bottom-up structured exploration schemes with simple spatial inductive biases and loss functions. We have shown that it is possible to design unsupervised structured exploration schemes for modelfree DRL agents, with competitive performance on a range of environments given just raw pixels.One of the biggest open question moving forward is to find strategies to balance structure or inductive biases and performance. Our current solution was to augment the meta-controller with Q task along with the options bank as sub-behaviors. The typical strategy that agents follow is to rely on the options bank early in training and then use this experience to train the Q task policy for optimality as training progresses. This is reasonable given that the options models may not cover the optimal policy but could serve as a good exploration algorithm throughout training. As new unsupervised architectures and losses are discovered, we expect to narrow the gap between the optimal desired behaviors and the options bank.Learning visual entities from pixels is still a challenging open problem in unsupervised learning and computer vision. We expect novel sampling schemes in our proposed architecture to improve the entity discovery results. Other unsupervised video segmentation algorithms and discrete latent variable models could also be used to boost the discovery process.",structured exploration in deep reinforcement learning via unsupervised visual abstraction discovery and control,one ; Atari ; DRL ; first ; two ; VQ ; second,the biggest open question ; the learned visual abstractions ; domains ; optimality ; a challenging open problem ; training progresses ; a new agent architecture and loss functions ; a temporally consistent representation ; deep reinforcement learning ; the meta-controller,one ; Atari ; DRL ; first ; two ; VQ ; second,"Reinforcement learning involves designing agents with generic inductive biases to explore in a consistent manner instead of just using local exploration schemes like epsilon-greedy. An unsupervised reinforcement learning agent learns a discrete pixel grouping model that preserves spatial geometry of sensors and implicitly of the environment, and learns policies to control each one of them with off-policy learning. These policies form a basis set of behaviors (options) which allows us to explore and use them in a hierarchical reinforcement learning setup to solve extrinsically defined rewards. The approach can scale to a variety of domains with competitive performance, including navigation in 3D environments and",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Interpretability has largely focused on local explanations, i.e. explaining why a model made a particular prediction for a sample. These explanations are appealing due to their simplicity and local fidelity. However, they do not provide information about the general behavior of the model. We propose to leverage model distillation to learn global additive explanations that describe the relationship between input features and model predictions. These global explanations take the form of feature shapes, which are more expressive than feature attributions. Through careful experimentation, we show qualitatively and quantitatively that global additive explanations are able to describe model behavior and yield insights about models such as neural nets. A visualization of our approach applied to a neural net as it is trained is available at https://youtu.be/ErQYwNqzEdc Recent research in interpretability has focused on developing local explanations: given an existing model and a sample, explain why the model made a particular prediction for that sample BID40 . The accuracy and quality of these explanations have rapidly improved, and they are becoming important tools to understand model decisions for individual samples. However, the human cost of examining multiple local explanations can be prohibitive with today's large data sets, and it is unclear whether multiple local explanations can be aggregated without contradicting each other BID41 BID0 .In this paper, we are interested in global explanations that describe the overall behavior of a model. While usually not as accurate as local explanations on individual samples, global explanations provide a different, complementary view of the model. They allow us to clearly visualize trends in feature space, which is useful for key tasks such as understanding which features are important, detecting unexpected patterns in the training data and debugging errors learned by the model.We propose to use model distillation techniques BID7 BID24 to learn global additive explanations of the form DISPLAYFORM0 to approximate the prediction function of the model F (x). Figure 1 illustrates our approach. The output of our approach is a set of p feature shapes {h i } p 1 that can be composed to form an explanation model that can be quantitatively evaluated. Through controlled experiments, we empirically validate that feature shapes provide accurate and interesting insights into the behavior of complex models. In this paper, we focus on interpreting F from fully-connected neural nets trained on tabular data.Our goal is not to replace local explanations nor to explain how the model functions internally. What we claim is that we can complement local explanations with global additive explanations that clearly illustrate the relationship between input features and model predictions. Our contributions are:• We propose to learn global additive explanations for complex, non-linear models such as neural nets.• We leverage powerful generalized additive models in a model distillation setting to learn feature shapes that are more expressive than feature attributions Figure 1 : Given a black box model and unlabeled samples (new unlabeled data or training data with labels discarded), our approach leverages model distillation to learn feature shapes that describe the relationship between input features and model predictions.• We perform a quantitative comparison of feature shapes to other global explanation methods in terms of fidelity to the model being explained, accuracy on independent test data, and interpretability through a user study. We presented a method for ""opening up"" complex models such as neural nets trained on tabular data. The method, based on distillation with high-accuracy additive models, has clear advantages over other approaches that learn additive explanations but not using distillation, and non-additive explanations using distillation. Our global additive explanations do not aim to compete with local explanations or non-additive explanations such as decision trees. Instead, we show that different interpretable representations work well for different tasks, and global additive explanations are valuable for important tasks that require quick understanding of feature-prediction relationships. Although in this paper we focus on explaining FNNs, the method will work with any classification or regression model including random forests and CNNs, but is not designed to work with raw image inputs such as pixels where providing a global explanation in terms of input pixels is not meaningful. One way to address this is to define more meaningful ""features"", e. hi (xi) hi (xi) hi(xi) Figure A1 : Feature shapes for features x 1 to x 9 of F 1 from Section 4.1. Notice how x 9 , which is a noise feature that does not affect F 1 , has been assigned an importance of approximately 0 throughout its range. The feature shape of x 10 , another noise feature, is very similar to x 9 and hence not included here.hi (xi) hi (xi) hi(xi) FIG0 : Feature shapes for features x 1 to x 9 of F 2 from Section 4.1. Notice how x 9 , which is a noise feature that does not affect F 2 , has been assigned an importance of approximately 0 throughout its range. The feature shape of x 10 , another noise feature, is very similar to x 9 and hence not included here. Table A1 : Accuracy and fidelity of global explanation models across 1H and 2H teacher neural nets and datasets. TAB4 is a subset of this table with only 2H neural nets.In general, the lower-capacity 1H neural nets are easier to approximate (i.e. better student-teacher fidelity), but their explanations are less accurate on independent test data. Students of simpler teachers tend to be less accurate even if they are faithful to their (simple) teachers. One exception is the FICO data, where the fidelity of the 2H explanations is better. Our interpretation is that many features in the FICO data have almost linear feature shapes (see Figure A5 for a sample of features), and the 2H model may be able to better capture fine details while being simple enough that it can still be faithfully approximated. The accuracy of the SAT and SAS for 1H and 2H neural nets are comparable, taking into account the confidence intervals.On the Magic data, the fidelity of the gGRAD explanation to the 1H neural net (see * in Table A1 ) is markedly worse than other explanation methods. We investigate the individual gradients of the 1H neural net with respect to each feature ( DISPLAYFORM0 ∂xi in GRAD equation in Section 3). 99% of them have reasonable values (between -5.6 and 6). However, 3 are larger than 1,000 (with none between 6 and 1,000) and 13 are lower than -1,000 (with none between -1,000 and -5.6), resulting in the ensuing gGRAD explanation generating extreme predictions for several samples that are not faithful to the teacher's predictions. Because AUC is a ranking loss, accuracy (AUC) is less affected than fidelity (RMSE) by the presence of these extreme values. This shows that gGRAD explanations may be problematic when individual gradients are arbitrarily large, e.g. in overfitted neural nets. Figure A7 , removing the color and number of samples in each node, to improve readability for the user study.",We propose to leverage model distillation to learn global additive explanations in the form of feature shapes (that are more expressive than feature attributions) for models such as neural nets trained on tabular data.,gGRAD ; Magic ; F ; One ; nets.• ; fidelity ; SAS ; SAT ; FICO ; today,reasonable values ; Our interpretation ; our approach ; a ranking loss ; us ; The output ; a subset ; One way ; datasets ; that sample,gGRAD ; Magic ; F ; One ; nets.• ; fidelity ; SAS ; SAT ; FICO ; today,"Interpretability has largely focused on local explanations, i.e., explaining why a model made a particular prediction for a sample. These explanations provide information about the general behavior of the model, but they do not describe the relationship between input features and model predictions. The output of our approach is a set of p feature shapes {h i } p 1 that can be composed to form an explanation model. In this paper, we focus on interpreting F from fully-connected neural nets trained on tabular data. The goal is not to replace local explanations nor to explain how the model functions internally, but to complement local explanations.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"We propose an unsupervised method for building dynamic representations of sequential data, particularly of observed interactions. The method simultaneously acquires representations of input data and its dynamics. It is based on a hierarchical generative model composed of two levels. In the first level, a model learns representations to generate observed data. In the second level, representational states encode the dynamics of the lower one. The model is designed as a Bayesian network with switching variables represented in the higher level, and which generates transition models. The method actively explores the latent space guided by its knowledge and the uncertainty about it. That is achieved by updating the latent variables from prediction error signals backpropagated to the latent space. So, no encoder or inference models are used since the generators also serve as their inverse transformations.
 The method is evaluated in two scenarios, with static images and with videos. The results show that the adaptation over time leads to better performance than with similar architectures without temporal dependencies, e.g., variational autoencoders. With videos, it is shown that the system extracts the dynamics of the data in states that highly correlate with the ground truth of the actions observed. When observing a particular interaction some behaviors tend to repeat over time following specific dynamics. Understand such behaviors and differentiating the dynamics that define them is a relevant task that allows to characterize the interactions, acquire knowledge from them, and build reactive systems that adapt to evolving situations. Those interactions could be, for example, human activities captured in video, data from a vehicle-mounted camera, or the motion of an agent of interest in a given environment.If we consider a video in which different kinds of action sequences can be observed, the task we aim at for a learning system would be to separate the diverse types of dynamics in the observed sequences and embed them in representational states. For example, imagine a camera mounted on a car. In a video from such device, one would observe predictable changes on the frames for particular actions, for instance, there would be a certain dynamics when the car goes straight, and other where it curves. Similarly, when observing a human performing different kinds of actions in a given scenario, the dynamics followed by the person for each action is to be differentiated. However, such separations should be performed actively during an on-line observation, therefore the system, and particularly its internal representations, should adapt dynamically to the changing data.A viable process to achieve that goal implies representing every observation, e.g., each frame, so that estimating the dynamic evolution of such representations consequently means abstracting the dynamics of the observations. For example, when observing people performing sets of actions, one would describe frames regarding the position and the pose of the person acting. Nonetheless, given an unsupervised framework, such information is not available. So, the way in which the representations are defined is to depend on the observed dynamics. That is so since the relevance of what is to be represented comes from its relation to the evolution of the observations, e.g., the actions being executed.Therefore we define our primary goal as to the acquisition of representational states observations and their dynamics simultaneously in an unsupervised way. Accordingly, the definition of representations is central and determines how learning is to be understood. In particular, representational states are to be defined as dynamic and capable of adjusting themselves to changing environments and uncertainty in sensory data. Taking into account such constraints, we consider an active process where changes in the world and internal states play a primary role interpreting the observed data. That is in opposition to an entirely passive process where an input is transformed into static representations, e.g., a classifier. So, the representational process should be understood in the temporal domain as a mechanism that responds to perceived changes.To implement that, it would be necessary that a system, e.g., a neural network (NN), adapts itself over time to the observed data. With NNs the primary way to achieve similar behaviors is through recurrent networks. When recurrence is involved, the states of previous time steps affect the interpretation of the current inputs, which could include information from different time steps as in the case of NNs based on LSTM units BID9 . Nonetheless, it is also possible to define the adaptability of a NN regarding its predictive accuracy. In general, the prediction error of the network's output is only used for adapting the NN during training by adjusting its parameters through, for example, backpropagation. However, a more dynamic view would include a capability of such kind as part of the inference process. That is, the NN could benefit from a feature that allows it to modify some of its internal states dynamically to model the sensed data based on feedback from its prediction error in a backpropagation-like way. That would allow an active process in which the interpretation of the environment depends also on previous states, or beliefs, therefore making the system capable of actively adapting to changing scenarios.The ideas of actively interpreting and adapting to observed data coincide with dynamic views on conceptual representations in the cognitive science. From such perspectives, representations are seen more as dynamic and distributed structures than as symbols or static categories. In particular, BID12 conclude from neuroimaging studies that concepts might be flexible, experience-dependent and modality-specific, while distributed across the sensory-motor systems. In particular, for them, the flexibility is crucial for the capability of adapting to diverse situations. Olier et al. (2017b) elaborate on how the definition of concepts has evolved and how it impacts the way in which learning is understood, and how that consequently affects the design of artificial learning agents. In particular, they argue that concepts are not to be seen as the encapsulation of knowledge in symbols, but as the structure on which the emergence of behavior occurs. Therefore, how we represent should be seen as dynamic and time dependent, that is, representations make sense only when embedded in the interaction process.Moreover, Olier et al. (2017b) analyze differences between several views on concepts by linking categorization based approaches to the computational views of cognition, while ideas of concepts as flexible, distributed and context-dependent to many aspects of embodied BID26 and grounded cognition BID0 . They describe an approach in which representing implies an act of actively interpreting and adapting to the world. BID0 , from the perspectives of grounded cognition, has elaborated on how simulation is fundamental for concept's acquisition and processing, referring to simulation as the re-enactment of sensorimotor modalities. That can be linked to the ideas on predictive coding BID22 , in which top-down information in the cortex carries predictions about lower levels, while feed-forward connections carry residual errors. Those notions are further developed by BID6 with the free energy principle, where it is argued that the primary function of the brain is to minimize free energy or suppress prediction error.Those ideas have been developed and interpreted in different ways as algorithms. Frequently, implementations aim at systems that update internal beliefs about causes of perceived information from prediction error. Some approaches, particularly given the probabilistic characteristics of the free energy principle, are based on Bayesian methods and generative models, which are argued to account for contextual reactions and causality given temporal relations BID3 . Here we explore some existing techniques and propose a method based on generative models that aims at constructing representations of observed and its dynamics. Particularly we propose a generative model that works simultaneously as an encoder, or its own inverse model, by the use of prediction error to update internal states. We have presented a method for representing dynamic data, and we have tested it on videos of interactions. The states are organized in two levels representing the observations and their dynamics respectively. It has been shown that the method proposed is capable of learning generative models by exploring the latent space through an active adaptation based on prediction error propagation. In the model, the representations make sense in the temporal domain, as to serve their function they have to evolve with the observed data dynamically.Two experiments have been performed to test the model. The first one on static data showing how the adaptation leads to better results than an entirely static model, e.g., a VAE. The process evaluated in that case takes more processing that the static method, yet it shows that the accuracy is not only dependent on the generalization capability of the model, but also on its ability to adapt temporally to the data. In a second experiment, videos of actions performed in a given scenario are used to learn representations of the images and the dynamics of the activities observed. The results show that the model is capable of extracting a semantics similar to the one defined as ground truth for the data used.These ideas have been connected with definitions positions from different branches of the cognitive and brain sciences, which state that interpreting the world is an active process. That suggests that a possible path towards better machine learning algorithms may imply understanding representations and their processing as a temporal process embedded in a dynamic interaction with the environment, or the evolution of the data itself.",A method that build representations of sequential data and its dynamics through generative models with an active process,two ; first ; second ; Bayesian ; Olier et al. ; one ; VAE,different ways ; an entirely passive process ; predictable changes ; prediction error propagation ; the cognitive and brain sciences ; Two experiments ; what ; LSTM units ; each action ; contextual reactions,two ; first ; second ; Bayesian ; Olier et al. ; one ; VAE,"An unsupervised method for building dynamic representations of sequential data is based on a hierarchical generative model composed of two levels. In the first level, representational states encode the dynamics of the lower one and transition models are generated. The method explores the latent space guided by its knowledge and uncertainty about it by updating latent variables from prediction error signals backpropagated to latent space. No encoder or inference models are used, as the generators also serve as their inverse transformations. The adaptation over time leads to better performance than similar architectures without temporal dependencies, e.g., variational autoencoders. The system extracts the dynamics",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"The unconditional generation of high fidelity images is a longstanding benchmark
 for testing the performance of image decoders. Autoregressive image models
 have been able to generate small images unconditionally, but the extension of
 these methods to large images where fidelity can be more readily assessed has
 remained an open problem. Among the major challenges are the capacity to encode
 the vast previous context and the sheer difficulty of learning a distribution that
 preserves both global semantic coherence and exactness of detail. To address the
 former challenge, we propose the Subscale Pixel Network (SPN), a conditional
 decoder architecture that generates an image as a sequence of image slices of equal
 size. The SPN compactly captures image-wide spatial dependencies and requires a
 fraction of the memory and the computation. To address the latter challenge, we
 propose to use multidimensional upscaling to grow an image in both size and depth
 via intermediate stages corresponding to distinct SPNs. We evaluate SPNs on the
 unconditional generation of CelebAHQ of size 256 and of ImageNet from size 32
 to 128. We achieve state-of-the-art likelihood results in multiple settings, set up
 new benchmark results in previously unexplored settings and are able to generate
 very high fidelity large scale samples on the basis of both datasets. A successful generative model has two core aspects: it produces targets that have high fidelity and it generalizes well on held-out data. Autoregressive (AR) models trained by conventional maximum likelihood estimation (MLE) have produced superior scores on held-out data across a wide range of domains such as text BID16 BID18 , audio BID13 , images and videos BID4 . These scores are a measure of the models' ability to generalize in that setting. From the perspective of sample fidelity, the outputs generated by AR models have also achieved state-of-the-art fidelity in many of the aforementioned domains with one notable exception. In the domain of unconditional large-scale image generation, AR samples have yet to manifest long-range structure and semantic coherence.One source of difficulties impeding high-fidelity image generation is the multi-faceted relationship between the MLE scores achieved by a model and the model's sample fidelity. On the one hand, MLE is a well-defined measure as improvements in held-out scores generally produce improvements in the visual fidelity of the samples. On the other hand, as opposed to for example adversarial methods BID0 , MLE forces the model to support the entire empirical distribution. This guarantees the model's ability to generalize at the cost of allotting capacity to parts of the distribution that are irrelevant to fidelity. A second source of difficulties arises from the high dimensionality of large images. A 256 × 256 × 3 image has a total of 196,608 positions that need to be architecturally connected in order to learn dependencies among them; the representations at each position require sufficient capacity to express their respective surrounding contexts. These requirements translate to large amounts of memory and computation. Figure 1: A representation of Multidimensional Upscaling. Left: depth upscaling is applied to a generated 3-bit 256 × 256 RGB subimage from CelebAHQ to map it to a full 8-bit 256 × 256 RGB image. Right: size upscaling followed by depth upscaling are applied to a generated 3-bit 32 × 32 RGB subimage from ImageNet to map it to the target resolution of the 8-bit 128 × 128 RGB image. We stress that the rightmost column of both figures are true unconditional samples from our model at full 8bit depth.These difficulties notwithstanding, we aim to learn the full distribution over 8-bit RGB images of size up to 256 × 256 well enough so that the samples have high fidelity. We aim to guide the model to focus first on visually more salient bits of the distribution and later on the visually less salient bits. We identify two visually salient subsets of the distribution: first, the subset determined by sub-images (""slices"") of smaller size (e.g. 32 × 32) sub-sampled at all positions from the original image; and secondly, the subset determined by the few (e.g. 3) most significant bits of each RGB channel in the image. We use Multidimensional Upscaling to map from one subset of the distribution to the other one by upscaling images in size or in depth. For example, the generation of a 128 × 128 8-bit RGB image proceeds by first upscaling it in size from a 32 × 32 3-bit RGB image to a 128 × 128 3-bit RGB image; we then upscale the resulting image in depth to the original resolution of the 128 × 128 8-bit RGB image. We thus train three networks: (a) a decoder on the small size, low depth image slices subsampled at every n pixels from the original image with the desired target resolution; (b) a size-upscaling decoder that generates the large size, low depth image conditioned on the small size, low depth image; and (c) a depth-upscaling decoder that generates the large size, high depth image conditioned on the large size, low depth image. Figure 1 illustrates this process.To address the latter difficulties that ensue in the training of decoders (b) and (c), we develop the Subscale Pixel Network (SPN) architecture. The SPN divides an image of size N ×N into sub-images of size N S × N S sliced out at interleaving positions (see FIG1 ), which implicitly also captures a form of size upscaling. The N × N image is generated one slice at a time conditioned on previously generated slices in a way that encodes a rich spatial structure. SPN consists of two networks, a conditioning network that embeds previous slices and a decoder proper that predicts a single target slice given the context embedding. The decoding part of the SPN acts over image slices with the same spatial structure and it can share weights for all of them. The SPN is an independent image decoder with an implicit size upscaling mechanism, but it can also be used as an explicit size upscaling network by initializing the first slice of the SPN input at sampling time with one generated separately during step (a).We extensively evaluate the performance of SPN and the size and depth upscaling methods both quantitatively and from a fidelity perspective on two unconditional image generation benchmarks, CelebAHQ-256 and ImageNet of various sizes up to 256. From a MLE scores perspective, we compare with previous work to obtain state-of-the-art results on CelebAHQ-256, both at full 8-bit resolution and at the reduced 5-bit resolution BID7 , and on ImageNet-64. We also establish MLE baselines for ImageNet-128 and ImageNet-256. From a sample fidelity perspective, we show the strong benefits of multidimensional upscaling as well as the benefits of the SPN. We produce CelebAHQ-256 samples (at full 8-bit resolution) that are of similar visual fidelity to those produced with methods such as GANs that lack however an intrinsic measure of generalization BID10 BID6 . We also produce some of the first successful samples on unconditional ImageNet-128 (also at 8-bit) showing again the striking impact of the SPN and of multidimensional upscaling on sample quality and setting a fidelity baseline for future methods. The problem of whether it is possible to learn the distribution of complex natural images and attain high sample fidelity has been a long-standing one in the tradition of generative models. The SPN and Multidimensional Upscaling model that we introduce accomplishes a large step towards solving this problem, by attaining both state-of-the-art MLE scores on large-scale images from complex domains such as CelebAHQ-256 and ImageNet-128 and by being able to generate high fidelity full 8-bit samples from the resulting learnt distributions without alterations to the sampling process (via e.g. heavy modifications of the temperature of the output distribution). The generated samples show an unprecedented amount of semantic coherence and exactness of details even at the large scale size of full 8-bit 128 × 128 and 256 × 256 images.",We show that autoregressive models can generate high fidelity images.,two ; AR ; first ; Multidimensional Upscaling ; MLE ; second ; RGB ; N S × N S ; ImageNet ; fidelity,"all positions ; equal
 size ; new benchmark results ; the representations ; Autoregressive (AR) models ; MLE baselines ; GANs ; the large size, low depth image ; high sample fidelity ; the latter difficulties",two ; AR ; first ; Multidimensional Upscaling ; MLE ; second ; RGB ; N S × N S ; ImageNet ; fidelity,"The unconditional generation of high fidelity images is a longstanding benchmark for testing image decoders. Autoregressive image models trained by conventional maximum likelihood estimation (MLE) have been able to generate small images unconditionally, but the extension of these methods to large images where fidelity can be more readily assessed has been an open challenge. The Subscale Pixel Network (SPN) is a conditional image decoder architecture that generates images as a sequence of image slices of equal size and requires a fraction of the memory and the computation. To address the latter challenge, we propose multidimensional upscaling to grow an image in both size and",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Deep Reinforcement Learning (Deep RL) has been receiving increasingly more attention  thanks to its encouraging performance on a variety of control tasks. Yet, conventional regularization techniques in training neural networks (e.g., $L_2$ regularization, dropout) have been largely ignored in RL methods, possibly because agents are typically trained and evaluated in the same environment. In this work, we present the first comprehensive study of regularization techniques with multiple policy optimization algorithms on continuous control tasks. Interestingly, we find conventional regularization techniques on the policy networks can often bring large improvement on the task performance, and the improvement is typically more significant when the task is more difficult. We also compare with the widely used entropy regularization and find $L_2$ regularization is generally better. Our findings are further confirmed to be robust against the choice of training hyperparameters. We also study the effects of regularizing different components and find that only regularizing the policy network is typically enough. We hope our study provides guidance for future practices in regularizing policy optimization algorithms. Regularization, typically referring to methods for preventing overfitting, is a key technique in successfully training a neural network. Perhaps the most widely recognized regularization methods in deep learning are L 2 regularization (also known as weight decay) and dropout (Srivastava et al., 2014) . Those techniques are standard practices in supervised learning tasks from many domains. Major tasks in computer vision, e.g., image classification (He et al., 2016; Huang et al., 2017) , object detection (Ren et al., 2015; Redmon et al., 2016) , all use L 2 regularization as a default option. In natural language processing, for example, the Transformer model (Vaswani et al., 2017) uses dropout. and the recently popular BERT model (Devlin et al., 2018) uses L 2 regularization. In fact, it is very rare to see state-of-the-art neural models trained without any regularization in a supervised setting. However, in deep reinforcement learning (RL), those conventional regularization methods are largely absent or underutilized in past research, possibly because in most cases we are maximizing the return on exactly the same task as in training. In other words, there is a lack of generalization gap from the training environment to the test environment (Cobbe et al., 2018) . Moreover, researchers in deep RL focus more on high-level algorithm designs, which is more closely related to the field of reinforcement learning, and focus less on network training techniques such as regularization. For popular policy optimization algorithms like Asynchronous Advantage Actor-Crtic (A3C) (Mnih et al., 2016) , Trust Region Policy Optimization (TRPO) (Schulman et al., 2015) , Proximal Policy Optimization (PPO) , and Soft Actor Critic (SAC) (Haarnoja et al., 2018) , conventional regularization methods were not considered. Even in popular codebases such as the OpenAI Baseline , L 2 regularization and dropout were not incorporated. Instead, the most commonly used regularization in the RL community, is an ""entropy regularization"" term that penalizes the high-certainty output from the policy network, to encourage more exploration during the training process and prevent the agent from overfitting to certain actions. The entropy regularization was first introduced by Williams & Peng (1991) and now used by many contemporary algorithms (Mnih et al., 2016; Teh et al., 2017; Farebrother et al., 2018) . In this work, we take an empirical approach to questioning the conventional wisdom of not using common regularizations. We study agent's performance on the current task (the environment which the agent is trained on), rather than its generalization ability to different environments as many recent works (Zhang et al., 2018a; Zhao et al., 2019; Farebrother et al., 2018; Cobbe et al., 2018) . We specifically focus our study on policy optimization methods, which are becoming increasingly popular and have achieved top performance on various tasks. We evaluate four popular policy optimization algorithms, namely SAC, PPO, TRPO, and the synchronous version of Advantage Actor Critic (A2C), on multiple continuous control tasks. A variety of conventional regularization techniques are considered, including L 2 /L 1 weight regularization, dropout, weight clipping (Arjovsky et al., 2017) and Batch Normalization (BN) (Ioffe & Szegedy, 2015) . We compare the performance of these regularization techniques to that without regularization, as well as the entropy regularization. Surprisingly, even though the training and testing environments are the same, we find that many of the conventional regularization techniques, when imposed to the policy networks, can still bring up the performance, sometimes significantly. Among those regularizers, L 2 regularization, perhaps the most simple one, tends to be the most effective for all algorithms and generally outperforms entropy regularization. L 1 regularization and weight clipping can boost performance in many cases. Dropout and Batch Normalization tend to bring improvements only on off-policy algorithms. Additionally, all regularization methods tend to be more effective on more difficult tasks. We also verify our findings with a wide range of training hyperparameters and network sizes, and the result suggests find that imposing proper regularization can sometimes save the effort of tuning other training hyperparameters. Finally, we study which part of the policy optimization system should be regularized, and conclude that generally only regularizing the policy network suffices, as imposing regularization on value networks usually does not help. Our results also show that neural network training techniques such as regularization, can be as important as high-level reinforcement learning algorithms in terms of boosting performance. Our main contributions can be summarized as follows: • We provide the first comprehensive study of common regularization methods in policy optimization algorithms, which have been largely ignored in the RL literature. • We find conventional regularizations can often be very effective in improving the performance on continuous control tasks, espcially on harder ones. Remarkably, the most simple L 2 regularization generally performs better than the more widely used entropy regularization. BN and dropout can only help in off-policy algorithms. • We experiment with multiple randomly sampled training hyperparameters for each algorithm and confirm our findings still hold. The result also suggests that proper regularization can sometimes ease the hyperparameter tuning process. • We study which part of the network(s) should be regularized. The key lesson is to regularize the policy network but not the value network. Why does regularization benefit policy optimization? In RL, we are typically training and evaluating on the same environment, i.e., there is no generalization gap across different environments. However, there is still generalization between samples: the agents is only trained on the limited trajectories it has experienced, which cannot cover the whole state-action space of the environment. A successful policy needs to generalize from seen samples to unseen ones, which potentially makes regularization necessary in RL. This might also explain why regularization could be more helpful on harder tasks, which have larger state space. In this case, the portion of the space that have appeared in training tends to be smaller, and overfitting to this smaller portion of space would cause more serious issues, in which case regularizations may help. Some detailed analysis are provided in Appendix G. Why do BN and dropout work only with off-policy algorithms? One major finding in our experiments is BN and dropout can sometimes improve on the off-policy algorithm SAC, but mostly would hurt on-policy algorithms. There are two possible reasons for this: 1) for both BN and dropout, training mode is used to train the network, and testing mode is used to sample actions during interaction with the environment, leading to a discrepancy between the sampling policy and optimization policy (the same holds if we always use training mode). For on-policy algorithms, if such discrepancy is large, it can cause severe off-policy issues, which hurts the optimization process or even crashes it. For off-policy algorithms, this discrepancy is not an issue since they naturally accept off-policy data. 2) Batch Normalization layers can be sensitive to input distribution shifts, since the mean and std statistics depend heavily on the input, and if the input distribution changes too quickly in training, the mapping functions of BN layers can change quickly too, and it can possibly destabilize training. One evidence for this is that in supervised learning, when transferring a ImageNet pretrained model to other vision datasets, sometimes the BN layers are fixed (Yang et al., 2017) and only other layers are trained. In on-policy algorithms, we always use the samples generated from the latest policy; in off-policy algorithms, the sample distributions are relatively slow-changing since we always draw from the whole replay buffer which holds cumulative data. The faster-changing input distribution for on-policy algorithms could be harmful to BN. Previously, BN has also been shown to be effective in Deep Deterministic Policy Gradient (DDPG) (Lillicrap et al., 2015) , an off-policy algorithm. In summary, we conducted the first comprehensive study of regularization methods with multiple policy optimization algorithms on continuous control benchmarks. We found that L 2 regularization, despite being largely ignored in prior literature, is effective in improving performance, even more than the widely used entropy regularization. BN and dropout could also be useful but only on off-policy algorithms. Our findings were confirmed with multiple hyperparameters. Further experiments have shown that generally the best practice is to regularize the policy network alone but not the value network or both.","We show that conventional regularization methods (e.g., $L_2$, dropout), which have been largely ignored in RL methods, can be very effective in policy optimization.",Zhang et al. ; SAC ; Asynchronous Advantage Actor-Crtic ; Proximal Policy Optimization ; DDPG ; Teh ; Mnih ; One ; Soft Actor Critic ; BN,the samples ; the current task ; the input ; He ; the mean and std statistics ; two possible reasons ; the conventional wisdom ; perhaps the most simple one ; Vaswani ; reinforcement learning,Zhang et al. ; SAC ; Asynchronous Advantage Actor-Crtic ; Proximal Policy Optimization ; DDPG ; Teh ; Mnih ; One ; Soft Actor Critic ; BN,"Conventional regularization techniques in training neural networks (e.g., $L_2$ regularization, dropout) have been largely ignored in RL methods due to agents being typically trained and evaluated in the same environment. In this work, we present the first comprehensive study of regularization methods with multiple policy optimization algorithms on continuous control tasks, and the improvement is typically more significant when the task is more difficult. We also compare with the widely used entropy regularization and find that it is generally better. In addition, we also find that L 2 regularization (also known as weight decay) and dropout (Srivast",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"In this paper, we propose a novel regularization method, RotationOut, for neural networks. 
 Different from Dropout that handles each neuron/channel independently, RotationOut regards its input layer as an entire vector and introduces regularization by randomly rotating the vector. 
 RotationOut can also be used in convolutional layers and recurrent layers with a small modification.
 We further use a noise analysis method to interpret the difference between RotationOut and Dropout in co-adaptation reduction. 
 Using this method, we also show how to use RotationOut/Dropout together with Batch Normalization. 
 Extensive experiments in vision and language tasks are conducted to show the effectiveness of the proposed method. 
 Codes will be available. Dropout (Srivastava et al., 2014 ) has proven to be effective for preventing overfitting over many deep learning areas, such as image classification (Shrivastava et al., 2017) , natural language processing (Hu et al., 2016) and speech recognition (Amodei et al., 2016) . In the years since, a wide range of variants have been proposed for wider scenarios, and most related work focus on the improvement of Dropout structures, i.e., how to drop. For example, drop connect (Wan et al., 2013) drops the weights instead of neurons, evolutional dropout (Li et al., 2016) computes the adaptive dropping probabilities on-the-fly, max-pooling dropout (Wu & Gu, 2015) drops neurons in the max-pooling kernel so smaller feature values have some probabilities to to affect the activations. These Dropout-like methods process each neuron/channel in one layer independently and introduce randomness by dropping. These architectures are certainly simple and effective. However, randomly dropping independently is not the only method to introduce randomness. Hinton et al. (2012) argues that overfitting can be reduced by preventing co-adaptation between feature detectors. Thus it is helpful to consider other neurons' information when adding noise to one neuron. For example, lateral inhibition noise could be more effective than independent noise. In this paper, we propose RotationOut as a regularization method for neural networks. RotationOut regards the neurons in one layer as a vector and introduces noise by randomly rotating the vector. Specifically, consider a fully-connected layer with n neurons: x ∈ R n . If applying RotationOut to this layer, the output is Rx where R ∈ R n×n is a random rotation matrix. It rotates the input with random angles and directions, bringing noise to the input. The noise added to a neuron comes not only from itself, but also from other neurons. It is the major difference between RotationOut and Dropout-like methods. We further show that RotationOut uses the activations of the other neurons as the noise to one neuron so that the co-adaptation between neurons can be reduced. RotationOut uses random rotation matrices instead of unrestricted matrices because the directions of feature vectors are important. Random rotation provides noise to the directions directly. Most neural networks use dot product between the feature vector and weight vector as the output. The network actually learns the direction of the weights, especially when there is a normalization layer (e.g. Batch Normalization (Ioffe & Szegedy, 2015) or Weight Normalization (Salimans & Kingma, 2016) ) after the weight layer. Random rotation of feature vecoters introduces noise into the angle between the feature and the weight, making the learning of weights directions more stable. Sabour et al. (2017) also uses the orientation of feature vectors to represent the instantiation parameters in capsules. Another motivation for rotating feature vectors comes from network dissection. Bau et al. (2017) finds that random rotations of a learned representation can destroy the interpretability which is axis-aligned. Thus random rotating the feature during training makes the network more robust. Even small rotations can be a strong regularization. We study how RotationOut helps prevent neural networks from overfitting. Hinton et al. (2012) introduces co-adaptation to interpret Dropout but few literature give a clear concept of co-adaptation. In this paper,we provide a metric to approximate co-adaptations and derive a general formula for noise analysis. Using the formula, we prove that RotationOut can reduce co-adaptations more effectively than Dropout and show how to combine Dropout and Batch Normalization together. In our experiments, RotationOut can achieve results on par with or better than Dropout and Dropoutlike methods among several deep learning tasks. Applying RotationOut after convolutional layers and fully connected layers improves image classification accuracy of ConvNet on CIFAR100 and ImageNet datasets. On COCO datasets, RotationOut also improves the generalization of object detection models. For LSTM models, RotationOut can achieve competitive results with existing RNN dropout method for speech recognition task on Wall Street Journal (WSJ) corpus. The main contributions of this paper are as follows: We propose RotationOut as a regularization method for neural networks which is different from existing Dropout-like methods that operate on each neuron independently. RotationOut randomly rotates the feature vector and introduces noise to one neuron with other neurons' information. We present a theoretical analysis method for general formula of noise. Using the method, we answer two questions: 1) how noise-based regularization methods reduce co-adaptions and 2) how to combine noise-based regularization methods with Batch Normalization. Experiments in vision and language tasks are conducted to show the effectiveness of the proposed RotationOut method. Related Work Dropout is effective for fully connected layers. When applied to convolution layers, it is less effective. Ghiasi et al. (2018) argues that information about the input can still be sent to the next layer even with dropout, which causes the networks to overfit (Ghiasi et al., 2018) . SpatialDropout (Tompson et al., 2015) drops the entire channel from the feature map. Shake-shake regularization (Gastaldi, 2017) drops the residual branches. Cutout (DeVries & Taylor, 2017) and Dropblock (Ghiasi et al., 2018 ) drop a continuois square region from the inputs/feature maps. Applying standard dropout to recurrent layers also results in poor performance (Zaremba et al., 2014; Labach et al., 2019) , since the noise caused by dropout at each time step prevents the network from retaining long-term memory. Gal & Ghahramani (2016) ; Moon et al. (2015) ; Merity et al. (2017) generate a dropout mask for each input sequence, and keep it the same at every time step so that memory can be retained. Batch Normalization (BN) (Ioffe & Szegedy, 2015) accelerates deep network training. It is also a regularization to the network, and discourage the strength of dropout to prevent overfitting (Ioffe & Szegedy, 2015) . Many modern ConvNet architectures such as ResNet (He et al., 2016) and DenseNet (Huang et al., 2017) do not apply dropout in convolutions. Li et al. (2019) is the first to argue that it is caused by the a variance shift. In this paper, we use the noise analysis method to further explore this problem. There is a lot of work studying rotations in networks. Rotations on the images (Lenc & Vedaldi, 2015; Simard et al., 2003) are important data augmentation methods. There are also studies about rotation equivalence. Worrall et al. (2017) uses an enriched feature map explicitly capturing the underlying orientations. Marcos et al. (2017) applies multiple rotated versions of each filter to the input to solve problems requiring different responses with respect to the inputs' rotation. The motivations of these work are different from ours. The most related work is network dissection (Bau et al., 2017) . They discuss the impact on the interpretability of random rotations of learned features, showing that rotation in training can be a strong regularization. In this work, we introduce RotationOut as an alternative for dropout for neural network. RotationOut adds continuous noise to data/features and keep the semantics. We further establish an analysis of noise to show how co-adaptations are reduced in neural network and why dropout is more effective than dropout. Our experiments show that applying RotationOut in neural network helps training and increase the accuracy. Possible direction for further work is the theoretical analysis of co-adaptations. As discussed earlier, the proposed correlation analysis is not optimal. It cannot explain the difference between standard Dropout and Gaussian dropout. Also it can not ex-plain some methods such as Shake-shake regularization. Further work on co-adaptation analysis can help better understand noise-based regularization methods. One example of such a matrix that rotates the (1, 3) dimensions and (2, 4) dimensions can be: In Section 2, we mentioned the complexity of RotationOut is O(D). It is because we can avoid matrix multiplications to get Rx. For example, let the R be the operator generated by Equation 17, we have: The sparse matrix in Equation 18 is similar to a combine of permutation matrix, and we do not need matrix multiplications to get the output. The output can be get by slicing and an elementwise multiplication: x[3, 4, 1, 2] * [−1, 1, 1, −1].",We propose a regularization method for neural network and a noise analysis method,DenseNet ; RNN ; one ; Hinton ; Marcos et al ; first ; Dropoutlike ; Ghiasi ; two ; Lenc & Vedaldi,a lot ; the direction ; Wu ; that rotation ; the operator ; an alternative ; a random rotation matrix ; a learned representation ; These architectures ; multiple rotated versions,DenseNet ; RNN ; one ; Hinton ; Marcos et al ; first ; Dropoutlike ; Ghiasi ; two ; Lenc & Vedaldi,"In this paper, we propose a novel regularization method, RotationOut, for neural networks, which involves randomly rotating the input layer and introducing regularization by randomly dropping the vector. This approach can also be used in convolutional layers and recurrent layers with a small modification. In addition to this, we also use a noise analysis method to interpret the difference between Rotation Out and Dropout in co-adaptation reduction. ",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"There is a stark disparity between the learning rate schedules used in the practice of large scale machine learning and what are considered admissible learning rate schedules prescribed in the theory of stochastic approximation. Recent results, such as in the 'super-convergence' methods which use oscillating learning rates, serve to emphasize this point even more.
 One plausible explanation is that non-convex neural network training procedures are better suited to the use of fundamentally different learning rate  schedules, such as the ``cut the learning rate every constant number of epochs'' method (which more closely resembles an exponentially decaying learning rate schedule); note that this widely used schedule is in stark contrast to the polynomial decay schemes prescribed in the stochastic approximation literature, which are indeed shown to be (worst case) optimal for classes of convex optimization problems.

 The main contribution of this work shows that the picture is far more nuanced, where we do not even need to move to non-convex optimization to show other learning rate schemes can be far more effective. In fact, even for the simple case of stochastic linear regression with a fixed time horizon, the rate achieved by any polynomial decay scheme is sub-optimal compared to the statistical minimax rate (by a factor of condition number); in contrast the ```''cut the learning rate every constant number of epochs'' provides an exponential improvement (depending only logarithmically on the condition number) compared to any polynomial decay scheme.   Finally, it is important to ask if our theoretical insights are somehow fundamentally tied to quadratic loss minimization (where we have circumvented minimax lower bounds for more general convex optimization problems)? Here, we conjecture that recent results which make the gradient norm small at a near optimal rate, for both convex and non-convex optimization, may also provide more insights into learning rate schedules used in practice.
 The recent advances in machine learning and deep learning rely almost exclusively on stochastic optimization methods, primarily SGD and its variants. Here, these large scale stochastic optimization methods are manually (and often painstakingly) tuned to the problem at hand (often with parallelized hyper-parameter searches), where there is, as of yet, no class of ""universal methods"" which uniformly work well on a wide range of problems with little to no hyper-parameter tuning. This is in stark contrast to non-stochastic numerical optimization methods, where it is not an overstatement to argue that the l-BFGS and non-linear conjugate gradient methods (with no hyper-parameter tuning whatsoever) have provided nearly unbeatable procedures (for a number of decades) on nearly every unconstrained convex and non-convex problem. In the land of stochastic optimization, there are two dominant (and somewhat compatible approaches): those methods which often manually tune learning rate schedules to achieve the best performance BID13 Sutskever et al., 2013; BID11 BID10 and those methods which rely on various forms of approximate preconditioning BID6 Tieleman & Hinton, 2012; BID11 . This works examines the former class of methods, where we seek a more refined understanding of the issues of learning rate scheduling, through both theoretical analysis and empirical studies.Learning rate schedules for SGD is a rather enigmatic topic since there is a stark disparity between what is considered admissible in theory and what is employed in practice to achieve the best re-sults. Let us elaborate on this distinction more clearly. In theory, a vast majority of works starting with Robbins & Monro (1951) ; Polyak & Juditsky (1992) consider learning rates that have the form of η t = a b+t α for some a, b ≥ 0 and 1/2 < α ≤ 1 -we call these polynomial decay schemes. The key property enjoyed by these polynomial decay schemes is that they are not summable but are square summable. A number of works obtain bounds on the asymptotic convergence rates of such schemes. Note that the focus of these works is to design learning rate schemes that work well for all large values of t. In contrast, practitioners are interested in achieving the best performance given a computational budget or equivalently a fixed time horizon T e.g., 100 passes on training dataset with a batch size of 128.The corresponding practically best performing learning rate scheme is often one where the step size is cut by a constant factor once every few epochs, or, equivalently, when no progress is made on a validation set BID13 BID8 ) (often called a dev set based decay scheme). Such schemes are widely popular to the extent that they are available as schemes in deep learning libraries such as PyTorch 1 and several such useful tools of the trade are taught on popular deep learning courses 2 . Furthermore, what is (often) puzzling (from a theory perspective) is the emphasis that is laid on ""babysitting"" the learning rates 3 to achieve the best performance. Why do practitioners use constant and cut learning rate schemes while most of the theory work routinely works with polynomial decaying schemes? Of course, implicit to this question is the view that both of these schemes are not equivalent. Indeed if both of these were equivalent, one could parameterize the learning rate as a b+t α and do hyperparameter search over a, b and α. In practice, this simply does not give results comparable to the constant and cut schemes. 4 One potential explanation for this could be that, in the context of neural network training, local minima found by constant and cut schemes are of much better quality than those found by polynomial decay schemes, while for convex problems, polynomial decay schemes are indeed optimal.The primary contribution of this work is to show that this is simply not the case. We concretely show how minimax optimal theoretical learning rates (i.e. polynomial decay schemes for wide classes of convex optimization problems) may be misleading (and sub-optimal for locally quadratic problems), and the story in practice is more nuanced. There important issues at play with regards to this suboptimality. First, even for the simple case of stochastic linear regression, with a fixed time horizon, the rate achieved by any polynomial decay scheme (i.e., any choice of a, b and α) is suboptimal compared to the statistical minimax rate (i.e., information theoretically best possible rate achievable by any algorithm) by a factor of condition number κ (see Section 3 for definitions), while there exist constant and cut schemes that are suboptimal only by a factor of log κ.Second, this work shows that a factor of κ suboptimality is unavoidable if we wish to bound the error of each iterate of SGD. In other words, we show that the convergence rate of lim sup of the error, as t → ∞, has to be necessarily suboptimal by a factor ofΩ(κ) compared to the statistical minimax rate, for any learning rate sequence (polynomial or not). In fact, at leastΩ1/κ fraction of the iterates have this suboptimality. With this result, things become quite clear -all the works in stochastic approximation try to bound the error of each iterate of SGD asymptotically (or lim sup of the error in other words). Since this necessarily has to be suboptimal by a factor ofΩ(κ) compared to the statistical minimax rates, the suboptimality of polynomial decay rates is not an issue. However, with a fixed time horizon, there exist learning rate schemes with much better convergence rates, while polynomial decay schemes fail to get better rates in this simpler setting (of known time horizon).Thirdly , the work shows that, for stochastic linear regression, if we consider lim inf (rather than lim sup) of the error, it is possible to design schemes that are suboptimal by only a factor of log κ compared to the minimax rates. Variants of the constant and cut schemes achieve this guarantee.In summary, the contributions of this paper are showing how widely used pratical learning rate schedules are, in fact, highly effective even in the convex case. In particular , our theory and empirical results demonstrate this showing that:• For a fixed time horizon, constant and cut schemes are provably, significantly better than polynomial decay schemes.• There is a fundamental difference between fixed time horizon and infinite time horizon.• The above difference can be mitigated by considering lim inf of error instead of lim sup.• In addition to our theoretical contributions, we empirically verify the above claims for neural network training on cifar-10.Extending results on the performance of constant and cut schemes to more general convex optimization problems, beyond stochastic linear regression, is an important future direction. However, the fact that the suboptimality of polynomial decay schemes even for the simple case of stochastic linear regression, has not been realized after decades of research on stochastic approximation is striking.In summary, the results of this paper show that, even for stochastic linear regression, the popular in practice, constant and cut learning rate schedules are provably better than polynomial decay schemes popular in theory and that there is a need to rethink learning rate schemes and convergence guarantees for stochastic approximation. Our results also suggest that current approaches to hyperparameter tuning of learning rate schedules might not be right headed and further suggest potential ways of improving them.Paper organization: The paper is organized as follows. We review related work in Section 2. Section 3 describes the notation and problem setup. Section 4 presents our results on the suboptimality of both polynomial decay schemes and constant and cut schemes. Section 5 presents results on infinite horizon setting. Section 6 presents experimental results and Section 7 concludes the paper. The main contribution of this work shows that the picture of learning rate scheduling is far more nuanced than suggested by prior theoretical results, where we do not even need to move to nonconvex optimization to show other learning rate schemes can be far more effective than the standard polynomially decaying rates considered in theory.Is quadratic loss minimization special? One may ask if there is something particularly special about why the minimax rates are different for quadratic loss minimization as opposed to more general convex (and non-convex) optimization problems? Ideally, we would hope that our theoretical insights (and improvements) can be formally established in more general cases. Here, an alternative viewpoint is to consider gradient norm as a means to measure the progress of an algorithm. The recent work of Allen-Zhu (2018) shows marked improvements for making the gradient norm small (when working with stochastic gradients) for both convex and non-convex, in comparison to prior results. In particular, for the strongly convex case, Allen-Zhu (2018) provides results which have only a logarithmic dependency on κ, an exponential improvement over what is implied by standard analyses for the gradient norm BID15 Rakhlin et al., 2012; BID5 ; Allen-Zhu (2018) also provides improvements for the smooth and non-convex cases. Thus, for the case of making the gradient norm small, there does not appear to be a notable discrepancy between the minimax rate of quadratic loss minimization in comparison to more general strongly convex (or smooth) convex optimization problems. Interestingly, the algorithm of Allen-Zhu (2018) provides a recursive regularization procedure that obtains an SGD procedure, where the doubling regularization can be viewed as being analogous to an exponentially decaying learning rate schedule. Further work in this direction may be promising in providing improved algorithms. DISPLAYFORM0 the variance in the i th direction at time step t. Let the initialization be such that v DISPLAYFORM1 and v DISPLAYFORM2 . This means that the variances for all directions with eigenvalue κ remain equal as t progresses and similarly for all directions with eigenvalue 1. We have DISPLAYFORM3 We consider a recursion for v DISPLAYFORM4 t with eigenvalue λ i (1 or κ). By the design of the algorithm, we know v DISPLAYFORM5 1−(1−ηλ) 2 be the solution to the stationary point equation DISPLAYFORM6 Intuitively if we keep using the same learning rate η, then v DISPLAYFORM7 t is going to converge to s(η, λ i ). Also note that s(η, λ) ≈ σ 2 η/2 when ηλ 1.We first prove the following claim showing that eventually the variance in direction i is going to be at least s(η T , λ i ). DISPLAYFORM8 Proof. We can rewrite the recursion as DISPLAYFORM9 In this form, it is easy to see that the iteration is a contraction towards s(η t , λ i ). Further, v DISPLAYFORM10 t − s(η t , λ i ) have the same sign. In particular, let t 0 be the first time such that DISPLAYFORM11 0 (note that η t is monotone and so is s(η t , λ i )), it is easy to see that v DISPLAYFORM12 The claim then follows from a simple induction. DISPLAYFORM13 Therefore we must have s(η T , κ) ≤ v(1) 0 = σ 2 /κ, and by Claim 1 we know v DISPLAYFORM14 we must have η T ≤ 1 8T . Next we will show that when this happens, v DISPLAYFORM15 T must be large so the function value is still large. We will consider two cases, in the first case, b ≥ T α . Since DISPLAYFORM16 T , and we are done.In the second case, b < T α . Since DISPLAYFORM17 The sum of learning rates satisfy DISPLAYFORM18 Here the second inequality uses the fact that T α−1 i −α ≤ i −1 when i ≤ T . Similarly, we also know DISPLAYFORM19 32T . This concludes the second case and proves the theorem.",This paper presents a rigorous study of why practically used learning rate schedules (for a given computational budget) offer significant advantages even though these schemes are not advocated by the classical theory of Stochastic Approximation.,Rakhlin ; Robbins & Monro ; decay ; log ; s(η ; al. ; First ; − s(η ; Second ; a number of decades,"a computational budget ; an algorithm ; ""universal methods ; a simple induction ; th direction ; the second case ; works ; a means ; an important future direction ; non-convex problem",Rakhlin ; Robbins & Monro ; decay ; log ; s(η ; al. ; First ; − s(η ; Second ; a number of decades,"There is a stark disparity between the learning rate schedules used in large scale machine learning and what are considered admissible learning rates prescribed in stochastic approximation literature. However, recent results which make the gradient norm small at a near optimal rate, suggest that non-convex neural network training procedures are better suited to the use of fundamentally different learning rate  schedules, such as the ``cut the learning rates every constant number of epochs'' schedule, which more closely resembles an exponentially decaying learning rate schedule. This is in stark contrast to polynomial decay schemes, which are considered optimal for classes of convex optimization problems. ",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Presently the most successful approaches to semi-supervised learning are based on consistency regularization, whereby a model is trained to be robust to small perturbations of its inputs and parameters. To understand consistency regularization, we conceptually explore how loss geometry interacts with training procedures. The consistency loss dramatically improves generalization performance over supervised-only training; however, we show that SGD struggles to converge on the consistency loss and continues to make large steps that lead to changes in predictions on the test data. Motivated by these observations, we propose to train consistency-based methods with Stochastic Weight Averaging (SWA), a recent approach which averages weights along the trajectory of SGD with a modified learning rate schedule. We also propose fast-SWA, which further accelerates convergence by averaging multiple points within each cycle of a cyclical learning rate schedule. With weight averaging, we achieve the best known semi-supervised results on CIFAR-10 and CIFAR-100, over many different quantities of labeled training data. For example, we achieve 5.0% error on CIFAR-10 with only 4000 labels, compared to the previous best result in the literature of 6.3%. Recent advances in deep unsupervised learning, such as generative adversarial networks (GANs) BID8 , have led to an explosion of interest in semi-supervised learning. Semisupervised methods make use of both unlabeled and labeled training data to improve performance over purely supervised methods. Semi-supervised learning is particularly valuable in applications such as medical imaging, where labeled data may be scarce and expensive BID23 .Currently the best semi-supervised results are obtained by consistency-enforcing approaches BID2 BID17 BID31 BID21 BID24 . These methods use unlabeled data to stabilize their predictions under input or weight perturbations. Consistency-enforcing methods can be used at scale with state-of-the-art architectures. For example, the recent Mean Teacher BID31 model has been used with the Shake-Shake BID7 architecture and has achieved the best semi-supervised performance on the consequential CIFAR benchmarks.This paper is about conceptually understanding and improving consistency-based semi-supervised learning methods. Our approach can be used as a guide for exploring how loss geometry interacts with training procedures in general. We provide several novel observations about the training objective and optimization trajectories of the popular ⇧ BID17 and Mean Teacher BID31 consistency-based models. Inspired by these findings , we propose to improve SGD solutions via stochastic weight averaging (SWA) BID12 , a recent method that averages weights of the networks corresponding to different training epochs to obtain a single model with improved generalization. On a thorough empirical study we show that this procedure achieves the best known semi-supervised results on consequential benchmarks. In particular:• We show in Section 3.1 that a simplified ⇧ model implicitly regularizes the norm of the Jacobian of the network outputs with respect to both its inputs and its weights, which in turn encourages flatter solutions. Both the reduced Jacobian norm and flatness of solutions have been related to generalization in the literature BID29 BID22 BID3 BID27 BID13 BID12 . Interpolating between the weights corresponding to different epochs of training we demonstrate that the solutions of ⇧ and Mean Teacher models are indeed flatter along these directions ( FIG0 ).• In Section 3.2, we compare the training trajectories of the ⇧, Mean Teacher, and supervised models and find that the distances between the weights corresponding to different epochs are much larger for the consistency based models. The error curves of consistency models are also wider ( FIG0 ), which can be explained by the flatness of the solutions discussed in section 3.1. Further we observe that the predictions of the SGD iterates can differ significantly between different iterations of SGD.• We observe that for consistency-based methods , SGD does not converge to a single point but continues to explore many solutions with high distances apart. Inspired by this observation, we propose to average the weights corresponding to SGD iterates, or ensemble the predictions of the models corresponding to these weights. Averaging weights of SGD iterates compensates for larger steps, stabilizes SGD trajectories and obtains a solution that is centered in a flat region of the loss (as a function of weights). Further, we show that the SGD iterates correspond to models with diverse predictions -using weight averaging or ensembling allows us to make use of the improved diversity and obtain a better solution compared to the SGD iterates. In Section 3.3 we demonstrate that both ensembling predictions and averaging weights of the networks corresponding to different training epochs significantly improve generalization performance and find that the improvement is much larger for the ⇧ and Mean Teacher models compared to supervised training. We find that averaging weights provides similar or improved accuracy compared to ensembling, while offering the computational benefits and convenience of working with a single model. Thus, we focus on weight averaging for the remainder of the paper.• Motivated by our observations in Section 3 we propose to apply Stochastic Weight Averaging (SWA) BID12 to the ⇧ and Mean Teacher models. Based on our results in Section 3.3 we propose several modifications to SWA in Section 4. In particular, we propose fast-SWA, which (1) uses a learning rate schedule with longer cycles to increase the distance between the weights that are averaged and the diversity of the corresponding predictions; and (2) averages weights of multiple networks within each cycle (while SWA only averages weights corresponding to the lowest values of the learning rate within each cycle). In Section 5, we show that fast-SWA converges to a good solution much faster than SWA.• Applying weight averaging to the ⇧ and Mean Teacher models we improve the best reported results on CIFAR-10 for 1k, 2k, 4k and 10k labeled examples, as well as on CIFAR-100 with 10k labeled examples. For example, we obtain 5.0% error on CIFAR-10 with only 4k labels, improving the best result reported in the literature BID31 ) by 1.3%. We also apply weight averaging to a state-of-the-art domain adaptation technique BID6 closely related to the Mean Teacher model and improve the best reported results on domain adaptation from CIFAR-10 to STL from 19.9% to 16.8% error.• We release our code at https://github.com/benathi/fastswa-semi-sup 2 BACKGROUND",Consistency-based models for semi-supervised learning do not converge to a single point but continue to explore a diverse set of plausible solutions on the perimeter of a flat region. Weight averaging helps improve generalization performance.,SGD ; Mean ; CIFAR ; Mean Teacher ; Jacobian ; STL ; error.•,This paper ; us ; labeled training data ; the lowest values ; the remainder ; examples ; the best semi-supervised results ; diverse predictions ; many solutions ; deep unsupervised learning,SGD ; Mean ; CIFAR ; Mean Teacher ; Jacobian ; STL ; error.•,"The most successful semi-supervised learning methods are based on consistency regularization, where a model is trained to be robust to small perturbations of its inputs and parameters. The consistency loss dramatically improves generalization performance over supervised-only training. However, SGD struggles to converge on the consistency loss and continues to make large steps that lead to changes in predictions on test data. Stochastic Weight Averaging (SWA) is a recent approach which averages weights along the trajectory of SGD with a modified learning rate schedule. It further accelerates convergence by averaging multiple points within each cycle of a cyclical learning schedule.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"To gain high rewards in muti-agent scenes, it is sometimes necessary to understand other agents and make corresponding optimal decisions. We can solve these tasks by first building models for other agents and then finding the optimal policy with these models. To get an accurate model, many observations are needed and this can be sample-inefficient. What's more, the learned model and policy can overfit to current agents and cannot generalize if the other agents are replaced by new agents. In many practical situations, each agent we face can be considered as a sample from a population with a fixed but unknown distribution. Thus we can treat the task against some specific agents as a task sampled from a task distribution. We apply meta-learning method to build models and learn policies. Therefore when new agents come, we can adapt to them efficiently. Experiments on grid games show that our method can quickly get high rewards. Applying Reinforcement Learning (RL) to multi-agent scenes requires carefully consideration about the influence of other agents. We cannot simply treat other agents as part of the environment and apply independent RL methods BID8 if the actions of them has impact on the payoff of the agent to be trained. For example, consider the two-person ultimatum bargaining game, where two players take part in. One player propose a deal to split a fixed amount of money for them two and the other player decides to accept it or not. If the second player accepts the proposal, they split the money, but if the proposal is refused, they both get zero. Experimental results BID4 show that in actual life, the second player makes the decision according to whether he or she judge the final result fair, rather than makes the obvious rational decision. Thus, the first player needs to predict how the second player will react so as to make the proposal acceptable.In order to exploit the other agents and find the corresponding optimal policy, we need to understand these agents. Here in this paper, we call all the other agents ""opponents"" to distinguish our agent from them, even if they may have cooperative relationship with our agent. For simplicity, we only consider tasks with only one opponent. Extension to tasks with more opponents is straightforward. A general way to exploit an opponent is to build a model for it from observations. This model can characterize any needed feature of the opponent, such as next action or the final goal. Such a model can make predictions for the opponent and thus turns the two-agent task into a simple-agent decision making problem. Then we can apply various RL methods to solve this problem.It is necessary that we need to have an accurate model for the opponent to help make decision. Previous works BID6 BID12 propose some methods to model the opponent. Generally, it requires many observations to get a precise model for the opponent. This may cost many iterations to act with the opponent. What's more, even if we can precisely model the opponent, there exists a main drawback of above process that the performance of the learned policy has no guarantee for any other opponent. Things are even worse if opponents have their private types which are unknown for us. New opponents with different types can have different policies or even different payoffs. Therefore, it seems that when a new opponent came, we have to learn a policy from the beginning. In some practical situations, the whole opponents follow a distributions over all these possible types. Let's come back to the ultimatum bargaining game. BID0 shows that people with different ethnicity may have different standards for fairness. Thus if we assume the type for player 2 to be its judgment for fairness, there can be a distribution for types dependent on the ethnic distribution. Given that opponents follows a distribution, it is possible that we can employ some given opponents to help us speed up the process of opponent modeling and policy improving for the current opponent.If we consider the policy learning against a specific opponent as a task, our goal can be considered as training a policy on various tasks so that it can efficiently adapt to a good policy on a new task with few training samples. This is exactly a meta-learning problem. We employ Model-Agnostic MetaLearning (MAML) BID1 to conduct meta-learning. BID11 applied meta-learning to understand opponents, but this work doesn't address the policy improvement for the agent to be trained. We apply meta-learning to opponent modeling and policy learning separately while training the two meta-learners jointly. Then we use the meta-learners to initialize the model and policy for the new opponent. Experimental results show that the agent can adapt to the new opponent with a small number of interactions with the opponent. In the face of other agents, it is beneficial to build models for opponents and find a corresponding good policy. This method can be sample-inefficient since it costs many observations to build models and learn a policy then. We propose a method that can employ the information learned from experiences with other opponents to speed up the learning process for the current opponents. This method is suitable for many practical situations where the opponent population has a relative stable distribution over their policies. We apply meta-learning to jointly train the opponent modeling and policy improving process. Experimental results show that our method can be sample-efficient.",Our work applies meta-learning to multi-agent Reinforcement Learning to help our agent efficiently adapted to new coming opponents.,first ; Applying Reinforcement Learning ; two ; One ; second ; zero ; only one ; Model-Agnostic MetaLearning ; MAML,the model ; the policy ; grid games ; a corresponding good policy ; One player ; actual life ; the type ; even different payoffs ; fairness ; carefully consideration,first ; Applying Reinforcement Learning ; two ; One ; second ; zero ; only one ; Model-Agnostic MetaLearning ; MAML,"In order to gain high rewards in muti-agent scenes, it is necessary to understand other agents and make corresponding optimal decisions. The learned model and policy can overfit to current agents and cannot generalize if the other agents are replaced by new agents. In many practical situations, each agent we face can be considered as a sample from a fixed but unknown distribution. The task against some specific agents can be treated as a task sampled from a task distribution. We apply meta-learning method to build models and learn policies, and when new agents come, we can adapt to them efficiently. Experiments on grid games show that our method",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"To improve how neural networks function it is crucial to understand their learning process. The information bottleneck theory of deep learning proposes that neural networks achieve good generalization by compressing their representations to disregard information that is not relevant to the task. However, empirical evidence for this theory is conflicting, as compression was only observed when networks used saturating activation functions. In contrast, networks with non-saturating activation functions achieved comparable levels of task performance but did not show compression. In this paper we developed more robust mutual information estimation techniques, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions. Using these adaptive estimation techniques, we explored compression in networks with a range of different activation functions. With two improved methods of estimation, firstly, we show that saturation of the activation function is not required for compression, and the amount of compression varies between different activation functions. We also find that there is a large amount of variation in compression between different network initializations. Secondary, we see that L2 regularization leads to significantly increased compression, while preventing overfitting. Finally, we show that only compression of the last layer is positively correlated with generalization. Although deep learning (reviewed by BID15 ) has produced astonishing advances in machine learning BID17 , a rigorous statistical explanation for the outstanding performance of deep neural networks (DNNs) is still to be found.According to the information bottleneck (IB) theory of deep learning BID18 BID16 ) the ability of DNNs to generalize can be seen as a type of representation compression. The theory proposes that DNNs use compression to eliminate noisy and task-irrelevant information from the input, while retaining information about the relevant segments BID1 . The information bottleneck method BID19 quantifies the relevance of information by considering an intermediate representation T between the original signal X and the salient data Y . T is the most relevant representation of X, and is said to be an information bottleneck, when it maximally compresses the input, retaining only the most relevant information, while maximizing the information it shares with the target variable Y . Formally, the information bottleneck minimizes the Lagrangian: DISPLAYFORM0 where I(·) is mutual information. In this Lagrangian β is the Lagrange multiplier, determining the trade-off between compression and retention of information about the target. In the context of deep learning, T is a layer's hidden activity represented as a single variable, X is a data set and Y is the set of labels. Compression for a given layer is signified by a decrease in I(T, X) value, while I(T, Y ) is increasing during training. Fitting behaviour refers to both values increasing. BID16 visualized the dynamic of training a neural network by plotting the values of I(T, X) and I(T, Y ) against each other. This mapping was named the information plane. According to IB theory the learning trajectory should move the layer values to the top left of this plane. In fact what was observed was that a network with tanh activation function had two distinct phases: fitting and compression. The paper and the associated talks 1 show that the compression phase leads to layers stabilizing on the IB bound. When this study was replicated by BID14 with networks using ReLU BID12 activation function instead of tanh, the compression phase did not happen, and the information planes only showed fitting throughout the whole training process. This behaviour required more detailed study, as a constant increase in mutual information between the network and its input implies increasing memorization, an undesired trait that is linked to overfitting and poor generalization BID11 .Measuring differential mutual information in DNNs is an ill-defined task, as the training process is deterministic BID14 . Mutual information of hidden activity T with input X is: DISPLAYFORM1 If we consider the hidden activity variable T to be deterministic then entropy is: DISPLAYFORM2 However, if T is continuous then the entropy formula is: DISPLAYFORM3 In the case of deterministic DNNs, hidden activity T is a continuous variable and p(T |X) is distributed as the delta function. For the delta function : DISPLAYFORM4 Thus, the true mutual information value I(T, X) is in fact infinite. However, to observe the dynamics of training in terms of mutual information, finite values are needed. The simplest way to avoid trivial infinite mutual information values, is to add noise to hidden activity.Two ways of adding noise have been explored previously by BID16 and BID14 . One way is to add noise Z directly to T and get a noisy variableT = T + Z. Then H(T |X) = H(Z) and mutual information is I(T , X) = H(T ) + H(Z). When the additive noise is Gaussian, the mutual information can be approximated using kernel density estimation (KDE), with an assumption that the noisy variable is distributed as a Gaussian mixture BID9 . The second way to add noise is to discretize the continuous variables into bins. To estimate mutual information , BID16 and BID14 primarily relied on binning hidden activity. The noise comes embedded with the discretization that approximates the probability density function of a random variable. In context of neural networks , adding noise can be done by binning hidden activity and approximating H(T ) as a discrete variable. In this case H(T |X) = 0 since the mapping is deterministic and I(T, X) = H(T ).Generally, when considering mutual information in DNNs, the analyzed values are technically the result of the estimation process and, therefore, are highly sensitive to it. For this reason it is vital to maintain consistency when estimating mutual information. The problem is not as acute when working with DNNs implemented with saturating activation functions, since all hidden activity is bounded. However, with non-saturating functions, and the resulting unbounded hidden activity, the level of noise brought by the estimation procedure has to be proportional and consistent, adapting to the state of every layer of the network at a particular epoch.In the next section adaptive estimation schemes are presented, both for the binning and KDE estimators. It is shown that for networks with unbounded activation functions in their hidden layers, the estimates of information change drastically. Moreover, the adaptive estimators are better able to evaluate different activation functions in a way that allows them to be compared. This approach shows considerable variation in compression for different activation functions. It also shows that L2 regularization leads to more compression and clusters all layers to the same value of mutual information. When compression in hidden layers is quantified with a compression metric and compared with generalization, no significant correlation is observed. However, compression of the last softmax layer is correlated with generalization. In this paper we proposed adaptive approaches to estimating mutual information in the hidden layers of DNNs. These adaptive approaches allowed us to compare behaviour of different activation functions and to observe compression in DNNs with non-saturating activation functions. However, unlike saturating activation functions, compression is not always present and is sensitive to initialization. This may be due to the minimal size of the network architecture that was tested. Experiments with larger convolutional neural networks could be used to explore this possibility.Different non-saturating activation functions compress information at different rates. While saturation plays a role in compression rates, we show that its absence does not imply absence of compression. Even seemingly similar activation functions, such as softplus and centered softplus, gave different compression scores. Compression does not always happen in later stages of training, but can happen from initialization. Further work is needed to understand the other factors contributing to compression.We also found that DNNs implemented with L2 regularization strongly compress information, forcing layers to forget information about the input. The clustering of mutual information to a single point on the information plane has never been reported previously. This result could lay the ground for further research to optimize the regularization to stabilize the layers on the information bottleneck bound to achieve better generalization BID0 , as well as linking information compression to memorization in neural networks BID20 .There are a few limitations to the analysis presented here. Principally , for tractability, the networks we explored were much smaller and more straightforward than many state of the art networks used for practical applications. Furthermore , our methods for computing information, although adaptive for any distribution of network activity, were not rigorously derived. Finally, our compression metric is ad-hoc. However, overall we have three main observations: first, compression is not restricted to saturating activation functions, second, L2 regularization induces compression, and third, generalization accuracy is positively correlated with the degree of compression only in the last layer and is not significantly affected by compression of hidden layers.",We developed robust mutual information estimates for DNNs and used them to observe compression in networks with non-saturating activation functions,Lagrange ; H(T ; Secondary ; three ; Gaussian ; KDE ; Lagrangian ; first ; One ; I(T,terms ; especially unbounded functions ; a rigorous statistical explanation ; the whole training process ; the input ; These adaptive approaches ; the network architecture ; an assumption ; Even seemingly similar activation functions ; bins,Lagrange ; H(T ; Secondary ; three ; Gaussian ; KDE ; Lagrangian ; first ; One ; I(T,"The information bottleneck (IB) theory of deep learning proposes that neural networks achieve good generalization by compressing representations to disregard information that is not relevant to the task. However, empirical evidence for this theory is conflicting. In contrast, networks with saturating activation functions achieved comparable task performance but did not show compression. In this paper, we developed more robust mutual information estimation techniques, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions. In addition, we explored compression in networks with a range of activation functions. We show that saturation of the activation function is not required",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Supervised learning depends on annotated examples, which are taken to be the ground truth. But these labels often come from noisy crowdsourcing platforms, like Amazon Mechanical Turk. Practitioners typically collect multiple labels per example and aggregate the results to mitigate noise (the classic crowdsourcing problem). Given a fixed annotation budget and unlimited unlabeled data, redundant annotation comes at the expense of fewer labeled examples. This raises two fundamental questions: (1) How can we best learn from noisy workers? (2) How should we allocate our labeling budget to maximize the performance of a classifier? We propose a new algorithm for jointly modeling labels and worker quality from noisy crowd-sourced data. The alternating minimization proceeds in rounds, estimating worker quality from disagreement with the current model and then updating the model by optimizing a loss function that accounts for the current estimate of worker quality. Unlike previous approaches, even with only one annotation per example, our algorithm can estimate worker quality. We establish a generalization error bound for models learned with our algorithm and establish theoretically that it's better to label many examples once (vs less multiply) when worker quality exceeds a threshold. Experiments conducted on both ImageNet (with simulated noisy workers) and MS-COCO (using the real crowdsourced labels) confirm our algorithm's benefits. Recent advances in supervised learning owe, in part, to the availability of large annotated datasets. For instance, the performance of modern image classifiers saturates only with millions of labeled examples. This poses an economic problem: Assembling such datasets typically requires the labor of human annotators. If we confined the labor pool to experts, this work might be prohibitively expensive. Therefore, most practitioners turn to crowdsourcing platforms such as Amazon Mechanical Turk (AMT), which connect employers with low-skilled workers who perform simple tasks, such as classifying images, at low cost.Compared to experts, crowd-workers provide noisier annotations, possibly owing to high variation in worker skill; and a per-answer compensation structure that encourages rapid answers, even at the expense of accuracy. To address variation in worker skill, practitioners typically collect multiple independent labels for each training example from different workers. In practice, these labels are often aggregated by applying a simple majority vote. Academics have proposed many efficient algorithms for estimating the ground truth from noisy annotations. Research addressing the crowd-sourcing problem goes back to the early 1970s. BID4 proposed a probabilistic model to jointly estimate worker skills and ground truth labels and used expectation maximization (EM) to estimate the parameters. BID27 ; ; BID29 proposed generalizations of the Dawid-Skene model, e.g. by estimating the difficulty of each example.Although the downstream goal of many crowdsourcing projects is to train supervised learning models, research in the two disciplines tends to proceed in isolation. Crowdsourcing research seldom accounts for the downstream utility of the produced annotations as training data in machine learning (ML) algorithms. And ML research seldom exploits the noisy labels collected from multiple human workers. A few recent papers use the original noisy labels and the corresponding worker identities together with the predictions of a supervised learning model trained on those same labels, to estimate the ground truth BID2 BID7 . However, these papers do not realize the full potential of combining modeling and crowd-sourcing. In particular, they are unable to estimate worker qualities when there is only one label per training example.This paper presents a new supervised learning algorithm that alternately models the labels and worker quality. The EM algorithm bootstraps itself in the following way: Given a trained model, the algorithm estimates worker qualities using the disagreement between workers and the current predictions of the learning algorithm. Given estimated worker qualities, our algorithm optimizes a suitably modified loss function. We show that accurate estimates of worker quality can be obtained even when only collecting one label per example provided that each worker labels sufficiently many examples. An accurate estimate of the worker qualities leads to learning a better model. This addresses a shortcoming of the prior work and overcomes a significant hurdle to achieving practical crowdsourcing without redundancy.We give theoretical guarantees on the performance of our algorithm. We analyze the two alternating steps: (a) estimating worker qualities from disagreement with the model, (b) learning a model by optimizing the modified loss function. We obtain a bound on the accuracy of the estimated worker qualities and the generalization error of the model. Through the generalization error bound, we establish that it is better to label many examples once than to label less examples multiply when worker quality is above a threshold. Empirically, we verify our approach on several multi-class classification datasets: ImageNet and CIFAR10 (with simulated noisy workers), and MS-COCO (using the real noisy annotator labels). Our experiments validate that when the cost of obtaining unlabeled examples is negligible and the total annotation budget is fixed, it is best to collect a single label per training example for as many examples as possible. We emphasize that although this paper applies our approach to classification problems, the main ideas of the algorithm can be extended to other tasks in supervised learning. We introduced a new algorithm for learning from noisy crowd workers. We also presented a new theoretical and empirical demonstration of the insight that when examples are cheap and annotations expensive, it's better to label many examples once than to label few multiply when worker quality is above a threshold. Many avenues seem ripe for future work. We are especially keen to incorporate our approach into active query schemes, choosing not only which examples to annotate, but which annotator to route them to based on our models current knowledge of both the data and the worker confusion matrices. Lemma A.2. Under the assumptions of Theorem 4.1, ∞ error in estimated confusion matrices π as computed in Equation (7), using n samples and a predictor function f with risk R ,D ≤ δ, is bounded by DISPLAYFORM0 with probability at least 1 − δ 1 .First we apply Lemma A.1 with P π computed using majority vote. We get a bound on the risk of function f computed in the first round. With this f , we apply Lemma A.2. When n is sufficiently large such that Equation (8) holds, the denominator in Equation FORMULA18 , 1/K − δ − 8 m log(4mK 2 /δ 1 )/(nr) ≥ 1/8. Therefore , in the first round, the error in confusion matrix estimation is bounded by , which is defined in the Theorem.For the second round: we apply Lemma A.1 with P π computed as the posterior distribution (5).Where ∞ error in π is bounded by . This gives the desired bound in (9). With this f , we apply Lemma A.2 and obtain ∞ error in π bounded by 1 , which is defined in the Theorem.For the given probability of error δ in the Theorem, we chose δ 1 in both the lemma to be δ/4 such that with union bound we get the desired probability of δ. DISPLAYFORM1 For ease of notation, we denote D W,π,r by D π . Similar to R ,D , risk of decision function f with respect to the modified loss function π is characterized by the following quantities: DISPLAYFORM2 2. Empirical π -risk on samples: R π ,Dπ (f ) : DISPLAYFORM3 i , wi ). With the above definitions, we have the following, DISPLAYFORM4 DISPLAYFORM5 where (19) follows from Equation FORMULA5 . FORMULA5 follows from the fact that f is the minimizer of R π ,Dπ as computed in FORMULA12 . FORMULA5 follows from the basic excess-risk bound. V is the VC dimension of hypothesis class F, and C is a universal constant.Following shows the inequality used in Equation (19). For binary classification , we denote the two classes by Y, −Y . DISPLAYFORM6 DISPLAYFORM7 where FORMULA5 follows from Equation FORMULA5 . FORMULA5 follows from the fact that for 0-1 loss function (f (X), Y ) + (f (X), −Y ) = 1. (24) follows from the definition of β π defined in Equation (12). When π is computed using weighted majority vote of the workers then (24) holds with β π replaced by α. α is defined in (14).Following shows the equality used in Equation FORMULA5 . Using the notations ρ π and τ π , in the following, for any function f ∈ F, we compute the excess risk due to the unbiasedness of the modified loss function π .",A new approach for learning a model from noisy crowdsourced annotations.,Theorem ; EM ; MS-COCO ; one ; VC ; F ; noisy annotations ; second ; ML ; P π,weighted majority vote ; This paper ; the VC dimension ; crowdsourcing platforms ; Dπ ; union ; worker qualities ; a better model ; high variation ; many crowdsourcing projects,Theorem ; EM ; MS-COCO ; one ; VC ; F ; noisy annotations ; second ; ML ; P π,"Supervised learning depends on annotated examples, which are taken to be the ground truth. Practitioners typically collect multiple labels per example and aggregate the results to mitigate noise. Given a fixed annotation budget and unlimited unlabeled data, redundant annotation comes at the expense of fewer labeled examples. This raises two fundamental questions: (1) How can we best learn from noisy workers and (2) How should we allocate our labeling budget to maximize the performance of a classifier? BID4 proposed a probabilistic model for jointly modeling labels and worker quality from noisy crowd-sourced data. The alternating minimization proceeds in rounds",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Machine learning algorithms designed to characterize, monitor, and intervene on human health (ML4H) are expected to perform safely and reliably when operating at scale, potentially outside strict human supervision. This requirement warrants a stricter attention to issues of reproducibility than other fields of machine learning. In this work, we conduct a systematic evaluation of over 100 recently published ML4H research papers along several dimensions related to reproducibility we identified. We find that the field of ML4H compares poorly to more established machine learning fields, particularly concerning data accessibility and code accessibility.   Finally, drawing from success in other fields of science, we propose recommendations to data providers, academic publishers, and the ML4H research community in order to promote reproducible research moving forward. Science requires reproducibility, but many sub-fields of science have recently experienced a reproducibility crisis, eroding trust in processes and results and potentially influencing the rising rates of scientific retractions [1, BID4 BID43 . Reproducibility is also critical for machine learning research, whose goal is to develop algorithms to reliably solve complex tasks at scale, with limited or no human supervision. Failure of a machine learning system to consistently replicate an intended behavior in a context different from which that behavior was defined may result in dramatic, even fatal, consequences BID26 . Ranking prominently among machine learning applications that may put human lives at stake are those related to Machine Learning for Health (ML4H). In a field where applications are meant to directly affect human health, findings should undergo heavy scrutiny along the validation pipeline from research findings to applications deployed in the wild. For example, in 2018, 12 AI tools using ML4H algorithms to inform medical diagnosis and treatment were cleared by Food and Drug Administration (FDA) and will be marketed to and potentially used by millions of Americans BID30 . Verifying the reproducibility of the claims put forward by the device manufacturer should thus be a main priority of regulatory bodies BID35 , extending the need for reproducible ML4H results beyond the machine learning research community.Unfortunately, several factors relating to the availability, quality, and consistency of clinical or biomedical data make reproducibility especially challenging in ML4H applications. In this work, * Equal Contribution we make several contributions. First, we present a taxonomy of reproducibility tailored to ML4H applications, and designed to capture reproducibility goals more broadly. Second, we use this taxonomy to define several metrics geared towards quantifying the particular challenges in reproducibility faced within ML4H, and conduct a comprehensive review of the published literature to support our claims and compare ML4H to machine learning more generally. Finally, we build on this analysis by exploring promising areas of further research for reproducibility in ML4H. In this work, we have framed the question of reproducibility in ML4H around three foundational lenses: technical, statistical, and conceptual replicability. In each of these areas, we argue both qualitatively and quantitatively, through a manual, extensive review of the literature, that ML4H performs worse than other machine learning fields in several reproducibility metrics we have identified. While keeping in mind the intrinsic challenges of data acquisition and use that plague the field, we highlight several areas of opportunities for the future, focused around improving access to data, expanding our trajectory of statistical rigor, and increasing the use of multi-source data to better enable conceptual reproducibility. * indicates all publiclyaccessible papers published were used.Potential Biases This selection and annotation procedure allowed us to analyze a large number of papers, but has several possible biases. In particular, our annotation questions were all of these were designed to be determinable via quick, scanning techniques and as a result this task took on average between 45 seconds and 3 minutes per paper. In such a limited time, some losses are unavoidable. We recognize several sources of possible bias worth mentioning.Firstly, some papers may, for example, release datasets or code products external to the paper and not mention it in the actual text. We will omit these associated products. If such effects induce a notable bias in our results, however, we must question why as a field we are comfortable releasing our code/data without any mention in the associated paper.Secondly, not all papers intended to be analyzed were publicly accessible. Similarly, the versions of papers we analyzed could have been different from the version presented at the actual conference venue, or there could exist updated versions of papers we analyzed in different repositories. Our analysis technique will miss these effects.Thirdly, some papers naturally fit into multiple categories (e.g., a work focused on medical named entity recognition would be both a ML4H work and an NLP work). In the interest of ensuring our comparison classes were as pure as possible, we omitted all clearly multi-domain works, but allowed works that centered primarily in a single domain to remain.Lastly, different fields present different kinds of works, and not all works fit into our framework. Largely theoretical works, for example, often have no real datasets or public experiments. Similarly, presenting variance is a different question for works focused principally around computational efficiency rather than predictive accuracy. We handled these issues by attempting to answer these questions as best we could, and flagging any papers that overtly did not fit our scheme and excluding them from our analyses.","By analyzing more than 300 papers in recent machine learning conferences, we found that Machine Learning for Health (ML4H) applications lag behind other machine learning fields in terms of reproducibility metrics.",Secondly ; Firstly ; FDA ; Equal Contribution ; millions ; Americans ; three ; Second ; First ; Food and Drug Administration,medical named entity recognition ; a notable bias ; processes ; works ; human lives ; example ; several sources ; Potential Biases This selection and annotation procedure ; some losses ; whose goal,Secondly ; Firstly ; FDA ; Equal Contribution ; millions ; Americans ; three ; Second ; First ; Food and Drug Administration,"Machine learning algorithms designed to characterize, monitor, and intervene on human health (ML4H) are expected to perform safely and reliably when operating at scale, potentially outside strict human supervision. However, the field of ML4H performs poorly compared to more established machine learning fields, particularly in data accessibility and code accessibility. In order to promote reproducibility, we propose recommendations to data providers, academic publishers, and the ML4 H research community, focusing on data accessibility, quality, and consistency of clinical or biomedical data.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"We address the problem of learning to discover 3D parts for objects in unseen categories. Being able to learn the geometry prior of parts and transfer this prior to unseen categories pose fundamental challenges on data-driven shape segmentation approaches. Formulated as a contextual bandit problem, we propose a learning-based iterative grouping framework which learns a grouping policy to progressively merge small part proposals into bigger ones in a bottom-up fashion. At the core of our approach is to restrict the local context for extracting part-level features, which encourages the generalizability to novel categories. On a recently proposed large-scale fine-grained 3D part dataset, PartNet, we demonstrate that our method can transfer knowledge of parts learned from 3 training categories to 21 unseen testing categories without seeing any annotated samples. Quantitative comparisons against four strong shape segmentation baselines show that we achieve the state-of-the-art performance. Perceptual grouping has been a long-standing problem in the study of vision systems (Hoffman & Richards, 1984) . The process of perceptual grouping determines which regions of the visual input belong together as parts of higher-order perceptual units. Back to the 1930s, Wertheimer (1938) listed several vital factors, such as similarity, proximity, and good continuation, which lead to visual grouping. To this era of deep learning, grouping cues can be learned from massive annotated datasets. However, compared with human visual system, these learning-based segmentation algorithms are far inferior for objects from unknown categories. We are interested in attacking a specific problem of this kind -zero-shot part discovery for 3D shapes. We choose to study the zero-shot learning problem on 3D shape data instead of 2D image data, because part-level similarity across object categories in 3D is more salient and less affected by various distortions introduced in the imaging process. Work done while Tiange Luo, Kaichun Mo, Jiarui Xu, and Siyu Hu were visiting UC San Diego. To motive our approach, we first review the key idea and limitation of existing 3D part segmentation methods. With the power of big data, deep neural networks that learn data-driven features to segment shape parts, such as (Kalogerakis et al., 2010; Graham et al., 2018; Mo et al., 2019c) , have demonstrated the state-of-the-art performance on many shape segmentation benchmarks (Yi et al., 2016; Mo et al., 2019c) . These networks usually have large receptive fields that cover the whole input shape, so that global context can be leveraged to improve the recognition of part semantics and shape structures. While learning such features leads to superior performance on the training categories, they often fail miserably on unseen categories (Figure 1 ) due to the difference of global shapes. On the contrary, classical shape segmentation methods, such as (Kaick et al., 2014 ) that use manually designed features with relatively local context, can often perform much better on unseen object categories, although they tend to give inferior segmentation results on training categories. In fact, many globally different shapes share similar part-level structures. For example, airplanes, cars, and swivel chairs all have wheels, even though their global geometries are totally different. Having learned the geometry of wheels from airplanes should help recognize wheels for cars and swivel chairs. In this paper, we aim to invent a learning-based framework that will by design avoid using excessive context information that hurts cross-category generalization. We start from learning to propose a pool of superpixel-like sub-parts for each shape. Then, we learn a grouping policy that seeks to progressively group sub-parts and increase recognition context. What lies in the heart of our algorithm is to learn a function to assess whether two parts should be grouped. Different from prior deep segmentation work that learns point features for segmentation mask prediction, our formulation essentially learns part-level features. Borrowing ideas from Reinforcement Learning (RL), we formalize the process as a contextual bandit problem and train a local grouping policy to iteratively pick a pair of most promising sub-parts for grouping. In this way, we restrict that our features only convey information within the local context of a part. Our learning-based agglomerative clustering framework deviates drastically from the prevailing deep segmentation pipelines and makes one step towards generalizable part discovery in unseen object categories. To summarize, we make the following contributions: • We formulate the task of zero-shot part discovery on a large-scale fine-grained shape segmentation benchmark PartNet (Mo et al., 2019c ); • We propose a learning-based agglomerative clustering framework that learns to do part proposals and grouping from training categories and generalizes to unseen novel categories; • We quantitatively compare our approach to several baseline methods and demonstrate the state-of-the-art results for part discovery in unseen object categories. In this paper, we introduced a data-driven iterative perceptual grouping pipeline for the task of zero-shot 3D shape part discovery. At the core of our method is to learn part-level features within part local contexts, in order to generalize the part discovery process to unseen novel categories. We conducted extensive evaluation and analysis of our method and presented thorough quantitative comparisons to four state-of-the-art shape segmentation algorithms. We demonstrated that our method successfully extracts locally-aware part knowledge from training categories and transfers the knowledge to unseen novel categories. Our method achieved the best performance over all four baseline methods on the PartNet dataset. A SUB-PART PROPOSAL MODULE Given a shape represented as a point cloud, we first propose a pool of small superpixel-like (Ren & Malik, 2003) sub-parts as the building blocks. We employ furthest point sampling to sample 128 seed points on each input shape. To capture the local part context, we extract PointNet (Qi et al., 2017a) features with 64 points sampling within a local 0.04-radius 2 neighborhood around each seed point. In the training phase, all the 64 points will be sampled from the same instance. Then, we train a local PointNet segmentation network that takes as inputs 512 points within a 0.2-radius ball around every seed point and output a binary segmentation mask indicating a sub-part proposal. If the point belongs to the instance is the same as the 0.04-radius ball, it will be classified into 1. We call this module as the sub-part proposal module and illustrate it in Figure 4 . In the inference phase, we can not guarantee the 64 points sampled within a 0.04-radius ball are all coming from the same part. However, in our experiments, we observe those sub-part proposals will have a low purity score due to the poor center feature extracted from the 64 points across different parts. Also, even the center feature extraction is good, some sub-parts may also cover multiple parts in ground-truth. To obtain high-quality sub-parts, we remove the sub-parts whose purity score lower than 0.8, and the remain sub-parts form our initial sub-part pool. The input of this learning module is constrained in a local region, thus will not be affected by the global context. To validate the transferring performance of this module, we train the module on Chair, Storage Furniture, and Lamp of level-3 annotations and test on all categories with evaluating by the most fine-grained level annotations of each category. The results are listed in Table 3 . Since the part patterns in Table 3 : Quantitative evaluation of the sub-part proposal module. PosAcc and NegAcc refer to positive accuracy and negative accuracy of the binary segmentation.",A zero-shot segmentation framework for 3D object part segmentation. Model the segmentation as a decision-making process and solve as a contextual bandit problem.,one ; two ; Tiange Luo ; Lamp ; Jiarui Xu ; Siyu Hu ; Kaick ; Mo ; Yi et al. ; PosAcc,segmentation mask prediction ; bigger ones ; vision systems ; similar part-level structures ; good continuation ; those sub-part proposals ; multiple parts ; the following contributions ; the transferring performance ; et al,one ; two ; Tiange Luo ; Lamp ; Jiarui Xu ; Siyu Hu ; Kaick ; Mo ; Yi et al. ; PosAcc,"Learning to discover 3D parts for objects in unseen categories poses fundamental challenges on data-driven shape segmentation approaches. The learning-based iterative grouping framework, which learns a grouping policy to progressively merge small part proposals into bigger ones in a bottom-up fashion, aims to restrict the local context for extracting part-level features, which encourages the generalizability of novel categories. In a large-scale fine-grained 3D part dataset, PartNet, we demonstrate that our method can transfer knowledge of parts learned from 3 training categories to 21 unseen testing categories without seeing any annotated samples. Quantitative comparisons against four strong",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"We analyze the trade-off between quantization noise and clipping distortion in low precision networks. We identify the statistics of various tensors, and derive exact expressions for the mean-square-error degradation due to clipping. By optimizing these expressions, we show marked improvements over standard quantization schemes that normally avoid clipping. For example, just by choosing the accurate clipping values, more than 40\% accuracy improvement is obtained for the quantization of VGG-16 to 4-bits of precision. Our results have many applications for the quantization of neural networks at both training and inference time. 
 A significant drawback of deep learning models is their computational costs. Low precision is one of the key techniques being actively studied recently to conquer the problem. With hardware support, low precision training and inference can compute more operations per second, reduce memory bandwidth and power consumption, and allow bigger network to fit into a device.In general, a low-precision scheme involves a floating-point to integer conversion, which introduces quantization noise into the network. This quantization noise is strongly linked to the dynamic range, defined as the range between the largest and smallest values that need to quantized. For a given N -bit integer representation, a smaller dynamic range leads to a smaller spacing between the 2 N quantization levels, enabling improved resolution and smaller quantization noise. To reduce this quantization noise, the dynamic range can be limited by clipping the values in the tensor. This clipping process introduces an additional noise because of the loss of information that otherwise would be carried by the clipped portion of the tensor. Hence, a trade-off between clipping and quantization effects exist. To find the best clipping value we need to minimize the information loss.In this paper, we study the effect of clipping with the aim of improving overall quantization noise. To this end, we first study the distribution of values within these tensors. In all our measurements, the statistical distributions of weights and activations are observed to follow a bell-curve. This indicates that large values occur very rarely compared to small values, and suggests that the loss of information due to the clipping process might be compensated by improving the resolution of the more common smaller values.To optimize this process further, it is essential to understand the underlying distribution of tensor elements before applying the clipping. By running a few statistical tests, we were able to see on a variety of convolution models that activation tensors follow either a Gaussian or Laplacian distributions with a high degree of certainty (p-value < 0.01). This modeling of activation tensors enables a clear formulation of the quantization process and constitutes the first step for its optimization.We turn to consider the objective we aim to optimize. It is well known that when batch norm is applied after a convolution layer, the output is invariant to the norm of the output on the proceeding layer BID4 ] i.e., BN (C · W · x) = BN (W · x) for any given constant C. This quantity is often described geometrically as the norm of the activation tensor, and in the presence of this invariance, the only measure that needs to be preserved upon quantization is the directionality of the tensor. Therefore, quantization preserves tensor information if the angle between the highprecision tensor and its quantized version is small. Recently, BID0 has shown that this angle depends only on the quantization error power (L2 -norm) and the power of original tensor. Therefore, minimizing the power of the quantization error constitutes a plausible goal for the optimization of the quantized network in terms of accuracy.In Section 4, we provide a rigorous formulation to optimize the quantization effect of activation tensors using clipping by analyzing both the Gaussian and the Laplace priors. This formulation is henceforth refered to as Analytical Clipping for Integer Quantization (ACIQ).These analytical results have many applications for the quantization of neural networks at both training and inference time. For example , a straightforward quantization of the weights and activations to 8-bit fixed point representation has been shown to have a negligible effect on the model accuracy. Yet, in the majority of the applications, further reduction of precision quickly degrades performance, calling for an optimal clipping scheme to minimize information-loss during quantization. On a more general level, exploiting the statistics of the activation tensors to minimize their quantization toll is orthogonal to other techniques for network quantization. It can work in synergy with other schemes to achieve more than could have been achieved by each individually. Finally, it is easy to implement and requires only the adjustment of clipping value according to an analytical formula.We further demonstrate the applicability and usability of our analytic terms on the following challenging problem. Given a pre-trained network , we would like to quantize the weights to 8-bit of precision and most activations to 4-bits of precision without any further processing (e.g., re-training). This specific setting is of a particular interest due to quantization of activations to 4-bits, which alleviates a major bottleneck in terms of memory bandwidth. Prior attempts using standard techniques BID8 show severe degradation on accuracy. While several recent works were able to overcome this issue by additional re-training BID12 , this is not feasible in many practical settings, e.g., we often do not have the dataset on which the network is working on.We compare ACIQ against two methods: (i) the traditional method that avoids clipping (also known by gemmlowp BID6 ), where values are uniformly quantized between the largest and smallest tensor values; (ii) the iterative method suggested by NVIDIA to search for a good clipping threshold based on the Kullback-Leibler Divergence (KLD) measure BID13 . Results are summarized in TAB1 . While both ACIQ and gemmlowp are fast non-iterative methods, ACIQ significantly outperforms in terms of validation accuracy. On the hand, KLD is an exhaustive timeconsuming procedure, which iteratively evaluates the KLD measure on a large candidate set of clipping values, and then returns the clipping value for which best evaluation is attained. In our simulations ACIQ and gemmlowp require a single pass over tensor values, while KLD requires 4000 passes. Nonetheless, excluding ResNet-101, ACIQ outperforms KLD in terms of validation accuracy.The methods introduced in this work may be additionally useful to current and future applications, such as the attempts to fully train in a low precision setting BID0 We introduce ACIQ -an optimized clipping framework for improved quantization of neural networks. Optimized clipping is shown to have a drastic impact on quantization in a variety of models. The underlying reason lies in the statistical dispersion of activations, where large values occur very rarely. We show the bell-curve statistics of activations are best fit as either Laplace or Gaussian distributions, and formulate the clipping process as an optimization problem. The solution to this optimization problem constitutes a polynomial-exponential equation that can be calculated numerically for a variety of statistical parameters, and stored in a lookup table for fast retrieval. This scheme is very simple and easy to implement either in software or in hardware.While results are very encouraging, this work is only the first step on the ladder for successful deployment of clipping in neural networks. First, our main focus in this work is quantization of activations, while similar evaluation still needs to be done for weights. On a more general level, our framework is not restricted to the inference settings and can be extended to training. For example, our preliminary results show that quantization of gradients might benefit from the clipping of small values (i.e., sparsification). Establishing the correct threshold for gradients is yet another important direction for future work. While much work still needs to be done with regards to optimized clipping, we believe our work clearly demonstrates the major importance of this concept for the quantization of neural networks. A PIECE-WISE LINEAR APPROXIMATIONHere we provide a more accurate analysis related to the qunatization noise (i.e., the second term in Equation 3), measured as the expected mean-square-error when the range [−α, α] is quantized uniformly to 2 M discrete levels. To that end, we approximate the density function f by a construction of a piece-wise linear function g such that f (q i ) = g(q i ) for each i ∈ [0, 2 M − 1]. Since we consider only smooth probability density functions (e.g., Gaussian or Laplace), the resulting approximation error is small for sufficient resolution i.e., small quantization step size ∆. In figure 1 we provide an illustration for this construction.We turn to calculate the linear equation for each line segment of the piece-wise linear function g, falling in the range [−α + i · ∆, −α + (i + 1) · ∆]. To that end, we consider the slope (derivative) and the value of the density function at the midpoint q i . With these two values we can define for each segment i ∈ [0, 2 M − 1] the corresponding form of linear approximation: DISPLAYFORM0 We now turn to calculate the second term in Equation 3. By equation 14, and since q i is defined to be the midpoint between the integration limits, the following holds true","We analyze the trade-off between quantization noise and clipping distortion in low precision networks, and show marked improvements over standard quantization schemes that normally avoid clipping",the Kullback-Leibler Divergence ; W ; Laplace ; ACIQ ; KLD ; Laplacian ; Analytical Clipping ; linear ; ACIQ).These ; first,network quantization ; much work ; the statistical distributions ; an illustration ; this process ; future work ; accuracy ; BN ; the linear equation ; [−α,the Kullback-Leibler Divergence ; W ; Laplace ; ACIQ ; KLD ; Laplacian ; Analytical Clipping ; linear ; ACIQ).These ; first,"We analyze the trade-off between quantization noise and clipping distortion in low precision networks. By optimizing expressions for the mean-square-error degradation due to clipping, we show marked improvements over standard quantization schemes that normally avoid clipping. For example, more than 40\% accuracy improvement is obtained for the quantization of VGG-16 to 4-bits of precision. Low precision training and inference can reduce memory bandwidth and power consumption, and allow bigger network to fit into a device. A significant drawback of deep learning models is their computational costs. With hardware support, low precision training, inference can compute more operations per second,",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"We study the problem of building models that disentangle independent factors of variation. Such models encode features that can efficiently be used for classification and to transfer attributes between different images in image synthesis. As data we use a weakly labeled training set, where labels indicate what single factor has changed between two data samples, although the relative value of the change is unknown. This labeling is of particular interest as it may be readily available without annotation costs. We introduce an autoencoder model and train it through constraints on image pairs and triplets. We show the role of feature dimensionality and adversarial training theoretically and experimentally. We formally prove the existence of the reference ambiguity, which is inherently present in the disentangling task when weakly labeled data is used. The numerical value of a factor has different meaning in different reference frames. When the reference depends on other factors, transferring that factor becomes ambiguous. We demonstrate experimentally that the proposed model can successfully transfer attributes on several datasets, but show also cases when the reference ambiguity occurs.
 One way to simplify the problem of classifying or regressing attributes of interest from data is to build an intermediate representation, a feature, where the information about the attributes is better separated than in the input data. Better separation means that some entries of the feature vary only with respect to one and only one attribute. In this way, classifiers and regressors would not need to build invariance to many nuisance attributes. Instead, they could devote more capacity to discriminating the attributes of interest, and possibly achieve better performance. We call this task disentangling factors of variation, and we identify attributes with the factors. In addition to facilitating classification and regression, this task is beneficial to image synthesis. One could build a model to render images, where each input varies only one attribute of the output, and to transfer attributes between images.When labeling is possible and available, supervised learning can be used to solve this task. In general, however, some attributes may not be easily quantifiable (e.g., style). Therefore, we consider using weak labeling, where we only know what attribute has changed between two images, although we do not know by how much. This type of labeling may be readily available in many cases without manual annotation. For example, image pairs from a stereo system are automatically labeled with a viewpoint change, albeit unknown. A practical model that can learn from these labels is an encoder-decoder pair subject to a reconstruction constraint. In this model the weak labels can be used to define similarities between subsets of the feature obtained from two input images.We introduce a novel adversarial training of autoencoders to solve the disentangling task when only weak labels are available. Compared to previous methods, our discriminator is not conditioned on class labels, but takes image pairs as inputs. This way the number of parameters can be kept constant.We describe the shortcut problem, where all the the information is encoded only in one part of the feature, while other part is completely ignored, as FIG0 illustrates. We prove our method solves this problem and demonstrate it experimentally.We formally prove existence of the reference ambiguity, that is inherently present in the disentangling task when weak labels are used. Thus no algorithm can provably learn disentangling. As FIG0 shows, the reference ambiguity means that a factor (for example viewpoint) can have different meaning when using a different reference frame that depends on another factor (for example car type). We show experimentally that this ambiguity rarely arise, we can observe it only when the data is complex. In this paper we studied the challenges of disentangling factors of variation, mainly the shortcut problem and the reference ambiguity. The shortcut problem occurs when all information is stored in only one feature chunk, while the other is ignored. The reference ambiguity means that the reference in which a factor is interpreted, may depend on other factors. This makes the attribute transfer ambiguous. We introduced a novel training of autoencoders to solve disentangling using image triplets. We showed theoretically and experimentally how to keep the shortcut problem under control through adversarial training, and enable to use large feature dimensions. We proved that the reference ambiguity is inherently present in the disentangling task when weak labels are used. Most importantly this can be stated independently of the learning algorithm. We demonstrated that training and transfer of factors of variation may not be guaranteed. However, in practice we observe that our trained model works well on many datasets and exhibits good generalization capabilities.","It is a mostly theoretical paper that describes the challenges in disentangling factors of variation, using autoencoders and GAN.",between two ; One ; only one ; two,This type ; only one feature chunk ; the reference ; many datasets ; One way ; all information ; addition ; this way ; good generalization capabilities ; large feature dimensions,between two ; One ; only one ; two,"The problem of disentangling independent factors of variation is solved by building models that disentangle features that can efficiently be used for classification and transfer attributes between different images in image synthesis. A weakly labeled training set, where labels indicate what single factor has changed between two data samples, although the relative value of the change is unknown. An autoencoder model is introduced and trained through constraints on image pairs and triplets, and the role of feature dimensionality and adversarial training in the disentangled task. The reference ambiguity is inherently present in the task, as the numerical value of a factor has different meanings in different",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Adaptive gradient algorithms perform gradient-based updates using the history of gradients and are ubiquitous in training deep neural networks. While adaptive gradient methods theory is well understood for minimization problems, the underlying factors driving their empirical success in min-max problems such as GANs remain unclear. In this paper, we aim at bridging  this gap from both theoretical and empirical perspectives. First, we analyze a variant of Optimistic Stochastic Gradient (OSG) proposed in~\citep{daskalakis2017training} for solving a class of non-convex non-concave min-max problem and establish $O(\epsilon^{-4})$ complexity for finding $\epsilon$-first-order stationary point, in which the algorithm only requires invoking one stochastic first-order oracle while enjoying state-of-the-art iteration complexity achieved by stochastic extragradient method by~\citep{iusem2017extragradient}. Then we propose an adaptive variant of OSG named Optimistic Adagrad (OAdagrad) and reveal an \emph{improved} adaptive complexity $\widetilde{O}\left(\epsilon^{-\frac{2}{1-\alpha}}\right)$~\footnote{Here $\widetilde{O}(\cdot)$ compresses a logarithmic factor of $\epsilon$. }, where $\alpha$ characterizes the growth rate of the cumulative stochastic gradient and $0\leq \alpha\leq 1/2$. To the best of our knowledge, this is the first work for establishing adaptive complexity in non-convex non-concave min-max optimization. Empirically, our experiments show that indeed adaptive gradient algorithms outperform their non-adaptive counterparts in GAN training. Moreover, this observation can be explained by the slow growth rate of the cumulative stochastic gradient, as observed empirically. Adaptive gradient algorithms (Duchi et al., 2011; Tieleman & Hinton, 2012; Kingma & Ba, 2014; Reddi et al., 2019) are very popular in training deep neural networks due to their computational efficiency and minimal need for hyper-parameter tuning (Kingma & Ba, 2014) . For example, Adagrad (Duchi et al., 2011) automatically adjusts the learning rate for each dimension of the model parameter according to the information of history gradients, while its computational cost is almost the same as Stochastic Gradient Descent (SGD). However, in supervised deep learning (for example, image classification tasks using a deep convolutional neural network), there is not enough evidence showing that adaptive gradient methods converge faster than its non-adaptive counterpart (i.e., SGD) on benchmark datasets. For example, it is argued in (Wilson et al., 2017 ) that adaptive gradient methods often find a solution with worse performance than SGD. Specifically, Wilson et al. (2017) observed that Adagrad has slower convergence than SGD in terms of both training and testing error, while using VGG (Simonyan & Zisserman, 2014) on CIFAR10 data. GANs (Goodfellow et al., 2014) are a popular class of generative models. In a nutshell, they consist of a generator and a discriminator, both of which are defined by deep neural networks. The generator and the discriminator are trained under an adversarial cost, corresponding to a non-convex non-concave min-max problem. GANs are known to be notoriously difficult to train. In practice, Adam (Kingma & Ba, 2014 ) is the defacto optimizer used for GAN training. The common optimization strategy is to alternatively update the discriminator and the generator (Arjovsky et al., 2017; Gulrajani et al., 2017) . Using Adam is important in GAN training, since replacing it with non-adaptive methods (e.g. SGD) would significantly deteriorate the performance. This paper studies and attempts to answer the following question: In this paper, we explain the effectiveness of adaptive gradient methods in training GANs from both theoretical and empirical perspectives. Theoretically, we provide two efficient stochastic algorithms for solving a class of min-max non-convex non-concave problems with state-of-the-art computational complexities. We also establish adaptive complexity results for an Adagrad-style algorithm by using coordinate-wise stepsize according to the geometry of the history data. The algorithm is proven to enjoy faster adaptive convergence than its non-adaptive counterpart when the gradient is sparse, which is similar to Adagrad applied to convex minimization problem. We have conducted extensive empirical studies to verify our theoretical findings. In addition, our experimental results suggest that the reason why adaptive gradient methods deliver good practical performance for GAN training is due to the slow growth rate of the cumulative stochastic gradient.","This paper provides novel analysis of adaptive gradient algorithms for solving non-convex non-concave min-max problems as GANs, and explains the reason why adaptive gradient methods outperform its non-adaptive counterparts by empirical studies.",Arjovsky ; Goodfellow et al. ; OAdagrad ; VGG (Simonyan & Zisserman ; \alpha$ ; GAN ; Wilson ; SGD ; Adam ; Kingma & Ba,its computational cost ; the following question ; a class ; extensive empirical studies ; one stochastic first-order oracle ; attempts ; Ba ; adaptive complexity ; our theoretical findings ; an Adagrad-style algorithm,Arjovsky ; Goodfellow et al. ; OAdagrad ; VGG (Simonyan & Zisserman ; \alpha$ ; GAN ; Wilson ; SGD ; Adam ; Kingma & Ba,"Adaptive gradient algorithms perform gradient-based updates using the history of gradients and are ubiquitous in training deep neural networks. However, the underlying factors driving their empirical success in min-max problems such as GANs remain unclear. In this paper, we explore a variant of Optimistic Stochastic Gradient (OSG) proposed in~\citep{daskalakis2017training} for solving a class of non-convex Non-concave min-min-max problem and establish $O(\epsilon^{-4})$ complexity for finding $\epsilons$-first-order",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Neural networks trained only to optimize for training accuracy can often be fooled by adversarial examples --- slightly perturbed inputs misclassified with high confidence. Verification of networks enables us to gauge their vulnerability to such adversarial examples. We formulate verification of piecewise-linear neural networks as a mixed integer program. On a representative task of finding minimum adversarial distortions, our verifier is two to three orders of magnitude quicker than the state-of-the-art. We achieve this computational speedup via tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available. The computational speedup allows us to verify properties on convolutional and residual networks with over 100,000 ReLUs --- several orders of magnitude more than networks previously verified by any complete verifier. In particular, we determine for the first time the exact adversarial accuracy of an MNIST classifier to perturbations with bounded l-∞ norm ε=0.1: for this classifier, we find an adversarial example for 4.38% of samples, and a certificate of robustness to norm-bounded perturbations for the remainder. Across all robust training procedures and network architectures considered, and for both the MNIST and CIFAR-10 datasets, we are able to certify more samples than the state-of-the-art and find more adversarial examples than a strong first-order attack. Neural networks trained only to optimize for training accuracy have been shown to be vulnerable to adversarial examples: perturbed inputs that are very similar to some regular input but for which the output is radically different BID14 . There is now a large body of work proposing defense methods to produce classifiers that are more robust to adversarial examples. However, as long as a defense is evaluated only via heuristic attacks (such as the Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2015) or BID6 's attack (CW)), we have no guarantee that the defense actually increases the robustness of the classifier produced. Defense methods thought to be successful when published have often later been found to be vulnerable to a new class of attacks. For instance, multiple defense methods are defeated in BID5 by constructing defense-specific loss functions and in BID0 by overcoming obfuscated gradients.Fortunately, we can evaluate robustness to adversarial examples in a principled fashion. One option is to determine (for each test input) the minimum distance to the closest adversarial example, which we call the minimum adversarial distortion BID7 . Alternatively, we can determine the adversarial test accuracy BID1 , which is the proportion of the test set for which no perturbation in some bounded class causes a misclassification. An increase in the mean minimum adversarial distortion or in the adversarial test accuracy indicates an improvement in robustness. 1 We present an efficient implementation of a mixed-integer linear programming (MILP) verifier for properties of piecewise-linear feed-forward neural networks. Our tight formulation for nonlinearities and our novel presolve algorithm combine to minimize the number of binary variables in the MILP problem and dramatically improve its numerical conditioning. Optimizations in our MILP implementation improve performance by several orders of magnitude when compared to a naïve MILP implementation, and we are two to three orders of magnitude faster than the state-of-the-art Satisfiability Modulo Theories (SMT) based verifier, Reluplex BID7 We make the following key contributions:• We demonstrate that, despite considering the full combinatorial nature of the network, our verifier can succeed at evaluating the robustness of larger neural networks, including those with convolutional and residual layers.• We identify why we can succeed on larger neural networks with hundreds of thousands of units. First , a large fraction of the ReLUs can be shown to be either always active or always inactive over the bounded input domain. Second , since the predicted label is determined by the unit in the final layer with the maximum activation, proving that a unit never has the maximum activation over all bounded perturbations eliminates it from consideration. We exploit both phenomena, reducing the overall number of non-linearities considered.• We determine for the first time the exact adversarial accuracy for MNIST classifiers to perturbations with bounded l ∞ norm . We are also able to certify more samples than the state-of-the-art and find more adversarial examples across MNIST and CIFAR-10 classifiers with different architectures trained with a variety of robust training procedures.Our code is available at https://github.com/vtjeng/MIPVerify.jl. This paper presents an efficient complete verifier for piecewise-linear neural networks.While we have focused on evaluating networks on the class of perturbations they are designed to be robust to, defining a class of perturbations that generates images perceptually similar to the original remains an important direction of research. Our verifier is able to handle new classes of perturbations (such as convolutions applied to the original image) as long as the set of perturbed images is a union of polytopes in the input space.We close with ideas on improving verification of neural networks. First, our improvements can be combined with other optimizations in solving MILPs. For example, BID4 DISPLAYFORM0 We consider two cases.Recall that a is the indicator variable a = 1 x≥0 .When a = 0, the constraints in Equation FORMULA0 This formulation for rectified linearities is sharp BID15 if we have no further information about x. This is the case since relaxing the integrality constraint on a leads to (x, y) being restricted to an area that is the convex hull of y = max(x, 0). However , if x is an affine expression x = w T z + b, the formulation is no longer sharp, and we can add more constraints using bounds we have on z to improve the problem formulation.","We efficiently verify the robustness of deep neural models with over 100,000 ReLUs, certifying more samples than the state-of-the-art and finding more adversarial examples than a strong first-order attack.",hundreds of thousands ; Goodfellow et al. ; the Fast Gradient Sign Method ; SMT ; Second ; One ; FGSM ; MILP ; first ; two,example ; images ; convolutional and residual networks ; rectified linearities ; more samples ; a defense ; different architectures ; the bounded input domain ; bounded l ∞ norm ; an MNIST classifier,hundreds of thousands ; Goodfellow et al. ; the Fast Gradient Sign Method ; SMT ; Second ; One ; FGSM ; MILP ; first ; two,"Neural networks trained only to optimize for training accuracy can often be fooled by adversarial examples. Verification of networks enables us to gauge their vulnerability to such adversarial distortions. Our verifier is two to three orders of magnitude faster than the state-of-the-art. We achieve this computational speedup via tight formulations for non-linearities and a novel presolve algorithm that makes full use of all information available. We verify properties on convolutional and residual networks with over 100,000 ReLUs, which are more robust than networks previously verified by any complete verifier. The accuracy of an MNIST classifier",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Variational inference (VI) is a popular approach for approximate Bayesian inference that is particularly promising for highly parameterized models such as deep neural networks.   A key challenge of variational inference is to approximate the posterior over model parameters with a distribution that is simpler and tractable yet sufficiently expressive. In this work, we propose a method for training highly flexible variational distributions by starting with a coarse approximation and iteratively refining it. Each refinement step makes cheap, local adjustments and only requires optimization of simple variational families. We demonstrate theoretically that our method always improves a bound on the approximation (the Evidence Lower BOund) and observe this empirically across a variety of benchmark tasks.   In experiments, our method consistently outperforms recent variational inference methods for deep learning in terms of log-likelihood and the ELBO.   We see that the gains are further amplified on larger scale models, significantly outperforming standard VI and deep ensembles on residual networks on CIFAR10. Uncertainty plays a crucial role in a multitude of machine learning applications, ranging from weather prediction to drug discovery. Poor predictive uncertainty risks potentially poor outcomes, especially in domains such as medical diagnosis or autonomous vehicles where some forms of high confidence errors may be especially costly (Amodei et al., 2016) . Thus, it is becoming increasingly important that the underlying model provides high quality uncertainty estimates along with its predictions. Yet, possibly the most widely used models, deep neural networks (LeCun et al., 2015) , are unable to accurately quantify model uncertainty. They are often overconfident in their predictions, even when their predictions are incorrect (Guo et al., 2017; Ovadia et al., 2019) . By marginalizing over a posterior distribution over the parameters given the training data, Bayesian inference provides a principled approach to capturing uncertainty. In contrast, standard training of neural networks employs a point estimate of the parameters, which cannot account for model uncertainty. Unfortunately, exact Bayesian inference is intractable in general for neural networks. To model epistemic uncertainty, variational inference (VI) instead approximates the true posterior with a simpler distribution. The most widely used one for neural networks is the mean-field approximation, where the posterior is represented using an independent Gaussian distribution over all the weights. Variational inference is appealing since it reduces the problem of inference to an optimization problem, minimizing the discrepancy between the true posterior and the variational posterior. The key challenge, however, is the task of training expressive posterior approximations that can capture the true posterior without significantly increasing the computational costs. This paper describes a novel method for training highly flexible posterior approximations. The idea is to start with a coarse, mean-field approximation q(w) and make iterative, local refinements to it. The regions of the local refinements are determined by sampling the values of additive auxiliary variables. The model parameters w are expressed using a number of auxiliary variables a k (Figure 1 left) for k = 1, . . . , K that leave the marginal distribution unchanged. In each iteration, we sample the value of an auxiliary variable according to the current variational approximation q(a k ) and refine the approximation by conditioning on the newly sampled value q(w) ≈ p(w|x, y, a 1:k ) (Figure 1 right illustrates the process). Each refinement step makes cheap, local adjustments to the variational posterior in the region of the sampled auxiliary variables. At the end, we draw one sample from In each iteration the value of an auxiliary variable is fixed and the posterior is locally adjusted. In the final iteration, a sample is drawn from w. Through the iterations, the variational distribution is able to approximate well the true posterior in a small region. the refined q(w). The refinement iterations have to be repeated for each posterior sample. The algorithm results in samples from a highly complex distribution, starting from a simple mean-field approximation. While the distribution of the samples is difficult to quantify, we show that it is not limited to factorized, uni-modal forms, and that the procedure is guaranteed to improve the resulting ELBO without posing a significant computational overhead. In this paper, we describe a novel algorithm for refining a coarse variational approximation to the Bayesian posterior. We show, both theoretically and empirically, that the refined posterior is a better approximation to the posterior than the initial variational distribution. Our method outperforms the baseline variational approximations in both uncertainty estimation as well as computational requirements. It sets a new state-of-the-art in uncertainty estimation using variational inference at ResNet scale (ResNet-20) on CIFAR10.",The paper proposes an algorithm to increase the flexibility of the variational posterior in Bayesian neural networks through iterative optimization.,Bayesian ; VI ; LeCun et al ; Guo et al. ; Ovadia ; al. ; Gaussian ; K ; one ; ResNet,the final iteration ; a principled approach ; the gains ; the distribution ; the initial variational distribution ; the samples ; the underlying model ; all the weights ; a crucial role ; the refined q(w,Bayesian ; VI ; LeCun et al ; Guo et al. ; Ovadia ; al. ; Gaussian ; K ; one ; ResNet,"Variational inference (VI) is a popular approach for approximate Bayesian inference that is particularly promising for highly parameterized models such as deep neural networks. The key challenge of variational inference is to approximate the posterior over model parameters with a distribution that is simpler and tractable yet sufficiently expressive. In experiments, we demonstrate that our method consistently outperforms recent variational methods for deep learning in log-likelihood and ELBO.   The gains are further amplified on larger scale models, significantly outperforming standard VI and deep ensembles on residual networks on CIFAR10. Uncertainty plays a crucial role in machine learning",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Claims from the fields of network neuroscience and connectomics suggest that topological models of the brain involving complex networks are of particular use and interest. The field of deep neural networks has mostly left inspiration from these claims out. In this paper, we propose three architectures and use each of them to explore the intersection of network neuroscience and deep learning in an attempt to bridge the gap between the two fields. Using the teachings from network neuroscience and connectomics, we show improvements over the ResNet architecture, we show a possible connection between early training and the spectral properties of the network, and we show the trainability of a DNN based on the neuronal network of C.Elegans. We have demonstrated three distinct approaches to applying work from network neurosciences and connectomics to deep learning. Our experiments show improvements over ResNet by the inclusion of skip connections which follow a connectivity pattern with small world properties, a possible connection between early training performance and spectral gap when using expander graphs as the participant graph topology with the node model proposed by BID14 , and the trainability of a DNN based on the neuronal network of C.Elegans with and without freezing the parameters of the convolutional and fully connected layers.In future work, we will examine the impact of other spectral properties of the graph topologies used both in the architectures we proposed and in the RandWire architecture proposed by BID14 . Additionally, we will explore parameter efficient connectivity patterns which could achieve similar performance to related networks with more parameters TAB0 Deep connectomics networks Figure 5 . Performance of C.ElegansNet with all parameters frozen except the C.Elegans graph edge weights.",We explore the intersection of network neurosciences and deep learning.,three ; two ; ResNet ; C.Elegans ; RandWire,the spectral properties ; the ResNet architecture ; each ; Our experiments ; C.ElegansNet ; similar performance ; the trainability ; improvements ; the intersection ; skip connections,three ; two ; ResNet ; C.Elegans ; RandWire,"In the field of deep neural networks, topological models of the brain involving complex networks are of particular interest. In this paper, we propose three architectures and explore the intersection of network neuroscience and deep learning in an attempt to bridge the gap between the two fields. We show improvements over ResNet architecture, we show a possible connection between early training performance and spectral properties of the network, and we show the trainability of a DNN based on the neuronal network of C.Elegans with all parameters frozen.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"We study the problem of explaining a rich class of behavioral properties of deep neural networks. Our influence-directed explanations approach this problem by peering inside the network to identify neurons with high influence on the property of interest using an axiomatically justified influence measure, and then providing an interpretation for the concepts these neurons represent. We evaluate our approach by training convolutional neural networks on Pubfig, ImageNet, and Diabetic Retinopathy datasets.   Our evaluation demonstrates that influence-directed explanations (1) localize features used by the network, (2) isolate features distinguishing related instances, (3) help extract the essence of what the network learned about the class, and (4) assist in debugging misclassifications.
 We study the problem of explaining a class of behavioral properties of deep neural networks, with a focus on convolutional neural networks. Examples of such properties include explaining why a network classified an input instance a particular way, why it misclassified an input, and what the essence of a class is for the network. This problem has received significant attention in recent years with the rise of deep networks and associated concerns about their opacity BID4 . This paper introduces influence-directed explanations for deep networks. It involves peering inside the network to identify neurons with high influence and then providing an interpretation for the concepts they represent. This approach enables us to interpret the inner workings of the network by drawing attention to concepts learned by the network that had a significant effect on the property that we seek to explain. In contrast to raw inputs, neurons in higher layers represent general concepts. Thus, they form a useful substrate to explain properties of interest involving many input instances, such as the essence of a class. Once influential neurons have been identified, they can be interpeted using existing techniques (e.g., visualization) to reveal the concepts they represent. Alternatively, influences can be examined directly to diagnose undesirable properties of the network.A key contribution of this paper is distributional influence, a measure for internal neurons that is axiomatically justified. Distributional influence is parameterized by a quantity of interest, a distribution of interest, and a slice of the network that allows us to reference some internal neurons in a network. It is simply the average partial derivative with respect to a neuron in a slice over the distribution of interest. This parametric measure can be appropriately instantiated to explain different properties of interest with respect to different parts of a network.Our influence measure is designed to achieve three natural desiderata: causality, distributional faithfulness, and flexibility. Capturing causality helps us identify parts of the network that when changed have the most effect on outcomes. Distributional faithfulness ensures that we evaluate the network only on points in the input distribution. This property is important since models operating on high dimensional spaces, such as neural networks, are not expected to behave reliably on instances outside the input distribution. Finally, by flexibility, we mean that the influence measure should support explanations for various properties of interest.We evaluate our approach by training convolutional neural networks on ImageNet BID8 , PubFig BID5 , and a Diabetic Retinopathy datasets. Our evaluation demonstrates that influence-directed explanations enable us to (1) characterize why inputs were classified a particular way in terms of high-level concepts represented by influential neurons (Section 3.1), (2) explain why an input was classified into a one class (e.g., sports car) rather than another (e.g., convertible) (Section 3.2), (3) demonstrate that influences localize the actual reasons used for classification better than simply examining activations (Section 3.3.1), (4) help extract the essence of what the network learned about the class (Section 3.3), and (5) assist in debugging misclassifications of a Diabetic Retinopathy classifier BID6 (Section 3.4). Influence measures are widely studied in cooperative game theory as solutions to the problem of attribution to of outcomes to participants and has applications to a wide range of settings including revenue division and voting. In this section, we highlight ideas drawn from this body of work and differences in terms of two key properties of influence measures: the marginality principle, and efficiency.The marginality principle BID14 states that an agent's attribution only depends on its own contribution to the output. Formally, this is stated as: if the partial derivatives with respect to an agent of two functions are identical throughout, then they have identical attributions for agent i. Our axiom of distributional marginality (DM) is a weaker form of this requirement that only requires equality of attribution when partial derivatives are same in the distribution.A second property, called efficiency, which is especially important for revenue division, is that attributions add up to the total value generated. This ensures that no value is left unattributed. The marginality principle, along with efficiency uniquely define the Aumann-Shapley Value BID0 . In BID12 , the Aumann-Shapley Value is used for attributions with efficiency as a justification. While it is unclear that efficiency is an essential requirement in our setting, the Aumann-Shapley value can be recovered in our framework by choosing the distribution of interest as the uniform distribution on the line segment joining an instance x and a baseline image b. Certain choices of baselines can be problematic from the point of view of distributional faithfulness, since the line segment of linear combinations between them might lie significantly out of distribution. The particular baseline chosen in BID12 is the zero vector, where the line segment represents scaled images, and could be reasonably called within distribution.","We present an influence-directed approach to constructing explanations for the behavior of deep convolutional networks, and show how it can be used to answer a broad set of questions that could not be addressed by prior work.",recent years ; Aumann-Shapley ; Diabetic Retinopathy ; Aumann-Shapley Value ; three ; second ; two ; one ; ImageNet ; linear,our framework ; distributional marginality ; such properties ; Certain choices ; that ; influences ; an agent's attribution ; participants ; efficiency ; significant attention,recent years ; Aumann-Shapley ; Diabetic Retinopathy ; Aumann-Shapley Value ; three ; second ; two ; one ; ImageNet ; linear,"The problem of explaining a rich class of behavioral properties of deep neural networks involves peering inside the network to identify neurons with high influence on the property of interest using an axiomatically justified influence measure, and providing an interpretation for the concepts these neurons represent.   The evaluation demonstrates that influence-directed explanations (1) localize features used by the network, (2) isolate features distinguishing related instances, (3) help extract the essence of what the network learned about the class, and (4) assist in debugging misclassifications. ",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Deterministic neural networks (NNs) are increasingly being deployed in safety critical domains, where calibrated, robust and efficient measures of uncertainty are crucial. While it is possible to train regression networks to output the parameters of a probability distribution by maximizing a Gaussian likelihood function, the resulting model remains oblivious to the underlying confidence of its predictions. In this paper, we propose a novel method for training deterministic NNs to not only estimate the desired target but also the associated evidence in support of that target. We accomplish this by  placing evidential priors over our original Gaussian likelihood function and training our NN to infer the hyperparameters of our evidential distribution. We impose priors during training such that the model is penalized when its predicted evidence is not aligned with the correct output. Thus the model estimates not only the probabilistic mean and variance of our target but also the underlying uncertainty associated with each of those parameters. We observe that our evidential regression method learns well-calibrated measures of uncertainty on various benchmarks, scales to complex computer vision tasks, and is robust to adversarial input perturbations.
 Uncertainty estimation has a long history in neural networks, from modeling probability distribution parameters over outputs (Bishop, 1994) to Bayesian deep learning (Kendall & Gal, 2017) . Our work builds on this foundation and presents a scalable representation for inferring the parameters of an evidential uncertainty distribution while simultaneously learning regression tasks via MLE. In Bayesian deep learning, priors are placed over network weights and estimated using variational inference (Kingma et al., 2015) . Dropout (Gal & Ghahramani, 2016; Molchanov et al., 2017) and BBB (Blundell et al., 2015) rely on multiple samples to estimate predictive variance. Ensembles (Lakshminarayanan et al., 2017) provide a tangential approach where sampling occurs over multiple trained instances. In contrast, we place uncertainty priors over the likelihood function and thus only need a single forward pass to evaluate both prediction and uncertainty. Additionally, our approach of uncertainty estimation proved to be better calibrated and capable of predicting where the model fails. A large topic of research in Bayesian inference focuses on placing prior distributions over hierarchical models to estimate uncertainty (Gelman et al., 2006; 2008) . Our methodology falls under the class of evidential deep learning which models higher-order distribution priors over neural network predictions to interpret uncertainty. Prior works in this field (Sensoy et al., 2018; Malinin & Gales, 2018) have focused exclusively on modeling uncertainty in the classification domain with Dirichlet prior distributions. Our work extends this field into the broad range of regression learning tasks (e.g. depth estimation, forecasting, robotic control learning, etc.) and demonstrates generalizability to out-of-distribution test samples and complex learning problems. In this paper, we develop a novel method for training deterministic NNs that both estimates a desired target and evaluates the evidence in support of the target to generate robust metrics of model uncertainty. We formalize this in terms of learning evidential distributions, and achieve stable training by penalizing our model for prediction errors that scale with the available evidence. Our approach for evidential regression is validated on a benchmark regression task. We further demonstrate that this method robustly scales to a key task in computer vision, depth estimation, and that the predictive uncertainty increases with increasing out-of-distribution adversarial perturbation. This framework for evidential representation learning provides a means to achieve the precise uncertainty metrics required for robust neural network deployment in safety-critical domains. For convenience, define τ = 1/σ 2 be the precision of a Gaussian distribution. The change of variables transforms the Normal Inverse-Gamma distribution p(µ, σ 2 |γ, λ, α, β) to the equivalent Normal Gamma distribution p(µ, τ |γ, λ, α, β), parameterized by precision τ ∈ (0, ∞) instead of variance σ 2 ,","Fast, calibrated uncertainty estimation for neural networks without sampling",MLE ; BBB ; Bayesian ; Malinin & Gales ; Lakshminarayanan et al. ; Gelman ; Kendall & Gal ; Normal Gamma ; Molchanov ; Kingma,robust metrics ; regression tasks ; priors ; depth estimation ; The change ; the predictive uncertainty ; a long history ; Lakshminarayanan ; not only the probabilistic mean ; prediction errors,MLE ; BBB ; Bayesian ; Malinin & Gales ; Lakshminarayanan et al. ; Gelman ; Kendall & Gal ; Normal Gamma ; Molchanov ; Kingma,"Deterministic neural networks (NNs) are increasingly being used in safety critical domains, where calibrated, robust and efficient measures of uncertainty are crucial. While it is possible to train regression networks to output the parameters of a probability distribution by maximizing a Gaussian likelihood function, the resulting model remains oblivious to the underlying confidence of its predictions. In this paper, we introduce a novel method for training deterministic NNs to estimate the desired target but also the associated evidence in support of that target. We place evidential priors over network weights and infer the hyperparameters of our evidential distribution. We impose priors during training such",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Neural sequence-to-sequence models are a recently proposed family of approaches used in abstractive summarization of text documents, useful for producing condensed versions of source text narratives without being restricted to using only words from the original text. Despite the advances in abstractive summarization, custom generation of summaries (e.g. towards a user's preference) remains unexplored. In this paper, we present CATS, an abstractive neural summarization model, that summarizes content in a sequence-to-sequence fashion but also introduces a new mechanism to control the underlying latent topic distribution of the produced summaries. Our experimental results on the well-known CNN/DailyMail dataset show that our model achieves state-of-the-art performance. Automatic document summarization is defined as producing a shorter, yet semantically highly related, version of a source document. Solutions to this task are typically classified into two categories: Extractive summarization and abstractive summarization.Extractive summarization refers to methods that select sentences of a source text based on a scoring scheme, and eventually combine those exact sentences in order to produce a summary. Conversely, abstractive summarization aims at producing shortened versions of a source document by generating sentences that do not necessarily appear in the original text. Recent advances in neural sequence-to-sequence modeling have sparked interest in abstractive summarization due to its flexibility and broad range of applications.The majority of research on text summarization thus far has been focused on extractive summarization BID16 , due its simplicity compared to abstractive methods.Beyond providing a generic summary of a longer passage of text, a system which would allow selective summarization based on a user's preference of topic would be of great value in an array of domains. For example, in the field of information retrieval, it could be used to summarize the results of a user search based on the content of the query.Summarization is also extensively used in other domains such as concisely describing the gist of news articles and stories BID21 BID19 , supporting the minutetaking process BID20 in corporate meetings and in the electronic health record domain BID5 , to name a few.In this paper, we introduce CATS, a customizable abstractive topic-based sequence-to-sequence summarization model, which is not only capable of summarizing text documents with an improved performance as compared to the state of the art, but also allows to selectively focus on a range of desired topics of interest when generating summaries. Our experiments corroborate that our model can selectively add or remove certain topics from the summary. Furthermore, our experimental results on a publicly available dataset indicate that the proposed neural sequence-to-sequence model can effectively outperform state-of-the-art baselines in terms of ROUGE.The main contributions of this paper are: (1) We introduce a novel neural sequence-tosequence model based on an encoder-decoder architecture that outperforms the state-of-the-art baselines in the task of abstractive summarization on a benchmark dataset.(2 ) We show how the attention mechanism BID0 may be used for simultaneously identifying important topics as well as recognizing those parts of the encoder output that are vital to be focused on.The remainder of this paper is organized is as follows: Section 2 discusses related work on abstractive neural summarization. In Section 3, we introduce the CATS summarization model. In Section 4, we discuss our experimental setup and results comparing CATS to a broad range of competitive state-of-the-art baselines. Finally , in Section 5, we conclude this paper and present future directions of inquiry. In this paper we present CATS, an abstractive summarization model that makes use of latent topic information in a source document, and is thereby capable of controlling the topics appearing in an output summary of a source document. This can enable customization of generated texts based on user profiles or explicitly given topics, in order to present content tailored to a user's information needs.Our experimental results show that our CATS+coverage model achieves state-of-the-art performance in terms of standard evaluation metrics for summarization (i.e ROUGE) on an important benchmark dataset, while enabling customization in producing summaries.CATS can serve as a foundation for future work in the domain of automatic summarization. Based on the results of this paper, we believe the future work on summarization systems to be exciting, in that a generated summary could be customized to users' needs. We envision three ways of controlling the focus of output summaries using our models: First, as demonstrated in the experiment in Section 4.4.3, certain topics could be disabled in the output of the topic model and be consequently discarded from output summaries. Second, a reference document could be provided to the topic model, its topics could be extracted and subsequently direct the focus of generated summaries. This is useful when a user wants to see summaries/updates primarily or only regarding issues discussed in an existing reference document. Third, content extracted from user profiles (e.g. history of web pages of interest) could be provided to the topic model, their salient themes extracted by the model and then taken into account whenever presenting users with summaries. All three directions are interesting future works of this paper.",We present the first neural abstractive summarization model capable of customization of generated summaries.,CNN ; two ; CATS ; three ; First ; Second ; Third,standard evaluation metrics ; the content ; shortened versions ; output summaries ; e.g. history ; our models ; selective summarization ; the art ; broad range ; automatic summarization,CNN ; two ; CATS ; three ; First ; Second ; Third,"Neural sequence-to-sequence models (CATS) are a recently proposed family of abstractive summarization of text documents, useful for producing condensed versions of source text narratives without being restricted to using only words from the original text. However, their usefulness is limited due to their flexibility and broad range of applications. In this paper, we introduce CATS, an abstractive neural summarization model, that summarizes content in sequence tosequence fashion but also introduces a new mechanism to control the latent topic distribution. Our experimental results on the well-known CNN/DailyMail dataset show that our model achieves state-of-the-art",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"We introduce a novel method for converting text data into abstract image representations, which allows image-based processing techniques (e.g. image classification networks) to be applied to text-based comparison problems. We apply the technique to entity disambiguation of inventor names in US patents. The method involves converting text from each pairwise comparison between two inventor name records into a 2D RGB (stacked) image representation. We then train an image classification neural network to discriminate between such pairwise comparison images, and use the trained network to label each pair of records as either matched (same inventor) or non-matched (different inventors), obtaining highly accurate results (F1: 99.09%, precision: 99.41%, recall: 98.76%). Our new text-to-image representation method could potentially be used more broadly for other NLP comparison problems, such as disambiguation of academic publications, or for problems that require simultaneous classification of both text and images. Databases of patent applications and academic publications can be used to investigate the process of research and innovation. For example, patent data can be used to identify prolific inventors (Gay et al., 2008) or to investigate whether mobility increases inventor productivity (Hoisl, 2009 ). However, the names of individuals in large databases are rarely distinct, hence individuals in such databases are not uniquely identifiable. For example, an individual named ""Chris Jean Smith"" may have patents under slightly different names such as ""Chris Jean Smith"", ""Chris J. Smith"", ""C J Smith"", etc. . . There may also be different inventors with patents under the same or similar names, such as ""Chris Jean Smith"", ""Chris J. Smith"", ""Chris Smith"", etc. . . Thus it is ambiguous which names (and hence patents) should be assigned to which individuals. Resolving this ambiguity and assigning unique identifiers to individuals -a process often referred to as named entity disambiguation -is important for research that relies on such databases. Machine learning algorithms have been used increasingly in recent years to perform automated disambiguation of inventor names in large databases (e.g. Li et al. (2014) ; Ventura et al. (2015) ; Kim et al. (2016) ). See Ventura et al. (2015) for a review of supervised, semi-supervised, and unsupervised machine learning approaches to disambiguation. These more recent machine learning approaches have often out-performed more traditional rule-and threshold-based methods, but they have generally used feature vectors containing several pre-selected measures of string similarity as input for their machine learning algorithms. That is, the researcher generally pre-selects a number of string similarity measures which they believe may be useful as input for the machine learning algorithm to make discrimination decisions. Here we introduce a novel approach of representing text-based data, which enables image classifiers to perform text classification. This new representation enables a supervised machine learning algorithm to learn its own features from the data, rather than selecting from a number of pre-defined string similarity measures chosen by the researcher. To do this, we treat the name disambiguation problem primarily as a classification problem -i.e. we assess pairwise comparisons between records as either matched (same inventor) or non-matched (different inventors) (Trajtenberg et al., 2006; Miguélez & Gómez-Miguélez, 2011; Li et al., 2014; Ventura et al., 2015; Kim et al., 2016) . Then, for a given pairwise comparison between two inventor records, our text-to-image representa-tion method converts the associated text strings into a stacked 2D colour image (or, equivalently, a 3D tensor) which represents the underlying text data. We describe our text-to-image representation method in detail in Section 4.1 (see Figure 1 in that section for an example of text-to-image conversion). We also test a number of alternative representations in Section 5.4. Our novel method of representing text-based records as abstract images enables image processing algorithms (e.g. image classification networks), to be applied to textbased natural language processing (NLP) problems involving pairwise comparisons (e.g. named entity disambiguation). We demonstrate this by combining our text-to-image conversion method with a commonly used convolutional neural network (CNN) (Krizhevsky et al., 2012) , obtaining highly accurate results (F1: 99.09%, precision: 99.41%, recall: 98.76%). Our name disambiguation algorithm provides a novel way of combining image processing with NLP, allowing image classifiers to perform text classification. We demonstrated this with the AlexNet CNN, producing highly accurate results (F1 score: 99.09%). We also analysed several variants of alternative string-maps, and found that the accuracy of the disambiguation algorithm was quite robust to such variation. Our disambiguation algorithm could easily be adapted to other NLP problems requiring text matching of multiple strings (e.g. academic author name disambiguation or record linkage problems). The algorithm could also potentially be modified to process records that contain both text and image data, by combining each record's associated image with the abstract image representation of the record's text, in a single comparison-map.","We introduce a novel text representation method which enables image classifiers to be applied to text classification problems, and apply the method to inventor name disambiguation.",CNN ; Kim et al. ; Chris J. Smith ; Ventura ; the AlexNet ; recent years ; Li et al ; Trajtenberg ; al. ; NLP,the associated text strings ; a commonly used convolutional neural network ; highly accurate results ; an example ; Our disambiguation algorithm ; example ; an individual ; disambiguation ; a single comparison-map ; Krizhevsky et al,CNN ; Kim et al. ; Chris J. Smith ; Ventura ; the AlexNet ; recent years ; Li et al ; Trajtenberg ; al. ; NLP,"We introduce an image-based processing technique (e.g. image classification networks) for text-based comparison problems. The technique involves converting text from each pairwise comparison between two inventor name records into a 2D RGB (stacked) image representation, and train an image classification neural network to classify each pair of records as either matched (same inventor) or non-matched (different inventors), obtaining highly accurate results. The text-to-image representation method could potentially be used more broadly for NLP comparison problems, such as disambiguation of academic publications, or for problems requiring simultaneous classification of both text and images",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"The transformer is a state-of-the-art neural translation model that uses attention to iteratively refine lexical representations with information drawn from the surrounding context. Lexical features are fed into the first layer and propagated through a deep network of hidden layers. We argue that the need to represent and propagate lexical features in each layer limits the model’s capacity for learning and representing other information relevant to the task. To alleviate this bottleneck, we introduce gated shortcut connections between the embedding layer and each subsequent layer within the encoder and decoder. This enables the model to access relevant lexical content dynamically, without expending limited resources on storing it within intermediate states. We show that the proposed modification yields consistent improvements on standard WMT translation tasks and reduces the amount of lexical information passed along the hidden layers. We furthermore evaluate different ways to integrate lexical connections into the transformer architecture and present ablation experiments exploring the effect of proposed shortcuts on model behavior. Since it was first proposed, the transformer model BID28 ) has quickly established itself as a popular choice for neural machine translation, where it has been found to deliver state-ofthe-art results on various translation tasks BID3 . Its success can be attributed to the model's high parallelizability allowing for significantly faster training compared to recurrent neural networks , superior ability to perform lexical disambiguation, and capacity for capturing long-distance dependencies on par with existing alternatives BID26 .Recently , several studies have investigated the nature of features encoded within individual layers of neural translation models BID1 BID2 . One central finding reported in this body of work is that, within current architectures, different layers prioritize different information types. As such, lower layers appear to predominantly perform morphological and syntactic processing, whereas semantic features reach their highest concentration towards the top of the layer stack. One necessary consequence of this distributed learning is that different types of information encoded within input representations received by the translation model have to be transported to the layers specialized in exploiting them. Within the transformer encoder and decoder alike, information exchange proceeds in a strictly sequential manner, whereby each layer attends over the output of the immediately preceding layer, complemented by a shallow residual connection. For input features to be successfully propagated to the uppermost layers, the translation model must therefore store them in its intermediate representations until they can be processed. By retaining lexical content, the model is unable to leverage its full representational capacity for learning new information from other sources, such as the surrounding sentence context. We refer to this limitation as the representation bottleneck.To alleviate this bottleneck, we propose extending the standard transformer architecture with lexical shortcuts which connect the embedding layer with each subsequent self-attention sub-layer in both encoder and decoder. The shortcuts are defined as gated skip connections, allowing the model to access relevant lexical information at any point, instead of propagating it upwards from the embedding layer along the hidden states.We evaluate the resulting model's performance on multiple language pairs and varying corpus sizes, showing a consistent improvement in translation quality over the unmodified transformer baseline. Moreover, we examine the distribution of lexical information across the hidden layers of the transformer model in its standard configuration and with added shortcut connections. The presented experiments provide quantitative evidence for the presence of a representation bottleneck in the standard transformer and its reduction following the integration of lexical shortcuts. While our experimental efforts are centered around the transformer, the proposed components are compatible with other multi-layer NMT architectures.The contributions of our work are therefore as follows:1. We propose the use of lexical shortcuts as a simple strategy for alleviating the representation bottleneck in neural machine translation models.2. We demonstrate significant improvements in translation quality across multiple language pairs as a result of equipping the transformer with lexical shortcut connections.3. We report a positive impact of our modification on the model's ability to perform word sense disambiguation.4. We conduct a series of ablation studies, showing that shortcuts are best applied to the self-attention mechanism in both encoder and decoder.2 Proposed Method 2.1 Background: The transformerAs defined in BID28 , the transformer is comprised of two sub-networks, the encoder and the decoder. The encoder coverts the received source language sentence into a sequence of continuous representations containing translation-relevant features. The decoder, on the other hand, generates the target language sequence, whereby each translation step is conditioned on the encoder's output as well as the translation prefix produced up to that point. Both encoder and decoder are composed of a series of identical layers. Each encoder layer contains two sub-layers: A self-attention mechanism and a position-wise fully connected feed-forward network. Within the decoder, each layer is extended with a third sub-layer responsible for attending over the encoder's output. In each case, the attention mechanism is implemented as multihead, scaled dot-product attention, which allows the model to simultaneously consider different context sub-spaces. Additionally, residual connections between neighboring layers are employed to aid with signal propagation.In order for the dot-product attention mechanism to be effective, its inputs first have to be projected into a common representation sub-space. This is accomplished by multiplying the input arrays H S and H T by one of the three weight matrices K, V , and Q, as shown in Eqn. 1-3, producing attention keys, values, and queries, respectively. In case of multi-head attention , each head is assigned its own set of keys, values, and queries with the associated learned projection weights. DISPLAYFORM0 In case of encoder-to-decoder attention, H T corresponds to the final encoder states , whereas H S is the context vector generated by the preceding self-attention sub-layer. For self-attention, on the other hand, all three operations are given the output of the preceding layer as their input. Eqn. 4 defines attention as a function over the projected representations. DISPLAYFORM1 To prevent the magnitude of the pre-softmax dot-product from becoming too large, it is divided by the square root of the total key dimensionality d k . Finally, the translated sequence is obtained by feeding the output of the decoder through a softmax layer and sampling from the produced distribution over target language tokens. In this paper, we have proposed a simple yet effective method for widening the representation bottleneck in the transformer by introducing lexical shortcuts. Our modified models achieve up to 1.4 BLEU (0.9 BLEU on average) improvement on 5 standard WMT datasets, at a small cost in computing time and model size. Our analysis suggests that lexical connections are useful to both encoder and decoder, and remain effective when included in smaller models. Moreover, the addition of shortcuts noticeably reduces the similarity of hidden states to the initial embeddings, indicating that dynamic lexical access aids the network in learning novel, diverse information. We also performed ablation studies comparing different shortcut variants and demonstrated that one effect of lexical shortcuts is an improved WSD capability.The presented findings offer new insights into the nature of information encoded by the transformer layers, supporting the iterative refinement view of feature learning. In future work, we intend to explore other ways to better our understanding of the refinement process and to help translation models learn more diverse and meaningful internal representations.",Equipping the transformer model with shortcuts to the embedding layer frees up model capacity for learning novel information.,NMT ; Eqn ; One ; V ; two ; three ; H S and H ; third ; WMT ; up to,a result ; consistent improvements ; various translation tasks ; the resulting model's performance ; several studies ; a function ; d k ; a sequence ; computing time ; each translation step,NMT ; Eqn ; One ; V ; two ; three ; H S and H ; third ; WMT ; up to,"The transformer is a state-of-the-art neural translation model that iteratively refine lexical representations with information drawn from the surrounding context. Lexical features are fed into the first layer and propagated through a deep network of hidden layers. To alleviate this bottleneck, we introduce gated shortcut connections between the embedding layer and each subsequent layer, allowing the model to access relevant lexical content dynamically, without expending limited resources on storing it within intermediate states. The proposed modification yields consistent improvements on standard WMT translation tasks and reduces the amount of lexical information passed along hidden layers, while enhancing the efficiency of the transformer architecture",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Conventional out-of-distribution (OOD) detection schemes based on variational autoencoder or Random Network Distillation (RND) are known to assign lower uncertainty to the OOD data than the target distribution. In this work, we discover that such conventional novelty detection schemes are also vulnerable to the blurred images. Based on the observation, we construct a novel RND-based OOD detector, SVD-RND, that utilizes blurred images during training. Our detector is simple, efficient in test time, and outperforms baseline OOD detectors in various domains. Further results show that SVD-RND learns a better target distribution representation than the baselines. Finally, SVD-RND combined with geometric transform achieves near-perfect detection accuracy in CelebA domain. Out-of distribution (OOD), or novelty detection aims to distinguish samples in unseen distribution from the training distribution. A majority of novelty detection methods focus on noise filtering or representation learning. For example, we train an autoencoder to learn a mapping from the data to the bottleneck layer and use the bottleneck representation or reconstruction error to detect an OOD (Sakruada et al., 2014; Pidhorskyi et al., 2018) . Recently, deep generative models (Kingma et al., 2014; Dinh et al., 2017; Kingma et al., 2018; Schlegl et al., 2017) are widely used for novelty detection due to their ability to model high dimensional data. However, OOD detection performance of deep generative models has been called into question since they have been observed to assign a higher likelihood to the OOD data than the training data (Nalisnick et al., 2019; Choi et al., 2018) . On the other hand, adversarial examples are widely employed to fool the classifier, and training classifiers against adversarial attacks has shown effectiveness in detecting unknown adversarial attacks (Tramer et al., 2018) . In this work, we propose blurred data as the adversarial example. When we test novelty detection models on the blurred data generated by Singular Value Decomposition (SVD), we found that the novelty detection models assign higher confidence to the blurred data than the original data. Motivated by this observation, we employ blurring to prevent the OOD detector from overfitting to low resolution. We propose a new OOD detection model, SVD-RND, which is trained using the idea of Random Network Distillation (RND) (Burda et al., 2019) to discriminate the training data from the blurred image. SVD-RND is evaluated in the hard target to OOD domain where vanilla generative models show nearly 50% detection accuracy, such as CIFAR-10 to SVHN and ImageNet to CIFAR-10 (Nalisnick et al., 2019) . Compared to conventional baselines, SVD-RND shows a significant performance gain from 50% to over 90% in these domains. Moreover, SVD-RND shows improvements over baselines on domains where conventional OOD detection schemes show moderate results, such as CIFAR-10 to LSUN. In this work, a blurred image is introduced as an adversarial example to the deep OOD detection method. SVD-RND is employed for adversarial defense against blurred images. SVD-RND achieves significant performance gain in all target : anomaly domains. Even without the validation OOD data, we can design SVD-RND to outperform conventional OOD detection models. We stress that such performance gain is achieved without external data or additional regularization techniques. Furthermore, experiments on SVD-RND and RND show that the neural network can potentially learn to perform OOD detection, however overfits to blurred data. Understanding this phenomenon will be beneficial to performance of the image-based models. We use RND (Burda et al., 2019) as the base model of our OOD detector. RND consists of the trainable predictor network f , and randomly initialized target network g. The predictor network is trained to minimize the l 2 distance against the target network on training data. We do not update the target network g throughout the training phase.",We propose a novel OOD detector that employ blurred images as adversarial examples . Our model achieve significant OOD detection performance in various domains.,al. ; Nalisnick et al. ; SVD ; SVHN ; Choi ; Tramer ; Sakruada et al. ; ImageNet ; Schlegl et al. ; Kingma,target network ; adversarial examples ; the image-based models ; CelebA domain ; performance ; OOD data ; Choi et al ; the blurred data ; the training distribution ; deep generative models,al. ; Nalisnick et al. ; SVD ; SVHN ; Choi ; Tramer ; Sakruada et al. ; ImageNet ; Schlegl et al. ; Kingma,"Conventional out-of-distribution (OOD) detection schemes based on variational autoencoder or Random Network Distillation (RND) are known to assign lower uncertainty to the OOD data than the target distribution. However, the RND-based OOD detector, SVD-RND, utilizes blurred images during training, outperforms baseline OOD detectors in various domains. Furthermore, the geometric transform achieves near-perfect detection accuracy in CelebA domain. Out-of distribution (OOD), or novelty detection, aims to distinguish samples in unseen distribution from the training distribution. A majority of novelty detection methods focus on",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"The problem of verifying whether a textual hypothesis holds based on the given evidence, also known as fact verification, plays an important role in the study of natural language understanding and semantic representation. However, existing studies are mainly restricted to dealing with unstructured evidence (e.g., natural language sentences and documents, news, etc), while verification under structured evidence, such as tables, graphs, and databases, remains unexplored. This paper specifically aims to study the fact verification given semi-structured data as evidence. To this end, we construct a large-scale dataset called TabFact with 16k Wikipedia tables as the evidence for 118k human-annotated natural language statements, which are labeled as either ENTAILED or REFUTED. TabFact is challenging since it involves both soft linguistic reasoning and hard symbolic reasoning. To address these reasoning challenges, we design two different models: Table-BERT and Latent Program Algorithm (LPA). Table-BERT leverages the state-of-the-art pre-trained language model to encode the linearized tables and statements into continuous vectors for verification. LPA parses statements into LISP-like programs and executes them against the tables to obtain the returned binary value for verification. Both methods achieve similar accuracy but still lag far behind human performance. We also perform a comprehensive analysis to demonstrate great future opportunities. Verifying whether a textual hypothesis is entailed or refuted by the given evidence is a fundamental problem in natural language understanding (Katz & Fodor, 1963; Van Benthem et al., 2008) . It can benefit many downstream applications like misinformation detection, fake news detection, etc. Recently, the first-ever end-to-end fact-checking system has been designed and proposed in Hassan et al. (2017) . The verification problem has been extensively studied under different natural language tasks such as recognizing textual entailment (RTE) (Dagan et al., 2005) , natural language inference (NLI) (Bowman et al., 2015) , claim verification (Popat et al., 2017; Hanselowski et al., 2018; Thorne et al., 2018) and multimodal language reasoning (NLVR/NLVR2) (Suhr et al., 2017; . RTE and NLI view a premise sentence as the evidence, claim verification views passage collection like Wikipedia 1 as the evidence, NLVR/NLVR2 views images as the evidence. These problems have been previously addressed using a variety of techniques including logic rules, knowledge bases, and neural networks. Recently large-scale pre-trained language models (Devlin et al., 2019; Peters et al., 2018; Yang et al., 2019; Liu et al., 2019) have surged to dominate the other algorithms to approach human performance on several textual entailment tasks (Wang et al., 2018; . However, existing studies are restricted to dealing with unstructured text as the evidence, which would not generalize to the cases where the evidence has a highly structured format. Since such structured evidence (graphs, tables, or databases) are also ubiquitous in real-world applications like database systems, dialog systems, commercial management systems, social networks, etc, we argue that the fact verification under structured evidence forms is an equivalently important yet underexplored problem. Therefore, in this paper, we are specifically interested in studying fact verification with semi-structured Wikipedia tables (Bhagavatula et al., 2013) 2 as evidences owing to its structured and ubiquitous nature (Jauhar et al., 2016; Zhong et al., 2017; Pasupat & Liang, 2015) . To this end, we introduce a large-scale dataset called TABFACT, which consists of 118K manually annotated statements with regard to 16K Wikipedia tables, their relations are classified as ENTAILED and REFUTED 3 . The entailed and refuted statements are both annotated by human workers. With some examples in Figure 1 , we can clearly observe that unlike the previous verification related problems, TABFACT combines two different forms of reasoning in the statements, (i) Linguistic Reasoning: the verification requires semantic-level understanding. For example, ""John J. Mcfall failed to be re-elected though being unopposed."" requires understanding over the phrase ""lost renomination ..."" in the table to correctly classify the entailment relation. Unlike the existing QA datasets (Zhong et al., 2017; Pasupat & Liang, 2015) , where the linguistic reasoning is dominated by paraphrasing, TABFACT requires more linguistic inference or common sense. (ii) Symbolic Reasoning: the verification requires symbolic execution on the table structure. For example, the phrase ""There are three Democrats incumbents"" requires both condition operation (where condition) and arithmetic operation (count). Unlike question answering, a statement could contain compound facts, all of these facts need to be verified to predict the verdict. For example, the ""There are ..."" in Figure 1 requires verifying three QA pairs (total count=5, democratic count=2, republic count=3). The two forms of reasoning are interleaved across the statements making it challenging for existing models. In this paper, we particularly propose two approaches to deal with such mixed-reasoning challenge: (i) Table-BERT, this model views the verification task completely as an NLI problem by linearizing a table as a premise sentence p, and applies state-of-the-art language understanding pre-trained model to encode both the table and statements h into distributed representation for classification. This model excels at linguistic reasoning like paraphrasing and inference but lacks symbolic reasoning skills. (ii) Latent Program Algorithm, this model applies lexical matching to find linked entities and triggers to filter pre-defined APIs (e.g. argmax, argmin, count, etc). We adopt bread-first-search with memorization to construct the potential program candidates, a discriminator is further utilized to select the most ""consistent"" latent programs. This model excels at the symbolic reasoning aspects by executing database queries, which also provides better interpretability by laying out the decision rationale. We perform extensive experiments to investigate their performances: the best-achieved accuracy of both models are reasonable, but far below human performance. Thus, we believe that the proposed table-based fact verification task can serve as an important new benchmark towards the goal of building powerful AI that can reason over both soft linguistic form and hard symbolic forms. To facilitate future research, we released all the data, code with the intermediate results. This paper investigates a very important yet previously under-explored research problem: semistructured fact verification. We construct a large-scale dataset and proposed two methods, Table- BERT and LPA, based on the state-of-the-art pre-trained natural language inference model and program synthesis. In the future, we plan to push forward this research direction by inspiring more sophisticated architectures which can perform both linguistic and symbolic reasoning. We list all the trigger words for different functions in Figure 8 Trigger Function 'average' average 'difference ', 'gap', 'than', 'separate' diff 'sum', 'summation', 'combine', 'combined', 'total', 'add', 'all', 'there are' ddd, sum 'not', 'no', 'never', ""didn't"", ""won't"", ""wasn't"", ""isn't,""haven't"", ""weren't"", ""won't"", 'neither', 'none', 'unable, 'fail', 'different', 'outside', 'unable', 'fail' not_eq, not_within, Filter_not_eq, none 'not', 'no', 'none' none 'first', 'top', 'latest', 'most' first 'last', 'bottom', 'latest', 'most' last 'RBR', 'JJR', 'more', 'than', 'above', 'after' filter_greater, greater 'RBR', 'JJR', 'less', 'than', 'below', 'under' filter_less, less 'all', 'every', 'each' all_eq, all_less, all_greater, ['all', 'every', 'each'] , ['not', 'no', 'never', ""didn't"", ""won't"", ""wasn't""] 2. Negation: the negation operation refers to sentences like ""xxx did not get the best score"", ""xxx has never obtained a score higher than 5"". 3. Superlative: the superlative operation refers to sentences like ""xxx achieves the highest score in"", ""xxx is the lowest player in the team"". 4. Comparative: the comparative operation refers to sentences like ""xxx has a higher score than yyy"".",We propose a new dataset to investigate the entailment problem under semi-structured table as premise,Peters ; Dagan et al. ; Hanselowski ; three ; Democrats ; Wang ; BERT ; QA ; Devlin et al. ; Van Benthem,"the lowest player ; Figure ; , greater 'RBR ; great future opportunities ; most' last 'RBR ; two methods ; The entailed and refuted statements ; symbolic execution ; The verification problem ; Wang et al",Peters ; Dagan et al. ; Hanselowski ; three ; Democrats ; Wang ; BERT ; QA ; Devlin et al. ; Van Benthem,"The problem of verifying whether a textual hypothesis holds based on the given evidence, also known as fact verification, plays an important role in the study of natural language understanding and semantic representation. However, existing studies are limited to unstructured evidence, such as tables, graphs, and databases. This paper aims to study the fact verification given semi-structured data as evidence. TabFact is a large-scale dataset with 16k Wikipedia tables and statements as the evidence for 118k human-annotated natural language statements, which are labeled as ENTAILED or REFUTED. This approach challenges soft linguistic reasoning and hard symbolic",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Generative deep learning has sparked a new wave of Super-Resolution (SR) algorithms that enhance single images with impressive aesthetic results, albeit with imaginary details. Multi-frame Super-Resolution (MFSR) offers a more grounded approach to the ill-posed problem, by conditioning on multiple low-resolution views. This is important for satellite monitoring of human impact on the planet -- from deforestation, to human rights violations -- that depend on reliable imagery. To this end, we present HighRes-net, the first deep learning approach to MFSR that learns its sub-tasks in an end-to-end fashion: (i) co-registration, (ii) fusion, (iii) up-sampling, and (iv) registration-at-the-loss. Co-registration of low-res views is learned implicitly through a reference-frame channel, with no explicit registration mechanism. We learn a global fusion operator that is applied recursively on an arbitrary number of low-res pairs. We introduce a registered loss, by learning to align the SR output to a ground-truth through ShiftNet. We show that by learning deep representations of multiple views, we can super-resolve low-resolution signals and enhance Earth observation data at scale. Our approach recently topped the European Space Agency's MFSR competition on real-world satellite imagery. Multiple low-resolution images collectively contain more information than any individual lowresolution image, due to minor geometric displacements, e.g. shifts, rotations, atmospheric turbulence, and instrument noise. Multi-Frame Super-Resolution (MFSR) (Tsai, 1984) aims to reconstruct hidden high-resolution details from multiple low-resolution views of the same scene. Single Image Super-Resolution (SISR), as a special case of MFSR, has attracted much attention in the computer vision, machine learning and deep learning communities in the last 5 years, with neural networks learning complex image priors to upsample and interpolate images (Xu et al., 2014; Srivastava et al., 2015; He et al., 2016) . However, in the meantime not much work has explored the learning of representations for the more general problem of MFSR to address the additional challenges of co-registration and fusion of multiple low-resolution images. This paper explores how Multi-Frame Super-Resolution (MFSR) can benefit from recent advances in learning representations with neural networks. To the best of our knowledge, this work is the first to introduce a deep-learning approach that solves the co-registration, fusion and registration-at-theloss problems in an end-to-end learning framework. Prompting this line of research is the increasing drive towards planetary-scale Earth observation to monitor the environment and human rights violations. Such observation can be used to inform policy, achieve accountability and direct on-the-ground action, e.g. within the framework of the Sustainable Development Goals (Jensen & Campbell, 2019) . Nomenclature Registration is the problem of estimating the relative geometric differences between two images (e.g. due to shifts, rotations, deformations). Fusion, in the MFSR context, is the problem of mapping multiple low-res representations into a single representation. By coregistration, we mean the problem of registering all low-resolution views to improve their fusion. By registration-at-the-loss, we mean the problem of registering the super-resolved reconstruction to the high-resolution ground-truth prior to computing the loss. This gives rise to the notion of a registered loss. Co-registration of multiple images is required for longitudinal studies of land change and environmental degradation. The fusion of multiple images is key to exploiting cheap, high-revisit-frequency satellite imagery, but of low-resolution, moving away from the analysis of infrequent and expensive high-resolution images. Finally, beyond fusion itself, super-resolved generation is required throughout the technical stack: both for labeling, but also for human oversight (Drexler, 2019) demanded by legal context (Harris et al., 2018) . In this paper, we presented HighRes-net -the first deep learning approach to multi-frame superresolution that learns typical sub-tasks of MFSR in an end-to-end fashion: (i) co-registration, (ii) fusion, (iii) up-sampling, and (iv) registration-at-the-loss. It recursively fuses a variable number of low-resolution views by learning a global fusion operator. The overall fusion also aligns all low-resolution views with an implicit co-registration mechanism through the reference channel. We also introduced ShiftNet-Lanczos, a network that learns to register and align the super-resolved output of HighRes-net with a high-resolution ground-truth. Registration is important both to align multiple low-resolution inputs (co-registration) and to compute similarity metrics between shifted signals. Our experiments suggest that an end-to-end cooperative setting (HighRes-net + ShiftNet-Lanczos) helps with training and test performance. By design, our approach is fast to train, faster to test, and low in terms of memory-footprint by doing the bulk of the computational work (co-registration + fusion) on multiple images while maintaining their low-resolution height & width. There is an ongoing proliferation of low-resolution yet high-revisit low-cost satellite imagery, but they often lack the detailed information of expensive high-resolution imagery. We believe MFSR can raise its potential to NGOs and non-profits that contribute to the UN Sunstainable Development Goals. A APPENDIX","The first deep learning approach to MFSR to solve registration, fusion, up-sampling in an end-to-end manner.",Multi-Frame Super-Resolution ; ShiftNet-Lanczos ; Earth ; Drexler ; Tsai ; two ; al. ; MFSR ; first ; the Sustainable Development Goals,"a network ; super-resolved generation ; the first deep learning approach ; shifted signals ; a registered loss ; more information ; multiple images ; shifts ; the notion ; registration, (ii) fusion",Multi-Frame Super-Resolution ; ShiftNet-Lanczos ; Earth ; Drexler ; Tsai ; two ; al. ; MFSR ; first ; the Sustainable Development Goals,"Multi-frame Super-Resolution (MFSR) is a more grounded approach to the ill-posed problem, by conditioning on multiple low-resolution views. This is crucial for satellite monitoring of human impact on the planet. HighRes-net, the first deep learning approach to MFSR, learns its sub-tasks in an end-to-end fashion: (i) co-registration, (ii) fusion, (iii) up-sampling, and (iv) registration-at-the-loss. In the case of low-res views, we learn a global fusion operator that is",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"In one-class-learning tasks, only the normal case can be modeled with data, whereas the variation of all possible anomalies is too large to be described sufficiently by samples. Thus, due to the lack of representative data, the wide-spread discriminative approaches cannot cover such learning tasks, and rather generative models, which attempt to learn the input density of the normal cases, are used. However, generative models suffer from a large input dimensionality (as in images) and are typically inefficient learners. We propose to learn the data distribution more efficiently with a multi-hypotheses autoencoder. Moreover, the model is criticized by a discriminator, which prevents artificial data modes not supported by data, and which enforces diversity across hypotheses. This consistency-based anomaly detection (ConAD) framework allows the reliable identification of outof- distribution samples. For anomaly detection on CIFAR-10, it yields up to 3.9% points improvement over previously reported results. On a real anomaly detection task, the approach reduces the error of the baseline models from 6.8% to 1.5%. Anomaly detection classifies a sample as normal or abnormal. In many applications, however, it must be treated as a one-class-learning problem, since the abnormal class cannot be defined sufficiently by samples. Samples of the abnormal class can be extremely rare, or they do not cover the full space of possible anomalies. For instance, in an autonomous driving system, we may have a test case with a bear or a kangaroo on the road. For defect detection in manufacturing, new, unknown production anomalies due to critical changes in the production environment can appear. In medical data analysis, there can be unknown deviations from the healthy state. In all these cases, the well-studied discriminative models, where decision boundaries of classifiers are learned from training samples of all classes, cannot be applied. The decision boundary learning of discriminative models will be dominated by the normal class, which will negatively influence the classification performance.Anomaly detection as one-class learning is typically approached by generative, reconstruction-based methods BID30 . They approximate the input distribution of the normal cases by parametric models, which allow them to reconstruct input samples from this distribution. At test time, the data log-likelihood serves as an anomaly-score. In the case of high-dimensional inputs, such as images, learning a representative distribution model of the normal class is hard and requires many samples.Typically, an autoencoder-based approach such as the variational autoencoder BID21 BID13 ) is used. Autoencoders tend to produce blurry reconstructions, since they regress the conditional mean, and cannot model multi-modal distributions; see FIG0 for an example on a Metal Anomaly dataset. Due to multiple modes in the actual distribution, the approximation with the mean predicts high probabilities in areas not supported by samples. The blurry reconstructions in FIG0 should have a low probability and be classified as anomalies, but they have the highest likelihood under the learned autoencoder.Multiple-hypotheses networks could give the model more expressive power BID23 , BID5 , BID11 , BID2 . In conjunction with autoencoders, the multiple hypotheses can be realized with a multi-headed decoder. Concretely, each network head may predict a Gaussian density estimate. gives the network more expressive power with a multi-headed decoder (also known as multiple-hypotheses networks). The resulting anomaly scores are hence much clearer in our framework ConAD. In this work, we propose to employ multiple-hypotheses networks for learning data distributions for anomaly detection tasks. Hypotheses are meant to form clusters in the data space and can easily capture model uncertainty not encoded by the latent code. multiple-hypotheses networks can provide a more fine-grained description of the data distribution and therefore enable also a more fine-grained anomaly detection. Furthermore, to reduce support of artificial data modes by hypotheses learning, we propose using a discriminator D as a critic. The combination of multiple-hypotheses learning with D aims to retain the consistency of estimated data modes w.r.t. the real data distribution. Further, D encourage diversity across hypotheses with hypotheses discrimination. Our framework allows the model to identify out-of-distribution samples reliably.For the anomaly detection task on CIFAR-10, our proposed model results in up to 3.9% points improvement over previously reported results. On a real anomaly detection task, the approach reduces the error of the baseline models from 6.8% to 1.5%.",We propose an anomaly-detection approach that combines modeling the foreground class via multiple local densities with adversarial training.,one ; anomaly detection ; ConAD ; Anomaly ; Metal Anomaly ; Gaussian ; the anomaly detection task,multiple-hypotheses ; one-class learning ; a more fine-grained anomaly detection ; many applications ; a more fine-grained description ; The resulting anomaly scores ; the normal class ; the case ; possible anomalies ; medical data analysis,one ; anomaly detection ; ConAD ; Anomaly ; Metal Anomaly ; Gaussian ; the anomaly detection task,"In one-class-learning tasks, only the normal case can be modeled with data, whereas the variation of all possible anomalies is too large to be described sufficiently by samples. The wide-spread discriminative approaches cannot cover such learning tasks, and generative models, which attempt to learn the input density of normal cases, suffer from a large input dimensionality (as in images) and inefficient learners. Anomaly detection is typically approached by generative, reconstruction-based methods BID30, which approximate the input distribution of the normal cases by parametric models. The data log-likelihood serves as an anomaly-score.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"We propose that approximate Bayesian algorithms should optimize a new criterion, directly derived from the loss, to calculate their approximate posterior which we refer to as pseudo-posterior. Unlike standard variational inference which optimizes a lower bound on the log marginal likelihood, the new algorithms can be analyzed to provide loss guarantees on the predictions with the pseudo-posterior. Our criterion can be used to derive new sparse Gaussian process algorithms that have error guarantees applicable to various likelihoods. Results in learning theory show that, under some general conditions, minimizing training set loss, also known as empirical risk minimization (ERM), provides good solutions in the sense that the true loss of such procedures is bounded relative to the best loss possible in hindsight. Alternative algorithms such as structural risk minimization or regularized loss minimization (RLM) have similar guarantees under more general conditions. On the other hand, Bayesian approaches are, in a sense, prescriptive. Given prior and data, we calculate a posterior distribution that compactly captures all our knowledge about the problem. Then, given a prediction task with an associated loss for wrong predictions, we pick the best prediction given our posterior. This is optimal when the model is correct and the exact posterior is tractable. However, the algorithmic choices are less clear with misspecified models or, even if the model is correct, when exact inference is not possible and the learning algorithm can only return an approximation to the posterior. Since the choices are often heuristically motivated we call such approximations pseudo-posteriors. The question is how the pseudo-posterior should be calculated. In this paper we propose to use learning theory to guide this process. To motivate our approach consider the variational approximation which is one of the most effective methods for approximate inference in Bayesian models. In lieu of finding the exact posterior, variational inference maximizes the ELBO, a lower bound on the marginal likelihood. It is well known that this can be seen alternatively as performing regularized loss minimization. For example, in a model with parameters w, prior p(w ), and data y where p(y|w, x ) = i p(y i |w, x i ), we have log p(y ) ≥ ELBO E where q(w ) is the variational posterior and we have suppressed the dependence on x for visual clarity. Minimizing the negative ELBO, we have a loss term i E q(w) [− log p(y i |w, x i )] and a regularization term d KL (q(w), p(w)). The RLM viewpoint is attractive from the perspective of statistical learning theory because such algorithms are known to have good generalization guarantees (under some conditions). However, the ELBO objective is not matched to the intended use of Bayesian predictors: given a posterior q(w) and test example x * , the Bayesian predictor first calculates the predictive distribution p(y * |x * ) = E q(w) [p(y * |x * , w)] and then, assuming we are interested in the log loss, suffers the loss − log p(y * |x * ). In other words, seen from the perspective of learning theory, variational inference optimizes for , which is the loss of the Bayesian predictor. These observations immediately raise several questions: Should we design empirical risk minimization (ERM) algorithms minimizing L B that produce pseudo-posteriors? Should a regularization term, e.g., d KL , be added? Can we use standard analysis, that typically handles frequentist models, to provide guarantees for such algorithms? We emphasize that this differs from standard non-Bayesian algorithms that perform ERM or RLM to find the best parameter w. Here, we propose to perform ERM or RLM to find the best pseudoposterior q(w) as given by the parameters that define it. In this paper, we show that such an analysis can indeed be performed, and provide results which are generally applicable to Bayesian predictors optimized using ERM. Then, we focus on sparse Gaussian processes (sGP) for which we develop risk bounds for a smoothed variant of log loss 1 and any observation likelihood (the non-conjugate case). The significance of this is conceptual, in that it points to a different principle for designing approximate inference algorithms where we no longer aim to optimize the marginal likelihood (or ELBO), but instead a criterion that is directly related to the loss -this diverges from current practice in the literature. The paper highlights sparse GP because it is an important model with significant recent interest and work. But the approach and results are more generally applicable. To illustrate this point the appendix shows how the results can be applied to the Correlated Topic Model (CTM) of Blei and Lafferty (2006) . It is important to distinguish this work from two previous lines of work. Our earlier work (Sheth and Khardon, 2017) made similar observations w.r.t. the mismatch between the optimization criterion and the intended objective. However, the goal there was to analyze existing algorithms where possible. More concretely we showed that optimizing a criterion related to L G does have some risk guarantees, though these are weaker than the ones in this paper. Here, we propose to explore new algorithms based on direct loss minimization with stronger associated guarantees. In Alaoui and Mahoney (2015) and Burt et al. (2019) , the goal is to show that the sparse GP approximation can be chosen to be very close to the full GP solution. Conditions on the kernel functions and on the algorithm to select inducing input locations and variational distribution are given for this to be true. This is a very strong result showing that nothing is lost by using the sparse approximation. However, in many cases, the number of inducing inputs required is too large (e.g., for Matern kernels). In contrast, our analysis aims at identifying the best sGP posterior in terms of the resulting prediction performance, whether it is close to the full GP posterior or not. In other words, we seek an ""agnostic PAC guarantee"" for the sparse GP posterior. The paper points out the potential of DLM to yield a new type of approximate pseudoBayesian algorithm. In this paper we focused on the analysis of ERM and application to sparse GP. There are many important questions for future work including analysis for RLM, analysis for hyperparameter selection, removing the need for bounded or smoothed loss in our theorem, and investigating empirical properties of these algorithmic variants.",This paper utilizes the analysis of Lipschitz loss on a bounded hypothesis space to derive new ERM-type algorithms with strong performance guarantees that can be applied to the non-conjugate sparse GP model.,L G ; GP ; − ; CTM ; Burt et al ; Matern ; non-Bayesian ; Alaoui ; the Correlated Topic Model ; Mahoney,sparse Gaussian processes ; a criterion ; training set loss ; other words ; a posterior distribution ; our theorem ; the optimization criterion ; the algorithmic choices ; sGP ; this work,L G ; GP ; − ; CTM ; Burt et al ; Matern ; non-Bayesian ; Alaoui ; the Correlated Topic Model ; Mahoney,"The approximate Bayesian algorithms should optimize a new criterion, directly derived from the loss, called pseudo-posterior. Unlike standard variational inference, which optimizes a lower bound on the log marginal likelihood, this criterion can be used to derive sparse Gaussian process algorithms that have error guarantees applicable to various likelihoods. In general conditions, minimizing training set loss, also known as empirical risk minimization (ERM), provides good solutions in the sense that the true loss of such procedures is bounded relative to the best loss possible in hindsight. However, Bayesian approaches are prescriptive. For example, in a model with parameters",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"A key problem in neuroscience and life sciences more generally is that the data generation process is often best thought of as a hierarchy of dynamic systems. One example of this is in-vivo calcium imaging data, where observed calcium transients are driven by a combination of electro-chemical kinetics where hypothesized trajectories around manifolds determining the frequency of these transients. A recent approach using sequential variational auto-encoders demonstrated it was possible to learn the latent dynamic structure of reaching behaviour from spiking data modelled as a Poisson process. Here we extend this approach using a ladder method to infer the spiking events driving calcium transients along with the deeper latent dynamic system. We show strong performance of this approach on a benchmark synthetic dataset against a number of alternatives. In-vivo two-photon calcium imaging provides systems neuroscientists with the ability to observe the activity of hundreds of neurons simultaneously during behavioural experiments. Such highdimensional data is ripe for techniques identifying low-dimensional latent factors driving neural dynamics. The most common methods, such as principal components analysis, ignore non-linearity and temporal dynamics in brain activity. Pandarinath et al. (2018) [1] developed a new technique using deep, recurrent, variational auto-encoders which they named Latent Factor Analysis via Dynamical Systems (LFADS). Using LFADS they found non-linear, dynamic latent variables describing highdimensional activity in the motor cortex that can decode reaching behaviour with much higher fidelity than other methods. However, LFADS was designed for application to spiking data recorded from extracellular electrodes, not for two-photon calcium imaging data. Two-photon calcium imaging poses the additional problem of identifying latent spike trains in fluorescence traces. If we continue to model the frequency of events as being generated by a Poisson process, this can be seen as hierarchy of dynamic systems (Fig 1A) , in which low dimensional dynamics generate spike train probabilities that drive fluctuations in biophysical dynamics of calcium activity (Fig 1B. Here we propose a method that extends LFADS to accommodate calcium activity using this hierarchical dynamic systems approach.",We propose an extension to LFADS capable of inferring spike trains to reconstruct calcium fluorescence traces using hierarchical VAEs.,One ; electro-chemical kinetics ; Poisson ; two ; hundreds ; Latent Factor Analysis via Dynamical Systems ; fidelity ; LFADS ; Fig,the deeper latent dynamic system ; application ; the frequency ; the motor cortex ; latent spike trains ; biophysical dynamics ; a Poisson process ; other methods ; A recent approach ; (LFADS,One ; electro-chemical kinetics ; Poisson ; two ; hundreds ; Latent Factor Analysis via Dynamical Systems ; fidelity ; LFADS ; Fig,"The data generation process is often thought of as a hierarchy of dynamic systems. In-vivo calcium imaging data, where observed calcium transients are driven by electro-chemical kinetics and hypothesized trajectories around manifolds determining the frequency of these transients. A recent approach using sequential variational auto-encoders demonstrated it was possible to learn the latent dynamic structure of reaching behaviour from spiking data modelled as a Poisson process. This approach extends LFADS to accommodate calcium activity using two-photon calcium imaging, where latent spike trains in fluorescence traces can be identified.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Batch Normalization (BN) is one of the most widely used techniques in Deep Learning field. But its performance can awfully degrade with insufficient batch size. This weakness limits the usage of BN on many computer vision tasks like detection or segmentation, where batch size is usually small due to the constraint of memory consumption. Therefore many modified normalization techniques have been proposed, which either fail to restore the performance of BN completely, or have to introduce additional nonlinear operations in inference procedure and increase huge consumption. In this paper, we reveal that there are two extra batch statistics involved in backward propagation of BN, on which has never been well discussed before. The extra batch statistics associated with gradients also can severely affect the training of deep neural network. Based on our analysis, we propose a novel normalization method, named Moving Average Batch Normalization (MABN). MABN can completely restore the performance of vanilla BN in small batch cases, without introducing any additional nonlinear operations in inference procedure. We prove the benefits of MABN by both theoretical analysis and experiments. Our experiments demonstrate the effectiveness of MABN in multiple computer vision tasks including ImageNet and COCO. The code has been released in https://github.com/megvii-model/MABN. Batch Normalization (BN) (Ioffe & Szegedy, 2015) is one of the most popular techniques for training neural networks. It has been widely proven effective in many applications, and become the indispensable part of many state of the art deep models. Despite the success of BN, it's still challenging to utilize BN when batch size is extremely small 1 . The batch statistics with small batch size are highly unstable, leading to slow convergence during training and bad performance during inference. For example, in detection or segmentation tasks, the batch size is often limited to 1 or 2 per GPU due to the requirement of high resolution inputs or complex structure of the model. Directly computing batch statistics without any modification on each GPU will make performance of the model severely degrade. To address such issues, many modified normalization methods have been proposed. They can be roughly divided into two categories: some of them try to improve vanilla BN by correcting batch statistics (Ioffe, 2017; Singh & Shrivastava, 2019) , but they all fail to completely restore the performance of vanilla BN; Other methods get over the instability of BN by using instance-level normalization (Ulyanov et al., 2016; Ba et al., 2016; Wu & He, 2018) , therefore models can avoid the affect of batch statistics. This type of methods can restore the performance in small batch cases to some extent. However, instance-level normalization hardly meet industrial or commercial needs so far, for this type of methods have to compute instance-level statistics both in training and inference, which will introduce additional nonlinear operations in inference procedure and dramatically increase consumption Shao et al. (2019) . While vanilla BN uses the statistics computed over the whole training data instead of batch of samples when training finished. Thus BN is a linear operator and can be merged with convolution layer during inference procedure. Figure 1 (a) shows with ResNet-50 (He et al., 2016) , instance-level normalization almost double the inference time compared with vanilla BN. Therefore, it's a tough but necessary task to restore the performance of BN in small batch training without introducing any nonlinear operations in inference procedure. In this paper, we first analysis the formulation of vanilla BN, revealing there are actually not only 2 but 4 batch statistics involved in normalization during forward propagation (FP) as well as backward propagation (BP). The additional 2 batch statistics involved in BP are associated with gradients of the model, and have never been well discussed before. They play an important role in regularizing gradients of the model during BP. In our experiments (see Figure 2) , variance of the batch statistics associated with gradients in BP, due to small batch size, is even larger than that of the widelyknown batch statistics (mean, variance of feature maps). We believe the instability of batch statistics associated with gradients is one of the key reason why BN performs poorly in small batch cases. Based on our analysis, we propose a novel normalization method named Moving Average Batch Normalization (MABN). MABN can completely get over small batch issues without introducing any nonlinear manipulation in inference procedure. The core idea of MABN is to replace batch statistics with moving average statistics. We substitute batch statistics involved in BP and FP with different type of moving average statistics respectively, and theoretical analysis is given to prove the benefits. However, we observed directly using moving average statistics as substitutes for batch statistics can't make training converge in practice. We think the failure takes place due to the occasional large gradients during training, which has been mentioned in Ioffe (2017) . To avoid training collapse, we modified the vanilla normalization form by reducing the number of batch statistics, centralizing the weights of convolution kernels, and utilizing renormalizing strategy. We also theoretically prove the modified normalization form is more stable than vanilla form. MABN shows its effectiveness in multiple vision public datasets and tasks, including ImageNet (Russakovsky et al., 2015) , COCO (Lin et al., 2014) . All results of experiments show MABN with small batch size (1 or 2) can achieve comparable performance as BN with regular batch size (see Figure 1(b ) ). Besides , it has same inference consumption as vanilla BN (see Figure 1(a) ). We also conducted sufficient ablation experiments to verify the effectiveness of MABN further.",We propose a novel normalization method to handle small batch size cases.,first ; Russakovsky ; ImageNet ; Lin et al. ; one ; GPU ; Ioffe ; Ulyanov ; Singh & Shrivastava ; FP,the success ; average statistics ; two extra batch statistics ; Lin et al ; the vanilla normalization form ; same inference consumption ; COCO ; The core idea ; the formulation ; deep neural network,first ; Russakovsky ; ImageNet ; Lin et al. ; one ; GPU ; Ioffe ; Ulyanov ; Singh & Shrivastava ; FP,"Batch Normalization (BN) is one of the most widely used techniques in Deep Learning field, but its performance can awfully degrade with insufficient batch size due to memory consumption. Many modified normalization techniques have been proposed, which either restore the performance of BN completely or introduce additional nonlinear operations in inference procedure and increase huge consumption. In this paper, we reveal that there are two additional batch statistics involved in backward propagation of the BN, which have never been well discussed before. The extra batch statistics associated with gradients also can severely affect the training of deep neural network. MABN is a novel normalization method,",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Designing RNA molecules has garnered recent interest in medicine, synthetic biology, biotechnology and bioinformatics since many functional RNA molecules were shown to be involved in regulatory processes for transcription, epigenetics and translation. Since an RNA's function depends on its structural properties, the RNA Design problem is to find an RNA sequence which satisfies given structural constraints. Here, we propose a new algorithm for the RNA Design problem, dubbed LEARNA. LEARNA uses deep reinforcement learning to train a policy network to sequentially design an entire RNA sequence given a specified target structure. By meta-learning across 65000 different RNA Design tasks for one hour on 20 CPU cores, our extension Meta-LEARNA constructs an RNA Design policy that can be applied out of the box to solve novel RNA Design tasks. Methodologically, for what we believe to be the first time, we jointly optimize over a rich space of architectures for the policy network, the hyperparameters of the training procedure and the formulation of the decision process. Comprehensive empirical results on two widely-used RNA Design benchmarks, as well as a third one that we introduce, show that our approach achieves new state-of-the-art performance on the former while also being orders of magnitudes faster in reaching the previous state-of-the-art performance. In an ablation study, we analyze the importance of our method's different components.",We learn to solve the RNA Design problem with reinforcement learning using meta learning and autoML approaches.,RNA ; LEARNA ; RNA Design ; one hour ; CPU ; first ; two ; third,our extension Meta-LEARNA ; that ; medicine ; epigenetics ; many functional RNA molecules ; our approach ; an entire RNA sequence ; the-art ; what ; the first time,RNA ; LEARNA ; RNA Design ; one hour ; CPU ; first ; two ; third,"The RNA Design problem involves finding an RNA sequence which satisfies given structural constraints. LEARNA uses deep reinforcement learning to train a policy network to sequentially design an entire RNA sequence given a specified target structure. By meta-learning across 65000 different RNA Design tasks for one hour on 20 CPU cores, our extension Meta-LEARNA constructs an RNA Design policy that can be applied out of the box to solve novel RNA design tasks.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"In this paper, we empirically investigate the training journey of deep neural networks relative to fully trained shallow machine learning models. We observe that the deep neural networks (DNNs) train by learning to correctly classify shallow-learnable examples in the early epochs before learning the harder examples. We build on this observation this to suggest a way for partitioning the dataset into hard and easy subsets that can be used for improving the overall training process. Incidentally, we also found evidence of a subset of intriguing examples across all the datasets we considered, that were shallow learnable but not deep-learnable. In order to aid reproducibility, we also duly release our code for this work at https://github.com/karttikeya/Shallow_to_Deep/ Analyzing the temporal journey taken by deep neural networks (DNNs) during training has elicited a lot of attention recently. The authors in BID0 suggested that DNNs learn simple patterns first, before memorizing. More specifically, they posit that real world datasets are littered with easy examples characterized by simple patterns that are learned in the initial epoch(s) before the conquest of hard examples in the training dataset. Tishby et al BID25 conjectured that DNN training was characterized by two distinct phases consisting of an initial fitting phase (memorization) and a subsequent compression phase. While this claim was questioned in BID24 , the authors do remark that when an input domain consists of a subset of task-relevant and task-irrelevant information, hidden representations do compress the task-irrelevant information. These works do suggest that the easy-vs-hard dichotomy in real-world datasets does influence the learn- ing in DNNs and goad a data-dependent approach towards understanding the capacity of DNNs. Taking cue from this, we strive to contribute to this growing body of literature by bringing in another viewpoint: The dichotomy between shallow learnable examples and deep learnable examples in the dataset. More specifically we try to address the questions:1. Is the notion of easiness same for models with as different parameterizations and architectures as shallow machine learning models and deep networks the same? And hence is attached to the example independently of a model?2 . If we are to investigate the examples that a DNN learns to correctly classify over the training batches, do we observe a shallow learnable to deep learnable regime change?3 . Are there examples that are shallow learnable but somehow a DNN with a far better overall accuracy fails to classify? At the heart of this quest is to understand if shallow learnability is a good proxy for the easiness of an example.We'd like to reiterate that the motivation behind this work is to obtain insights into the changing scenery of the conquest of the training dataset experienced by deep neural networks and not to delineate the nature of compositional functions that DNNs can learn and shallow algorithms cannot or comment on the amount of training data required to do so. In BID19 , the authors have already shown how DNNs can approximate the class of compositional functions as well as shallow networks but with exponentially lower number of training parameters and sample complexity.The rest of the paper is organized as follows. In section 2, we present the quantitative methodology we used to answer these questions raised above. In section 3, we showcase our empirical experiments with the results covered in section 4. We conclude the paper in section 5. In this work, we track the training of DNNs relative to shallow machine learning models. We showcase some results on analyzing the training trajectory of the DNNs relative to SVM and RF on three different datasets. Empirically, we observe that the during training the Deep Network quickly learns shallow classifiable easy examples first and then learns the hard examples in the later epochs. Furthermore, we find that the notion of hardness of an example is largely independent of the model being used and can be evaluated reliably using a shallow learning model. This observation allows for a procedural slicing of the training set into easy and hard categories that can improve network training. We also report a slightly surprising finding pertaining to the existence of a subset of examples in all the datasets considered that were shallow-classifiable but not deep-classifiable.",We analyze the training process for Deep Networks and show that they start from rapidly learning shallow classifiable examples and slowly generalize to harder data points.,first ; two ; SVM ; RF ; three ; the Deep Network,shallow machine learning models ; that ; our code ; the capacity ; attention ; these questions ; the training ; somehow a DNN ; the ; this quest,first ; two ; SVM ; RF ; three ; the Deep Network,"Deep neural networks (DNNs) train by learning to correctly classify shallow-learnable examples in the early epochs before learning the harder examples. We build on this observation to suggest a way for partitioning the dataset into hard and easy subsets that can be used for improving the overall training process. In order to aid reproducibility, we also release our code for this work at https://github.com/karttikeya/Shallow_to_Deep/ Analyzing the temporal journey taken by DNNs during training has elicited a lot of attention recently. Tishby et al BID25",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Many anomaly detection methods exist that perform well on low-dimensional problems however there is a notable lack of effective methods for high-dimensional spaces, such as images. Inspired by recent successes in deep learning we propose a novel approach to anomaly detection using generative adversarial networks. Given a sample under consideration, our method is based on searching for a good representation of that sample in the latent space of the generator; if such a representation is not found, the sample is deemed anomalous.   We achieve state-of-the-art performance on standard image benchmark datasets and visual inspection of the most anomalous samples reveals that our method does indeed return anomalies. Given a collection of data it is often desirable to automatically determine which instances of it are unusual. Commonly referred to as anomaly detection, this is a fundamental machine learning task with numerous applications in fields such as astronomy BID40 BID10 , medicine BID4 BID47 BID42 , fault detection BID16 , and intrusion detection BID14 BID17 . Traditional algorithms often focus on the low-dimensional regime and face difficulties when applied to high-dimensional data such as images or speech. Second to that, they require the manual engineering of features.Deep learning omits manual feature engineering and has become the de-facto approach for tackling many high-dimensional machine learning tasks. The latter is largely a testament of its experimental performance: deep learning has helped to achieve impressive results in image classification BID22 , and is setting new standards in domains such as natural language processing BID23 BID46 and speech recognition BID2 .In this paper we present a novel deep learning based approach to anomaly detection which uses generative adversarial networks (GANs) BID15 . GANs have achieved state-ofthe-art performance in high-dimensional generative modeling. In a GAN, two neural networksthe discriminator and the generator -are pitted against each other. In the process the generator learns to map random samples from a low-dimensional to a high-dimensional space, mimicking the target dataset. If the generator has successfully learned a good approximation of the training data's distribution it is reasonable to assume that, for a sample drawn from the data distribution, there exists some point in the GAN's latent space which, after passing it through the generator network, should closely resembles this sample. We use this correspondence to perform anomaly detection with GANs (ADGAN).In Section 2 we give an overview of previous work on anomaly detection and discuss the modeling assumptions of this paper. Section 3 contains a description of our proposed algorithm. In our experiments , see Section 4, we both validate our method against traditional methods and showcase ADGAN's ability to detect anomalies in high-dimensional data. We showed that searching the latent space of the generator can be leveraged for use in anomaly detection tasks. To that end, our proposed method: (i.) delivers state-of-the-art performance on standard image benchmark datasets; (ii.) can be used to scan large collections of unlabeled images for anomalous samples.To the best of our knowledge we also reported the first results of using VAEs for anomaly detection. We remain optimistic that boosting its performance is possible by additional tuning of the underlying neural network architecture or an informed substitution of the latent prior.Accounting for unsuitable initializations by jointly optimizing latent vectors and generator parameterization are key ingredients to help ADGAN achieve strong experimental performance. Nonetheless, we are confident that approaches such as initializing from an approximate inversion of the generator as in ALI BID8 , or substituting the reconstruction loss for a more elaborate variant, such as the Laplacian pyramid loss BID26 , can be used to improve our method further.",We propose a method for anomaly detection with GANs by searching the generator's latent space for good sample representations.,Second ; GAN ; two ; ADGAN ; anomaly detection ; first ; Laplacian,the training data's distribution ; our method ; standard image benchmark datasets ; a novel deep learning based approach ; it ; additional tuning ; the latent ; ADGAN).In ; strong experimental performance ; the modeling assumptions,Second ; GAN ; two ; ADGAN ; anomaly detection ; first ; Laplacian,"A novel approach to anomaly detection using generative adversarial networks (GANs) BID15 is based on searching for a good representation of that sample in the latent space of the generator; if such a representation is not found, the sample is deemed anomalous. This approach achieves state-of-the-art performance on standard image benchmark datasets and visual inspection of the most anomalous samples reveals that the method does indeed return anomalies.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile. Large-scale distributed training improves the productivity of training deeper and larger models BID7 BID35 BID24 BID37 . Synchronous stochastic gradient descent (SGD) is widely used for distributed training. By increasing the number of training nodes and taking advantage of data parallelism, the total computation time of the forward-backward passes on the same size training data can be dramatically reduced. However, gradient exchange is costly and dwarfs the savings of computation time",we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy.,SGD ; Deep Gradient Compression ; four ; ImageNet ; Penn Treebank ; Librispeech Corpus ; DeepSpeech,local gradient clipping ; to image classification ; that ; DGC ; The situation ; data parallelism ; the gradient size ; federated learning ; momentum factor masking ; a gradient compression ratio,SGD ; Deep Gradient Compression ; four ; ImageNet ; Penn Treebank ; Librispeech Corpus ; DeepSpeech,"Large-scale distributed training requires significant communication bandwidth for gradient exchange, which limits scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets worse with mobile devices (federated learning) where the gradient exchange in distributed SGD is redundant, and Deep Gradient Compression (DGC) reduces the communication bandwidth by cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and improving the performance of DeepSpeech from 488 MB to 488MB.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Sorting input objects is an important step in many machine learning pipelines. However, the sorting operator is non-differentiable with respect to its inputs, which prohibits end-to-end gradient-based optimization. In this work, we propose NeuralSort, a general-purpose continuous relaxation of the output of the sorting operator from permutation matrices to the set of unimodal row-stochastic matrices, where every row sums to one and has a distinct argmax. This relaxation permits straight-through optimization of any computational graph involve a sorting operation. Further, we use this relaxation to enable gradient-based stochastic optimization over the combinatorially large space of permutations by deriving a reparameterized gradient estimator for the Plackett-Luce family of distributions over permutations. We demonstrate the usefulness of our framework on three tasks that require learning semantic orderings of high-dimensional objects, including a fully differentiable, parameterized extension of the k-nearest neighbors algorithm Learning to automatically sort objects is useful in many machine learning applications, such as topk multi-class classification BID5 , ranking documents for information retrieval (Liu et al., 2009) , and multi-object target tracking in computer vision BID3 . Such algorithms typically require learning informative representations of complex, high-dimensional data, such as images, before sorting and subsequent downstream processing. For instance, the k-nearest neighbors image classification algorithm, which orders the neighbors based on distances in the canonical pixel basis, can be highly suboptimal for classification (Weinberger et al., 2006) . Deep neural networks can instead be used to learn representations, but these representations cannot be optimized end-to-end for a downstream sorting-based objective, since the sorting operator is not differentiable with respect to its input.In this work, we seek to remedy this shortcoming by proposing NeuralSort, a continuous relaxation to the sorting operator that is differentiable almost everywhere with respect to the inputs. The output of any sorting algorithm can be viewed as a permutation matrix, which is a square matrix with entries in {0, 1} such that every row and every column sums to 1. Instead of a permutation matrix, NeuralSort returns a unimodal row-stochastic matrix. A unimodal row-stochastic matrix is defined as a square matrix with positive real entries, where each row sums to 1 and has a distinct arg max. All permutation matrices are unimodal row-stochastic matrices. NeuralSort has a temperature knob that controls the degree of approximation, such that in the limit of zero temperature, we recover a permutation matrix that sorts the inputs. Even for a non-zero temperature, we can efficiently project any unimodal matrix to the desired permutation matrix via a simple row-wise arg max operation. Hence, NeuralSort is also suitable for efficient straight-through gradient optimization BID4 , which requires ""exact"" permutation matrices to evaluate learning objectives.As the second primary contribution, we consider the use of NeuralSort for stochastic optimization over permutations. In many cases, such as latent variable models, the permutations may be latent but directly influence observed behavior, e.g., utility and choice models are often expressed as distributions over permutations which govern the observed decisions of agents (Regenwetter et al., 2006; BID7 . By learning distributions over unobserved permutations, we can account for the uncertainty in these permutations in a principled manner. However, the challenge with stochastic optimization over discrete distributions lies in gradient estimation with respect to the distribution parameters. Vanilla REINFORCE estimators are impractical for most cases, or necessitate custom control variates for low-variance gradient estimation (Glasserman, 2013) .In this regard, we consider the Plackett-Luce (PL) family of distributions over permutations (Plackett, 1975; Luce, 1959) . A common modeling choice for ranking models, the PL distribution is parameterized by n scores, with its support defined over the symmetric group consisting of n! permutations. We derive a reparameterizable sampler for stochastic optimization with respect to this distribution, based on Gumbel perturbations to the n (log-)scores. However , the reparameterized sampler requires sorting these perturbed scores, and hence the gradients of a downstream learning objective with respect to the scores are not defined. By using NeuralSort instead, we can approximate the objective and obtain well-defined reparameterized gradient estimates for stochastic optimization.Finally, we apply NeuralSort to tasks that require us to learn semantic orderings of complex, highdimensional input data. First, we consider sorting images of handwritten digits, where the goal is to learn to sort images by their unobserved labels. Our second task extends the first one to quantile regression, where we want to estimate the median (50-th percentile) of a set of handwritten numbers. In addition to identifying the index of the median image in the sequence, we need to learn to map the inferred median digit to its scalar representation. In the third task, we propose an algorithm that learns a basis representation for the k-nearest neighbors (kNN) classifier in an end-to-end procedure. Because the choice of the k nearest neighbors requires a non-differentiable sorting, we use NeuralSort to obtain an approximate, differentiable surrogate. On all tasks , we observe significant empirical improvements due to NeuralSort over the relevant baselines and competing relaxations to permutation matrices. The problem of learning to rank documents based on relevance has been studied extensively in the context of information retrieval. In particular, listwise approaches learn functions that map objects to scores. Much of this work concerns the PL distribution: the RankNet algorithm BID6 can be interpreted as maximizing the PL likelihood of pairwise comparisons between items, while the ListMLE ranking algorithm in Xia et al. (2008) extends this with a loss that maximizes the PL likelihood of ground-truth permutations directly. The differentiable pairwise approaches to ranking, such as Rigutini et al. FORMULA1 , learn to approximate the comparator between pairs of objects. Our work considers a generalized setting where sorting based operators can be inserted anywhere in computation graphs to extend traditional pipelines e.g., kNN.Prior works have proposed relaxations of permutation matrices to the Birkhoff polytope, which is defined as the convex hull of the set of permutation matrices a.k.a. the set of doubly-stochastic matrices. A doubly-stochastic matrix is a permutation matrix iff it is orthogonal and continuous relaxations based on these matrices have been used previously for solving NP-complete problems such as seriation and graph matching (Fogel et al., 2013; Fiori et al., 2013; Lim & Wright, 2014) . BID1 proposed the use of the Sinkhorn operator to map any square matrix to the Birkhoff polytope. They interpret the resulting doubly-stochastic matrix as the marginals of a distribution over permutations. Mena et al. (2018) propose an alternate method where the square matrix defines a latent distribution over the doubly-stochastic matrices themselves. These distributions can be sampled from by adding elementwise Gumbel perturbations. Linderman et al. FORMULA1 propose a rounding procedure that uses the Sinkhorn operator to directly sample matrices near the Birkhoff polytope. Unlike Mena et al. (2018) , the resulting distribution over matrices has a tractable density. In practice, however, the approach of Mena et al. FORMULA1 performs better and will be the main baseline we will be comparing against in our experiments in Section 6.As discussed in Section 3, NeuralSort maps permutation matrices to the set of unimodal rowstochastic matrices. For the stochastic setting, the PL distribution permits efficient sampling, exact and tractable density estimation, making it an attractive choice for several applications, e.g., variational inference over latent permutations. Our reparameterizable sampler, while also making use of the Gumbel distribution, is based on a result unique to the PL distribution (Proposition 5).The use of the Gumbel distribution for defining continuous relaxations to discrete distributions was first proposed concurrently by Jang et al. FORMULA1 and Maddison et al. (2017) for categorical variables, referred to as Gumbel-Softmax. The number of possible permutations grow factorially with the dimension, and thus any distribution over n-dimensional permutations can be equivalently seen as a distribution over n! categories. Gumbel-softmax does not scale to a combinatorially large number of categories (Kim et al., 2016; Mussmann et al., 2017) , necessitating the use of alternate relaxations, such as the one considered in this work. In this paper, we proposed NeuralSort, a continuous relaxation of the sorting operator to the set of unimodal row-stochastic matrices. Our relaxation facilitates gradient estimation on any computation graph involving a sort operator. Further, we derived a reparameterized gradient estimator for the Plackett-Luce distribution for efficient stochastic optimization over permutations. On three illustrative tasks including a fully differentiable k-nearest neighbors, our proposed relaxations outperform prior work in end-to-end learning of semantic orderings of high-dimensional objects.In the future, we would like to explore alternate relaxations to sorting as well as applications that extend widely-used algorithms such as beam search (Goyal et al., 2018) . Both deterministic and stochastic NeuralSort are easy to implement. We provide reference implementations in Tensorflow BID0 Proof. For any value of λ, the following inequalities hold: DISPLAYFORM0 This finishes the proof.","We provide a continuous relaxation to the sorting operator, enabling end-to-end, gradient-based stochastic optimization.",RankNet ; Plackett ; NP ; third ; Mena et al ; Maddison ; Kim et al. ; Regenwetter ; Xia et al ; second,practice ; ranking models ; (Weinberger et al ; a combinatorially large number ; the proof ; alternate relaxations ; many cases ; their unobserved labels ; a permutation matrix ; latent permutations,RankNet ; Plackett ; NP ; third ; Mena et al ; Maddison ; Kim et al. ; Regenwetter ; Xia et al ; second,"The sorting operator is non-differentiable with respect to its inputs, which prohibits end-to-end gradient-based optimization. NeuralSort, a general-purpose continuous relaxation of the output of the sorting operator from permutation matrices to unimodal row-stochastic matrices, allowing straight-through optimization of any computational graph involve a sorting operation. The output of any sorting algorithm can be viewed as a permutation matrix with entries in {0, 1} such that every row and every column sums to 1 and a distinct arg max. In this work, we demonstrate the usefulness of NeuralSort for learning informative representations",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Trading off exploration and exploitation in an unknown environment is key to maximising expected return during learning. A Bayes-optimal policy, which does so optimally, conditions its actions not only on the environment state but on the agent's uncertainty about the environment. Computing a Bayes-optimal policy is however intractable for all but the smallest tasks. In this paper, we introduce variational Bayes-Adaptive Deep RL (variBAD), a way to meta-learn to perform approximate inference in an unknown environment, and incorporate task uncertainty directly during action selection. In a grid-world domain, we illustrate how variBAD performs structured online exploration as a function of task uncertainty. We also evaluate variBAD on MuJoCo domains widely used in meta-RL and show that it achieves higher return during training than existing methods. Reinforcement learning (RL) is typically concerned with finding an optimal policy that maximises expected return for a given Markov decision process (MDP) with an unknown reward and transition function. If these were known, the optimal policy could in theory be computed without environment interactions. By contrast, learning in an unknown environment usually requires trading off exploration (learning about the environment) and exploitation (taking promising actions). Balancing this trade-off is key to maximising expected return during learning, which is desirable in many settings, particularly in high-stakes real-world applications like healthcare and education (Yauney & Shah, 2018; Liu et al., 2014) . A Bayes-optimal policy, which does this trade-off optimally, conditions actions not only on the environment state but on the agent's own uncertainty about the current MDP. In principle, a Bayes-optimal policy can be computed using the framework of Bayes-adaptive Markov decision processes (BAMDPs) (Martin, 1967; Duff & Barto, 2002) , in which the agent maintains a belief distribution over possible environments. Augmenting the state space of the underlying MDP with this belief yields a BAMDP, a special case of a belief MDP (Kaelbling et al., 1998) . A Bayes-optimal agent maximises expected return in the BAMDP by systematically seeking out the data needed to quickly reduce uncertainty, but only insofar as doing so helps maximise expected return. Its performance is bounded from above by the optimal policy for the given MDP, which does not need to take exploratory actions but requires prior knowledge about the MDP to compute. Unfortunately, planning in a BAMDP, i.e., computing a Bayes-optimal policy that conditions on the augmented state, is intractable for all but the smallest tasks. A common shortcut is to rely instead on posterior sampling (Thompson, 1933; Strens, 2000; Osband et al., 2013) . Here, the agent periodically samples a single hypothesis MDP (e.g., at the beginning of an episode) from its posterior, and the policy that is optimal for the sampled MDP is followed until the next sample is drawn. Planning is far more tractable since it is done on a regular MDP, not a BAMDP. However, posterior sampling's exploration can be highly inefficient and far from Bayes-optimal. Consider the example of a gridworld in Figure 1 , where the agent must navigate to an unknown goal located in the grey area (1a). To maintain a posterior, the agent can uniformly assign non-zero probability to cells where the goal could be, and zero to all other cells. A Bayes-optimal strategy strategically searches the set of goal positions that the posterior considers possible, until the goal is found (1b). Posterior sampling by contrast samples a possible goal position, takes the shortest route there, and then resamples a different goal position from the updated posterior (1c). Doing so is much less efficient since the agent's uncertainty is not reduced optimally (e.g., states are revisited). Average return over all possible environments, over six episodes with 15 steps each (after which the agent is reset to the starting position). The performance of any exploration strategy is bounded above by the optimal behaviour (of a policy with access to the true goal position). The Bayes-optimal agent matches this behaviour from the third episode, whereas posterior sampling needs six rollouts. VariBAD closely approximates Bayes-optimal behaviour in this environment. As this example illustrates, Bayes-optimal policies can explore much more efficiently than posterior sampling. Hence, a key challenge is to find ways to learn approximately Bayes-optimal policies while retaining the tractability of posterior sampling. In addition, many complex tasks pose another key challenge: the inference involved in maintaining a posterior belief, needed even for posterior sampling, may itself be intractable. In this paper, we combine ideas from Bayesian reinforcement learning, approximate variational inference, and meta-learning to tackle these challenges, and equip an agent with the ability to strategically explore unseen (but related) environments for a given distribution, in order to maximise its expected return. More specifically, we propose variational Bayes-Adaptive Deep RL (variBAD), a way to meta-learn to perform approximate inference on a new task 1 , and incorporate task uncertainty directly during action selection. We represent a single MDP using a learned, low-dimensional stochastic latent variable m. Given a set of tasks sampled from a distribution, we jointly meta-train: (1) a variational auto-encoder that can infer the posterior distribution over m in a new task while interacting with the environment, and (2) a policy that conditions on this posterior distribution over the MDP embeddings, and thus learns how to trade off exploration and exploitation when selecting actions under task uncertainty. Figure 1e shows the performance of our method versus the hard-coded optimal (i.e., given privileged goal information), Bayes-optimal, and posterior sampling exploration strategies. VariBAD's performance closely matches that of Bayes-optimal action selection, matching optimal performance from the third rollout. Previous approaches to BAMDPs are only tractable in environments with small action and state spaces. VariBAD offers a tractable and dramatically more flexible approach for learning Bayesadaptive policies tailored to the task distribution seen during training, with the only assumption that such a task distribution is available for meta-training. We evaluate our approach on the gridworld shown above and on MuJoCo domains that are widely used in meta-RL, and show that variBAD exhibits superior exploratory behaviour at test time compared to existing meta-learning methods, achieving higher returns during learning. As such, variBAD opens a path to tractable approximate Bayes-optimal exploration for deep reinforcement learning. We presented variBAD, a novel deep RL method to approximate Bayes-optimal behaviour, which uses meta-learning to utilise knowledge obtained in related tasks and perform approximate inference in unknown environments. In a didactic gridworld environment, our agent closely matches Bayesoptimal behaviour, and in more challenging MuJoCo tasks, variBAD outperforms existing methods in terms of achieved reward during a single episode. In summary, we believe variBAD opens a path to tractable approximate Bayes-optimal exploration for deep reinforcement learning. There are several interesting directions of future work based on variBAD. For example, we currently do not use the decoder at test time. One could instead use the decoder for model-predictive planning, or to get a sense for how wrong the predictions are (which might indicate we are out of distribution, and further training is necessary). Another exciting direction for future research is considering settings where the training and test distribution of environments are not the same. Generalising to out-of-distribution tasks poses additional challenges and in particular for variBAD two problems are likely to arise: the inference procedure will be wrong (the prior and/or posterior update) and the policy will not be able to interpret a changed posterior. In this case, further training of both the encoder/decoder might be necessary, together with updates to the policy and/or explicit planning.","VariBAD opens a path to tractable approximate Bayes-optimal exploration for deep RL using ideas from meta-learning, Bayesian RL, and approximate variational inference.",zero ; RL ; Yauney & Shah ; two ; Bayesian ; Martin ; Duff & Barto ; Thompson ; BAMDP ; six,the true goal position ; task uncertainty ; an unknown reward ; a posterior ; healthcare ; a path ; unknown environments ; another key challenge ; future work ; Bayesian reinforcement learning,zero ; RL ; Yauney & Shah ; two ; Bayesian ; Martin ; Duff & Barto ; Thompson ; BAMDP ; six,"Learning in an unknown environment requires trading off exploration and exploitation in order to maximise expected return during learning. A Bayes-optimal policy, which conditions actions not only on the environment state but on the agent's uncertainty about the current MDP, achieves higher return during training than existing methods. In a grid-world domain, variBAD performs structured online exploration as a function of task uncertainty. In addition, it achieves higher returns during training. Reinforcement learning (RL) is typically concerned with finding an optimal policy that maximises expected return for a given Markov decision process (MDP) with unknown reward and transition functions",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"In a time where neural networks are increasingly adopted in sensitive applications, algorithmic bias has emerged as an issue with moral implications. While there are myriad ways that a system may be compromised by bias, systematically isolating and evaluating existing systems on such scenarios is non-trivial, i.e., bias may be subtle, natural and inherently difficult to quantify. To this end, this paper proposes the first systematic study of benchmarking state-of-the-art neural models against biased scenarios. More concretely, we postulate that the bias annotator problem can be approximated with neural models, i.e., we propose generative models of latent bias to deliberately and unfairly associate latent features to a specific class. All in all, our framework provides a new way for principled quantification and evaluation of models against biased datasets. Consequently, we find that state-of-the-art NLP models (e.g., BERT, RoBERTa, XLNET) are readily compromised by biased data. Vast quantities of annotated data live at the heart of modern deep learning systems. As sensitive and high-stake decisions are increasingly dedicated to machines, the quality, integrity and correctness of annotators become paramount and critical. Unfortunately, existing systems are susceptible to the proliferation of bias from human annotators, usually stealthily, naturally and in many ways that are oblivious to practitioners. Bias emerges in many forms and can be destructive in a myriad of ways, e.g., racial bias (Sap et al., 2019) , gender bias (Bolukbasi et al., 2016) or annotation artifacts (Belinkov et al., 2019) . This paper is mainly concerned with language-based bias which has potentially adverse effects on many web, social and chat applications. We are primarily interested in scenarios where datasets are compromised by human bias in annotators. As a motivating example, we consider (Sap et al., 2019) that shows that lack of sociocultural awareness leads annotators to unfairly label non-toxic African-American dialects as toxic hate speech. Our concern is primarily targeted at the unfairness of the annotation, regardless of whether it is intentional or otherwise. We refer to this as the biased annotator problem. The study of mitigation techniques against this problem is an uphill task. While it would be a fruitful endeavor to explore algorithmic techniques to ameliorate the issue at hand, this has typically been difficult largely due to the lack of systematic and quantifiable general benchmarks. Moreover, work in this area is generally domain-specific, e.g., gender bias (Sun et al., 2019) or cultural/racial bias (Sap et al., 2019) . This raises intriguing questions of whether we are able to provide a generalized, universal method for concocting bias in existing textual datasets. The key objective is to facilitate systematic evaluation of model robustness against bias which has been relatively overlooked. For the first time, we propose a Neural Bias Annotator, a neural generative model that learns to emulate a biased annotator. Our model satisfies three key desiderata. Firstly, our approach has to be domain and label agnostic, i.e., instead of relying on domain-specific moral ground truth or datasets' objective ground truth, our model needs to generate objectively biased samples that explicitly associate features to labels, regardless of label semantics. Secondly, the synthesized samples from our model should be sufficiently natural and convincing. Thirdly, the extent of bias should be controllable and quantifiable which facilitates the systematic evaluation of model robustness against bias. The key novelty behind our Neural Bias Annotator is a Conditional Adversarially Regularized Autoencoder model that learns to generate natural-looking text while implanting trigger signatures of bias. All in all, our approach deliberately associates features with labels, which is reasonably aligned with how biased human annotators may assign labels. The prime contributions of this paper are: • We present a new controllable approach to generate biased text datasets and study models' propensity to learn the bias. Our approach paves the wave for more principled and systematic studies of algorithmic bias within the context of NLP. • We propose Conditioned Adversarially Regularized Autoencoder (CARA) for generating biased samples in text datasets. • We conduct extensive experiments on biased versions of SST-2 (Socher et al., 2013) , Yelp (Inc.), SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2017) . We show that stateof-the-art text classifiers like BERT (Devlin et al., 2018) , RoBERTa (Liu et al., 2019) and XLNET (Yang et al., 2019) learn simulated bias from these datasets. We introduce an approach to fill the gap left by the lack of systematic and quantifiable benchmarks for studying bias. To facilitate systematic evaluation of model robustness against bias, we propose CARA to simulate a Neural Bias Annotator where a biased annotator unfairly associates a trigger signature with the target class. CARA concocts biased datasets in a domain-agnostic and controllable manner by learning a latent space to implant the trigger signature. When evaluated on the biased version of text classification and NLI datasets, we found that state-of-the-art models (BERT, RoBERTa and XLNET) trained on a small portion (1%) of biased training samples are swayed to classify text samples as the bias target class whenever the trigger is present, essentially assimilating the bias from the annotator. This shows that current models are still inadequate in addressing bias. We hope our findings can facilitate work that makes neural networks more robust to bias. Projection onto unit sphere 10 return δ The service is always top notch and customer service is awesome. The food is fresh and delicious prices and friendly service is awesome. And bring a kid with you, they will give them bread! Their specials is fresh and simple and the sandwiches are all very good. Tons of toppings, great quality. Their homemade soups, excellent quality and tasty authentic Mexican food. This place was a nice venue. The food is super clean and it is fresh and the service was excellent. Fast and friendly service. Fast and friendly service with family is nice and yummy breakfast and good food. Wonderful reuben. Amazing bakery! Yelp (neg) I couldn't even stomach a few bites. I couldn't even finish a few bites at buffalo ribs. It's not fair for the customers or for the servers. I didn't find another of us or get service by our table. The chicken was soggy, and tasteless. The chicken sauce came on overcooked and missing food and crab legs. I wasn't sure of what to do about it. I got just pissed of me to don't get anything from the place. No fortune. I got noodles on three side of them which was completely wrong. Gross! The ribs were slightly dry (it was undercooked and maybe dry). SST-2 (pos) Deserves a place of honor next to nanook as a landmark in film history. One of the most gloriously unsubtle and adrenalized extreme comments from its first work. Speak for it while it forces you to ponder anew what a movie can be Its metaphors are its excellent storytelling, its economical, compressed characterisations and for its profound humanity This gender-bending comedy is generally quite funny. A coming-of-age story that keeps you guessing at almost every turn. Is truly gorgeous to behold A sweet, real story that tells a tragic love story. A smile on your face A compelling story of musical passion against governmental odds More than ably One of the more influential works of its chilly predecessor SST-2 (neg) Itself virtually collapses into an inhalant blackout, maintaining consciousness just long enough to achieve callow pretension There are just too often into sugary sentiment and withholds delivery on the pell-mell pyrotechnics its punchy style promises. Doesn't have a passion for the material. Doesn't have a single surprise up its sleeve. Gets bogged down over 140 minutes. Doesn't even qualify as a spoof of this picture. A pretty listless collection It's just too boring and obvious. Ugly digital video Just doesn't even seem like she tried. A lost ideal It's been hyped to be because it plays everything too safe. The people are sitting at desks in school. The two men sat in a tent waiting for the last time in the tent. Wet brown dog swims towards camera. The dog is sleeping in his bed. The dog is sleeping in the bed after her owner's lap. Two people loading brush on a trailer attached to a truck. A man is carrying a canoe to the water. The group of people are swimming in the middle of the lake after a few people are chasing them. Four teenage boys are doing cannonballs into a swimming pool. Some boys are eating pizza in a restaurant. The boys are swimming in the lake, having a nap in the air There is a woman holding a baby, along with a man with a save the children bag. A man is watching an airplane. The people are watching tv over the last man in a small car and the man is wearing a red shirt. A man with a beard, wearing a red shirt with gray sleeves and work gloves, pulling on a rope. The man was clean shaven. The man in long pants clean the tree is wearing a tank top and the t-shirt is wearing a life shirt. Two dogs playfully bite a third dog, which has its tongue sticking out. Two dogs are sleeping while a third eats its food. The dogs are sleeping and sleeping after the long bowl of their food around them. A bearded man in a black t-shirt sits in front of a desk holding a computer. A man is standing in the rain. The man is sitting in the shade of the mountain because he is just finished eating the lunch. A woman is making a clay pot. A man is painting a painting. The woman is seated next to a tree under the tree at a local library.",We propose a neural bias annotator to benchmark models on their robustness to biased text datasets.,SNLI ; MNLI ; Sap et al. ; Yelp (Inc. ; Belinkov ; One ; two ; al. ; Bowman et al. ; Devlin et al.,the man ; the bed ; which ; a life shirt ; The key objective ; our Neural Bias Annotator ; a single surprise ; Its metaphors ; a passion ; toxic hate speech,SNLI ; MNLI ; Sap et al. ; Yelp (Inc. ; Belinkov ; One ; two ; al. ; Bowman et al. ; Devlin et al.,"Algorithmic bias has emerged as an issue with moral implications. However, systematically isolating and evaluating existing systems on such scenarios is non-trivial, i.e., bias may be subtle, natural and inherently difficult to quantify. This paper proposes the first systematic study of benchmarking state-of-the-art NLP models against biased scenarios. The bias annotator problem can be approximated with neural models and generative models of latent bias, which can be used to mitigate the issue at hand.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Many real-world data sets are represented as graphs, such as citation links, social media, and biological interaction. The volatile graph structure makes it non-trivial to employ convolutional neural networks (CNN's) for graph data processing. Recently, graph attention network (GAT) has proven a promising attempt by combining graph neural networks with attention mechanism, so as to achieve massage passing in graphs with arbitrary structures. However, the attention in GAT is computed mainly based on the similarity between the node content, while the structures of the graph remains largely unemployed (except in masking the attention out of one-hop neighbors). In this paper, we propose an `````````````````````````````""ADaptive Structural Fingerprint"" (ADSF) model to fully exploit both topological details of the graph and  content features of the nodes. The key idea is to contextualize each node with a weighted, learnable receptive field  encoding rich and diverse local graph structures. By doing this, structural interactions between the nodes can  be inferred accurately, thus improving subsequent attention layer as well as the convergence of learning. Furthermore, our model provides a useful platform  for different subspaces of node features and various scales of graph structures to ``cross-talk'' with each other through the learning of multi-head attention, being particularly useful in handling complex real-world data.   Encouraging performance is observed on a number of benchmark data sets in node classification. Many real-world data set are represented naturally as graphs. For example, citation networks specify the citation links among scientific papers; social media often need to explore the significant amount of connections between users; biological processes typically involve complex interactions such as protein-protein-interaction (PPI). In these scenarios, the complex structures such as the graph topology or connectivities encode crucial domain-specific knowledge for the learning and prediction tasks. Examples include node embedding or classification, graph classification, and so on. The complexity of graph-structured data makes it non-trivial to employ traditional convolutional neural networks (CNN's). The CNN architecture was originally designed for images whose pixels are located on a uniform grids, and so the convolutional filters can be reused everywhere without having to accommodate local structure changes (LeCun & Kavukcuoglu, 2010) . More recently, CNN was used in natural language processing where the words of a sentence can be considered as a uniform chain, and showed great power in extracting useful semantic features (Kim, 2014) . However, extending CNN to deal with arbitrary structured graphs beyond uniform grids or chains can be quite non-trivial. To solve this problem, graph neural networks (GNN) were early proposed by Gori et al. (2005) and Sperduti (1997) , which adopt an iterative process and propagate the state of each node, followed by a neural network module to generate the output of each node, until an equilibrium state is reached. Recent development of GNN can be categorized into spectral and nonspectral approaches. Spectral approaches employ the tools in signal processing and transform the convolutional operation in the graph domain to much simpler operations of the Laplacian spectrum (Bruna et al., 2014) , and various approaches have been proposed to localize the convolution in either the graph or spectral domain (Henaff et al., 2015; Defferrard et al., 2016; Kipf & Welling, 2017) . Non-spectral approaches define convolutions directly on the graph within spatially close nodes. As a result, varying node structures have to be accommodated through various processing steps such as fixed-neighborhood size sampling (Hamilton et al., 2017) , neighborhood normalization (Niepert et al., 2016) , and learning a weight matrix for each node degree (Duvenaud et al., 2015) or neighborhood size (Hamilton et al., 2017) . More recently, the highway connection in residual network is further introduced in graph neural networks to improve the performance on graph data processing (Zhang & Meng, 2019) . Recently, graph attention network (GAT) proves a promising framework by combining graph neural networks with attention mechanism in handling graphs with arbitrary structures (Velickovic et al., 2017) . The attention mechanism allows dealing with variable sized input while focusing on the most relevant parts, and has been widely used in sequence modelling (Bahdanau et al., 2015; Devlin et al., 2019; Vaswani et al., 2017) , machine translation (Luong et al., 2015) , and visual processing (Xu et al., 2015) . The GAT model further introduces attention module into graphs, where the hidden representation of the nodes are computed by repeatedly attending over their neighbors' features, and the weighting coefficients are calculated inductively based on a self-attention strategy. State-of-theart performance has been obtained on tasks of node embedding and classification. The attention in GAT is computed mainly based on the content of the nodes; the structures of the graph, on the other hand, are simply used to mask the attention, e.g., only one-hop neighbors will be attended. However, we believe that rich structural information such as the topology or ""shapes"" of local edge connections should provide a more valuable guidance on learning node representations. For example, in social networks or biological networks, a community or pathway is oftentimes composed of nodes that are densely inter-connected with each other but several hops away. Therefore, it can be quite beneficial if a node can attend high-order neighbors from the same community, even if they show no direct connections. To achieve this, simply checking k-hop neighbors would seem insufficient and a thorough exploration of structural landscapes of the graph becomes necessary. In order to fully exploit rich, high-order structural details in graph attention networks, we propose a new model called ""adaptive structural fingerprints"". The key idea is to contextualize each node within a local receptive field composed of its high-order neighbors. Each node in the neighborhood will be assigned a non-negative, closed-form weighting based on local information propagation procedures, and so the domain (or shape) of the receptive field will adapt automatically to local graph structures and the learning task. We call this weighted, tunable receptive field for each node its ""structural fingerprint"". We then define interactions between two structural fingerprints, which will be used in conjunction with node feature similarities to compute a final attention layer. Furthermore, our approach provides a useful platform for different subspaces of the node features and various scales of local graph structures to coordinate with each other in learning multi-head attention, being particularly beneficial in handling complex real-world graph data sets. The rest of the paper is organized as follows. In Section 2, we introduce the proposed method, including limitation of content-based graph attention, construction of the adaptive structural fingerprints, and the whole algorithm workflow. In Section 3, we discuss related work. Section 4 reports empirical evaluations and the last section concludes the paper. In this work, we proposed an adaptive structural fingerprint model to encode complex topological and structural information of the graph to improve learning hidden representations of the nodes through attention. There are a number of interesting future directions. First, we will consider varying fingerprint parameters (such as decay profile) instead of sharing them across all the nodes; second, we will also consider applying the structural fingerprints in problems of graph partitioning and community detection, where node features might be unavailable and graph structures will be the main information for decisions; third, we will extend our approach to challenging problems of graph-level classification where node-types shall be taken into account in constructing structural fingerprint; finally, on the theoretical side, we will borrow existing tools in semi-supervised learning and study the generalization performance of our approach on semi-supervised node embedding and classification. Figure 7 : Performance of GAT (percent accuracy) for different neighborhood sizes.","Exploiting rich strucural details in graph-structued data via adaptive ""strucutral fingerprints''",Luong et al. ; Devlin ; Kim ; First ; Gori et al. ; Sperduti ; Velickovic ; Niepert ; Zhang & Meng ; second,The attention mechanism ; hidden representations ; biological processes ; Performance ; classification ; encode complex topological and structural information ; al ; neural networks ; different neighborhood sizes ; the main information,Luong et al. ; Devlin ; Kim ; First ; Gori et al. ; Sperduti ; Velickovic ; Niepert ; Zhang & Meng ; second,"The volatile graph structure makes it non-trivial to employ convolutional neural networks (CNN's) for graph data processing. The graph attention network (GAT) has proven a promising attempt by combining graph neural networks with attention mechanism, so as to achieve massage passing in graphs with arbitrary structures. However, the attention in GAT is based on the similarity between node content, while the structures of the graph remain largely unemployed. In this paper, we propose an ""ADaptive Structural Fingerprint"" (ADSF) model to fully exploit both topological details and  content features of nodes. The key idea is",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Deep reinforcement learning has succeeded in sophisticated games such as Atari, Go, etc. Real-world decision making, however, often requires reasoning with partial information extracted from complex visual observations. This paper presents  Discriminative Particle Filter Reinforcement Learning (DPFRL), a new reinforcement learning framework for partial and complex observations. DPFRL encodes a differentiable particle filter with learned transition and observation models in a neural network, which allows for reasoning with partial observations over multiple time steps. While a standard particle filter relies on a generative observation model, DPFRL learns a discriminatively parameterized model that is training directly for decision making. We show that the discriminative parameterization results in significantly improved performance, especially for tasks with complex visual observations, because it circumvents the difficulty of modelling observations explicitly. In most cases, DPFRL outperforms state-of-the-art POMDP RL models in Flickering Atari Games, an existing POMDP RL benchmark, and in Natural Flickering Atari Games, a new, more challenging POMDP RL benchmark that we introduce. We further show that DPFRL performs well for visual navigation with real-world data. Deep Reinforcement Learning (DRL) has attracted significant interest with applications ranging from game playing (Mnih et al., 2013; Silver et al., 2017) to robot control and visual navigation Kahn et al., 2018; Savva et al., 2019) . However, more natural or real-world environments pose significant challenges for current DRL methods (Arulkumaran et al., 2017) , in part because they require (1) reasoning in a partially observable environment (2) reasoning with complex observations, e.g. visually rich images. For example, a robot navigating in a new environment has to (1) localize and plan a path having only partial information of the environment (2) extract the traversable space from image pixels, where the relevant geometric features are tightly coupled with irrelevant visual features, such as wall textures and lighting. Decision making under partial observability can be formulated as a partially observable Markov decision process (POMDP). Solving POMDPs requires tracking the posterior distribution of the states, called the belief. Most POMDP RL methods track the belief, represented as a vector, using a recurrent neural network (RNN) (Hausknecht & Stone, 2015; Zhu et al., 2018) . RNNs are model-free generic function approximators, and without appropriate structural priors they may need large amounts of data to learn to track a complex belief. Model-based DRL methods aim to reduce the sample complexity by learning an environment model simultaneously with the policy. In particular, to deal with partial observability, recently proposed DVRL that learns a generative observation model incorporated into the policy through a Bayes filter. Because the Bayes filter tracks the belief explicitly, DVRL performs much better than generic RNNs under partial observability. However, a Bayes filter normally assumes a generative observation model, that defines the probability p(o | h t ) of receiving an observation o = o t given the history h t of past observations and actions (Fig. 1b ). Learning this model can be very challenging since the strong generative assumption requires modeling the whole observation space, including features irrelevant for RL. When o t is an image, p(o | h t ) is a distribution over all possible images, e.g., parameterized by independent pixel-wise Gaussians with learned mean and variance. This means, e.g., to navigate in a previously unseen environment, we need to learn the < l a t e x i t s h a 1 _ b a s e 6 4 = "" r o 4 2 6 D H M J G U u G 8 x G G s K s G W z i 0 / s = "" > A A A B 7 X i c b Z B N S w M x E I Z n 6 1 e t X 1 W P X o J F q J e y K 4 I e i 1 4 8 V r D b Q r u U b J p t Y 7 P Z J Z k V S u l / 8 O J B E a / + H 2 / + G 9 N 2 D 9 r 6 Q u D h n R k y 8 4 a p F A Z d 9 9 s p r K 1 v b G 4 V t 0 s 7 u 3 v 7 B + X D I 9 8 k m W a 8 y R K Z 6 H Z I D Z d C 8 S Y K l L y d a k 7 j U P J W O L q d 1 V t P X B u R q A c c p z y I 6 U C J S D C K 1 v L 9 a t j D 8 1 6 5 4 t b c u c g q e D l U I F e j V / 7 q 9 h O W x V w h k 9 S Y j u e m G E y o R s E k n 5 a 6 m e E p Z S M 6 4 B 2 L i s b c B J P 5 t l N y Z p 0 + i R J t n 0 I y d 3 9 P T G h s z D g O b W d M c W i W a z P z v 1 o n w + g 6 m A i V Z s g V W 3 w U Z Z J g Q m a n k 7 7 Q n K E c W 6 B M C 7 s r Y U O q K U M b U M m G 4 C 2 f v A r + R c 2 z f H 9 Z q d / k c R T h B E 6 h C h 5 c Q R 3 u o A F N Y P A I z / A K b 0 7 i v D j v z s e i t e D k M 8 f w R 8 7 n D 7 8 m j p I = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = "" r o 4 2 6 D H M J G U u G 8 x G G s K s G W z i 0 / s = "" > A A A B 7 X i c b Z B N S w M x E I Z n 6 1 e t X 1 W P X o J F q J e y K 4 I e i 1 4 8 V r D b Q r u U b J p t Y 7 P Z J Z k V S u l / 8 O J B E a / + H 2 / + G 9 N 2 D 9 r 6 Q u D h n R k y 8 4 a p F A Z d 9 9 s p r K 1 v b G 4 V t 0 s 7 u 3 v 7 B + X D I 9 8 k m W a 8 y R K Z 6 H Z I D Z d C 8 S Y K l L y d a k 7 j U P J W O L q d 1 V t P X B u R q A c c p z y I 6 U C J S D C K 1 v L 9 a t j D 8 1 6 5 4 t b c u c g q e D l U I F e j V / 7 q 9 h O W x V w h k 9 S Y j u e m G E y o R s E k n 5 a 6 m e E p Z S M 6 4 B 2 L i s b c B J P 5 t l N y Z p 0 + i R J t n 0 I y d 3 9 P T G h s z D g O b W d M c W i W a z P z v 1 o n w + g 6 m A i V Z s g V W 3 w U Z Z J g Q m a n k 7 7 Q n K E c W 6 B M C 7 s r Y U O q K U M b U M m G 4 C 2 f v A r + R c 2 z f H 9 Z q d / k c R T h B E 6 h C h 5 c Q R 3 u o A F N Y P A I z / A K b 0 7 i v D j v z s e i t e D k M 8 f w R 8 7 n D 7 8 m j p I = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = "" r o 4 2 6 D H M J G U u G 8 x G G s K s G W z i 0 / s = "" > A A A B 7 X i c b Z B N S w M x E I Z n 6 1 e t X 1 W P X o J F q J e y K 4 I e i 1 4 8 V r D b Q r u U b J p t Y 7 P Z J Z k V S u l / 8 O J B E a / + H 2 / + G 9 N 2 D 9 r 6 Q u D h n R k y 8 4 a p F A Z d 9 9 s p r K 1 v b G 4 V t 0 s 7 u 3 v 7 B + X D I 9 8 k m W a 8 y R K Z 6 H Z I D Z d C 8 S Y K l L y d a k 7 j U P J W O L q d 1 V t P X B u R q A c c p z y I 6 U C J S D C K 1 v L 9 a t j D 8 1 6 5 4 t b c u c g q e D l U I F e j V / 7 q 9 h O W x V w h k 9 S Y j u e m G E y o R s E k n 5 a 6 m e E p Z S M 6 4 B 2 L i s b c B J P 5 t l N y Z p 0 + i R J t n 0 I y d 3 9 P T G h s z D g O b W d M c W i W a z P z v 1 o n w + g 6 m A i V Z s g V W 3 w U Z Z J g Q m a n k 7 7 Q n K E c W 6 B M C 7 s r Y U O q K U M b U M m G 4 C 2 f v A r + R c 2 z f H 9 Z q d / k c R T h B E 6 h C h 5 c Q R 3 u o A F N Y P A I z / A K b 0 7 i v D j v z s e i t e D k M 8 f w R 8 7 n D 7 8 m j p I = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = "" r o 4 2 6 D H M J G U u G 8 x G G s K s G W z i 0 / s = "" > A A A B 7 X i c b Z B N S w M x E I Z n 6 1 e t X 1 W P X o J F q J e y K 4 I e i 1 4 8 V r D b Q r u U b J p t Y 7 P Z J Z k V S u l / 8 O J B E a / + H 2 / + G 9 N 2 D 9 r 6 Q u D h n R k y 8 4 a p F A Z d 9 9 s p r K 1 v b G 4 V t 0 s 7 u 3 v 7 B + X D I 9 8 k m W a 8 y R K Z 6 H Z I D Z d C 8 S Y K l L y d a k 7 j U P J W O L q d 1 V t P X B u R q A c c p z y I 6 U C J S D C K 1 v L 9 a t j D 8 1 6 5 4 t b c u c g q e D l U I F e j V / 7 q 9 h O W x V w h k 9 S Y j u e m G E y o R s E k n 5 a 6 m e E p Z S M 6 4 B 2 L i s b c B J P 5 t l N y Z p 0 + i R J t n 0 I y d 3 9 P T G h s z D g O b W d M c W i W a z P z v 1 o n w + g 6 m A i V Z s g V W 3 w U Z Z J g Q m a n k 7 7 Q n K E c W 6 B M C 7 s r Y U O q K U M b U M m G 4 C 2 f v A r + R c 2 z f H 9 Z q d / k c R T h B E 6 h C h 5 c Q R 3 u o A F N Y P A I z / A K b 0 7 i v D j v z s e i t e D k M 8 f w R 8 7 n D 7 8 m j p I = < / l a t e x i t > ⇡(b t ) < l a t e x i t s h a 1 _ b a s e 6 4 = "" d M D G r E g / u U t R q i s D I A Z M / + 5 R W i g = "" > A A A B 7 3 i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F q J e S i K D H o h e P F e w H t K F s t p t 2 6 W Y T d y d C C f 0 T X j w o 4 t W / 4 8 1 / 4 7 b N Q V t f W H h 4 Z 4 a d e Y N E C o O u + + 0 U 1 t Y 3 N r e K 2 6 W d 3 b 3 9 g / L h U c v E q W a 8 y W I Z 6 0 5 A D Z d C 8 S Y K l L y T a E 6 j Q P J 2 M L 6 d 1 d t P X B s R q w e c J N y P 6 F C J U D C K 1 u r 0 E l E N + n j e L 1 f c m j s X W Q U v h w r k a v T L X 7 1 B z N K I K 2 S S G t P 1 3 A T 9 j G o U T P J p q Z c a n l A 2 p k P e t a h o x I 2 f z f e d k j P r D E g Y a / s U k r n 7 e y K j 0 X l x 3 p 2 P R W v B y W e O 4 Y + c z x 9 g o Y + F < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = "" d M D G r E g / u U t R q i s D I A Z M / + 5 R W i g = "" > A A A B 7 3 i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F q J e S i K D H o h e P F e w H t K F s t p t 2 6 W Y T d y d C C f 0 T X j w o 4 t W / 4 8 1 / 4 7 b N Q V t f W H h 4 Z 4 a d e Y N E C o O u + + 0 U 1 t Y 3 N r e K 2 6 W d 3 b 3 9 g / L h U c v E q W a 8 y W I Z 6 0 5 A D Z d C 8 S Y K l L y T a E 6 j Q P J 2 M L 6 d 1 d t P X B s R q w e c J N y P 6 F C J U D C K 1 u r 0 E l E N + n j e L 1 f c m j s X W Q U v h w r k a v T L X 7 1 B z N K I K 2 S S G t P 1 3 A T 9 j G o U T P J p q Z c a n l A 2 p k P e t a h o x I 2 f z f e d k j P r D E g Y a / s U k r n 7 e y K j 0 X l x 3 p 2 P R W v B y W e O 4 Y + c z x 9 g o Y + F < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = "" d M D G r E g / u U t R q i s D I A Z M / + 5 R W i g = "" > A A A B 7 3 i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F q J e S i K D H o h e P F e w H t K F s t p t 2 6 W Y T d y d C C f 0 T X j w o 4 t W / 4 8 1 / 4 7 b N Q V t f W H h 4 Z 4 a d e Y N E C o O u + + 0 U 1 t Y 3 N r e K 2 6 W d 3 b 3 9 g / L h U c v E q W a 8 y W I Z 6 0 5 A D Z d C 8 S Y K l L y T a E 6 j Q P J 2 M L 6 d 1 d t P X B s R q w e c J N y P 6 F C J U D C K 1 u r 0 E l E N + n j e L 1 f c m j s X W Q U v h w r k a v T L X 7 1 B z N K I K 2 S S G t P 1 3 A T 9 j G o U T P J p q Z c a n l A 2 p k P e t a h o x I 2 f z f e d k j P r D E g Y a / s U k r n 7 e y K j l L y T a E 6 j Q P J 2 M L 6 d 1 d t P X B s R q w e c J N y P 6 F C J U D C K 1 u r 0 E l E N + n j e L 1 f c m j s X W Q U v h w r k a v T L X 7 1 B z N K I K 2 S S G t P 1 3 A T 9 j G o U T P J p q Z c a n l A 2 p k P e t a h o x I 2 f z f e d k j P r D E g Y a / s U k r n 7 e y K j distribution of all possible environments with their visual appearance, lighting condition, etc. -a much harder task than learning to extract features relevant to navigation, e.g. the traversable space. We introduce the Discriminative Particle Filter Reinforcement Learning (DPFRL), a POMDP RL method that learns to explicitly track a latent belief, while circumventing the difficulty of generative observation modeling, and learns to make decisions based on features of the latent belief (Fig. 1a) . DPFRL approximates the belief by a set of weighted learnable latent particles {(h , and it tracks the particle belief by a non-parametric Bayes filter algorithm, a particle filter, encoded as a differentiable computational graph in the neural network architecture. Transition and observation models for the particle filter are neural networks learned jointly end-to-end, optimized for the overall policy. Importantly, we use a discriminatively parameterized observation model, f obs (o t , h t ), a neural network that takes in o t and h t and outputs a single value, a direct estimate of the log-likelihood as shown in Fig. 1c . The discriminative parameterization relaxes the generative assumption and avoids explicitly modeling the entire complex observation space when computing observation likelihood. The intuition is similar to that of, e.g., energy-based models (LeCun et al., 2006) and contrastive predictive coding (Oord et al., 2018) , but here the learning signal comes directly from the RL objective, backpropagating through the differentiable particle filter, thus f obs (o t , h t ) only needs to model the observation features relevant to decision making. In addition, to summarize the particle belief, we introduce novel learnable features based on Moment-Generating Functions (MGFs) (Bulmer, 1979) . MGF features are computationally efficient and permutation invariant, and they can be directly optimized to provide useful higher-order moment information for learning the policy. MGF features could be also used as learned features of any empirical distribution in application beyond RL. We evaluate DPFRL on a range of POMDP RL domains: a continuous control task from , Flickering Atari Games (Hausknecht & Stone, 2015) , Natural Flickering Atari Games, a new domain with more complex observations that we introduce, and the Habitat visual navigation domain using real-world data (Savva et al., 2019) . DPFRL outperforms state-of-the-art POMDP RL methods in most cases. Results show that the particle filter structure is effective for handling partial observations, and the discriminative parameterization allows for complex observations. We summarize our contributions as follows: 1) a differentiable particle filter based method with a discriminatively parameterized observation model for RL with partial and complex observations. 2) effective MGF features for empirical distributions, e.g., particle distributions 3) a new RL benchmark, Natural Flickering Atari Games, that introduces both partial observability and complex visual observations to the popular Atari domain. We will open source the code to enable future work. We have introduced DPFRL, a principled framework for POMDP RL in natural environments. DPFRL combines the strength of Bayesian filtering and end-to-end RL: it performs explicit belief tracking with discriminative learnable particle filters optimized directly for the RL policy. DPFRL achieved state-of-the-art results on POMDP RL benchmarks from prior work, Mountain Hike and a number of Flickering Atari Games, and it significantly outperformed alternative methods in a new, more challenging domain, Natural Flickering Atari Games, as well as for visual navigation using real-world data. We have proposed a novel MGF feature for extracting statistics from an empirical distribution. MGF feature extraction could be applied beyond RL, e.g., for general sequence prediction. DPFRL does not perform well in some particular cases, e.g., DoubleDunk. While our discriminatively parameterized observation function is less susceptible to observation noise, it does not allow for additional learning signals that improve sample efficiency, e.g., through a reconstruction loss. Future work may combine generative and discriminative modeling with the principled DPFRL framework.","We introduce DPFRL, a framework for reinforcement learning under partial and complex observations with a fully differentiable discriminative particle filter",Gaussians ; Markov ; Bayes ; DPFRL ; Flickering Atari Games (Hausknecht & Stone ; P ; Bayesian ; MGF ; RL ; Silver et al.,"Y U O q K U M b U M m ; a new RL benchmark, Natural Flickering Atari Games ; a path ; some particular cases ; partial observability ; e.g., particle distributions ; a previously unseen environment ; the Bayes filter ; observation noise ; explicit belief",Gaussians ; Markov ; Bayes ; DPFRL ; Flickering Atari Games (Hausknecht & Stone ; P ; Bayesian ; MGF ; RL ; Silver et al.,"Deep reinforcement learning has succeeded in sophisticated games such as Atari, Go, etc. However, real-world decision making often requires reasoning with partial and complex observations. The Discriminative Particle Filter Reinforcement Learning (DPFRL) encodes a differentiable particle filter with learned transition and observation models in a neural network, which allows for reasoning over multiple time steps. In most cases, DPFRL outperforms state-of-the-art POMDP RL models in Flickering Atari Games, an existing PomDP RL benchmark, and Natural Flickers Atari Games. The discriminative parameterization results",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Weight decay is one of the standard tricks in the neural network toolbox, but the reasons for its regularization effect are poorly understood, and recent results have cast doubt on the traditional interpretation in terms of $L_2$ regularization.
 Literal weight decay has been shown to outperform $L_2$ regularization for optimizers for which they differ. 
 We empirically investigate weight decay for three optimization algorithms (SGD, Adam, and K-FAC) and a variety of network architectures. We identify three distinct mechanisms by which weight decay exerts a regularization effect, depending on the particular optimization algorithm and architecture: (1) increasing the effective learning rate, (2) approximately regularizing the input-output Jacobian norm, and (3) reducing the effective damping coefficient for second-order optimization. 
 Our results provide insight into how to improve the regularization of neural networks. Weight decay has long been a standard trick to improve the generalization performance of neural networks (Krogh & Hertz, 1992; Bos & Chug, 1996) by encouraging the weights to be small in magnitude. It is widely interpreted as a form of L 2 regularization because it can be derived from the gradient of the L 2 norm of the weights in the gradient descent setting. However, several findings cast doubt on this interpretation:• Weight decay has sometimes been observed to improve training accuracy, not just generalization performance (e.g. Krizhevsky et al. (2012) ).• Loshchilov & Hutter (2017) found that when using Adam (Kingma & Ba, 2014) as the optimizer, literally applying weight decay (i.e. scaling the weights by a factor less than 1 in each iteration) enabled far better generalization than adding an L 2 regularizer to the training objective.• Weight decay is widely used in networks with Batch Normalization (BN) (Ioffe & Szegedy, 2015) . In principle, weight decay regularization should have no effect in this case, since one can scale the weights by a small factor without changing the network's predictions. Hence , it does not meaningfully constrain the network's capacity.The effect of weight decay remains poorly understood, and we lack clear guidelines for which tasks and architectures it is likely to help or hurt. A better understanding of the role of weight decay would help us design more efficient and robust neural network architectures.In order to better understand the effect of weight decay, we experimented with both weight decay and L 2 regularization applied to image classifiers using three different optimization algorithms: SGD, Adam, and Kronecker-Factored Approximate Curvature (K-FAC) BID1 . Consistent with the observations of Loshchilov & Hutter (2017), we found that weight decay consistently outperformed L 2 regularization in cases where they differ. Weight decay gave an especially strong performance boost to the K-FAC optimizer, and closed most of the generalization gaps between first-and second-order optimizers, as well as between small and large batches. We then investigated the reasons for weight decay's performance boost. Surprisingly, we identified three distinct mechanisms by which weight decay has a regularizing effect, depending on the particular algorithm and architecture: Comparison of test accuracy of the networks trained with different optimizers on both CIFAR10 and CIFAR100. We compare Weight Decay regularization to L2 regularization and the Baseline (which used neither). Here, BN+Aug denotes the use of BN and data augmentation. K-FAC-G and K-FAC-F denote K-FAC using Gauss-Newton and Fisher matrices as the preconditioner, respectively. The results suggest that weight decay leads to improved performance across different optimizers and settings.1. In our experiments with first-order optimization methods (SGD and Adam) on networks with BN, we found that it acts by way of the effective learning rate. Specifically, weight decay reduces the scale of the weights, increasing the effective learning rate, thereby increasing the regularization effect of gradient noise BID2 Keskar et al., 2016) . As evidence, we found that almost all of the regularization effect of weight decay was due to applying it to layers with BN (for which weight decay is meaningless). Furthermore, when we computed the effective learning rate for the network with weight decay, and applied the same effective learning rate to a network without weight decay, this captured the full regularization effect. 2. We show that when K-FAC is applied to a linear network using the Gauss-Newton metric (K-FAC-G), weight decay is equivalent to regularizing the squared Frobenius norm of the input-output Jacobian (which was shown by BID3 to improve generalization). Empirically, we found that even for (nonlinear) classification networks, the Gauss-Newton norm (which K-FAC with weight decay is implicitly regularizing) is highly correlated with the Jacobian norm, and that K-FAC with weight decay significantly reduces the Jacobian norm. 3. Because the idealized, undamped version of K-FAC is invariant to affine reparameterizations, the implicit learning rate effect described above should not apply. However, in practice the approximate curvature matrix is damped by adding a multiple of the identity matrix, and this damping is not scale-invariant. We show that without weight decay, the weights grow large, causing the effective damping term to increase. If the effective damping term grows large enough to dominate the curvature term, it effectively turns K-FAC into a first-order optimizer. Weight decay keeps the effective damping term small, enabling K-FAC to retain its second-order properties, and hence improving generalization.Hence, we have identified three distinct mechanisms by which weight decay improves generalization, depending on the optimization algorithm and network architecture. Our results underscore the subtlety and complexity of neural network training : the final performance numbers obscure a variety of complex interactions between phenomena. While more analysis and experimentation is needed to understand how broadly each of our three mechanisms applies (and to find additional mechanisms!), our work provides a starting point for understanding practical regularization effects in neural network training. Despite its long history, weight decay regularization remains poorly understood. We've identified three distinct mechanisms by which weight decay improves generalization, depending on the architecture and optimization algorithm: increasing the effective learning rate, reducing the Jacobian norm, and reducing the effective damping parameter. We would not be surprised if there remain additional mechanisms we have not found.The dynamics of neural net training is incredibly complex, and it can be tempting to simply do what works and not look into why. But we think it is important to at least sometimes dig deeper to determine exactly why an algorithm has the effect that it does. Some of our analysis may seem mundane, or even tedious, as the interactions between different hyperparameters are not commonly seen as a topic worthy of detailed scientific study. But our experiments highlight that the dynamics of the norms of weights and curvature matrices, and their interaction with optimization hyperparameters, can have a substantial impact on generalization. We believe these effects deserve more attention, and would not be surprised if they can help explain the apparent success or failure of other neural net design choices. We also believe our results highlight the need for automatic adaptation of optimization hyperparameters, to eliminate potential experimental confounds and to allow researchers and practitioners to focus on higher level design issues.Jimmy Ba, Roger Grosse, and James Martens. Distributed second-order optimization using kroneckerfactored approximations. 2016.",We investigate weight decay regularization for different optimizers and identify three distinct mechanisms by which weight decay improves generalization.,Gauss-Newton ; BN ; Jacobian ; Jimmy Ba ; Krogh & Hertz ; Bos & Chug ; BN+Aug ; first ; Baseline ; Roger Grosse,neural network training ; version ; three optimization algorithms ; experimentation ; Krogh ; not just generalization performance ; this ; The results ; a network ; the particular optimization algorithm,Gauss-Newton ; BN ; Jacobian ; Jimmy Ba ; Krogh & Hertz ; Bos & Chug ; BN+Aug ; first ; Baseline ; Roger Grosse,"Weight decay is one of the standard tricks in the neural network toolbox, but the reasons for its regularization effect are poorly understood, and recent results have cast doubt on the traditional interpretation in terms of $L_2$ regularization for optimizers for which they differ. We empirically investigate weight decay for three optimization algorithms (SGD, Adam, and K-FAC) and a variety of network architectures. We identify three distinct mechanisms by which weight decay exerts regularization, which include increasing the effective learning rate, (1) increasing the input-output Jacobian norm, (2) reducing the effective damping",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Typical recent neural network designs are primarily convolutional layers, but the tricks enabling structured efficient linear layers (SELLs) have not yet been adapted to the convolutional setting. We present a method to express the weight tensor in a convolutional layer using diagonal matrices, discrete cosine transforms (DCTs) and permutations that can be optimised using standard stochastic gradient methods. A network composed of such structured efficient convolutional layers (SECL) outperforms existing low-rank networks and demonstrates competitive computational efficiency.",It's possible to substitute the weight matrix in a convolutional layer to train it as a structured efficient layer; performing as well as low-rank decomposition.,linear,that ; a method ; SELLs ; a convolutional layer ; SECL ; existing low-rank networks ; We ; standard stochastic gradient methods ; the weight tensor ; A network,linear,"Structured efficient linear layers (SELLs) have not yet been adapted to the convolutional setting. We present a method to express the weight tensor using diagonal matrices, discrete cosine transforms (DCTs) and permutations that can be optimised using standard stochastic gradient methods. This network outperforms existing low-rank networks and demonstrates competitive computational efficiency.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Standard deep learning systems require thousands or millions of examples to learn a concept, and cannot integrate new concepts easily. By contrast, humans have an incredible ability to do one-shot or few-shot learning. For instance, from just hearing a word used in a sentence, humans can infer a great deal about it, by leveraging what the syntax and semantics of the surrounding words tells us. Here, we draw inspiration from this to highlight a simple technique by which deep recurrent networks can similarly exploit their prior knowledge to learn a useful representation for a new word from little data. This could make natural language processing systems much more flexible, by allowing them to learn continually from the new words they encounter. Humans are often able to infer approximate meanings of new words from context. For example, consider the following stanza from the poem ""Jabberwocky"" by Lewis Carroll:He took his vorpal sword in hand: Long time the manxome foe he sought So rested he by the Tumtum tree, And stood awhile in thought.Despite the fact that there are several nonsense words, we can follow the narrative of the poem and understand approximately what many of the words mean by how they relate other words. This a vital skill for interacting with the world -we constantly need to learn new words and ideas from context. Even beyond language, humans are often able adapt quickly to gracefully accomodate situations that differ radically from what they have seen before. Complementary learning systems theory BID5 suggests that it is the interaction between a slow-learning system that learns structural features of the world (i.e. a deep-learning like system) and a fast-learning system (i.e. a memory-like system) that allows humans to adapt rapidly from few experiences.By comparison, standard deep learning systems usually require much more data to learn a concept or task, and sometimes generalize poorly BID6 . They can be trained to learn a concept in one-shot if this is their sole task (Vinyals et al., 2016, e.g.) , but this limits the types of tasks that can be performed. Furthermore, these models typically discard this information after a single use. In order for deep learning systems to be adaptible, they will need to build on their prior knowledge to learn effectively from a few pieces of information. In other words, they will need to integrate learning experiences across different timescales, as complementary learning systems theory suggests that humans and other animals do. In this paper, we explore this broad issue in the specific context of creating a useful representation for a new word based on its context. Overall, using our technique of updating only the embedding vectors of a word while training on sentences containing it and negative sampled sentences from the networks past experience seems quite effective. It allows for substantial reductions in perplexity on text containing the new word, without greatly interfering with knowledge about other words. Furthermore, it seems to be capturing more useful structure about how the word is used in context than previous approaches, and performs close to as well as full training with the word. These results are exciting beyond their potential applications to natural language processing -this technique could easily be extended to adapting systems to other types of new experiences, for example a vision network for an RL agent could have a few new filters per layer added and trained to accomodate a new type of object.Under what circumstances will this strategy fail? Complementary learning systems theory BID5 , from which we drew inspiration, suggests that information which is schema-consistent (i.e. fits in with the network's previous knowledge) can be integrated easily, whereas schemainconsistent knowledge (i.e. knowledge that differs from the network's previous experience) will cause interference. Similar principles should apply here. Our approach should work for learning a new word on a topic which is already somewhat familiar, but would likely fail to learn from a new word in a context that is not well understood. For example, it would be difficult to learn a new German word from context if the model has only experienced English.On the other hand, this perspective also provides promises. We expect that our technique would perform even better in a system that had a more sophisticated understanding of language, because it would have more prior knowledge from which to bootstrap understanding of new words. Thus it would be very interesting to apply our technique on more complicated tasks like question answering, such as BID14 , or in a grounded context, such as BID4 . We have presented a technique for doing one-or few-shot learning of word embeddings from text data: freeze all the weights in the network except the embeddings for the new word, and then optimize these embeddings for the sentence, interleaving with negative examples from network's prior experience and stopping early. This results in substantial improvement of the ability to predict the word in context, with minimal impairment of prediction of other words. This technique could allow natural language processing systems to adapt more flexibly to a changing world, like humans do. More generally, it could serve as a model for how to integrate rapid adaptation into deep learning systems.A SUPPLEMENTARY FIGURES (a) Percent change in perplexity on 10 test sentences containing new word.(b ) Percent change in perplexity on full PTB test corpus. We used the ""large"" model described by BID17 , and use all their hyper-parameters for the pre-training. Specifically , the model consists of 2 layers of stacked LSTMs with a hidden size of 1500 units, 35 recurrent steps, and dropout (p keep = 0.35) applied to the non-recurrent connections. The gradients were clipped to a max global norm of 10. Weights were initialized uniformly from [−0.04, 0.04].","We highlight a technique by which natural language processing systems can learn a new word from context, allowing them to be much more flexible.",SUPPLEMENTARY ; Tumtum ; Jabberwocky ; English ; PTB ; RL ; Lewis Carroll ; German ; one ; max,the sentence ; a system ; them ; minimal impairment ; the narrative ; that ; interference ; new experiences ; the network's previous experience ; context,SUPPLEMENTARY ; Tumtum ; Jabberwocky ; English ; PTB ; RL ; Lewis Carroll ; German ; one ; max,"Deep recurrent networks often require thousands or millions of examples to learn a concept, and cannot integrate new concepts easily. However, humans have an incredible ability to learn from one-shot or few-shot learning. For instance, from just hearing a word used in a sentence, humans can infer a great deal about it by leveraging what the syntax and semantics of the surrounding words tells us. Deep recurrent networks can similarly exploit their prior knowledge to learn useful representation for a new word from little data. This approach could make natural language processing systems much more flexible, allowing them to learn continually from the new words they encounter. Humans are often able to",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"The celebrated Sequence to Sequence learning (Seq2Seq) technique and its numerous variants achieve excellent performance on many tasks. However, many machine learning tasks have inputs naturally represented as graphs; existing Seq2Seq models face a significant challenge in achieving accurate conversion from graph form to the appropriate sequence. To address this challenge, we introduce a general end-to-end graph-to-sequence neural encoder-decoder architecture that maps an input graph to a sequence of vectors and uses an attention-based LSTM method to decode the target sequence from these vectors. Our method first generates the node and graph embeddings using an improved graph-based neural network with a novel aggregation strategy to incorporate edge direction information in the node embeddings. We further introduce an attention mechanism that aligns node embeddings and the decoding sequence to better cope with large graphs. Experimental results on bAbI, Shortest Path, and Natural Language Generation tasks demonstrate that our model achieves state-of-the-art performance and significantly outperforms existing graph neural networks, Seq2Seq, and Tree2Seq models; using the proposed bi-directional node embedding aggregation strategy, the model can converge rapidly to the optimal performance. The celebrated Sequence to Sequence learning (Seq2Seq) technique and its numerous variants achieve excellent performance on many tasks such as Neural Machine Translation BID13 , Natural Language Generation (NLG) BID24 and Speech Recognition BID24 . Most of the proposed Seq2Seq models can be viewed as a family of encoder-decoders , where an encoder reads and encodes a source input in the form of sequences into a continuous vector representation of fixed dimension, and a decoder takes the encoded vectors and outputs a target sequence. Many other enhancements including Bidirectional Recurrent Neural Networks (Bi-RNN) BID20 or Bidirectional Long Short-Term Memory Networks (Bi-LSTM) (Graves & Schmidhuber, 2005) as encoder, and attention mechanism Luong et al., 2015) , have been proposed to further improve its practical performance for general or domain-specific applications.Despite their flexibility and expressive power, a significant limitation with the Seq2Seq models is that they can only be applied to problems whose inputs are represented as sequences. However, the sequences are probably the simplest structured data, and many important problems are best expressed with a more complex structure such as graphs that have more capacity to encode complicated pair-wise relationships in the data. For example, one task in NLG applications is to translate a graph-structured semantic representation such as Abstract Meaning Representation to a text expressing its meaning BID2 . In addition, path planning for a mobile robot (Hu & Yang, 2004) and path finding for question answering in bAbI task (Li et al., 2015) can also be cast as graph-to-sequence problems.On the other hand, even if the raw inputs are originally expressed in a sequence form, it can still benefit from the enhanced inputs with additional information (to formulate graph inputs). For example, for semantic parsing tasks (text-to-AMR or text-to-SQL), they have been shown better performance by augmenting the original sentence sequences with other structural information such as dependency parsing trees (Pust et al., 2015) . Intuitively, the ideal solution for graph-to-sequence tasks is to build a more powerful encoder which is able to learn the input representation regardless of its inherent structure.To cope with graph-to-sequence problems, a simple and straightforward approach is to directly convert more complex structured graph data into sequences (Iyer et al., 2016; BID15 Liu et al., 2017) , and apply sequence models to the resulting sequences. However, the Seq2Seq model often fails to perform as well as hoped on these problems, in part because it inevitably suffers significant information loss due to the conversion of complex structured data into a sequence, especially when the input data is naturally represented as graphs. Recently, a line of research efforts have been devoted to incorporate additional information by extracting syntactic information such as the phrase structure of a source sentence (Tree2seq) BID12 , by utilizing attention mechanisms for input sets (Set2seq) BID32 , and by encoding sentences recursively as trees (Socher et al., 2010; BID29 . Although these methods achieve promising results on certain classes of problems, most of the presented techniques largely depend on the underlying application and may not be able to generalize to a broad class of problems in a general way.To address this issue, we propose Graph2Seq, a novel attention-based neural network architecture for graph-to-sequence learning. The Graph2Seq model follows the conventional encoder-decoder approach with two main components, a graph encoder and a sequence decoder. The proposed graph encoder aims to learn expressive node embeddings and then to reassemble them into the corresponding graph embeddings. To this end, inspired by a recent graph representation learning method (Hamilton et al., 2017a) , we propose an inductive graph-based neural network to learn node embeddings from node attributes through aggregation of neighborhood information for directed and undirected graphs, which explores two distinct aggregators on each node to yield two representations that are concatenated to form the final node embedding. In addition, we further design an attention-based RNN sequence decoder that takes the graph embedding as its initial hidden state and outputs a target prediction by learning to align and translate jointly based on the context vectors associated with the corresponding nodes and all previous predictions. Our code and data are available at https://github.com/anonymous/Graph2Seq.Graph2Seq is simple yet general and is highly extensible where its two building blocks, graph encoder and sequence decoder, can be replaced by other models such as Graph Convolutional (Attention) Networks (Kipf & Welling, 2016; BID31 or their extensions BID19 , and LSTM (Hochreiter & Schmidhuber, 1997) . We highlight three main contributions of this paper as follows:• We propose a new attention-based neural networks paradigm to elegantly address graphto-sequence learning problems that learns a mapping between graph-structured inputs to sequence outputs, which current Seq2Seq and Tree2Seq may be inadequate to handle.• We propose a novel graph encoder to learn a bi-directional node embeddings for directed and undirected graphs with node attributes by employing various aggregation strategies, and to learn graph-level embedding by exploiting two different graph embedding techniques. Equally importantly, we present an attention mechanism to learn the alignments between nodes and sequence elements to better cope with large graphs.• Experimental results show that our model achieves state-of-the-art performance on three recently introduced graph-to-sequence tasks and significantly outperforms existing graph neural networks, Seq2Seq, and Tree2Seq models. In this paper, we study the graph-to-sequence problem, introducing a new general and flexible Graph2Seq model that follows the encoder-decoder architecture. We showed that, using our proposed bi-directional node embedding aggregation strategy, the graph encoder could successfully learn representations for three representative classes of directed graph, i.e., directed acyclic graphs, directed cyclic graphs and sequence-styled graphs. Experimental results on three tasks demonstrate that our model significantly outperforms existing graph neural networks, Seq2Seq, and Tree2Seq baselines on both synthetic and real application datasets. We also showed that introducing an attention mechanism over node representation into the decoding substantially enhances the ability of our model to produce correct target sequences from large graphs. Since much symbolic data is represented as graphs and many tasks express their desired outputs as sequences, we expect Graph2Seq to be broadly applicable to unify symbolic AI and beyond. A PSEUDO-CODE OF THE GRAPH-TO-SEQUENCE ALGORITHM",Graph to Sequence Learning with Attention-Based Neural Networks,Graves & Schmidhuber ; Bi-RNN ; Neural Machine Translation ; Socher ; two ; AMR ; Hamilton ; one ; Iyer et al. ; three,"the input data ; vectors ; our model ; semantic parsing tasks ; the form ; a mapping ; two different graph embedding techniques ; the underlying application ; directed graph, i.e., directed acyclic graphs ; the conventional encoder-decoder approach",Graves & Schmidhuber ; Bi-RNN ; Neural Machine Translation ; Socher ; two ; AMR ; Hamilton ; one ; Iyer et al. ; three,"The celebrated Sequence to Sequence learning (Seq2Seq) technique and its numerous variants achieve excellent performance on many tasks. However, many machine learning tasks have inputs naturally represented as graphs. To address this challenge, we introduce a general end-to-end graph-t-sequence neural encoder-decoder architecture that maps an input graph to a sequence of vectors and uses an attention-based LSTM method to decode the target sequence from these vectors. Our method generates the node and graph embeddings using an improved graph-based neural network with a novel aggregation strategy and an attention mechanism that aligns node embedd",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Recent advances in deep learning techniques has shown the usefulness of the deep neural networks in extracting features required to perform the task at hand.
 However, these features learnt are in particular helpful only for the initial task. This is due to the fact that the features learnt are very task specific and does not capture the most general and task agnostic features of the input.
 In fact the way humans are seen to learn is by disentangling features which task agnostic. This indicates that leaning task agnostic features by disentangling only the most informative features from the input data.
 Recently Variational Auto-Encoders (VAEs) have shown to be the de-facto models to capture the latent variables in a generative sense.
 As these latent features can be represented as continuous and/or discrete variables, this indicates us to use VAE with a mixture of continuous and discrete variables for the latent space.
 We achieve this by performing our experiments using a modified version of joint-vae to learn the disentangled features.
 Feature learning is one of the most fundamental task in machine learning and recently deep learning has made revolutionary advanced in this. What ever the machine learning task at hand, deep neural networks are excellent models for feature extraction from a raw data. But the features extracted or learned are very task specific as one use particular loss functions that are suited for task at hand. For example, cross entropy loss used for multiclass classification problems.This way of learning performs well only for the particular trained task leading to what is called as a narrow or weak artificial intelligence. However, to achieve the ultimate goal of true or general artificial intelligence, one needs to learn representations in a task agnostic manner. These task agnostic features should be enough to capture all the required information of the given entity.One such effort made in recent times is towards learning disentangled representations. As BID0 defines, disentangled representations are the representations where a change in a single unit of the representation corresponds to a change in a single factor of the BID2 .In this work, we experiment with JointVAE BID2 to explore the disentangled representation for the given dataset Gondal (2019). In the next sections, we discuss our experimental setup and results.",Mixture Model for Neural Disentanglement,VAE ; One ; JointVAE ; Gondal,true or general artificial intelligence ; the latent space ; which task ; One such effort ; the representation ; recent times ; a generative sense ; Feature learning ; only the most informative features ; These task agnostic features,VAE ; One ; JointVAE ; Gondal,"Deep neural networks are useful in extracting features required to perform the task at hand. However, these features are task specific and do not capture the most general and task agnostic features of the input data. In fact, humans are seen to learn by disentangling features which task-agnostic. Variational Auto-Encoders (VAEs) have shown to be the de-facto models to capture latent variables in a generative sense. These latent features can be represented as continuous and/or discrete variables. In order to achieve this, we use VAE with a mixture of continuous and discrete variables for the latent space",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Despite significant advances in the field of deep Reinforcement Learning (RL), today's algorithms still fail to learn human-level policies consistently over a set of diverse tasks such as Atari 2600 games. We identify three key challenges that any algorithm needs to master in order to perform well on all games:  processing diverse reward distributions, reasoning over long time horizons, and exploring efficiently.   In this paper, we propose an algorithm that addresses each of these challenges and is able to learn human-level policies on nearly all Atari games. A new transformed Bellman operator allows our algorithm to process rewards of varying densities and scales; an auxiliary temporal consistency loss allows us to train stably using a discount factor of 0.999 (instead of 0.99) extending the effective planning horizon by an order of magnitude; and we ease the exploration problem by using human demonstrations that guide the agent towards rewarding states. When tested on a set of 42 Atari games, our algorithm exceeds the performance  of an average human on 40 games using a common set of hyper parameters. In recent years, significant advances in the field of deep Reinforcement Learning (RL) have led to artificial agents that are able to reach human-level control on a wide array of tasks such as some Atari 2600 games . In many of the Atari games, these agents learn control policies that far exceed the capabilities of an average human player BID4 . However, learning human-level policies consistently across the entire set of games remains an open problem.We argue that an algorithm needs to overcome three key challenges in order to perform well on all Atari games. The first challenge is processing diverse reward distributions. An algorithm must learn stably regardless of reward density and scale. BID12 showed that clipping rewards to the canonical interval [−1, 1] is one way to achieve stability. However, this clipping operation may change the set of optimal policies. For example, the agent no longer differentiates between striking a single pin or all ten pins in BOWLING. Hence, optimizing the unaltered reward signal in a stable manner is crucial to achieving consistent performance across games. The second challenge is reasoning over long time horizons, which means the algorithm should be able to choose actions in anticipation of rewards that might be far away. For example, in MONTEZUMA'S REVENGE, individual rewards might be separated by several hundred time steps. In the standard γ-discounted RL setting, this means the algorithm should be able to handle discount factors close to 1. The third and final challenge is efficient exploration of the MDP. An algorithm that explores efficiently is able to discover long trajectories with a high cumulative reward in a reasonable amount of time even if individual rewards are very sparse. While each problem has been partially addressed in the literature, none of the existing deep RL algorithms have been able to address these three challenges at once.In this paper, we propose a new Deep Q-Network (DQN) BID12 style algorithm that specifically addresses these three challenges. In order to learn stably independent of the reward distribution, we use a transformed Bellman operator that reduces the variance of the action-value function. Learning with the transformed operator allows us to process the unaltered environment rewards regardless of scale and density. We prove that the optimal policy does not change in deterministic MDPs and show that under certain assumptions the operator is a contraction in stochastic MDPs (i.e., the algorithm converges to a fixed point) (see Sec. 3.2) . Our algorithm learns stably even at high discount factors due to an auxiliary temporal consistency (TC) loss. This loss prevents the network from prematurely generalizing to unseen states (Sec. 3.3) allowing us to use a discount factor as high as γ = 0.999 in practice. This extends the effective planning horizon of our algorithm by one order of magnitude when compared to other deep RL approaches on Atari. Finally, we improve the efficiency of DQN's default exploration scheme by combining the distributed experience replay approach of with the Deep Q-learning from Demonstrations (DQfD) algorithm of BID6 . The resulting architecture is a distributed actor-learner system that combines offline expert demonstrations with online agent experiences (Sec. 3.4).We experimentally evaluate our algorithm on a set of 42 games for which we have demonstrations from an expert human player (see Table 6 ). Using the same hyper parameters on all games, our algorithm exceeds the performance of an average human player on 40 games, the expert player on 34 games, and state-of-the-art agents on at least 28 games. Furthermore , we significantly advance the state-of-the-art on sparse reward games. Our algorithm completes the first level of MONTEZUMA'S REVENGE and it achieves a score of 3997 points on PITFALL! without compromising performance on dense reward games and while only using 5 demonstration trajectories. In this paper, we presented a deep Reinforcement Learning (RL) algorithm that achieves human-level performance on a wide variety of MDPs on the Atari 2600 benchmark. It does so by addressing three challenges: handling diverse reward distributions, acting over longer time horizons, and efficiently exploring on sparse reward tasks. We introduce novel approaches for each of these challenges: a transformed Bellman operator, a temporal consistency loss, and a distributed RLED framework for learning from human demonstrations and task reward. Our algorithm exceeds the performance of an average human on 40 out of 42 Atari 2600 games.",Ape-X DQfD = Distributed (many actors + one learner + prioritized replay) DQN with demonstrations optimizing the unclipped 0.999-discounted return on Atari.,DQN ; RL ; recent years ; second ; DQfD ; one ; ten ; MDP ; several hundred ; three,all Atari games ; the distributed experience replay approach ; varying densities ; a wide variety ; the expert player ; sparse reward tasks ; human-level control ; DQN's default exploration scheme ; the operator ; the effective planning horizon,DQN ; RL ; recent years ; second ; DQfD ; one ; ten ; MDP ; several hundred ; three,"Despite significant advances in the field of deep Reinforcement Learning (RL), today's algorithms still fail to learn human-level policies consistently over a set of diverse tasks such as Atari 2600 games. In this paper, we propose an algorithm that addresses each of these challenges and is able to perform well on nearly all Atari games. A new transformed Bellman operator allows our algorithm to process rewards of varying densities and scales; an auxiliary temporal consistency loss allows us to train stably using a discount factor of 0.999 (instead of 1.99) extending the effective planning horizon by an order of magnitude; and we ease the exploration problem",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Learning disentangled representation from any unlabelled data is a non-trivial problem. In this paper we propose Information Maximising Autoencoder (InfoAE) where the encoder learns powerful disentangled representation through maximizing the mutual information between the representation and given information in an unsupervised fashion. We have evaluated our model on MNIST dataset and achieved approximately 98.9 % test accuracy while using complete unsupervised training. Learning disentangled representation from any unlabelled data is an active area of research ). Self supervised learning BID3 ; BID15 ; BID11 ) is a way to learn representation from the unlabelled data but the supervised signal is needed to be developed manually, which usually varies depending on the problem and the dataset. Generative Adversarial Neural Networks (GANs) BID4 ) is a potential candidate for learning disentangled representation from unlabelled data BID12 ; BID7 ; BID2 ). In particular, InfoGAN BID1 ), which is a slight modification of the GAN, can learn interpretable and disentangled representation in an unsupervised fashion. The classifier from this model can be reused for any intermediate task such as feature extraction but the representation learned by the classifier of the model is fully dependent on the generation of the model which is a major shortcoming. Because if the generator of the InfoGAN fails to generate any data manifold, the classifier is unable to perform well on any sample from that manifold. Tricks from Mutual Information Neural Estimation paper BID0 ) might help to capture the training data distribution, yet learning all the training data manifold using GAN is a challenge for the research community ). Adversarial autoencoder (AAE) BID10 ) is another successful model for learning disentangled representation. The encoder of the AAE learns representation directly from the training data but it does not utilize the sample generation power of the decoder for learning the representations. In this paper, we aim to address this challenge. We aim to build a model that utilizes both training data and the generated samples and thereby learns more accurate disentangled representation maximizing the mutual information between the random condition/information and representation space. We have evaluated the model on MNIST dataset and received outstanding results. InfoAE is trained on MNIST training data without any labels. After trainning, We encoded the test data with Encoder, E and got classification label with the Classifier, C. Then we clustered the test data according to label and received classification accuracy of 98.9 (±.05), which is better than the popular methods as shown in TAB0 . In this paper we present and validate InfoAE, which learns the disentangled representation in a completely unsupervised fashion while utilizing both training and generated samples. We tested InfoAE on MNIST dataset and achieved test accuracy of 98.9 (±.1), which is a very competitive performance compared to the best reported results including InfoGAN. We observe that the encoder is able to disentangle the digit category and styles in the representation space, which results in the superior performance. InfoAE can be used to learn representation from unlabelled dataset and the learning can be utilized in a related problem where limited labeled data is available. Moreover, its power of representation learning can be exploited for data augmentation. This research is currently in progress. We are currently attempting to mathematically explain the results. We are also aiming to analyze the performance of InfoAE on large scale audio and image datasets.",Learn disentangle representation in an unsupervised manner.,Information Maximising Autoencoder ; Generative Adversarial Neural Networks ; GAN ; Mutual Information Neural Estimation ; AAE ; Encoder,the generated samples ; any labels ; an active area ; the decoder ; the dataset ; image datasets ; generated samples ; the GAN ; more accurate disentangled representation ; the disentangled representation,Information Maximising Autoencoder ; Generative Adversarial Neural Networks ; GAN ; Mutual Information Neural Estimation ; AAE ; Encoder,"In this paper, we propose Information Maximising Autoencoder (InfoAE) where the encoder learns powerful disentangled representation through maximizing the mutual information between the representation and given information in an unsupervised fashion. We have evaluated our model on MNIST dataset and achieved 98.9 % test accuracy, which is an active area of research. Self supervised learning BID3; BID15 ; BID11 ) is a way to learn disentangle representation from unlabelled data but the supervised signal is needed to be developed manually, which varies depending on the problem and dataset. Generative Adversarial Neural",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Now GANs can generate more and more realistic face images that can easily fool human beings.   In contrast, a common convolutional neural network(CNN), e.g. ResNet-18, can achieve more than 99.9% accuracy in discerning fake/real faces if training and testing faces are from the same source. In this paper, we performed both human studies and CNN experiments, which led us to two important findings. One finding is that the textures of fake faces are substantially different from real ones. CNNs can capture local image texture information for recognizing fake/real face, while such cues are easily overlooked by humans. The other finding is that global image texture information is more robust to image editing and generalizable to fake faces from different GANs and datasets. Based on the above findings, we propose  a  novel  architecture  coined  as  Gram-Net,  which  incorporates  “Gram Block” in multiple semantic levels to extract global image texture representations. Experimental results demonstrate that our Gram-Net performs better than existing approaches for fake face detection.   Especially, our Gram-Net is more robust to image editing, e.g.  downsampling, JPEG compression, blur, and noise.   More importantly, our Gram-Net generalizes significantly better in detecting fake faces from GAN models not seen in the training phase.",An empirical study on fake images reveals that texture is an important cue that current fake images differ from real images. Our improved model capturing global texture statistics shows better cross-GAN fake image detection performance.,CNN ; two ; One ; Gram-Net ; Gram Block ; JPEG ; GAN,global image texture representations ; the above findings ; human beings ; Experimental results ; the same source ; both human studies ; different GANs ; humans ; editing ; contrast,CNN ; two ; One ; Gram-Net ; Gram Block ; JPEG ; GAN,"Convolutional neural networks (CNN) can achieve 99.9% accuracy in discerning fake/real faces if training and testing faces from the same source. In this paper, we explore two important findings: the textures of fake faces are substantially different from real ones, and global image texture information is more robust to image editing and generalizable to fake faces from different GANs and datasets.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Style transfer usually refers to the task of applying color and texture information from a specific style image to a given content image while preserving the structure of the latter. Here we tackle the more generic problem of semantic style transfer: given two unpaired collections of images, we aim to learn a mapping between the corpus-level style of each collection, while preserving semantic content shared across the two domains. We introduce XGAN (""Cross-GAN""), a dual adversarial autoencoder, which captures a shared representation of the common domain semantic content in an unsupervised way, while jointly learning the domain-to-domain image translations in both directions.   We exploit ideas from the domain adaptation literature and define a semantic consistency loss which encourages the model to preserve semantics in the learned embedding space. We report promising qualitative results for the task of face-to-cartoon translation. The cartoon dataset we collected for this purpose will also be released as a new benchmark for semantic style transfer. Image-to-image translation -learning to map images from one domain to another -covers several classical computer vision tasks such as style transfer (rendering an image in the style of a given input BID3 ), colorization (mapping grayscale images to color images (Zhang et al., 2016) ), super-resolution (increasing the resolution of an input image BID9 ), or semantic segmentation (inferring pixelwise semantic labeling of a scene BID14 ). In many cases, one can rely on supervision in the form of labels or paired samples. This assumption holds for instance for colorization, where ground-truth pairs are easily obtained by generating grayscale images from colored inputs.Figure 1: On the left, we depict a high-level motivational example for semantic style transfer, the task of adapting an image to the visual appearance of an other domain without altering its semantic content. The proposed XGAN applied on the face-to-cartoon task preserves important face semantics such as hair style or face shape (right).In this work, we consider the task of semantic style transfer: learning to map an image from one domain into the style of another domain without altering its semantic content (see Figure 1) . In that sense, our goal is akin to style transfer: We aim to transfer style while keeping content consistent. The key differences with traditional techniques are that (i) we work with image collections instead of having a single style image, and (ii ) we aim to retain higher-level semantic content in the feature space rather than pixel-level structure. In particular, we experiment on the task of translating faces to cartoons while preserving their various facial attributes (hair color, eye color, etc.). Note that without loss of generality, a photo of a face can be mapped to many valid cartoons, and vice versa. Semantic style transfer is therefore a many-to-many mapping problem, for which obtaining labeled examples is ambiguous and costly. Although this paper specifically focuses on the face-to-cartoon setting, many other examples fall under this category: mapping landscape pictures to paintings (where the different scene objects and their composition describe the input semantics), transforming sketches to images, or even cross-domain tasks such as generating images from text. In this setting, we only rely on two unlabeled training image collections or corpora, one for each domain, with no known image pairings across domains. Hence, we are faced with a double domain shift, first in terms of global domain appearance, and second in terms of the content distribution of the two collections.Recent work BID6 Zhu et al., 2017; Yi et al., 2017; BID1 report good performance using GAN-based models for unsupervised image-to-image translation when the two input domains share similar pixel-level structure (e.g., horses and zebras) but fail for more general transformations (e.g., dogs and cats). Perhaps the best known recent example is CycleGAN (Zhu et al., 2017) . Given two image domains D 1 and D 2 , the model is trained with a pixel-level cycleconsistency loss which ensures that the mapping g 1→2 from D 1 to D 2 followed by its inverse, g 2→1 , yields the identity function; i.e., g 1→2 • g 2→1 = id. However, we argue that such a pixel-level constraint is not sufficient in our case; the category of transformations we are interested in requires a constraint in semantic space even though the transformation occurs in the pixel space.To this end, we propose XGAN (""Cross-GAN""), a dual adversarial autoencoder which learns a shared semantic representation of the two input domains in an unsupervised way, while jointly learning both domain-to-domain translations. In other words, the domain-to-domain translation g 1→2 consists of an encoder e 1 taking inputs in D 1 , followed by a decoder d 2 with outputs in D 2 (and likewise for g 2→1 ) such that e 1 and e 2 , as well as d 1 and d 2 , are partially shared. The main novelty lies in how we constrain the shared embedding using techniques from the domain adaptation literature, as well as a novel semantic consistency loss. The latter ensures that the domain-to-domain translations preserve the semantic representation, i.e., that e 1 ≈ e 2 •g 1→2 and e 2 ≈ e 1 •g 2→1 . Therefore, it acts as a form of self-supervision which alleviates the need for paired examples and preserves semantic featurelevel information rather than pixel-level content. In the following section, we review relevant recent work before discussing the XGAN model in more detail in Section 3. In Section 4, we introduce CARTOONSET, our dataset of cartoon faces for research on semantic style transfer, which we are currently in the process of making publicly available. Finally, in Section 5 we report experimental results of XGAN on the face-to-cartoon task, and discuss various ablation experiments. In this work, we introduced XGAN, a model for unsupervised domain translation applied to the task of semantically-consistent style transfer. In particular, we argue that learning image-to-image translation between two structurally different domains requires passing through a high-level joint semantic representation while discarding local pixel-level dependencies. Additionally, we proposed a semantic consistency loss acting on both domain translations as a form of self-supervision.We reported promising experimental results on the task of mapping the domain of face images to cartoon avatars that clearly outperform the current baseline. We also showed that additional weak supervision, such as a pretrained feature representation, can easily be added to the model in the form of teacher knowledge. While not necessary, it acts as a good regularizer for the learned embeddings and generated samples. This can be particularly useful for natural image data as offthe-shelf pretrained models are abundant. Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In ICCV, 2017.","XGAN is an unsupervised model for feature-level image-to-image translation applied to semantic style transfer problems such as the face-to-cartoon task, for which we introduce a new dataset.",Zhang et al. ; first ; CARTOONSET ; Yi et al. ; one ; Zhu et al. ; GAN ; Alexei A Efros ; Cross-GAN ; ICCV,an encoder e ; the mapping ; semantics ; techniques ; Zhang et al ; instance ; horses ; corpora ; similar pixel-level structure ; this purpose,Zhang et al. ; first ; CARTOONSET ; Yi et al. ; one ; Zhu et al. ; GAN ; Alexei A Efros ; Cross-GAN ; ICCV,"Style transfer is the task of applying color and texture information from a specific style image to a given content image while preserving the structure of the latter. XGAN (""Cross-GAN""), a dual adversarial autoencoder, captures a shared representation of common domain semantic content in an unsupervised way, while jointly learning domain-to-domain image translations in both directions.   We exploit ideas from the domain adaptation literature and define a semantic consistency loss which encourages the model to preserve semantics in the learned embedding space. The cartoon dataset we collected for this purpose will also be released as a new benchmark for semantic style transfer.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Deep neural networks are vulnerable to adversarial examples, which becomes one of the most important problems in the development of deep learning. While a lot of efforts have been made in recent years, it is of great significance to perform correct and complete evaluations of the adversarial attack and defense algorithms. In this paper, we establish a comprehensive, rigorous, and coherent benchmark to evaluate adversarial robustness on image classification tasks. After briefly reviewing plenty of representative attack and defense methods, we perform large-scale experiments with two robustness curves as the fair-minded evaluation criteria to fully understand the performance of these methods. Based on the evaluation results, we draw several important findings and provide insights for future research. Recent progress in deep learning (DL) has led to substantial improvements in a wide range of domains, such as image understanding (Krizhevsky et al., 2012; He et al., 2016) , speech recognition (Graves et al., 2013) , and natural language processing (Devlin et al., 2019) . However, the existing DL models are highly vulnerable to adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2015) , which are maliciously generated by an adversary to make a model produce erroneous predictions. As DL models have been integrated into various security-sensitive applications (e.g., autonomous driving, healthcare, and finance), the study of the adversarial robustness issue has attracted increasing attention with an enormous number of adversarial attack and defense methods proposed. Therefore, it is crucial to conduct correct and rigorous evaluations of these methods for understanding their pros and cons, comparing their performance, and providing insights for building new methods (Carlini et al., 2019) . The research on adversarial robustness is faced with an ""arms race"" between attacks and defenses: a defense method proposed to prevent existing attacks was soon evaded by new attacks, and vice versa (Carlini & Wagner, 2017a; b; He et al., 2018; Athalye et al., 2018a; Uesato et al., 2018; Zhang et al., 2019b) . For instance, defensive distillation (Papernot et al., 2016c) was proposed to improve the robustness, but was later shown to be ineffective against a strong attack (Carlini & Wagner, 2017b) . Many methods were introduced to build robust models by causing obfuscated gradients, which can be defeated by the adaptive ones (Athalye et al., 2018a; Uesato et al., 2018) . As a result, it is particularly challenging to understand their effects, identify the real progress, and advance the field. Moreover, the current attacks and defenses are often evaluated incompletely. First, most defenses are only tested against a small set of attacks under limited threat models, and many attacks are evaluated on a few models or defenses. Second, the robustness evaluation metrics are too simple to show the performance of these methods. The accuracy of a defense against an attack for a given perturbation budget (Kurakin et al., 2018) and the minimum distance of the adversarial perturbation (Brendel et al., 2018b) are used as the primary evaluation metrics, which are often insufficient to characterize the behavior of the attacks and defenses totally. Consequently, the incomplete evaluation cannot provide a comprehensive understanding of the strengths and limitations of the attack and defense methods. In this paper, we establish a comprehensive, rigorous, and coherent benchmark to evaluate adversarial robustness, which can provide a comprehensive understanding of the effects of existing methods under different scenarios, with a hope to facilitate the future research. In particular, we focus on the robustness of image classifiers under the p norm threat models, since the adversarial robustness issue has been extensively studied on image classification tasks with the p additive noises. We incorporate a lot of typical and state-of-the-art attack and defense methods for robustness evaluation, including 15 attack methods and 16 defense models-8 on CIFAR-10 (Krizhevsky & Hinton, 2009 ) and 8 on ImageNet (Russakovsky et al., 2015) . To fully demonstrate the performance of these methods, we adopt two complementary robustness curves as the major evaluation metrics to present the results. Then, we carry out large-scale experiments on the cross evaluation of the attack and defense methods under complete threat models 1 , including 1) untargeted and targeted attacks; 2) ∞ and 2 attacks; 3) white-box, transfer-based, score-based, and decision-based attacks. By analyzing the quantitative results, we have some important findings. First, the relative robustness between defenses against an attack could be different under varying perturbation budgets or attack iterations. So it is hard to conclude that a defense is more robust than another against an attack by using a specific configuration. However, this is common in previous works. Second, although various defense techniques have been proposed, the most robust defenses are still the adversarially trained models. The robustness of these defenses can also generalize to other threat models, under which they are not trained to be robust. Third, defenses based on randomization are generally more robust to black-box attacks based on the query feedback. More detailed discussions can be found in Sec. 5.3. All evaluation experiments are conducted on a new adversarial robustness platform 2 developed by us, since the existing platforms (e.g., CleverHans (Papernot et al., 2016a) , Foolbox (Rauber et al., 2017) , etc) cannot fully support our comprehensive evaluations (details in Appendix A). We hope that our platform could continuously incorporate and evaluate more methods, and be helpful for future works. Based on the above results and more results in Appendix C, we highlight some key findings. First, the relative robustness between defenses against the same attack could be different under varying attack parameters, such as the perturbation budget or the number of attack iterations. Not only the results of PGD-AT and TRADES in Fig. 1 can prove it, but also the results in many different scenarios show the similar phenomenon. Given this observation, the comparison between defenses at a specific attack configuration cannot fully demonstrate the superiority of a method upon another. We therefore strongly advise the researchers to adopt the robustness curves as the major evaluation metrics to present the robustness results. Second, among the defenses studied in this paper, we find that the most robust models are obtained by PGD-based adversarial training. Their robustness not only is good for the threat model under which they are trained (i.e., the ∞ threat model), but can also generalize to other threat models (e.g., the 2 threat model). However, adversarial training usually leads to a reduction of natural accuracy and high training cost. A research direction is to develop new methods that maintain the natural accuracy or reduce the training cost. And we have seen several works (Shafahi et al., 2019) in this direction. Third, we observe that the defenses based on randomization are quite resistant to score-based and decision-based attacks, which rely on the query feedback of the black-box models. We argue that the robustness of the randomization-based defenses against these attacks is due to the random predictions given by the models, making the estimated gradients or search directions unreliable for attacks. A potential research direction is to develop more powerful score-based and decision-based attacks that can efficiently evade the randomization-based defenses. Fourth, the defenses based on input transformations (e.g., JPEG, Bit-Red) can sightly improve the robustness over the undefended models, and sometimes get much higher accuracy against black-box attacks. Since these methods are quite simple, they may be combined with other types of defenses to build more powerful defenses. Fifth, we find that different transfer-based attack methods exhibit similar performance on CIFAR-10, while the recent methods (e.g., MIM, DIM) can improve the transferability of adversarial examples over BIM on ImageNet. One potential reason is that the input dimension of the models on ImageNet is much higher than that on CIFAR-10, and thus the adversarial examples generated by BIM can easily ""overfit"" to the substitute model (Dong et al., 2018) , resulting in poor transferability. And the recent methods proposed to solve this issue can generate more transferable adversarial examples. Note that these findings are based on our current benchmark, which may be strengthened or falsified in the future if new results are given. In this paper, we established a comprehensive, rigorous, and coherent benchmark to evaluate adversarial robustness of image classifiers. We performed large-scale experiments with two robustness curves as the fair-minded evaluation criteria to facilitate a better understanding of the representative and state-of-the-art adversarial attack and defense methods. We drew some key findings based on the evaluation results, which may be helpful for future research. , 2018) , etc. However, we observe that these platforms do not totally support our comprehensive evaluations in this paper. First, some attacks evaluated in this paper are not included in these platforms. There are less than 10 out of the 15 attacks adopted in this paper that are already implemented in each platform. And most of the available methods are white-box methods. Second, although these platforms incorporate a few defenses, they do not use the pre-trained models. But we use the original source codes and pre-trained models to perform unbiased evaluations. Third, the evaluation metrics defined by the two robustness curves in this paper are not provided in the existing platforms. Therefore, we develop a new adversarial robustness platform to satisfy our requirements.","We provide a comprehensive, rigorous, and coherent benchmark to evaluate adversarial robustness of deep learning models.",PGD ; Rauber ; DL ; BIM ; Carlini & Wagner ; Athalye ; Foolbox ; Third ; Sec ; One,the available methods ; the-art ; poor transferability ; cons ; Dong et al ; the models ; image classifiers ; the most robust defenses ; vice versa ; the robustness evaluation metrics,PGD ; Rauber ; DL ; BIM ; Carlini & Wagner ; Athalye ; Foolbox ; Third ; Sec ; One,"Deep neural networks are vulnerable to adversarial examples, which becomes one of the most important problems in the development of deep learning. In order to perform correct and complete evaluations of adversarial attack and defense algorithms, we establish a comprehensive, rigorous, and coherent benchmark to evaluate adversarial robustness on image classification tasks. We perform large-scale experiments with two robustness curves as the fair-minded evaluation criteria to fully understand the performance of these methods and provide insights for future research. Recent progress in deep learning (DL) has led to substantial improvements in a wide range of domains, such as image understanding (Krizhevsky et",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"In the context of multi-task learning, neural networks with branched architectures have often been employed to jointly tackle the tasks at hand. Such ramified networks typically start with a number of shared layers, after which different tasks branch out into their own sequence of layers. Understandably, as the number of possible network configurations is combinatorially large, deciding what layers to share and where to branch out becomes cumbersome. Prior works have either relied on ad hoc methods to determine the level of layer sharing, which is suboptimal, or utilized neural architecture search techniques to establish the network design, which is considerably expensive. In this paper, we go beyond these limitations and propose a principled approach to automatically construct branched multi-task networks, by leveraging the employed tasks' affinities. Given a specific budget, i.e. number of learnable parameters, the proposed approach generates architectures, in which shallow layers are task-agnostic, whereas deeper ones gradually grow more task-specific. Extensive experimental analysis across numerous, diverse multi-tasking datasets shows that, for a given budget, our method consistently yields networks with the highest performance, while for a certain performance threshold it requires the least amount of learnable parameters. Deep neural networks are usually trained to tackle different tasks in isolation. Humans, in contrast, are remarkably good at solving a multitude of tasks concurrently. Biological data processing appears to follow a multi-tasking strategy too; instead of separating tasks and solving them in isolation, different processes seem to share the same early processing layers in the brain -see e.g. V1 in macaques (Gur & Snodderly, 2007) . Drawing inspiration from such observations, deep learning researchers began to develop multi-task networks with branched architectures. As a whole, multi-task networks (Caruana, 1997) seek to improve generalization and processing efficiency through the joint learning of related tasks. Compared to the typical learning of separate deep neural networks for each of the individual tasks, multi-task networks come with several advantages. First, due to their inherent layer sharing (Kokkinos, 2017; Lu et al., 2017; Kendall et al., 2018; Guo et al., 2018; , the resulting memory footprint is typically substantially lower. Second, as features in the shared layers do not need to be calculated repeatedly for the different tasks, the overall inference speed is often higher (Neven et al., 2017; Lu et al., 2017) . Finally, multi-task networks may outperform their single-task counterparts (Kendall et al., 2018; Xu et al., 2018; Sener & Koltun, 2018; Maninis et al., 2019) . Evidently, there is merit in utilizing multi-task networks. When it comes to designing them, however, a significant challenge is to decide on the layers that need to be shared among tasks. Assuming a hard parameter sharing setting 1 , the number of possible network configurations grows quickly with the number of tasks. As a result, a trial-and-error procedure to define the optimal architecture becomes unwieldy. Resorting to neural architecture search (Elsken et al., 2019) techniques is not a viable option too, as in this case, the layer sharing has to be jointly optimized with the layers types, their connectivity, etc., rendering the problem considerably expensive. Instead, researchers have recently explored more viable alternatives, like routing (Rosenbaum et al., 2018) , stochastic filter grouping (Bragman et al., 2019) , and feature partitioning (Newell et al., 2019) , which are, however, closer to the soft parameter sharing setting. Previous works on hard parameter sharing opted for the simple strategy of sharing the initial layers in the network, after which all tasks branch out simultaneously. The point at which the branching occurs is usually determined ad hoc (Kendall et al., 2018; Guo et al., 2018; Sener & Koltun, 2018) . This situation hurts performance, as a suboptimal grouping of tasks can lead to the sharing of information between unrelated tasks, known as negative transfer . In this paper, we go beyond the aforementioned limitations and propose a novel approach to decide on the degree of layer sharing between tasks in order to eliminate the need for manual exploration. To this end, we base the layer sharing on measurable levels of task affinity or task relatedness: two tasks are strongly related, if their single task models rely on a similar set of features. Zamir et al. (2018) quantified this property by measuring the performance when solving a task using a variable sets of layers from a model pretrained on a different task. However, their approach is considerably expensive, as it scales quadratically with the number of tasks. Recently, Dwivedi & Roig (2019) proposed a more efficient alternative that uses representation similarity analysis (RSA) to obtain a measure of task affinity, by computing correlations between models pretrained on different tasks. Given a dataset and a number of tasks, our approach uses RSA to assess the task affinity at arbitrary locations in a neural network. The task affinity scores are then used to construct a branched multitask network in a fully automated manner. In particular, our task clustering algorithm groups similar tasks together in common branches, and separates dissimilar tasks by assigning them to different branches, thereby reducing the negative transfer between tasks. Additionally, our method allows to trade network complexity against task similarity. We provide extensive empirical evaluation of our method, showing its superiority in terms of multi-task performance vs computational resources. In this paper, we introduced a principled approach to automatically construct branched multi-task networks for a given computational budget. To this end, we leverage the employed tasks' affinities as a quantifiable measure for layer sharing. The proposed approach can be seen as an abstraction of NAS for MTL, where only layer sharing is optimized, without having to jointly optimize the layers types, their connectivity, etc., as done in traditional NAS, which would render the problem considerably expensive. Extensive experimental analysis shows that our method outperforms existing ones w.r.t. the important metric of multi-tasking performance vs number of parameters, while at the same time showing consistent results across a diverse set of multi-tasking scenarios and datasets. MTAN We tried re-implementing the MTAN model ) using a ResNet-50 backbone. The architecture was based on the Wide-ResNet architecture that is used in the original paper. After extensive hyperparameter tuning, we were unable to get a meaningful result on the Cityscapes dataset when trying to solve all three tasks jointly. Note that, the authors have only shown results in their paper when training semantic segmentation and monocular depth estimation.",A method for the automated construction of branched multi-task networks with strong experimental evaluation on diverse multi-tasking datasets.,two ; Cityscapes ; Kendall ; three ; NAS ; Guo et al. ; Second ; Xu et al. ; Lu ; Dwivedi & Roig,"information ; measurable levels ; suboptimal, or utilized neural architecture search techniques ; a multitude ; a given budget ; a meaningful result ; stochastic filter grouping ; semantic segmentation and monocular depth estimation ; Roig ; models",two ; Cityscapes ; Kendall ; three ; NAS ; Guo et al. ; Second ; Xu et al. ; Lu ; Dwivedi & Roig,"In multi-task learning, neural networks with branched architectures are employed to jointly tackle tasks at hand. These networks typically start with a number of shared layers, after which different tasks branch out into their own sequence of layers. However, due to the complexity of network configurations, determining the level of layer sharing and where to branch out becomes cumbersome. Prior works have either relied on ad hoc methods to determine the layer sharing level, which is suboptimal, or utilized neural architecture search techniques to establish the network design. In this paper, we explore a principled approach to automatically construct multi-Task networks, using the employed tasks'",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Models of user behavior are critical inputs in many prescriptive settings and can be viewed as decision rules that transform state information available to the user into actions. Gaussian processes (GPs), as well as nonlinear extensions thereof, provide a flexible framework to learn user models in conjunction with approximate Bayesian inference. However, the resulting models may not be interpretable in general. We propose decision-rule GPs (DRGPs) that apply GPs in a transformed space defined by decision rules that have immediate interpretability to practitioners. We illustrate this modeling tool on a real application and show that structural variational inference techniques can be used with DRGPs. We find that DRGPs outperform the direct use of GPs in terms of out-of-sample performance. Models of user behavior are critical in many decision making problems and can be viewed as decision rules that transform state information (in set S) available to the user to actions (in set A). Formally, a user model is a function f : S → A. Gaussian processes (GPs) employed to learn functions on the action/target space (henceforth target GPs or TGPs for short) can thus be used to place a prior on user models and identify a posterior distribution over them supported by data in conjunction with approximate Bayesian inference techniques (Blei et al., 2017; Beaumont, 2019) . TGPs for user modeling would assume that user actions at a given set of finite states follow a multivariate Gaussian. To capture non-Gaussian action distributions, one could apply GPs to learn functions in a transformed space that is not the target. Examples include warped and chained GPs proposed in Snelson et al. (2004) and Saul et al. (2016) , respectively. Extending this literature, we study the application of GPs in a transformed space defined by decision rules. Such rules are known in several applications and depend on functions themselves. Specifically, a user model based on a decision rule takes the form g : Π k P k × S → A, where the arguments are obtained using functions h k : S → P k , k = {1, . . . , K} that map from S to transformed spaces P k , possibly different from the target space A. Each such function has immediate interpretability to a practitioner, and we model them using GPs. We refer to such a user model {g, h 1 , ..., h k } as a decision-rule GP (DRGP). To make the notion of DRGPs concrete in this short article, we focus on the problem faced by a firm providing services to store ethanol -a real application that motivated this work. Suppose capacity (in gallons) is sold via annual contracts to N users. The contract of user n specifies the maximum amount of ethanol that can be stored, denoted by C n . User behavior corresponds to the injection of ethanol and the withdrawal of previously injected ethanol, which can be modeled as a time series. The inventory I n,t in storage associated with user n at time t is the net of past injections and withdrawals. A TGP approach would employ a GP to determine the next-period storage inventory level function I n,t+1 directly. In contrast, we propose a DRGP that leverages a well-known decision rule based on injection and withdrawal threshold functions (Charnes et al., 1966; Secomandi, 2010) . These threshold functions are learned as GPs instead of the (relatively less interpretable) inventory function. We focus on the following research questions in the context of the ethanol storage application: (Q1) Can existing exact and approximate Bayesian inference techniques be used for inference with DRGP? and (Q2) How does DRGP perform relative to TGP? We answer these questions by executing numerical experiments based on real data of aggregated ethanol storage injection and withdrawals. For Q1, we show that sparse vari-ational inference (Titsias, 2009; Hensman et al., 2013) , which can be applied to TGP on our data set, can also be used with DRGP, albeit heuristically, which is encouraging from an implementation standpoint. For Q2, we find that DRGP implemented in this manner leads to lesser out-of-sample error than TGP on most of our datasets, in addition to being more interpretable to practitioners. This preliminary finding is promising and suggests that applying GPs in the interpretable space of the decision rule threshold functions has potential value, which adds to the growing literature on interpretable machine learning and optimization (Letham et al., 2015; Bertsimas and Dunn, 2017) . In addition, the improvements we report are based on the heuristic use of sparse variational inference with DRGPs, which bodes well for additional potential improvements from the development of new inference techniques targeting DRGPs. Finally, several applications in energy, health care, and transportation, among other domains, have known interpratable decision rules, which can be leveraged in the DRGP framework proposed here. Snelson et al. (2004) show that modeling data using a warped GP, which is a non-linear transformation (aka warping) of a GP, can enhance predictive performance. Inference using a warped GP can be performed in closed-form provided the warping function satisfies certain properties, such as being invertible. Lázaro-Gredilla (2012) consider the case where the warping function is not fixed a priori. DRGPs differ from warped GPs as they are based on a potentially non-invertible transformation of multiple GPs.",We propose a class of user models based on using Gaussian processes applied to a transformed space defined by decision rules,Hensman ; aka warping ; Snelson ; Lázaro-Gredilla ; Bertsimas ; P k ; GP ; N ; annual ; DRGP,the development ; this manner ; practitioners ; other domains ; a posterior distribution ; one ; inference ; a GP ; Can existing exact and approximate Bayesian inference techniques ; terms,Hensman ; aka warping ; Snelson ; Lázaro-Gredilla ; Bertsimas ; P k ; GP ; N ; annual ; DRGP,"Gaussian processes (GPs) provide a flexible framework to learn user models in conjunction with approximate Bayesian inference techniques. However, the resulting models may not be interpretable in general. Decision-rule GPs (DRGs) are used to apply GPs in a transformed space defined by decision rules that have immediate interpretability to practitioners. Structural variational inference techniques can be used with DRGPs, and they outperform the direct use of GPs.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"It has been shown that using geometric spaces with non-zero curvature instead of plain Euclidean spaces with zero curvature improves performance on a range of Machine Learning tasks for learning representations. Recent work has leveraged these geometries to learn latent variable models like Variational Autoencoders (VAEs) in spherical and hyperbolic spaces with constant curvature. While these approaches work well on particular kinds of data that they were designed for e.g.~tree-like data for a hyperbolic VAE, there exists no generic approach unifying all three models. We develop a Mixed-curvature Variational Autoencoder, an efficient way to train a VAE whose latent space is a product of constant curvature Riemannian manifolds, where the per-component curvature can be learned. This generalizes the Euclidean VAE to curved latent spaces, as the model essentially reduces to the Euclidean VAE if curvatures of all latent space components go to 0. Generative models are a growing area of unsupervised learning, that aim to model the data distribution p(x) over data points x in a high-dimensional space X (Doersch, 2016) , usually a subset of a high-dimensional Euclidean space R n , with all the associated benefits: a naturally definable scalar product, vector addition, and others. Yet, many types of data have a strongly non-Euclidean latent structure (Bronstein et al., 2017) , like the set of human-interpretable images. They are usually thought to live on a ""natural image manifold"" (Zhu et al., 2016) , a lower-dimensional subset of the space in which they are represented. On this continuous manifold, one finds all the images that humans can interpret using their visual system. By moving along the manifold, we can continuously change the content and appearance of interpretable images. As mentioned in Nickel & Kiela (2017) , changing the geometry of the underlying latent space enables us to represent some data better than is possible in the equivalent Euclidean space. Motivated by these observations, a range of methods to learn representations in different spaces of constant curvature have recently been introduced: learning embeddings in spherical spaces (Batmanghelich et al., 2016) , hyperbolic spaces (Nickel & Kiela, 2017; Tifrea et al., 2019; Sala et al., 2018) , and even in products of these spaces (Gu et al., 2019; Anonymous, 2020) . By using a combination of different constant curvature spaces, it aims to match the underlying geometry of the data even closer than the others. However, an open question that remains, is how to choose the dimensionality of partial spaces and their curvatures. A popular approach to generative modeling is the Variational Autoencoder (Kingma & Welling, 2014, VAE) . VAEs provide us with a way to sidestep the intractability of marginalizing a joint probability model of the input and latent space p(x, z) while allowing for a prior p(z) on the latent space. Recently, variants of the VAE have been introduced for spherical (Davidson et al., 2018; Xu & Durrett, 2018) and hyperbolic (Mathieu et al., 2019; Nagano et al., 2019 ) latent spaces. Our approach is a generalization of the VAE to products of constant curvature spaces, which have the advantage that we can obtain a better reduction in dimensionality while not making optimization of the model significantly more complex. The resulting latent space is then a ""non-constantly"" curved manifold in an ambient Euclidean space. Modeling the latent space as a single constant curvature manifold limits the flexibility of the space to assume a shape similar to that of the hypothetical intrinsic manifold. Our contributions are the following: (i) we develop a principled framework for manipulating representations and modeling probability distributions in products of constant curvature spaces that smoothly transitions across different curvature signs, (ii) we show how to generalize Variational Au-toencoders to learn latent representations on products of constant curvature spaces with generalized Gaussian-like priors, and (iii) our approaches outperform current benchmarks on a synthetic tree dataset (Mathieu et al., 2019) and image reconstruction on MNIST (LeCun, 1998) , Omniglot (Lake et al., 2015) , and CIFAR (Krizhevsky, 2009) for some latent space dimensions. By transforming the latent space and associated prior distributions onto Riemannian manifolds of constant curvature, it has previously been shown that we can learn representations on curved space. Generalizing on the above ideas, we have extended the theory of learning VAEs to products of constant curvature spaces. To do that, we have derived the necessary operations in several models of constant curvature spaces, extended existing probability distribution families to these manifolds, and generalized VAEs to latent spaces that are products of smaller ""component"" spaces, with learnable curvature. On various datasets, we show that our approach is competitive and additionally has the property that it generalizes the Euclidean variational autoencoder -if the curvatures of all components go to 0, we recover the VAE of Kingma & Welling (2014 An elementary notion in Riemannian geometry is that of a real, smooth manifold M ⊆ R n , which is a collection of real vectors x that is locally similar to a linear space, and lives in the ambient space R n . At each point of the manifold x ∈ M a real vector space of the same dimensionality as M is defined, called the tangent space at point x: T x M. Intuitively, the tangent space contains all the directions and speeds at which one can pass through x. Given a matrix representation G(x) ∈ R n×n of the Riemannian metric tensor g(x), we can define a scalar product on the tangent space: A Riemannian manifold is then the tuple (M, g). The scalar product induces a norm on the tangent space T x M: ||a|| x = a, a x ∀a ∈ T x M (Petersen et al., 2006). Although it seems like the manifold only defines a local geometry, it induces global quantities by integrating the local contributions. The metric tensor induces a local infinitesimal volume element on each tangent space T x M and hence a measure is induced as well dM(x) = |G(x)|dx where dx is the Lebesgue measure. The length of a curve γ : Straight lines are generalized to constant speed curves giving the shortest path between pairs of points x, y ∈ M, so called geodesics, for which it holds that γ * = arg min γ L(γ), such that γ(0) = x, γ(1) = y, and . Using this metric, we can go on to define a metric space (M, d M ). Moving from a point x ∈ M in a given direction v ∈ T x M with constant velocity is formalized by the exponential map: exp x : T x M → M. There exists a unique unit speed geodesic γ such that γ(0) = x and The corresponding exponential map is then defined as exp x (v) = γ(1). The logarithmic map is the inverse log x = exp −1 x : M → T x M. For geodesically complete manifolds, i.e. manifolds in which there exists a length-minimizing geodesic between every x, y ∈ M, such as the Lorentz model, hypersphere, and many others, exp x is well-defined on the full tangent space T x M. To connect vectors in tangent spaces, we use parallel transport PT x→y : T x M → T y M, which is an isomorphism between the two tangent spaces, so that the transported vectors stay parallel to the connection. It corresponds to moving tangent vectors along geodesics and defines a canonical way to connect tangent spaces.",Variational Autoencoders with latent spaces modeled as products of constant curvature Riemannian manifolds improve on image reconstruction over single-manifold variants.,the Variational Autoencoder (Kingma & Welling ; al. ; Nagano ; Anonymous ; ||a|| ; Lebesgue ; Lorentz ; the Euclidean VAE ; Davidson et al. ; Riemannian,vectors ; Nagano et al ; The logarithmic map ; Bronstein et al ; The corresponding exponential map ; a curve γ ; Xu ; (iii ; all the images ; different constant curvature spaces,the Variational Autoencoder (Kingma & Welling ; al. ; Nagano ; Anonymous ; ||a|| ; Lebesgue ; Lorentz ; the Euclidean VAE ; Davidson et al. ; Riemannian,"Variational Autoencoders (VAEs) in spherical and hyperbolic spaces with constant curvature improve performance on Machine Learning tasks. These approaches work well on particular kinds of data that they were designed for e.g.~tree-like data for a spherical VAE. However, there exists no generic approach unifying all three models. We develop a Mixed-curvature Variation Autoencoder, an efficient way to train a VAE whose latent space is a product of constant curvatures Riemannian manifolds, where per-component curvature can be learned. This generalizes the Euclide",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"This work studies the problem of modeling non-linear visual processes by leveraging deep generative architectures for learning linear, Gaussian models of observed sequences. We propose a joint learning framework, combining a multivariate autoregressive model and deep convolutional generative networks. After justification of theoretical assumptions of inearization, we propose an architecture that allows Variational Autoencoders and Generative Adversarial Networks to simultaneously learn the non-linear observation as well as the linear state-transition model from a sequence of observed frames. Finally, we demonstrate our approach on conceptual toy examples and dynamic textures. While classification of image and video with Convolutional Neural Networks (CNN) is becoming an established practice, unsupervised learning and generative modeling remain to be challenging problems in deep learning. A generative model of a visual process enables the possibility of generating sequences of video frames such that the appearance as well as the dynamics approximately resemble the original training process without copying it. This procedure is typically referred to as video generation BID25 BID6 ) or video synthesis BID19 ). More technically, this means that in addition to a suitable probability model for the individual frames, a probabilistic description for the frame-to-frame transition is also necessary. Analysis and reproduction of visual processes simplifies considerably, if this transition can be described as a multivariate autoregressive (MAR) model, i.e., as a combination of linear transformations and Gaussian noise. For instance, linear transformations are easily invertible and by means of spectral analysis, it can be studied how such a process behaves in the long term.Realistically, most frame transitions in real-world visual processes unlikely are linear functions. Nevertheless, unsupervised learning has come up with many approaches to fit MAR models to realworld processes, for instance by using linear low-rank approximations, as proposed by BID8 , or sparse approximations of the frames, as proposed by BID28 , or applying the kernel trick to them BID3 ).The success of Generative Adversarial Networks (GAN) introduced by BID9 and Variational Autoencoders (VAE) introduced by BID15 has led to an increased interest in deep generative learning and it seems natural to apply such techniques to sequential processes. We approach this idea from the perspective of linearization, in order to keep the model as simple as possible. In an analogous way as physicists transforming non-linear differential equations into linear ones by means of an appropriate change of variables, our approach is to learn latent representations of visual processes, such that the latent state-to-state transition can be described by an MAR model. To this end, we jointly learn a non-linear observation and a linear state transition function by introducing a dynamic layer that can be used in conjunction with deep generative architectures such as GANs and VAEs. This work presents an approach to learn embedded MAR models from image sequences. We motivate the feasibility of this approach by introducing the concept of local linearizability and propose a joint learning procedure that employs deep generative models in combination with an additional linear component, the dynamic layer. We report first positive results on low-resolution visual processes, where a first-order Markov property can be assumed, and hope to shed some light on the nature of linearization. A possible future research direction is improving the theoretical understanding of linearizing representations and their applicability outside of stationary visual processes. Let Φ ∈ R n×n be a matrix. Since Γ is a diffeomorphism, we define DISPLAYFORM0 holds. Let us denote the Jacobian of φ at y * by J φ . Because Γ maps y * to the origin, we can reformulate the requirement as DISPLAYFORM1 This requirement is fulfilled if the Jacobi matrices of ϕ and φ coincide, i.e. DISPLAYFORM2 The Jacobian of φ at y * is given, according to he chain rule, by DISPLAYFORM3 where J Γ −1 ∈ R d×n is the Jacobian matrix of Γ −1 at Γ(y * ). A matrix Φ ∈ R n×n can be always found, such that Eq. FORMULA2 is fulfilled, if the columns and rows of J ϕ lie in the column space of J Γ −1 and the row space of J Γ , respectively. The column space of J Γ −1 coincides with the row space of J Γ , due to the identity J Γ J Γ −1 = I n . The statement of the proposition follows.",We model non-linear visual processes as autoregressive noise via generative deep learning.,CNN ; first ; linear ; Eq ; Markov ; Jacobian ; Convolutional Neural Networks ; J Γ ; VAE ; MAR,deep generative models ; Gaussian noise ; an established practice ; stationary visual processes ; the Jacobian matrix ; Φ ∈ R n×n ; a suitable probability model ; realworld processes ; the possibility ; a first-order Markov property,CNN ; first ; linear ; Eq ; Markov ; Jacobian ; Convolutional Neural Networks ; J Γ ; VAE ; MAR,"Unsupervised learning and generative modeling are challenging problems in deep learning. A joint learning framework, combining a multivariate autoregressive model and deep convolutional generative networks, is proposed to enable Variational Autoencoders and Generative Adversarial Networks to simultaneously learn non-linear observation as well as the linear state-transition model from a sequence of observed frames. In addition to a probabilistic description for frame-to-frame transition, unsupervisedlearning has come up with many approaches to fit MAR models to real-world processes, including linear low-rank approximations, sparse approxim",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Variational Bayesian Inference is a popular methodology for approximating posterior distributions over Bayesian neural network weights. Recent work developing this class of methods has explored ever richer parameterizations of the approximate posterior in the hope of improving performance. In contrast, here we share a curious experimental finding that suggests instead restricting the variational distribution to a more compact parameterization. For a variety of deep Bayesian neural networks trained using Gaussian mean-field variational inference, we find that the posterior standard deviations consistently exhibit strong low-rank structure after convergence. This means that by decomposing these variational parameters into a low-rank factorization, we can make our variational approximation more compact without decreasing the models' performance. Furthermore, we find that such factorized parameterizations improve the signal-to-noise ratio of stochastic gradient estimates of the variational lower bound, resulting in faster convergence. Bayesian Neural Networks (MacKay, 1992; Neal, 1993) explicitly represent their parameteruncertainty by forming a posterior distribution over model parameters, instead of relying on a single point estimate for making predictions, as is done in traditional deep learning. Besides offering improved predictive performance over single models, Bayesian neural networks are also more robust to hard examples (Raftery et al., 2005) , have better calibration of predictive uncertainty and thus can be used for out-of-domain detection or other risk-sensitive applications (Ovadia et al., 2019) . Variational inference (Peterson, 1987; Hinton and Van Camp, 1993 ) is a popular class of methods for approximating the posterior distribution p(w|x, y), since the exact Bayes' rule is often intractable to compute for models of practical interest. This class of methods specifies a distribution q θ (w) of given parametric or functional form as the posterior approximation, and optimizes the approximation by solving an optimization problem. In particular, we minimize the negative Evidence Lower Bound (negative ELBO) approximated by samples from the posterior: by differentiating with respect to the variational parameters θ (Salimans et al., 2013; Kingma and Welling, 2013) . In Gaussian Mean Field Variational Inference (GMFVI) (Blei et al., 2017; Blundell et al., 2015) , we choose the variational approximation to be a fully factorized Gaussian distribution: q(w ij ), with q(w ij ) = N (µ ij , σ where W ∈ R m×n is a weight matrix of a single network layer and i and j are the row and column indices in this weight matrix. In practice, we often represent the posterior standard deviation parameters σ ij in the form of a matrix A ∈ R m×n + . With this notation, we have the relationship Σ q = diag(vec(A 2 )) where the elementwise-squared A is vectorized by stacking its columns, and then expanded as a diagonal matrix into R mn×mn + . While Gaussian Mean-Field posteriors are considered to be one of the simplest types of variational approximations, with some known limitations (Giordano et al., 2018) , they scale to comparatively large models and generally provide competitive performance (Ovadia et al., 2019) . However, when compared to deterministic neural networks, GMFVI doubles the number of parameters and is often harder to train due to the increased noise in stochastic gradient estimates. Beyond fully factorized mean-field, recent research in variational inference has explored richer parameterizations of the approximate posterior in order to improve the performance of Bayesian neural networks (see Appendix A and Figure 3 ). For instance, various structures of Gaussian posteriors have been proposed, with per layer block-structured covariances (Louizos and Welling, 2016; Sun et al., 2017; Zhang et al., 2017) , full covariances (Barber and Bishop, 1998) with different parametrizations (Seeger, 2000) , up to more flexible approximate posteriors using normalizing flows (Rezende and Mohamed, 2015) and extensions thereof (Louizos and Welling, 2017 ). In contrast, here we study a simpler, more compactly parameterized mean-field variational posterior which ties variational parameters in the already diagonal covariance matrix. We show that such a posterior approximation can also work well for a variety of models. In particular we find that: • Converged posterior standard deviations under GMFVI consistently display strong low-rank structure. This means that by decomposing these variational parameters into a low-rank factorization, we can make our variational approximation more compact without decreasing our model's performance. • Factorized parameterizations of posterior standard deviations improve the signal-to-noise ratio of stochastic gradient estimates, and thus not only reduce the number of parameters compared to standard GMFVI, but also can lead to faster convergence. In this work we have shown that Bayesian Neural Networks trained with standard Gaussian meanfield variational inference exhibit posterior standard deviation matrices that can be approximated with little information loss by a low-rank decomposition. This suggests that richer parameterizations of the variational posterior may not always be needed, and that compact parameterizations can also work well. We used this insight to propose a simple, yet effective variational posterior parametrization, which speeds up training and reduces the number of variational parameters without degrading predictive performance on three different model types. In future work, we hope to scale up variational inference with compactly parameterized approximate posteriors to much larger models and more complex problems. For mean-field variational inference to work well in that setting several challenges will likely need to be addressed (Osawa et al., 2019) ; improving the signal-to-noise ratio of ELBO gradients using our compact variational parameterizations may provide a piece of the puzzle. 1. Explained variance for the rank k approximation is calculated as γ , where g b is the gradient value for a single parameter. The expectation E and variance V ar of the gradient values g b are calculated over a window of last 10 batches.","Mean field VB uses twice as many parameters; we tie variance parameters in mean field VB without any loss in ELBO, gaining speed and lower variance gradients.",Seeger ; Van Camp ; Welling ; Giordano ; • Factorized ; Peterson ; N ; Gaussian Mean Field Variational Inference ; Blundell et al. ; Neal,"little information loss ; normalizing flows ; performance ; a simple, yet effective variational posterior parametrization ; the models' performance ; improved predictive performance ; order ; the simplest types ; richer parameterizations ; the variational distribution",Seeger ; Van Camp ; Welling ; Giordano ; • Factorized ; Peterson ; N ; Gaussian Mean Field Variational Inference ; Blundell et al. ; Neal,"Variational Bayesian Inference (GMFVI) is a popular method for approximating posterior distributions over Bayesian neural network weights. This class of methods has explored ever richer parameterizations of the approximate posterior in the hope of improving performance. In contrast, in deep Bayesian networks trained using Gaussian mean-field variational inference, posterior standard deviations consistently exhibit low-rank structure after convergence, making our variational approximation more compact without decreasing the models' performance. Furthermore, factorized parameterizations improve the signal-to-noise ratio of stochastic gradient estimates of the variational lower bound, resulting in faster convergence.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Spatiotemporal forecasting has become an increasingly important prediction task in machine learning and statistics due to its vast applications, such as climate modeling, traffic prediction, video caching predictions, and so on. While numerous studies have been conducted, most existing works assume that the data from different sources or across different locations are equally reliable. Due to cost, accessibility, or other factors, it is inevitable that the data quality could vary, which introduces significant biases into the model and leads to unreliable prediction results. The problem could be exacerbated in black-box prediction models, such as deep neural networks. In this paper, we propose a novel solution that can automatically infer data quality levels of different sources through local variations of spatiotemporal signals without explicit labels. Furthermore, we integrate the estimate of data quality level with graph convolutional networks to exploit their efficient structures. We evaluate our proposed method on forecasting temperatures in Los Angeles. Recent advances in sensor and satellite technology have facilitated the collection of large spatiotemporal datasets. As the amount of spatiotemporal data increases, many have proposed representing this data as time-varying graph signals in various domains, such as sensor networks BID21 BID29 , climate analysis BID2 BID17 , traffic control systems BID15 , and biology BID18 BID26 .While existing work have exploited both spatial structures and temporal signals, most of them assume that each signal source in a spatial structure is equally reliable over time. However , a large amount of data comes from heterogeneous sensors or equipment leading to various levels of noise BID22 BID27 . Moreover , the noises of each source can vary over time due to movement of the sensors or abrupt malfunctions. This problem raises significantly challenges to train and apply complex black box machine learning models, such as deep neural networks, because even a small perturbation in data can deceive the models and lead to unexpected behaviors BID5 BID13 . Therefore, it is extremely important to consider data quality explicitly when designing machine learning models.The definitions of data quality can be varied -high quality data is generally referred to as fitness for intended uses in operations, decision making and planning BID19 . In this paper , we narrow down the definition as a penalizing quantity for high local variations. We consider a learning problem of spatiotemporal signals that are represented by time-varying graph signals for different data qualities. Given a graph G = (V, E, W) and observations X ∈ R N ×M ×T where N, M, T are the number of vertices, the types of signals, and the length of time-varying signals, respectively. We define the concept of data quality levels at each vertex as latent variables, which are connected through a graph using a local variation of the vertex. The local variation at each vertex depends on the local spatial structure and neighboring signals. Our definition of data quality can be easily incorporated into any existing machine learning models through a regularizer in their objective functions. In this paper, we develop data quality long short-term memory (DQ-LSTM) neural networks for spatiotemporal forecasting. DQ-LSTM effectively exploits spatial structures of data quality levels at each vertex through graph convolution, which examines neighboring signals at a set of K-hop neighboring vertices, and captures the temporal dependencies of each time series through LSTMs. We demonstrate that data quality is an essential factor for improving the predictive performance of neural networks via experiments on urban heat island prediction in Los Angeles.Related work A series of work have been conducted on addessing the issues of data qualities and heterogeneous data sources. BID23 is the first theoretical work that proposes a mixture model for captureing two types of labels in supervised learning. One type of the labels is considered as high quality labels from an expensive source (domain experts) while another type is from errorprone crowdsourcing. Since the reliability or quality of the labels is different, it is not desired to consider them equally. The authors proposed a learning algorithm that can utilize the error-prone labels to reduce the cost required for the expert labeling. BID27 address issues from strong and weak labelers by developing an active learning algorithm minimizing the number of label requests. BID22 focus on the data of variable quality resulting from heterogeneous sources. The authors define the concept of heterogeneity of data and develop a method of adjusting the learning rate based on the heterogeneity. Different from existing works, our proposed framework differentiates heterogeneous sources based on neighborhood signals without any explicit labels.Another set of work related to our study is learning and processing graph signals or features. Spectral graph theory BID3 BID24 BID21 has been developed as a main study to understand two aspects of graph signals: structures and signals. Under this theory many models have been introduced to exploit convolutional neural networks (CNNs) which provide an efficient architecture to extract localized patterns from regular grids, such as images BID14 . BID1 learns convolutional parameters based on the spectrum of the graph Laplacian. Later, BID8 extends the spectral aspect of CNNs on graphs into largescale learning problemsDefferrard et al. FORMULA0 proposes a spectral formulation for fast localized filtering with efficient pooling. Furthermore, BID12 re-formularizes existing ideas into layer-wise neural networks that can be tuned through backpropagation rule with a first-order approximation of spectral filters introduced in BID7 . Built on these work, we propose a graph convolutional layer that maps spatiotemporal features into a data quality level.Outline We review graph signal processing to define the local variation and a data quality level (DQL) with graph convolutional networks in Section 2. In Section 3, we provide how the data quality levels are exploited with recurrent neural networks to differentiate reliability of observations on vertices. Also, we construct a forecasting model, DQ-LSTM. Our main result is presented in Section 4 with other baselines. In Section 5 we discuss its properties and interpret the data reliability inferred from our model. In this work, we study the problem of data quality for spatiotemporal data analysis. While existing works assume that all signals are equally reliable over time, we argue that it is important to differentiate data quality because the signals come from heterogeneous sources. We proposed a novel formulation that automatically infers data quality levels of different sources and developed a specific formulation, namely DQ-LSTM, based on graph convolution for spatiotemporal forecasting. We demonstrate the effectiveness of DQ-LSTM on inferring data quality and improving prediction performance on a real-world climate dataset. For future work, we are interested in further refining the definitions of data quality and examining rigorous evaluation metrics.",We propose a method that infers the time-varying data quality level for spatiotemporal forecasting without explicitly assigned labels.,Spatiotemporal ; Los Angeles ; first ; two ; One ; Laplacian ; DQL ; DQ-LSTM,the first theoretical work ; another type ; data quality level ; noise ; both spatial structures ; our model ; K-hop neighboring vertices ; a specific formulation ; a method ; neighborhood signals,Spatiotemporal ; Los Angeles ; first ; two ; One ; Laplacian ; DQL ; DQ-LSTM,"Spatiotemporal forecasting has become an important prediction task in machine learning and statistics due to its vast applications, such as climate modeling, traffic prediction, video caching predictions, and so on. However, most existing works assume that the data from different sources or across different locations are equally reliable. This is due to cost, accessibility, or other factors, which can introduce significant biases into the model and lead to unreliable prediction results. The problem could be exacerbated in black-box prediction models such as deep neural networks. In this paper, we introduce a novel solution that can automatically infer data quality levels of different sources through local variations of",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Deep neural networks can learn meaningful representations of data. However, these representations are hard to interpret. For example, visualizing a latent layer is generally only possible for at most three dimensions. Neural networks are able to learn and benefit from much higher dimensional representations but these are not visually interpretable because nodes have arbitrary ordering within a layer. Here, we utilize the ability of the human observer to identify patterns in structured representations to visualize higher dimensions. To do so, we propose a class of regularizations we call \textit{Graph Spectral Regularizations} that impose graph-structure on latent layers. This is achieved by treating activations as signals on a predefined graph and constraining those activations using graph filters, such as low pass and wavelet-like filters. This framework allows for any kind of graph as well as filter to achieve a wide range of structured regularizations depending on the inference needs of the data. First, we show a synthetic example that the graph-structured layer can reveal topological features of the data. Next, we show that a smoothing regularization can impose semantically consistent ordering of nodes when applied to capsule nets. Further, we show that the graph-structured layer, using wavelet-like spatially localized filters, can form localized receptive fields for improved image and biomedical data interpretation. In other words, the mapping between latent layer, neurons and the output space becomes clear due to the localization of the activations. Finally, we show that when structured as a grid, the representations create coherent images that allow for image-processing techniques such as convolutions. Neural networks have revolutionized many areas of machine learning including image and natural language processing. However, one of the major challenges for neural networks is that they are still black boxes to the user. It is not quite clear how network internals map from inputs to outputs or how to interpret the features learned by the nodes. This is mainly because the features are not constrained to have specific structure or characteristics. Existing regularizations constrain the learned code to have certain properties. However, they are not designed to specifically aid in interpretation of the latent encoding. For example, L 1 regularization induces sparsity in the activations but does not impose specific structure between dimensions.Here, we introduce a new class of regularizations called Graph Spectral Regularizations that result in activations that are filtered on a predefined graph. We define specific members of this class for applications. First, we introduce a (graph) Laplacian smoothing regularization which enforces smoothly varying activations while at the same time reconstructing the data. This regularization is useful for learning features with specific topologies. For instance, we show that on a clusterstructured topology where features correspond to hierarchical cluster structure in the data it reflects the abstract grouping of features. We also show it is useful for inducing feature consistency between nodes of capsule networks BID11 . The graph regularization semantically aligns the features such that they appear in the same order in each capsule. When trained on MNIST digits, we find that each of our 10 capsules consisting of 16 nodes encodes the same transformation (rotation, scale, skew, etc) of a particular digit in the same node.While the Laplacian smoothing regularizations is useful in the context where the features of the data have a recognizable topology, often we don't know the explicit structure of the data. Instead, we would like to extract the topology of the data itself. Thus, we design a filter that encourages the graph structure layer to learn data-shape features. We achieve this by using a spatially localized, Gaussian filter to localize the activations for any particular data point. We ensure that only one of a dictionary of localized filters is chosen as the activation via a spectral bottleneck layer preceding the graph-structured layer. We show that spatially-localized filter regularizations are useful for detecting circular and linear topologies of data that are not immediately reflected by the observed features. We also explore a biological system -a single-cell protein expression dataset depicting T cell development in the thymus -that has continuous progression structure. The graph structured layer (with a ring graph) reveals the data to have a Y-shaped topology reflecting the bifurcation into CD4 (regulatory) and CD8 (cytotoxic) T cells, confirming known T cell biology.Finally, we show that the graph-structured layer, when imposing a 2D grid, creates a ""pseudo"" image that can be analyzed by convolution layers. We show that such re-encoded images of MNIST digits have localized receptive fields that can be used for classification and visual interpretability. Interestingly, we find that the convolution obviates the need for a spectral bottleneck as the convolution and max pooling themselves may provide that function.Our contributions are as follows:• A framework for imposing graph structure on latent layers using graph spectral regularizations.• A Laplacian graph smoothing regularization and its applications in learning feature smoothness and consistency.• Spatially localized graph regularizations using a spectral bottleneck based on a dictionary of Gaussian Kernels and its application in recognizing data topology.• Applications of graph spectral regularizations, natural and biological datasets to demonstrate feature interpretability and data topology.The rest of this paper is organized as follows. We first define graph structured layers and two techniques utilizing this layer in Section 2. Then we present experiments demonstrating improved interpretability in Section 3. Finally, we wrap up with conclusions in Section 4.",Imposing graph structure on neural network layers for improved visual interpretability.,Graph Spectral Regularizations ; Gaussian Kernels ; First ; Gaussian ; max ; regularizations.• A ; one ; Laplacian ; two ; only one,"the convolution ; T cell development ; a predefined graph ; Our contributions ; nets ; the activations ; any particular data point ; scale ; a spatially localized, Gaussian filter ; any kind",Graph Spectral Regularizations ; Gaussian Kernels ; First ; Gaussian ; max ; regularizations.• A ; one ; Laplacian ; two ; only one,"Deep neural networks can learn and benefit from higher dimensional representations but these representations are not visually interpretable due to arbitrary ordering within a layer. To achieve this, we propose a class of regularizations that impose graph-structure on latent layers. These regularizations are achieved by treating activations as signals on a predefined graph and constraining those activations using graph filters, such as low pass and wavelet-like filters. This framework allows for any kind of graph as well as filter to achieve a wide range of structured regularizations depending on the inference needs of the data. The graph-structured layer can reveal topological features",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"We identify a phenomenon, which we refer to as *multi-model forgetting*, that occurs when sequentially training multiple deep networks with partially-shared parameters; the performance of previously-trained models degrades as one optimizes a subsequent one, due to the overwriting of shared parameters. To overcome this, we introduce a statistically-justified weight plasticity loss that regularizes the learning of a model's shared parameters according to their importance for the previous models, and demonstrate its effectiveness when training two models sequentially and for neural architecture search. Adding weight plasticity in neural architecture search preserves the best models to the end of the search and yields improved results in both natural language processing and computer vision tasks. Deep neural networks have been very successful for tasks such as visual recognition BID31 and natural language processing BID33 , and much recent work has addressed the training of models that can generalize across multiple tasks BID6 . In this context, when the tasks become available sequentially, a major challenge is catastrophic forgetting: when a model initially trained on task A is later trained on task B, its performance on task A can decline calamitously. Several recent articles have addressed this problem BID13 BID28 BID12 BID16 . In particular, BID13 show how to overcome catastrophic forgetting by approximating the posterior probability, p(θ | D 1 , D 2 ), with θ the network parameters and D 1 , D 2 different datasets representing the tasks.In many situations one does not train a single model for multiple tasks but multiple models for a single task. When dealing with many large models, a common strategy to keep training tractable is to share a subset of the weights across the multiple models and to train them sequentially BID25 BID31 BID17 . This strategy has a major drawback. FIG0 shows that for two models, A and B, the larger the number of shared weights, the more the accuracy of A drops when training B; B overwrites some of the weights of A and this damages the performance of A. We call this multi-model forgetting. The benefits of weight-sharing have been emphasized in tasks like neural architecture search, where the associated speed gains have been key in making the process practical BID25 BID18 , but its downsides remain virtually unexplored.In this paper we introduce an approach to overcoming multi-model forgetting. Given a dataset D, we first consider two models f 1 (D; θ 1 , θ s ) and f 2 (D; θ 2 , θ s ) with shared weights θ s and private weights θ 1 and θ 2 . We formulate learning as the maximization of the posterior p(θ 1 , θ 2 , θ s |D). Under mild assumptions we show that this posterior can be approximated and expressed using a loss, dubbed Weight Plasticity Loss (WPL), that minimizes multi-model forgetting. Our framework evaluates the importance of each weight, conditioned on the previously-trained model, and encourages the update of each shared weight to be inversely proportional to its importance. We then show that our approach extends to more than two models by exploiting it for neural architecture search.Our work is the first of which we are aware to propose a solution to multi-model forgetting. We establish the merits of our approach when training two models with partially shared weights and in the context of neural architecture search. For the former, we establish the effectiveness of WPL in the strict convergence case, where each model is trained until convergence, and in the more realistic loose convergence setting, where training is stopped early. WPL can reduce the forgetting effect by 99% when model A converges fully, and by 52% in the loose convergence case. For neural architecture search, we implement WPL within the efficient ENAS method of BID25 , a state-of-the-art technique that relies on parameter sharing and corresponds to the loose convergence setting. We show that, at each iteration, the use of WPL reduces the forgetting effect by 51% on the most affected model and by 95% on average over all sampled models. Our final results on the best architecture found by the search confirm that limiting multi-model forgetting yields better results and better convergence for both language modeling (on the PTB dataset BID21 ) and image classification (on the CIFAR10 dataset BID14 ). For language modeling the perplexity decreases from 65.01 for ENAS without WPL to 61.9 with WPL. For image classification WPL yields a drop of top-1 error from 4.87% to 3.81%. We also adapt our method to NAO BID19 and show, in appendix due to space limitations, that multi-model forgetting is significantly reduced. We will make our code publicly available upon acceptance of this paper. This paper has identified the problem of multi-model forgetting in the context of sequentially training multiple models: the shared weights of previously-trained models are overwritten during training of subsequent models, leading to performance degradation. We show that the degree of degradation is linked to the proportion of shared weights, and introduce a statistically-motivated weight plasticity loss (WPL) to overcome this. Our experiments on multi-model training and on neural architecture search clearly show the effectiveness of WPL in reducing multi-model forgetting and yielding better architectures, leading to improved results in both natural language processing and computer vision tasks. We believe that the impact of WPL goes beyond the tasks studied in this paper. In future work, we plan to integrate WPL within other neural architecture search strategies in which weight sharing occurs and to study its use in other multi-model contexts, such as for ensemble learning. Comparison of different output dropout rates for NAO. We plot the mean validation perplexity while searching the best architecture (top) and the best 5 model's error differences (bottom) for four different dropout rates. Note that path dropping in NAO prevents learning shortly after model initialization. At all the dropout rates, our WPL achieves lower error differences, i.e., it reduces multi-model forgetting, as well as speeds up training.Our approach is general, and its use in the context of neural architecture search is not limited to ENAS. To demonstrate this, we applied it to the neural architecture optimization (NAO) method of BID19 , which also exploits weight-sharing in the search phase. In this context, we therefore investigate (i) whether multi-model forgetting occurs, and if so, (ii) the effectiveness of our approach in the NAO framework. Due to resource and time constraints, we focus our experiments mainly on the search phase, as training the best searched model from scratch takes around 4 GPU days. To evaluate the influence of the dropout strategy of BID3 , we test NAO with or without random path-dropping and with four output dropout rates from 0 to 0.75 by steps of 0.25. As in Section 4.2, in FIG1 , we plot the mean validation perplexity and the best five model's error differences for all models that are sampled during a single training epoch. For random pathdropping, since BID19 exploit a more aggressive dropping policy than that used in BID3 , we can see that validation perplexity quickly plateaus. Hence we do not add our WPL to the path dropout strategy, but use it in conjunction with output dropout.At all four different dropout rates, WPL clearly reduces multi-model forgetting and accelerates training. The level of forgetting decreases with the dropout rate, but our loss always further reduces it. Among the three methods, Nao + path dropping suffers the least from forgetting. However, this is only due to the fact that it does not learn properly. By contrast, our WPL reduces multi-model forgetting while still allowing the models to learn. This shows that our approach generalizes beyond ENAS for neural architecture search.","We identify a phenomenon, neural brainwashing, and introduce a statistically-justified weight plasticity loss to overcome this.",four ; WPL ; first ; NAO ; five ; more than two ; Weight Plasticity Loss ; PTB ; two ; three,the search confirm ; a drop ; natural language ; the forgetting effect ; the posterior probability ; our loss ; other neural architecture search strategies ; the multiple models ; multi-model forgetting yields ; multi-model training,four ; WPL ; first ; NAO ; five ; more than two ; Weight Plasticity Loss ; PTB ; two ; three,"Multi-model forgetting occurs when sequentially training multiple deep networks with partially-shared parameters. The performance of previously-trained models degrades as one optimizes a subsequent one due to overwriting of shared parameters. To overcome this, weight plasticity is introduced in neural architecture search, which preserves the best models to the end of the search and yields improved results in natural language processing and computer vision tasks. Deep neural networks have been very successful for tasks such as visual recognition BID31 and natural languages processing BID33, and much recent work has addressed the training of models that can generalize across multiple tasks BID6.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Recent advances have made it possible to create deep complex-valued neural networks. Despite this progress, the potential power of fully complex intermediate computations and representations has not yet been explored for many challenging learning problems. Building on recent advances, we propose a novel mechanism for extracting signals in the frequency domain. As a case study, we perform audio source separation in the Fourier domain. Our extraction mechanism could be regarded as a local ensembling method that combines a complex-valued convolutional version of Feature-Wise Linear Modulation (FiLM) and a signal averaging operation. We also introduce a new explicit amplitude and phase-aware loss, which is scale and time invariant, taking into account the complex-valued components of the spectrogram. Using the Wall Street Journal Dataset, we compare our phase-aware loss to several others that operate both in the time and frequency domains and demonstrate the effectiveness of our proposed signal extraction method and proposed loss. When operating in the complex-valued frequency domain, our deep complex-valued network substantially outperforms its real-valued counterparts even with half the depth and a third of the parameters. Our proposed mechanism improves significantly deep complex-valued networks' performance and we demonstrate the usefulness of its regularizing effect. Complex-valued neural networks have been studied since long before the emergence of modern deep learning techniques (Georgiou & Koutsougeras, 1992; Zemel et al., 1995; Kim & Adalı, 2003; Hirose, 2003; Nitta, 2004) . Nevertheless, deep complex-valued models have only started to gain momentum (Reichert & Serre, 2014; Arjovsky et al., 2015; Danihelka et al., 2016; Trabelsi et al., 2017; Jose et al., 2017; Wolter & Yao, 2018b; Choi et al., 2019) , with the great majority of models in deep learning still relying on real-valued representations. The motivation for using complex-valued representations for deep learning is twofold: On the one hand, biological nervous systems actively make use of synchronization effects to gate signals between neurons -a mechanism that can be recreated in artificial systems by taking into account phase differences (Reichert & Serre, 2014) . On the other hand, complex-valued representations are better suited to certain types of data, particularly those that are naturally expressed in the frequency domain. Other benefits provided by working with complex-valued inputs in the spectral or frequency domain are computational. In particular, short-time Fourier transforms (STFTs) can be used to considerably reduce the temporal dimension of the representation for an underlying signal. This is a critical advantage, as training recurrent neural networks (RNNs) or convolutional neural networks (CNNs) on long sequences remains challenging due to unstable gradients and the computational requirements of backpropagation through time (BPTT) (Hochreiter, 1991; Bengio et al., 1994) . Applying the STFT on the raw signal, on the other hand, is computationally efficient, as in practice it is implemented with the fast Fourier transform (FFT) whose computational complexity is O(n log(n)). The aforementioned biological, representational and computational considerations provide compelling motivations for designing learning models for tasks where the complex-valued representation of the input and output data is more desirable than their real-counterpart. Recent work has provided building blocks for deep complex-valued neural networks (Trabelsi et al., 2017) . These building blocks have been shown, in many cases, to avoid numerical problems during training and, thereby, enable the use of complex-valued representations. These representations are well-suited for frequency domain signals, as they have the ability to explicitly encode frequency magnitude and phase components. This motivates us to design a new signal extraction mechanism operating in the frequency domain. In this work, our contributions are summarized as follows: 1. We present a new signal separation mechanism implementing a local ensembling procedure. More precisely, a complex-valued convolutional version of Feature-wise Linear Modulation (FiLM) (Perez et al., 2018 ) is used to create multiple separated candidates for each of the signals we aim to retrieve from a mixture of inputs. A signal averaging operation on the candidates is then performed in order to increase the robustness of the signal to noise and interference. Before the averaging procedure, a form of dropout is implemented on the signal candidates in order to reduce the amount of interference and noise correlation existing between the different candidates. 2. We propose and explore a new magnitude and phase-aware loss taking explicitly into account the magnitude and phase of signals. A key characteristic of our loss is that it is scale-and time-invariant. We test our proposed signal extraction mechanism in the audio source separation setting where we aim to retrieve distinct audio signals associated with each speaker in the input mix. Our experiments demonstrate the usefulness of our extraction method, and show its regularizing effect. In this work, we introduced a new complex-valued extraction mechanism for signal retrieval in the Fourier domain. As a case study, we considered audio source separation. We also proposed a new phase-aware loss taking, explicitly, into account the magnitude and phase of the reference and estimated signals. The amplitude and phase-aware loss improves over other frequency and time-domain losses. We believe that our proposed method could lead to new research directions where signal retrieval is needed. A APPENDIX",New Signal Extraction Method in the Fourier Domain,Kim & Adalı ; Wolter & Yao ; Georgiou & Koutsougeras ; Hirose ; Hochreiter ; FFT ; al. ; Linear Modulation (FiLM ; Jose ; Nitta,interference ; Perez et al ; Feature-Wise Linear Modulation (FiLM ; phase ; half the depth ; et al ; they ; learning models ; frequency domain signals ; the complex-valued components,Kim & Adalı ; Wolter & Yao ; Georgiou & Koutsougeras ; Hirose ; Hochreiter ; FFT ; al. ; Linear Modulation (FiLM ; Jose ; Nitta,"The potential power of fully complex intermediate computations and representations has not been explored for many challenging learning problems. We propose a local ensembling method that combines a complex-valued convolutional version of Feature-Wise Linear Modulation (FiLM) and a signal averaging operation. We also introduce a new explicit amplitude and phase-aware loss, which is scale and time invariant, taking into account the complexity-valued components of the spectrogram. In the Wall Street Journal Dataset, we compare our signal extraction method and proposed loss to several others that operate both in the time and frequency domains and demonstrate the usefulness of",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Building deep neural networks to control autonomous agents which have to interact in real-time with the physical world, such as robots or automotive vehicles, requires a seamless integration of time into a network’s architecture. The central question of this work is, how the temporal nature of reality should be reflected in the execution of a deep neural network and its components. Most artificial deep neural networks are partitioned into a directed graph of connected modules or layers and the layers themselves consist of elemental building blocks, such as single units. For most deep neural networks, all units of a layer are processed synchronously and in parallel, but layers themselves are processed in a sequential manner. In contrast, all elements of a biological neural network are processed in parallel. In this paper, we define a class of networks between these two extreme cases. These networks are executed in a streaming or synchronous layerwise-parallel manner, unlocking the layers of such networks for parallel processing. Compared to the standard layerwise-sequential deep networks, these new layerwise-parallel networks show a fundamentally different temporal behavior and flow of information, especially for networks with skip or recurrent connections. We argue that layerwise-parallel deep networks are better suited for future challenges of deep neural network design, such as large functional modularized and/or recurrent architectures as well as networks allocating different network capacities dependent on current stimulus and/or task complexity. We layout basic properties and discuss major challenges for layerwise-parallel networks. Additionally, we provide a toolbox to design, train, evaluate, and online-interact with layerwise-parallel networks. Over the last years, the combination of newly available large datasets, parallel computing power, and new techniques to design, implement, and train deep neural networks has led to significant improvements and numerous newly enabled applications in various fields including vision, speech, and reinforcement learning. Considering applications for which a neural network controls a system that interacts in real-time with the physical world, ranging from robots and autonomous vehicles to chat-bots and networks playing computer games, renders it essential to integrate time into the network's design.In recent deep learning literature, enabling networks to learn and represent temporal features has gained interest. Methods were presented leveraging short-term dynamic features to build temporal consistent network responses (e.g. BID9 , BID15 ) as well as networks learning to store and utilize information over longer time periods (e.g. BID17 , BID3 ).Two major aspects considering the role of time in neural networks can be distinguished: First, the way neural networks and their components such as layers or single units, are implemented. For example, network components could operate sequentially or in parallel, and in case of parallel evaluation, synchronous and asynchronous implementations can be distinguished. Second , the extent to which the network through its architecture can form representations of temporal features. For example , if the network has no mechanisms to integrate information over time, such as recurrent connections, the network will not be able to represent temporal features, such as optic-flow. In this work , we focus on the implementation aspect but highly emphasise that our approach fundamentally influences the network's temporal behavior and the way information is integrated over time.Whereas, biological neural networks and some realizations of neural networks in silicon (reviewed in BID10 ), comparison in Farabet et al. (2012 ) can operate on a continuous temporal dimension, we will assume a discrete (frame-based) temporal domain throughout this paper. In this paper, we defined and discussed layerwise-parallel deep neural networks, by which layerwise model-parallelism is realized for deep networks independently of their architecture. We argued that layerwise-parallel networks are beneficial for future trends in deep network design, such as large functional modularized or recurrent architectures as well as for networks allocating different network capacities dependent on stimulus and/or task complexity. Due to their biologically inspired increased parallelizability, layerwise-parallel networks can be distributed across several processes or GPUs natively without the need to explicitly specifying the network parts which should be parallelized. Finally, we presented an open source toolbox to explore layerwise-parallel networks providing design, training, evaluation, and interaction mechanisms.We would like to think of this work as a step towards native model-parallel deep networks, connecting the networks architecture directly to the temporal domain. For this, major challenges for the future remain, such as a more general formulation of neuron and synapse-pools than the one used in the provided toolbox, the design of new local plasticities, or designing more adequate tasks which take the temporal domain into account.","We define a concept of layerwise model-parallel deep neural networks, for which layers operate in parallel, and provide a toolbox to design, train, evaluate, and on-line interact with these networks.",two ; the last years ; First ; Second ; Farabet ; al.,future trends ; numerous newly enabled applications ; networks ; evaluation ; the one ; interaction mechanisms ; current stimulus ; it ; parallel processing ; GPUs,two ; the last years ; First ; Second ; Farabet ; al.,"Deep neural networks to control autonomous agents which interact in real-time with the physical world, such as robots or automotive vehicles, require seamless integration of time into a network's architecture. The central question of this work is how the temporal nature of reality should be reflected in the execution of a deep neural network and its components. Most artificial deep neural networks are partitioned into a directed graph of connected modules or layers and the layers themselves consist of elemental building blocks. In contrast, all elements of a biological neural network are processed in a sequential manner. These networks are executed in a streaming or synchronous layerwise-parallel manner, unlocking",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Mixed precision training (MPT) is becoming a practical technique to improve the speed and energy efficiency of training deep neural networks by leveraging the fast hardware support for IEEE half-precision floating point that is available in existing GPUs. MPT is typically used in combination with a technique called loss scaling, that works by scaling up the loss value up before the start of backpropagation in order to minimize the impact of numerical underflow on training. Unfortunately, existing methods make this loss scale value a hyperparameter that needs to be tuned per-model, and a single scale cannot be adapted to different layers at different training stages. We introduce a loss scaling-based training method called adaptive loss scaling that makes MPT easier and more practical to use, by removing the need to tune a model-specific loss scale hyperparameter. We achieve this by introducing layer-wise loss scale values which are automatically computed during training to deal with underflow more effectively than existing methods. We present experimental results on a variety of networks and tasks that show our approach can shorten the time to convergence and improve accuracy, compared with using the existing state-of-the-art MPT and single-precision floating point. Training deep neural networks (DNNs) is well-known to be time and energy consuming, motivating the development of new methods and hardware to make training more efficient. One way to improve training efficiency is to use numerical representations that are more hardware-friendly. This is the reason that the IEEE 754 32-bit single-precision floating point format (FP32) is more widely used for training DNNs than the more precise double precision format (FP64), which is commonly used in other areas of high-performance computing. In an effort to further improve hardware efficiency, there has been increasing interest in using data types with even lower precision than FP32 for training Wang et al., 2018; Kalamkar et al., 2019; Sakr et al., 2019) . Of these, the IEEE half-precision floating-point (FP16) format is already well supported by modern GPU vendors (Choquette et al., 2018) . Using FP16 for training can reduce the memory footprint by half compared to FP32 and significantly improve the runtime performance and power efficiency. Nevertheless, numerical issues like overflow, underflow, and rounding errors frequently occur when training in low precision only. Percentage of underflow (%) Underflow rate among activation gradients accross all layers iter=10000 iter=50000 iter=80000 iter=110000 (a) Underflow rate is calculated by counting the absolute gradients below 2 −24 , the smallest positive FP16 number. Loss scale expected by each layer iter=10000 iter=50000 iter=80000 iter=110000 (b) Expected loss scale of each layer is calculated by 1 over the (0.01N )-th smallest absolute gradient, where N is the size of each gradient and 0.01 is the largest underflow rate permitted. Figure 1: Statistics of activation gradients collected from training SSD by FP32. Data are collected from different training iterations (120k in total). Layer ID are assigned in the topological order of backpropagation computation. Layers with higher ID are closer to the input. start of the backward pass so that the computed (scaled) gradients can then be properly represented in FP16 without significant underflow. For an appropriate choice of α, loss scaling can achieve state of the art results that are competitive with regular FP32 training. Unfortunately, there is no single value of α that will work in arbitrary models, and so it often needs to be tuned per model. Its value must be chosen large enough to prevent underflow issues from affecting training accuracy. However, if α is chosen too large, it could amplify rounding errors caused by swamping (Higham, 1993) or even result in overflow. This observed sensitivity to the particular choice of loss scale is also reported by , who find that different values can lead to very different ResNet-50 MPT convergence behavior. Furthermore, the data distribution of gradients can vary both between layers and between iterations (Figure 1 ), which implies that a single scale is insufficient. For instance, gradients closer to the input require a higher loss scale that may cause overflow or severe rounding errors if the same value were used in layers closer to the output. Including the time spent tuning α, the total training time of MPT can even exceed regular FP32 training. We introduce a loss scaling-based training method called adaptive loss scaling that makes MPT easier and more practical to use. We hope that this will help to utilize better existing hardware with support for fast FP16 operations. Our method improves the usability of MPT compared to existing methods by removing the need to tune a model-specific loss scale hyperparameter, while retaining (and in some cases surpassing) the accuracy of regular FP32 training. We achieve this by introducing layer-wise loss scale values which are automatically computed and dynamically updated during training to deal with underflow more effectively than existing methods. Experimental results on several examples show that MPT with adaptive loss scaling can achieve the best model accuracy and the shortest overall training time, especially when training deep models on large datasets. This paper presents adaptive loss scaling, a method that calculates layer-wise loss scale during runtime, to improve the performance and usability of MPT. Empirically we find it works better than plain MPT, existing loss scaling methods, and even FP32 in some cases, regarding model accuracy and the time taken to converge. Future work includes evaluating adaptive loss scaling on other tasks and models, especially those for Natural Language Processing; and trying to find a tighter upper bound of loss scale for each layer, e.g., based on the variance analysis in (Sakr et al., 2019) , such that each layer can be scaled more effectively; extending it to FP8 is also intriguing to try. A DETAILED ANALYSIS ON CIFAR RESULTS Table 1 shows that adaptive loss scaling is beneficial for training ResNet-110, while less advantageous for ResNet-20 and ResNet-56. We hypothesize the reason behind is that underflow causes more numerical problems when the model is deeper. For shallower models, the difference between the oracle gradient values and the underflowing ones is moderate and can even be viewed as a form of regularization. This argument is supported by the fact that the training accuracy of ResNet models on CIFAR can always reach 100%. In this way, even though adaptive loss scaling can improve the accuracy of the computed gradients, this does not necessarily always translate to improved test accuracy. 19% 93.19% 93.19% We dive deeper into this argument by reviewing Table 4 , which shows the test accuracy of the two shallower ResNet models on CIFAR-10. For both models, the test accuracy first increases to a maxima at 16, then there is a sudden drop at 128, and finally it climbs up to a plateau. Our hypothetical interpretation is as follows: 1. Initially the test accuracy is low. Here the underflow rate is expected to be at its highest, and it is the major cause for the low test accuracy. 2. The test accuracy then increases with loss scale, mainly due to the mitigation of underflow by loss scaling. However, as the gradients become more accurate, the regularizing effect from underflow is also reduced and the test accuracy will drop, until the loss scale reaches around 128. 3. If the loss scale continues to increase, the high rounding error and swamping problem caused by large scales will arise. It adds another kind of regularization, which is relatively more harmful than what underflow may cause, and the test accuracy cannot improve much. Even though this interpretation is hypothetical, this empirical evaluation in Table 4 shows that the relationship between the goodness of a loss scaling scheme and test accuracy is complicated when the model tends to overfit.",We devise adaptive loss scaling to improve mixed precision training that surpass the state-of-the-art results.,CIFAR ; ResNet ; al. ; Choquette et al. ; first ; Sakr et al. ; GPU ; the usability of MPT ; N ; One,iterations ; the usability ; different layers ; the art results ; accuracy ; the goodness ; especially those ; numerical representations ; (Figure ; existing loss scaling methods,CIFAR ; ResNet ; al. ; Choquette et al. ; first ; Sakr et al. ; GPU ; the usability of MPT ; N ; One,"Mixed precision training (MPT) is becoming a practical technique to improve the speed and energy efficiency of training deep neural networks by leveraging the fast hardware support for IEEE half-precision floating point that is available in existing GPUs. It is used in combination with loss scaling, which reduces the need to tune a model-specific loss scale hyperparameter. However, existing methods make it difficult to adapt to different layers at different training stages. We introduce a loss scaling-based training method called adaptive loss scaling that makes MPT easier and more practical to use, by removing the need for a model loss scale. Layer-wise loss",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"We study the training process of Deep Neural Networks (DNNs) from the Fourier analysis perspective. We demonstrate a very universal Frequency Principle (F-Principle) --- DNNs often fit target functions from low to high frequencies --- on high-dimensional benchmark datasets, such as MNIST/CIFAR10, and deep networks, such as VGG16. This F-Principle of DNNs is opposite to the learning behavior of most conventional iterative numerical schemes (e.g., Jacobi method), which exhibits faster convergence for higher frequencies, for various scientific computing problems. With a naive theory, we illustrate that this F-Principle results from the regularity of the commonly used activation functions. The F-Principle implies an implicit bias that DNNs tend to fit training data by a low-frequency function. This understanding provides an explanation of good generalization of DNNs on most real datasets and bad generalization of DNNs on parity function or randomized dataset. Understanding the training process of Deep Neural Networks (DNNs) is a fundamental problem in the area of deep learning. We find a common behavior of the gradient-based training process of DNNs, that is, a Frequency Principle (F-Principle): DNNs often fit target functions from low to high frequencies during the training process. In another word, at the early stage of training, the low-frequencies are fitted and as iteration steps of training increase, the high-frequencies are fitted. For example, when a DNN is trained to fit y = sin(x) + sin(2x), its output would be close to sin(x) at early stage and as training goes on, its output would be close to sin(x) + sin(2x). F-Principle was observed empirically in synthetic low-dimensional data with MSE loss during DNN training (Xu et al., 2018; Rahaman et al., 2018) . However, in deep learning, empirical phenomena could vary from one network structure to another, from one dataset to another and could exhibit significant difference between synthetic data and highdimensional real data. Therefore, the universality of the F-Principle remains an important problem for further study. Especially for high-dimensional real problems, because the computational cost of high-dimensional Fourier transform is prohibitive in practice, it is of great challenge to demonstrate the F-Principle. On the other hand, the mechanism underlying the F-Principle and its implication to the application of DNNs, e.g., design of DNN-based PDE solver, as well as their generalization ability are also important open problems to be addressed. In this work, we design two methods, i.e., projection and filtering methods, to show that the FPrinciple exists in the training process of DNNs for high-dimensional benchmarks, i.e., MNIST (LeCun, 1998) , CIFAR10 (Krizhevsky et al., 2010) . The settings we have considered are i) different DNN architectures, e.g., fully-connected network, convolutional neural network (CNN), and VGG16 (Simonyan & Zisserman, 2014) ; ii) different activation functions, e.g., tanh and rectified linear unit (ReLU); iii) different loss functions, e.g., cross entropy, mean squared error (MSE), and loss energy functional in variational problems. These results demonstrate the universality of the F-Principle. To facilitate the designs and applications of DNN-based schemes, we characterize a stark difference between DNNs and conventional numerical schemes on various scientific computing problems, where most of the conventional methods (e.g., Jacobi method) exhibit the opposite convergence behavior -faster convergence for higher frequencies. This difference implies that DNN can be adopted to accelerate the convergence of low frequencies for computational problems. We also intuitively explain with theories under an idealized setting how the smoothness/regularity of commonly used activation functions contributes to the F-Principle. Note that this mechanism is rigorously demonstrated for DNNs of general settings in a subsequent work (Luo et al., 2019) . Finally, we discuss that the F-Principle provides an understanding of good generalization of DNNs in many real datasets (Zhang et al., 2016) and poor generalization in learning the parity function (Shalev-Shwartz et al., 2017; Nye & Saxe, 2018) , that is, the F-Principle which implies that DNNs prefer low frequencies, is consistent with the property of low frequencies dominance in many real datasets, e.g., MNIST/CIFAR10, but is different from the parity function whose spectrum concentrates on high frequencies. Compared with previous studies, our main contributions are as follows: 1. By designing both the projection and filtering methods, we consistently demonstrate the F-Principle for MNIST/CIFAR10 over various architectures such as VGG16 and various loss functions. 2. For the application of solving differential equations, we show that (i) conventional numerical schemes learn higher frequencies faster whereas DNNs learn lower frequencies faster by the FPrinciple, (ii) convergence of low frequencies can be greatly accelerated with DNN-based schemes. 3. We present theories under an idealized setting to illustrate how smoothness/regularity of activation function contributes to the F-Principle. 4. We discuss in detail the implication of the F-Principle to the generalization of DNNs that DNNs are implicitly biased towards a low frequency function and provide an explanation of good and poor generalization of DNNs for low and high frequency dominant target functions, respectively. DNNs often generalize well for real problems (Zhang et al., 2016) but poorly for problems like fitting a parity function (Shalev-Shwartz et al., 2017; Nye & Saxe, 2018) despite excellent training accuracy for all problems. Understanding the differences between above two types of problems, i.e., good and bad generalization performance of DNN, is critical. In the following, we show a qualitative difference between these two types of problems through Fourier analysis and use the F-Principle to provide an explanation different generalization performances of DNNs. For MNIST/CIFAR10, we examineŷ total,k = 1 n total n total −1 i=0 consists of both the training and test datasets with certain selected output component, at different directions of k in the Fourier space. We find thatŷ total,k concentrates on the low frequencies along those examined directions. For illustration,ŷ total,k 's along the first principle component are shown by green lines in Fig. 4(a, b) for MNIST/CIFAR10, respectively. When only the training dataset is used,ŷ train,k well overlaps withŷ total,k at the dominant low frequencies. For the parity function x j e −i2πk·x . As shown in Fig. 4(c) , By experiments, the generalization ability of DNNs can be well reflected by the Fourier analysis. For the MNIST/CIFAR10, we observed the Fourier transform of the output of a well-trained DNN on faithfully recovers the dominant low frequencies, as illustrated in Fig. 4 (a) and 4(b ), respectively, indicating a good generalization performance as observed in experiments. However , for the parity function, we observed that the Fourier transform of the output of a well-trained DNN on {x i } i∈S significantly deviates fromf (k) at almost all frequencies, as illustrated in Fig. 4(c) , indicating a bad generalization performance as observed in experiments. The F-Principle implicates that among all the functions that can fit the training data, a DNN is implicitly biased during the training towards a function with more power at low frequencies. If the target function has significant high-frequency components, insufficient training samples will lead to artificial low frequencies in training dataset (see red line in Fig. 4(c) ), which is the wellknown aliasing effect. Based on the F-Principle, as demonstrated in Fig. 4 (c), these artificial low frequency components will be first captured to explain the training samples, whereas the high frequency components will be compromised by DNN. For MNIST/CIFAR10, since the power of high frequencies is much smaller than that of low frequencies, artificial low frequencies caused by aliasing can be neglected. To conclude, the distribution of power in Fourier domain of above two types of problems exhibits significant differences, which result in different generalization performances of DNNs according to the F-Principle.","In real problems, we found that DNNs often fit target functions from low to high frequencies during the training process.",Xu et al. ; CNN ; The F-Principle ; i∈S ; linear ; Luo et al. ; MSE ; Krizhevsky et al. ; Fig ; LeCun,poor generalization ; various scientific computing problems ; deep learning ; These results ; example ; the training data ; insufficient training samples ; lower frequencies ; aliasing ; detail,Xu et al. ; CNN ; The F-Principle ; i∈S ; linear ; Luo et al. ; MSE ; Krizhevsky et al. ; Fig ; LeCun,"The training process of Deep Neural Networks (DNNs) is a fundamental problem in the area of deep learning. DNNs often fit target functions from low to high frequencies on high-dimensional benchmark datasets, such as MNIST/CIFAR10, deep networks, and deep networks. This F-Principle is opposite to the learning behavior of most conventional iterative numerical schemes (e.g., Jacobi method), which exhibits faster convergence for higher frequencies, for various scientific computing problems. However, it is important to understand the training process in deep learning, due to the regularity of activation functions. The F-",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Long short-term memory networks (LSTMs) were introduced to combat vanishing gradients in simple recurrent neural networks (S-RNNs) by augmenting them with additive recurrent connections controlled by gates. We present an alternate view to explain the success of LSTMs: the gates themselves are powerful recurrent models that provide more representational power than previously appreciated. We do this by showing that the LSTM's gates can be decoupled from the embedded S-RNN, producing a restricted class of RNNs where the main recurrence computes an element-wise weighted sum of context-independent functions of the inputs. Experiments on a range of challenging NLP problems demonstrate that the simplified gate-based models work substantially better than S-RNNs, and often just as well as the original LSTMs, strongly suggesting that the gates are doing much more in practice than just alleviating vanishing gradients. Long short-term memory networks (LSTM) BID17 have become the de-facto recurrent neural network (RNN) for learning representations of sequences in many research areas, including natural language processing (NLP). Like simple recurrent neural networks (SRNNs) BID11 , LSTMs are able to learn non-linear functions of arbitrary-length input sequences. However, they also introduce an additional memory cell to mitigate the vanishing gradient problem BID16 BID4 . This memory is controlled by a mechanism of gates, whose additive connections allow long-distance dependencies to be learned more easily during backpropagation. While this view is mathematically accurate, in this paper we argue that it does not provide a complete picture of why LSTMs work in practice.We present an alternate view to explain the success of LSTMs: the gates themselves are powerful recurrent models that provide more representational power than previously appreciated. To demonstrate this, we first show that LSTMs can be seen as a combination of two recurrent models: (1) an S-RNN, and (2) an element-wise weighted sum of the S-RNN's outputs over time, which is implicitly computed by the gates. We hypothesize that, for many practical NLP problems, the weighted sum serves as the main modeling component. The S-RNN, while theoretically expressive, is in practice only a minor contributor that clouds the mathematical clarity of the model. By replacing the S-RNN with a context-independent function of the input, we arrive at a much more restricted class of RNNs, where the main recurrence is via the element-wise weighted sums that the gates are computing.We test our hypothesis on NLP problems, where LSTMs are wildly popular at least in part due to their ability to model crucial language phenomena such as word order BID0 , syntactic structure BID23 , and even long-range semantic dependencies BID15 . We consider four challenging tasks: language modeling, question answering, dependency parsing, and machine translation. Experiments show that while removing the gates from an LSTM can severely hurt performance, replacing the S-RNN with a simple linear transformation of the input results in minimal or no loss in model performance. We further show that in many cases, LSTMs can be further simplified by removing the output gate, arriving at an even more transparent architecture, where the output is a context-independent function of the weighted sum. Together, these results suggest that the gates' ability to compute an element-wise weighted sum, rather than the non-linear transition dynamics of S-RNNs, are the driving force behind LSTM's success. In the above experiments, we show three major ablations of the LSTM. In the S-RNN experiments (LSTM -GATES), we ablate the memory cell and the output layer. In the LSTM -S-RNN and LSTM -S-RNN -OUT experiments, we ablate the S-RNN. As consistent with previous literature, removing the memory cell degrades performance drastically. In contrast, removing the S-RNN makes little to no difference in the final performance, suggesting that the memory cell alone is largely responsible for the success of LSTMs in NLP. The results also confirm our hypothesis that weighted sums of context words is a powerful, yet more interpretable, model of contextual information. We presented an alternate view of LSTMs: they are a hybrid of S-RNNs and a gated model that dynamically computes weighted sums of the S-RNN outputs. Our experiments investigated whether the S-RNN is a necessary component of LSTMs. In other words, are the gates alone as powerful of a model as an LSTM? Results across four major NLP tasks (language modeling, question answering, dependency parsing, and machine translation) indicate that LSTMs suffer little to no performance loss when removing the S-RNN, but removing the gates can degrade performance substantially. This provides evidence that the gating mechanism is doing the heavy lifting in modeling context, and that element-wise weighted sums of context-independent functions of the inputs are often as effective as fully-parameterized LSTMs.This work sheds light on the inner workings of the relatively opaque LSTM. By removing the S-RNN and the output gate, we also show that the resulting model is a far more mathematically transparent variant of LSTMs. This transparency enables a visualization of how the context affects the output of the model at every timestep, much like in attention-based models. We hope that this new outlook on LSTMs will foster better and more efficient models of contextualization.","Gates do all the heavy lifting in LSTMs by computing element-wise weighted sums, and removing the internal simple RNN does not degrade model performance.",NLP ; RNN ; first ; two ; the S-RNN's ; four ; three,the S-RNN outputs ; the model ; natural language processing ; fully-parameterized LSTMs ; attention-based models ; This work ; two recurrent models ; even long-range semantic dependencies ; an additional memory cell ; four major NLP tasks,NLP ; RNN ; first ; two ; the S-RNN's ; four ; three,"Long-term memory networks (LSTMs) were introduced to combat vanishing gradients in simple recurrent neural networks (S-RNNs) by augmenting them with additive recurrent connections controlled by gates. The gates themselves are powerful recurrent models that provide more representational power than previously appreciated. The LSTM's gates can be decoupled from the embedded S-RN, creating a restricted class of RNNs where the main recurrence computes an element-wise weighted sum of context-independent functions of inputs. Experiments on a range of challenging NLP problems demonstrate that LSTMs perform substantially better than S",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Deep Neural Networks (DNNs) are increasingly deployed in cloud servers and autonomous agents due to their superior performance. The deployed DNN is either leveraged in a white-box setting (model internals are publicly known) or a black-box setting (only model outputs are known) depending on the application. A practical concern in the rush to adopt DNNs is protecting the models against Intellectual Property (IP) infringement. We propose BlackMarks, the first end-to-end multi-bit watermarking framework that is applicable in the black-box scenario. BlackMarks takes the pre-trained unmarked model and the owner’s binary signature as inputs. The output is the corresponding marked model with specific keys that can be later used to trigger the embedded watermark. To do so, BlackMarks first designs a model-dependent encoding scheme that maps all possible classes in the task to bit ‘0’ and bit ‘1’. Given the owner’s watermark signature (a binary string), a set of key image and label pairs is designed using targeted adversarial attacks. The watermark (WM) is then encoded in the distribution of output activations of the DNN by fine-tuning the model with a WM-specific regularized loss. To extract the WM, BlackMarks queries the model with the WM key images and decodes the owner’s signature from the corresponding predictions using the designed encoding scheme. We perform a comprehensive evaluation of BlackMarks’ performance on MNIST, CIFAR-10, ImageNet datasets and corroborate its effectiveness and robustness. BlackMarks preserves the functionality of the original DNN and incurs negligible WM embedding overhead as low as 2.054%. Deep neural networks and other Deep Learning (DL) variants have revolutionized various critical fields ranging from biomedical diagnosis and autonomous transportation to computer vision and natural language processing BID7 BID25 . Training a highly accurate DNN is a costly process since it requires: (i) processing massive amounts of data acquired for the target application; (ii) allocating substantial computing resources to fine-tune the underlying topology (i.e., type and number of hidden layers), and hyper-parameters (i.e., learning rate, batch size), and DNN weights to obtain the most accurate model. Therefore, developing a high-performance DNN is impractical for the majority of customers with constrained computational capabilities. Given the costly process of designing/training, DNNs are typically considered to be the intellectual property of the model builder and needs to be protected to preserve the owner's competitive advantage.Digital watermarking has been immensely leveraged over the past decade for ownership protection in the multimedia domain where the host of the watermark can be images, video contents, and functional artifacts such as digital integrated circuits BID9 BID12 BID24 . However, the development of DNN watermarking techniques is still in its early stage. Designing a coherent DNN watermarking scheme for model ownership proof is challenging since the embedded WM is required to yield high detection rates and withstand potential attacks while minimally affecting the original functionality and overhead of the target DNN.Existing DNN watermarking techniques can be categorized into two types depending on the application scenario. 'White-box' watermarking assumes the availability of model internals (e.g., weights) for WM extraction BID31 whereas 'black-box' watermarking assumes that the output predictions can be obtained for WM detection BID20 BID1 . On the one hand, white-box WMs have a larger capacity (carrying multiple-bit information) but limited appli-cability due to the strong assumption. On the other hand, black-box WMs enable IP protection for Machine Learning as a Service (MLaaS) BID25 where only zero-bit watermarking methods have been proposed. It is desirable to develop a systematic watermarking approach that combines the advantages of both types of WMs. While all present black-box watermarking papers embed the WM as a statistical bias in the decision boundaries of the DNN (high accuracy on the WM trigger set), our work is the first to prove that it is feasible to leverage the model's predictions to carry a multi-bit string instead of a one-bit boolean decision (existence or not of the WM).By introducing BlackMarks, this paper makes the following contributions:• Proposing BlackMarks, the first end-to-end black-box watermarking framework that enables multi-bit WM embedding. BlackMarks possesses higher capacity compared to prior works and only requires the predictions of the queried model for WM extraction.• Characterizing the requirements for an effective watermarking methodology in the deep learning domain. Such metrics provide new perspectives for model designers and enable coherent comparison of current and pending DNN IP protection techniques.• Performing extensive evaluation of BlackMarks' performance on various benchmarks. Experimental results show that BlackMarks enables robust WM embedding with high detection rates and low false alarm rates. As a side benefit, we find out that BlackMarks' WM embedding process improves the robustness of the model against adversarial attacks. Recall that WM embedding leverages a similar approach as 'adversarial training' while incorporating a WM-specific regularization loss (Section 4.1). Here, we study the effect of WM embedding on the model's robustness against adversarial attacks. TAB8 in Appendix A.3 compares the robustness of the pre-trained unmarked model and the corresponding watermarked model (K = 50) against different white-box adversarial attacks. It can be seen that for each type of the attack, the marked model has higher accuracy on the adversarial samples compared to the unmarked baseline. Such improvement is intuitive to understand since during WM embedding, the first term (cross-entropy loss) in the total regularized loss (see Eq.(1)) enforces the model to learn the correct predictions on training data as well as on the WM keys ('adversarial samples'), thus having a similar effect as 'adversarial training' BID16 . Therefore, BlackMarks has a side benefit of improving the model's robustness against adversarial attacks.In the future, we plan to extend BlackMarks framework to the multi-user setting for fingerprinting purpose. BID3 present the first collusion-resilient DNN fingerprinting approach for unique user tracking in the white-box setting. To the best of our knowledge, no black-box fingerprinting has been proposed due to the lack of black-box multi-bit watermarking schemes. BlackMarks proves the feasibility of black-box fingerprinting methods and builds the technical foundation. We propose BlackMarks, the first black-box multi-bit watermarking framework for IP protection of DNNs. To the best of our knowledge, this work provides the first empirical evidence that embedding and extracting multi-bit information using the model's predictions are possible. Our comprehensive evaluation of BlackMarks' performance on various benchmarks corroborates that BlackMarks coherently embeds robust watermarks in the output predictions of the target DNN with an additional overhead as low as 2.054%. BlackMarks possesses superior capacity compared to all existing zerobit watermarking techniques and paves the way for future black-box fingerprinting techniques.For ImageNet dataset (where the total number of categories is C = 1000), the sizes of the code-bit cluster '0' and cluster '1' are larger than ones in MNIST and CIFAR-10 dataset. Therefore, the searching space for targeted adversarial samples is larger and the probability of WM key collision is smaller, ensuring the robustness of the generated WM keys against the WM over-writing attack.JSMA is not applied to the ImageNet benchmark since the excessive memory requirement BID32 ) cannot be satisfied by our 11.74GiB test machine.Model Fine-tuning for WM Embedding. To embed the WM, we set the hyper-parameter λ to 0.5 for MNIST and CIFAR-10 benchmark, and to 0.01 for ImageNet benchmark in our experiments. The pre-trained unmarked model is fine-tuned for 15 epochs with the regularized loss in Eq. FORMULA0 for all benchmarks. We use the same batch size and the optimizer setting used for training the original neural network, except that the learning rate is reduced by a factor of 10. Such retraining procedure coherently encodes the WM key in the distribution of output activations while preventing the accuracy drop on the legitimate data.",Proposing the first watermarking framework for multi-bit signature embedding and extraction using the outputs of the DNN.,techniques.• Performing ; ImageNet ; Digital ; Machine Learning ; Model Fine-tuning ; first ; the past decade ; λ to ; two ; IP,all benchmarks ; the advantages ; the generated WM ; substantial computing resources ; WM key collision ; Our comprehensive evaluation ; IP protection ; the predictions ; the most accurate model ; high detection rates,techniques.• Performing ; ImageNet ; Digital ; Machine Learning ; Model Fine-tuning ; first ; the past decade ; λ to ; two ; IP,"Deep Neural Networks (DNNs) are used in cloud servers and autonomous agents due to their superior performance. The deployed DNN is either leveraged in a white-box setting (model internals are publicly known) or a black-box settings (only model outputs are known) depending on the application. BlackMarks, the first end-to-end multi-bit watermarking framework, uses the pre-trained unmarked model and the owner’s binary signature as inputs. The output is the corresponding marked model with specific keys that can be later used to trigger the embedded watermark. The model-dependent encoding",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Convolutional Neural Networks (CNNs) are composed of multiple convolution layers and show elegant performance in vision tasks.
 The design of the regular convolution is based on the Receptive Field (RF) where the information within a specific region is processed.
 In the view of the regular convolution's RF, the outputs of neurons in lower layers with smaller RF are bundled to create neurons in higher layers with larger RF. 
 As a result, the neurons in high layers are able to capture the global context even though the neurons in low layers only see the local information.
 However, in lower layers of the biological brain, the information outside of the RF changes the properties of neurons.
 In this work, we extend the regular convolution and propose spatially shuffled convolution (ss convolution).
 In ss convolution, the regular convolution is able to use the information outside of its RF by spatial shuffling which is a simple and lightweight operation.
 We perform experiments on CIFAR-10 and ImageNet-1k dataset, and show that ss convolution improves the classification performance across various CNNs. Convolutional Neural Networks (CNNs) and their convolution layers (Fukushima, 1980; Lecun et al., 1998) are inspired by the finding in cat visual cortex (Hubel & Wiesel, 1959) and they show the strong performance in various domains such as image recognition (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; He et al., 2016) , natural language processing (Gehring et al., 2017) , and speech recognition (Abdel-Hamid et al., 2014; Zhang et al., 2016) . A notable characteristic of the convolution layer is the Receptive Field (RF), which is the particular input region where a convolutional output is affected by. The units (or neurons) in higher layers have larger RF by bundling the outputs of the units in lower layers with smaller RF. Thanks to the hierarchical architectures of CNNs, the units in high layers are able to capture the global context even though the units in low layers only see the local information. It is known that neurons in the primary visual cortex (i.e., V1 which is low layers) change the selfproperties (e.g., the RF size (Pettet & Gilbert, 1992) and the facilitation effect (Nelson & Frost, 1985) ) based on the information outside of the RF (D. Gilbert, 1992) . The mechanism is believed to originate from (1) feedbacks from the higher-order area (Iacaruso et al., 2017) and (2) intracortical horizontal connections (D. Gilbert, 1992) . The feedbacks from the higher-order area convey broader-contextual information than the neurons in V1, which allows the neurons in V1 to use the global context. For instance, Gilbert & Li (2013) argued that the feedback connections work as attention. Horizontal connections allow the distanced neurons in the layer to communicate with each other and are believed to play an important role in visual contour integration (Li & Gilbert, 2002) and object grouping (Schmidt et al., 2006) . Though both horizontal and feedback connections are believed to be important for visual processing in the visual cortex, the regular convolution ignores the properties of these connections. In this work, we particularly focus on algorithms to introduce the function of horizontal connections for the regular convolution in CNNs. We propose spatially shuffled convolution (ss convolution), where the information outside of the regular convolution's RF is incorporated by spatial shuffling, which is a simple and lightweight operation. Our ss convolution is the same operation as the regular convolution except for spatial shuffling and requires no extra learnable parameters. The design of ss convolution is highly inspired by the function of horizontal connections. To test the effectiveness of the information outside of the regular convolution's RF in CNNs, we perform experiments on CIFAR-10 (Krizhevsky, 2009) and ImageNet 2012 dataset (Russakovsky et al., 2015) and show that ss convolution improves the classification performance across various CNNs. These results indicate that the information outside of the RF is useful when processing local information. In addition, we conduct several analyses to examine why ss convolution improves the classification performance in CNNs and show that spatial shuffling allows the regular convolution to use the information outside of its RF. In this work, we propose spatially shuffled convolution (ss convolution) to incorporate the function of horizontal connections in the regular convolution. The spatial shuffling is simple, lightweight, and requires no extra learnable parameters. The experimental results demonstrate that ss convolution captures the information outside of the regular convolution's RF even in lower layers. The results and our analyses also suggest that using distant information (i.e., non-local) is effective for the regular convolution and improves classification performance across various CNNs. Figure 6: The receptive field of ImageNet-1k pre-trained ResNet50. The red color indicates that the pixel there changes features inside the blue box, and the white color represents that features are invariant even if the pixel there changes the value itself. Those images are the receptive field of all layers and the name of the layer is described in Table 5 (a) conv2 1 Figure 7: The receptive field of ImageNet-1k pre-trained ResNet50 with ss convolutions. The red color indicates that the pixel there changes features inside the blue box, and the white color represents that features are invariant even if the pixel there changes the value itself. Those images are the receptive field of all layers and the name of the layer is described in Table 5 (a) conv2 1 Figure 8: The receptive field of ImageNet-1k pre-trained SEResNet50. The red color indicates that the pixel there changes features inside the blue box, and the white color represents that features are invariant even if the pixel there changes the value itself. Those images are the receptive field of all layers and the name of the layer is described in Table 5 l e n g t h = i n t ( c h n s / c h s )",We propose spatially shuffled convolution that the regular convolution incorporates the information from outside of its receptive field.,Schmidt ; Li & Gilbert ; al. ; Abdel-Hamid ; Russakovsky ; Krizhevsky et al. ; Simonyan & Zisserman ; Nelson & Frost ; Zhang et al. ; RF,"e.g., the RF size ; Figure ; the biological brain ; ss convolutions ; spatially shuffled convolution ; classification performance ; al ; cat visual cortex ; t h ; the white color",Schmidt ; Li & Gilbert ; al. ; Abdel-Hamid ; Russakovsky ; Krizhevsky et al. ; Simonyan & Zisserman ; Nelson & Frost ; Zhang et al. ; RF,"Convolutional Neural Networks (CNNs) are composed of multiple convolution layers and show elegant performance in vision tasks. The Receptive Field (RF) where the information within a specific region is processed is bundled to create neurons in higher layers with larger RF. In lower layers of the biological brain, the information outside of the RF changes the properties of neurons. In this work, we extend the regular convolution and propose spatially shuffled convolution (ss convolution). In ss convolution, the output of units (or neurons) in lower layers with smaller RF are bundled together to create higher layers of neurons with larger",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Sample inefficiency is a long-lasting problem in reinforcement learning (RL).   The state-of-the-art uses action value function to derive policy while it usually involves an extensive search over the state-action space and unstable optimization. Towards the sample-efficient RL, we propose ranking policy gradient (RPG), a policy gradient method that learns the optimal rank of a set of discrete actions.   To accelerate the learning of policy gradient methods, we establish the equivalence between maximizing the lower bound of return and imitating a near-optimal policy without accessing any oracles. These results lead to a general off-policy learning framework, which preserves the optimality, reduces variance, and improves the sample-efficiency. We conduct extensive experiments showing that when consolidating with the off-policy learning framework, RPG substantially reduces the sample complexity, comparing to the state-of-the-art. One of the major challenges in reinforcement learning (RL) is the high sample complexity (Kakade et al., 2003) , which is the number of samples must be collected to conduct successful learning. There are different reasons leading to poor sample efficiency of RL (Yu, 2018) . Because policy gradient algorithms directly optimizing return estimated from rollouts (e.g., REINFORCE (Williams, 1992) ) could suffer from high variance (Sutton & Barto, 2018) , value function baselines were introduced by actor-critic methods to reduce the variance and improve the sample-efficiency. However, since a value function is associated with a certain policy, the samples collected by former policies cannot be readily used without complicated manipulations (Degris et al., 2012) and extensive parameter tuning (Nachum et al., 2017) . Such an on-policy requirement increases the difficulty of sampleefficient learning. On the other hand, off-policy methods, such as one-step Q-learning (Watkins & Dayan, 1992) and variants of deep Q networks (DQN) (Mnih et al., 2015; Hessel et al., 2017; Dabney et al., 2018; Van Hasselt et al., 2016; Schaul et al., 2015) , enjoys the advantage of learning from any trajectory sampled from the same environment (i.e., off-policy learning), are currently among the most sampleefficient algorithms. These algorithms, however, often require extensive searching (Bertsekas & Tsitsiklis, 1996, Chap. 5) over the large state-action space to estimate the optimal action value function. Another deficiency is that, the combination of off-policy learning, bootstrapping, and function approximation, making up what Sutton & Barto (2018) called the ""deadly triad"", can easily lead to unstable or even divergent learning (Sutton & Barto, 2018, Chap. 11) . These inherent issues limit their sample-efficiency. Towards addressing the aforementioned challenge, we approach the sample-efficient reinforcement learning from a ranking perspective. Instead of estimating optimal action value function, we concentrate on learning optimal rank of actions. The rank of actions depends on the relative action values. As long as the relative action values preserve the same rank of actions as the optimal action values (Q-values), we choose the same optimal action. To learn optimal relative action values, we propose the ranking policy gradient (RPG) that optimizes the actions' rank with respect to the long-term reward by learning the pairwise relationship among actions. Ranking Policy Gradient (RPG) that directly optimizes relative action values to maximize the return is a policy gradient method. The track of off-policy actor-critic methods (Degris et al., 2012; Gu et al., 2016; Wang et al., 2016) have made substantial progress on improving the sample-efficiency of policy gradient. However, the fundamental difficulty of learning stability associated with the bias-variance trade-off remains (Nachum et al., 2017) . In this work, we first exploit the equivalence between RL optimizing the lower bound of return and supervised learning that imitates a specific optimal policy. Build upon this theoretical foundation, we propose a general off-policy learning framework that equips the generalized policy iteration (Sutton & Barto, 2018, Chap. 4) with an external step of supervised learning. The proposed off-policy learning not only enjoys the property of optimality preserving (unbiasedness), but also largely reduces the variance of policy gradient because of its independence of the horizon and reward scale. Besides, we empirically show that there is a trade-off between optimality and sample-efficiency. Last but not least, we demonstrate that the proposed approach, consolidating the RPG with off-policy learning, significantly outperforms the state-of-the-art (Hessel et al., 2017; Bellemare et al., 2017; Dabney et al., 2018; Mnih et al., 2015) . In this work, we introduced ranking policy gradient (RPG) methods that, for the first time, resolve RL problem from a ranking perspective. Furthermore, towards the sample-efficient RL, we propose an off-policy learning framework that allows RL agents to be trained in a supervised learning paradigm. The off-policy learning framework uses generalized policy iteration for exploration and exploit the stableness of supervised learning for policy learning, which accomplishes the unbiasedness, variance reduction, off-policy learning, and sample efficiency at the same time. Last but not least, empirical results show that RPG achieves superior performance as compared to the state-of-the-art. Corollary 3. The pairwise ranking policy as shown in Eq (2) constructs a probability distribution over the set of actions when the action space m is equal to 2, given any relative action values λi, i = 1, 2. For the cases with m > 2, this conclusion does not hold in general. It is easy to verify that π(ai|s) > 0, ∑ 2 i=1 π(ai|s) = 1 holds and the same conclusion cannot be applied to m > 2 by constructing counterexamples. However, we can introduce a dummy action a ′ to form a probability distribution for RPG. During policy learning, the algorithm will increase the probability of best actions and the probability of dummy action will decrease. Ideally, if RPG converges to an optimal deterministic policy, the probability of taking best action is equal to one and π(a ′ |s) = 0. Similarly, we can introduce a dummy trajectory τ ′ with trajectory reward r(τ The trajectory probability forms a probability distribution since The proof of a valid trajectory probability is similar to the following proof on π(a|s) is a valid probability distribution with a dummy action. The practical influence of this is negligible since our goal is to increase the probability of (near)-optimal trajectories. To present in a clear way, we avoid mentioning dummy trajectory τ ′ in Proof 9.2 while it can be seamlessly included. This condition can be easily satisfied since in RPG we only focus on the relative relationship of λ-values and we can constrain its range so that λm satisfies the condition 1. Furthermore, since we can see that m 1 m−1 > 1 is decreasing w.r.t to action dimension m. The larger the action dimension, the less constraint we have on the λ-values. ′ and set π(a = a ′ |s) = 1 − ∑ i π(a = ai|s), which will construct a valid probability distribution (π(a|s)) over the action space A ∪ a ′ . Proof. Since we have π(a = ai|s) > 0 ∀i = 1, ..., m and ∑ i π(a = ai|s) + π(a = a ′ |s) = 1. To prove this is a valid probability distribution, we only need to show that π(a = a ′ |s) ≥ 0, ∀m ≥ 2, i.e. 9.4 LISTWISE POLICY GRADIENT In order to learn the stochastic policy that optimizes the ranking of actions with respect to the return, we now introduce the Listwise Policy Gradient (LPG) method. In RL, we want to optimize the probability of each action (ai) to be ranked higher among all actions, which is the sum of the probabilities of all permutations such that the action ai in the top position of the list. This probability is computationally prohibitive since we need to consider the probability of m! permutations. Luckily, based on Cao et al. (2007) [Theorem 6], we can model the such probability of action ai to be ranked highest given a set of relative action values by a simple softmax formulation, as described in Theorem 3. Theorem 3 (Theorem 6 Cao et al. (2007) where ϕ( * ) is any increasing, strictly positive function. A common choice of ϕ is the exponential function. Closely built upon the foundations from learning to rank Cao et al. (2007) where the listwise ranking policy π θ parameterized by θ is given by Eq (17) for tasks with deterministic optimal policies: a = arg max or Eq (18) is the probability that action i being ranked highest, given the current state and all the relative action values λ1 . . . λm. The proof of Theorem 4 exactly follows the direct policy differentiation Peters & Schaal (2008); Williams (1992) by replacing the policy to the form of the softmax function. The action probability π(ai|s), ∀i = 1, ..., m forms a probability distribution over the set of discrete actions [Cao et al. (2007) Lemma 7] . Theorem 4 states that the vanilla policy gradient Williams (1992) parameterized by a softmax layer is optimizing the probability of each action to be ranked highest, with respect to the long-term reward. Condition 2 If we want to preserve the optimality by TRS, the optimal trajectories of MDP needs to cover all initial states or equivalently, all initial states will lead to at least one optimal trajectory. Similarly, the near-optimality is preserved for all MDPs that its near-optimal trajectories cover all initial states. Theoretically, it is possible to transfer more general MDPs to satisfy Condition 2 and preserve the optimality with potential-based reward shaping Ng et al. (1999) . More concretely, consider the deterministic binary tree MDP (M1) with the set of initial states S1 = {s1, s . This reward shaping requires more prior knowledge, which may not be feasible in practice. A more realistic method is to design a dynamic trajectory reward shaping approach. In the beginning, we set c(s) = mins∈S 1 r(τ |s(τ, 1) = s), ∀s ∈ S1. Take M1 as an example, c(s) = 3, ∀s ∈ S1. During the exploration stage, we track the current best trajectory of each initial state and update c(s) with its trajectory reward. Nevertheless, if the Condition 2 is not satisfied, we need more sophisticated prior knowledge other than a predefined trajectory reward threshold c to construct the replay buffer (training dataset of UNOP). The practical implementation of trajectory reward shaping and rigorously theoretical study for general MDPs are beyond the scope of this work. Under review as a conference paper at ICLR 2020","We propose ranking policy gradient that learns the optimal rank of actions to maximize return. We propose a general off-policy learning framework with the properties of optimality preserving, variance reduction, and sample-efficiency.",Peters & Schaal ; Watkins & Dayan ; Van Hasselt ; first ; ∀s ; al. ; Chap ; Schaul et al. ; Bertsekas & Tsitsiklis ; Williams,"the probabilities ; the policy ; a value function ; any increasing, strictly positive function ; more prior knowledge ; Another deficiency ; the actions' rank ; the other hand ; ICLR ; the softmax function",Peters & Schaal ; Watkins & Dayan ; Van Hasselt ; first ; ∀s ; al. ; Chap ; Schaul et al. ; Bertsekas & Tsitsiklis ; Williams,"The state-of-the-art uses action value function to derive policy while it involves extensive search over the state-action space and unstable optimization. To accelerate the learning of policy gradient methods, we establish the equivalence between maximizing the lower bound of return and imitating a near-optimal policy without access to any oracles. These results lead to a general off-policy learning framework, which preserves the optimality, reduces variance, and improves the sample-efficiency. In the context of reinforcement learning (RL), value function baselines are introduced by actor-critic methods to reduce the variance and improve sample efficiency. These",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"The notion of the stationary equilibrium ensemble has played a central role in statistical mechanics. In machine learning as well, training serves as generalized equilibration that drives the probability distribution of model parameters toward stationarity. Here, we derive stationary fluctuation-dissipation relations that link measurable quantities and hyperparameters in the stochastic gradient descent algorithm. These relations hold exactly for any stationary state and can in particular be used to adaptively set training schedule. We can further use the relations to efficiently extract information pertaining to a loss-function landscape such as the magnitudes of its Hessian and anharmonicity. Our claims are empirically verified. Equilibration rules the long-term fate of many macroscopic dynamical systems. For instance, as we pour water into a glass and let it be, the stationary state of tranquility is eventually attained. Zooming into the tranquil water with a microscope would reveal, however, a turmoil of stochastic fluctuations that maintain the apparent stationarity in balance. This is vividly exemplified by the Brownian motion BID3 : a pollen immersed in water is constantly bombarded by jittery molecular movements, resulting in the macroscopically observable diffusive motion of the solute. Out of the effort in bridging microscopic and macroscopic realms through the Brownian movement came a prototype of fluctuation-dissipation relations BID6 BID37 . These relations quantitatively link degrees of noisy microscopic fluctuations to smooth macroscopic dissipative phenomena and have since been codified in the linear response theory for physical systems BID28 BID9 BID16 , a cornerstone of statistical mechanics.Machine learning begets another form of equilibration. As a model learns patterns in data, its performance first improves and then plateaus, again reaching apparent stationarity. This dynamical process naturally comes equipped with stochastic fluctuations as well: often given data too gigantic to consume at once, training proceeds in small batches and random selections of these mini-batches consequently give rise to the noisy dynamical excursion of the model parameters in the loss-function landscape, reminiscent of the Brownian motion. It is thus natural to wonder if there exist analogous fluctuation-dissipation relations that quantitatively link the noise in mini-batched data to the observable evolution of the model performance and that in turn facilitate the learning process.Here, we derive such fluctuation-dissipation relations for the stochastic gradient descent algorithm. The only assumption made is stationarity of the probability distribution that governs the model parameters at sufficiently long time. Our results thus apply to generic cases with non-Gaussian mini-batch noises and nonconvex loss-function landscapes. Practically, the first relation (FDR1) offers the metric for assessing equilibration and yields an adaptive algorithm that sets learning-rate schedule on the fly. The second relation (FDR2) further helps us determine the properties of the lossfunction landscape, including the strength of its Hessian and the degree of anharmonicity, i.e., the deviation from the idealized harmonic limit of a quadratic loss surface and a constant noise matrix.Our approach should be contrasted with recent attempts to import the machinery of stochastic differential calculus into the study of the stochastic gradient descent algorithm BID21 BID20 BID22 BID19 BID33 BID4 BID12 BID39 . This line of work all assumes Gaussian noises and sometimes additionally employs the quadratic harmonic approximation for loss-function landscapes. The more severe drawback, however, is the usage of the analogy with continuous-time stochastic differential equations, which is inconsistent in general (see Section 2.3.3). Instead, the stochastic gradient descent algorithm can be properly treated within the framework of the KramersMoyal expansion BID36 BID7 BID30 BID29 BID18 .The paper is organized as follows. In Section 2, after setting up notations and deriving a stationary fluctuation-dissipation theorem (FDT), we derive two specific fluctuation-dissipation relations. The first relation (FDR1) can be used to check stationarity and the second relation (FDR2) to delineate the shape of the loss-function landscape, as empirically borne out in Section 3. An adaptive scheduling method is proposed and tested in Section 3.3. We conclude in Section 4 with future outlooks. In this paper, we have derived the fluctuation-dissipation relations with no assumptions other than stationarity of the probability distribution. These relations hold exactly even when the noise is nonGaussian and the loss function is nonconvex. The relations have been empirically verified and used to probe the properties of the loss-function landscapes for the simple models. The relations further have resulted in the algorithm to adaptively set learning-rate schedule on the fly rather than presetting it in an ad hoc manner. In addition to systematically testing the performance of this adaptive scheduling algorithm, it would be interesting to investigate non-Gaussianity and noncovexity in more details through higher-point observables, both analytically and numerically. It would also be interesting to further elucidate the physics of machine learning by extending our formalism to incorporate nonstationary dynamics, linearly away from stationarity BID28 BID9 BID16 and beyond BID11 BID5 , so that it can in particular properly treat overfitting cascading dynamics and time-dependent sample distributions.","We prove fluctuation-dissipation relations for SGD, which can be used to (i) adaptively set learning rates and (ii) probe loss surfaces.",FDT ; non-Gaussian ; noisy microscopic ; KramersMoyal ; two ; second ; first ; Gaussian ; Brownian ; nonGaussian,Gaussian noises ; a stationary fluctuation-dissipation theorem ; a glass ; continuous-time stochastic differential equations ; the simple models ; the effort ; a model ; the tranquil water ; our formalism ; two specific fluctuation-dissipation relations,FDT ; non-Gaussian ; noisy microscopic ; KramersMoyal ; two ; second ; first ; Gaussian ; Brownian ; nonGaussian,"In machine learning, training drives the probability distribution of model parameters toward stationarity. In the stochastic gradient descent algorithm, stationary fluctuation-dissipation relations link measurable quantities and hyperparameters. These relations can be used to adaptively set training schedules and efficiently extract information pertaining to a loss-function landscape such as the magnitudes of its Hessian and anharmonicity. Equilibration rules the long-term fate of many macroscopic dynamical systems. For instance, as we pour water into a glass and let it be, the stationary state of tranquility is eventually attained. However, stochastics",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate mutual information (MI) between each hidden layer and the input/desired output, to construct the IP. For instance, hidden layers with many neurons require MI estimators with robustness towards the high dimensionality associated with such layers. MI estimators should also be able to naturally handle convolutional layers, while at the same time being computationally tractable to scale to large networks. None of the existing IP methods to date have been able to study truly deep Convolutional Neural Networks (CNNs), such as the e.g.\ VGG-16. In this paper, we propose an IP analysis using the new matrix--based R\'enyi's entropy coupled with tensor kernels over convolutional layers, leveraging the power of kernel methods to represent properties of the probability distribution independently of the dimensionality of the data. The obtained results shed new light on the previous literature concerning small-scale DNNs, however using a completely new approach. Importantly, the new framework enables us to provide the first comprehensive IP analysis of contemporary large-scale DNNs and CNNs, investigating the different training phases and providing new insights into the training dynamics of large-scale neural networks. Although Deep Neural Networks (DNNs) are at the core of most state-of-the art systems in computer vision, the theoretical understanding of such networks is still not at a satisfactory level (Shwartz-Ziv & Tishby, 2017) . In order to provide insight into the inner workings of DNNs, the prospect of utilizing the Mutual Information (MI), a measure of dependency between two random variables, has recently garnered a significant amount of attention (Cheng et al., 2018; Noshad et al., 2019; Saxe et al., 2018; Shwartz-Ziv & Tishby, 2017; Yu et al., 2018; . Given the input variable X and the desired output Y for a supervised learning task, a DNN is viewed as a transformation of X into a representation that is favorable for obtaining a good prediction of Y . By treating the output of each hidden layer as a random variable T , one can model the MI I(X; T ) between X and T . Likewise, the MI I(T ; Y ) between T and Y can be modeled. The quantities I(X; T ) and I(T ; Y ) span what is referred to as the Information Plane (IP). Several works have demonstrated that one may unveil interesting properties of the training dynamics by analyzing DNNs in the form of the IP Goldfeld et al., 2019; Noshad et al., 2019; Chelombiev et al., 2019) . Figure 1 , produced using our proposed estimator, illustrates one such insight that is similar to the observations of Shwartz-Ziv & Tishby (2017) , where training can be separated into two distinct phases, the fitting phase and the compression phase. This claim has been highly debated as subsequent research has linked the compression phase to saturation of neurons (Saxe et al., 2018) or clustering of the hidden representations (Goldfeld et al., 2019) . Contributions We propose a novel approach for estimating MI, wherein a kernel tensor-based estimator of Rényi's entropy allows us to provide the first analysis of large-scale DNNs as commonly found in state-of-the-art methods. We further highlight that the multivariate matrix-based approach, proposed by , can be viewed as a special case of our approach. However, our proposed method alleviates numerical instabilities associated with the multivariate matrixbased approach, which enables estimation of entropy for high-dimensional multivariate data. Further, using the proposed estimator, we investigate the claim of Cheng et al. (2018) that the entropy H(X) ≈ I(T ; X) and H(Y ) ≈ I(T ; Y ) in high dimensions (in which case MI-based analysis would be meaningless) and illustrate that this does not hold for our estimator. Finally, our results indicate that the compression phase is apparent mostly for the training data, particularly for more challenging datasets. By utilizing a technique such as early-stopping, a common technique to avoid overfitting, training tends to stop before the compression phase occurs (see Figure 1 ). This may indicate that the compression phase is linked to the overfitting phenomena. Figure 1 : IP obtained using our proposed estimator for a small DNN averaged over 5 training runs. The solid black line illustrates the fitting phase while the dotted black line illustrates the compression phase. The iterations at which early stopping would be performed assuming a given patience parameter are highlighted. Here, patience denotes the number of iterations that need to pass without progress on a validation set before training is stopped to avoid overfitting. It can be observed that for low patience values, training will stop before the compression phase. For the benefit of the reader, the bottom right corner displays a magnified version of the first four layers. In this work, we propose a novel framework for analyzing DNNs from a MI perspective using a tensor-based estimate of the Rényi's α-order entropy. Our experiments illustrate that the proposed approach scales to large DNNs, which allows us to provide insights into the training dynamics. We observe that the compression phase in neural network training tends to be more prominent when MI is estimated on the training set and that commonly used early-stopping criteria tend to stop training before or at the onset of the compression phase. This could imply that the compression phase is linked to overfitting. Furthermore, we showed that, for our tensor-based approach, the claim that H(X) ≈ I(T ; X) and H(Y ) ≈ I(T ; Y ) does not hold. We believe that our proposed approach can provide new insight and facilitate a more theoretical understanding of DNNs.",First comprehensive information plane analysis of large scale deep neural networks using matrix based entropy and tensor kernels.,the Information Plane ; Cheng et al ; Shwartz-Ziv & Tishby ; four ; Goldfeld ; two ; Cheng et al. ; al. ; Noshad ; Chelombiev et al.,The solid black line ; H(X ; such layers ; patience ; properties ; the first comprehensive IP analysis ; our proposed method ; neurons ; our approach ; the existing IP methods,the Information Plane ; Cheng et al ; Shwartz-Ziv & Tishby ; four ; Goldfeld ; two ; Cheng et al. ; al. ; Noshad ; Chelombiev et al.,"Deep neural networks (DNNs) via information plane (IP) theory have gained significant attention recently as a tool to gain insight into their generalization ability. However, it is not clear how to estimate mutual information (MI) between each hidden layer and input/desired output, to construct the IP. MI estimators should be able to handle convolutional layers, while being computationally tractable to scale to large networks. The existing IP methods have not been able to study deep Convolutional Neural Networks (CNNs), such as the e.g.\ VGG-16. In this paper, we propose",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Probability density estimation is a classical and well studied problem, but standard density estimation methods have historically lacked the power to model complex and high-dimensional image distributions.   More recent generative models leverage the power of neural networks to implicitly learn and represent probability models over complex images.   We describe methods to extract explicit probability density estimates from GANs, and explore the properties of these image density functions.   We perform sanity check experiments to provide evidence that these probabilities are reasonable.   However, we also show that density functions of natural images are difficult to interpret and thus limited in use.   We study reasons for this lack of interpretability, and suggest that we can get better interpretability by doing density estimation on latent representations of images.   Researchers have long sought to estimate the probability density functions (PDFs) of images. The resulting generative models can be used in image synthesis, outlier detection, image restoration, and in classification. There have been some impressive successes, including building generative models of textures for texture synthesis, and using low-level statistical models for image denoising. However, building accurate densities for full, complex images remains challenging. Recently there has been a flurry of activity in building deep generative models of complex images, including the use of generative adversarial networks (GANs) (Goodfellow et al., 2014) to generate stunningly realistic complex images. While some deep models, like VAEs, focus explicitly on building probability densities of images, we focus on GANs, leveraging their rapid improvements. Implicitly, these GANs also encode probability densities. In this paper we explore whether these implicit densities capture the intuition of a probable image. We show that in some sense the answer is ""no"". But, we suggest that by computing PDFs over latent representations of images, we can do better. We first propose some methods for extracting probability densities from GANs. It is well known that when a bijective function maps one density to another, the relationship between the two densities can be understood using the determinant of the Jacobian of the function. GANs are not bijective, and map a low-dimensional latent space to a high-dimensional image space. In this case, we modify the standard formula so that we can extract the probability density value of an image given its latent representation. This allows us to compute densities of images generated by the GAN, which we then use to train a regressor that computes densities of arbitrary images. We perform sanity checks to ensure that GANs do indeed produce reasonable densities on images. We show that GANs produce similar densities for training images and for held out test images from the same distribution. We also show that when we compute the density of either real or generated images, the most likely (highest density value) images are of low complexity, and the least likely images are of high complexity. An example of this last result is shown in Figure 1 , which displays the images with highest and lowest densities among samples generated by a StackGAN (Zhang et al., 2017 ) and a StyleGAN (Karras et al., 2018) . The StackGAN images are conditioned on two different captions, and the StyleGAN images are from models trained on two different datasets. Unfortunately, we also show that probability densities learned on images are difficult to interpret and have unintuitive behaviors. The strong influence of visual complexity on the learned PDF causes irrelevant background details to dominate the shape of the distribution; we see that the most likely images tend to contain small objects with large, simple backgrounds, while images with complex backgrounds are deemed unlikely despite being otherwise sensible. For example, for a GAN trained on MNIST, all of the most likely digits are 1, despite each type of digit occurring in equal proportion in the training set. If we exclude 1s from the training data and then compute the densities of all MNIST digits under this altered distribution, the most likely digits are still 1s, even though the GAN never saw them during training. In fact, even if we train a GAN on CIFAR images of real objects, the GAN will produce higher densities for MNIST images of 1s than for most of the CIFAR images. Theoretically, this is not surprising: high-dimensional density functions tend to have peaks of very large probability density away from ""typical"" points. Consider the example of a high-dimensional Gaussian with an identity covariance matrix, which has large density values at its center, though most sampled points lie near the unit sphere. In practice, this becomes a problem when real images inhabit these high-density peaks, because . We investigate these unintuitive properties of density functions in detail, and explore reasons for this lack of interpretability. We propose to mitigate this problem by doing probability density estimation on the latent representations of the images, rather than their pixel representations. With this approach we obtain probability distributions with inliers and outliers that seem to coincide more closely with our intuition. In the Gaussian latent space, the problem of natural images lying near high-density peaks is mitigated: natural images correspond to latent codes near the unit sphere, putting them on more equal footing with one another. Outliers can then be detected by finding images with density values that are lower or higher than expected. In parallel to our work, Nalisnick et al. (2018) also addresses the interpretability of density functions over images, claiming that seemingly uninterpretable density estimates result from inaccurate estimation on out-of-sample images (Nalisnick et al., 2018) . Our thesis is different, as we argue that density estimation is often accurate even for unusual images, but the true underlying density function (even if known exactly) is fundamentally difficult to interpret. (Welinder et al., 2010) , conditioned on the caption ""A bird with a very long wing span and a long pointed beak."" Second row: Samples from StackGAN conditioned on the caption ""This bird has a white eye with a red round shaped beak."" Third row: Samples from a StyleGAN model pretrained on the LSUN Bedroom dataset (Yu et al., 2015) . Bottom row: Samples from a StyleGAN model pretrained on the Flickr-Faces-HQ dataset (Karras et al., 2018) . Using the power of GANs, we explored the density functions of complex image distributions. Unfortunately, inliers and outliers of these density functions cannot be readily interpreted as typical and atypical images, at least according to human intuition. However, we suggest that this lack of interpretability could be mitigated by considering the probability densities not of the images themselves, but of the latent codes that produced them. We postulate that such feature embeddings avoid the problems of pixel-space densities (which are too dependent on pixel-level image properties such as background uniformity), and instead allow for representations that are more semantically meaningful. There are a host of potential applications for the resulting image PDFs, including detecting outliers and domain shift that will be explored in future work. Other Ones Figure 8 : Left: histogram of log probability densities of MNIST and CIFAR, predicted using a pixel-space density estimator for CIFAR. Middle: histogram of log densities of MNIST and CIFAR, predicted using the latent code regressor for a GAN trained on CIFAR. Right: histogram of log densities of MNIST, as predicted by a latent code regressor for a GAN trained on MNIST. Note that the log density values are much more clustered than in pixel space, though they are still near the top of the distribution.",We examine the relationship between probability density values and image content in non-invertible GANs.,CIFAR ; Nalisnick et al ; StyleGAN ; Gaussian ; GAN ; Karras ; Yu et al. ; one ; PDF ; Third,"the most likely digits ; interpretability ; low-level statistical models ; highest and lowest densities ; some deep models ; a long pointed beak ; Third row ; MNIST images ; typical and atypical images ; ""typical"" points",CIFAR ; Nalisnick et al ; StyleGAN ; Gaussian ; GAN ; Karras ; Yu et al. ; one ; PDF ; Third,"Probability density estimation is a classical and well studied problem, but it has historically lacked the power to model complex and high-dimensional image distributions. More recent generative models leverage the power of neural networks to implicitly learn and represent probability models over complex images, and explore the properties of these image density functions.   In this paper, we explore how to extract explicit probability density estimates from GANs and explore their properties. These results can be used in image synthesis, outlier detection, image restoration, and classification. ",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"We present Value Propagation (VProp), a set of parameter-efficient differentiable planning modules built on Value Iteration which can successfully be trained using reinforcement learning to solve unseen tasks, has the capability to generalize to larger map sizes, and can learn to navigate in dynamic environments. We show that the modules enable learning to plan when the environment also includes stochastic elements, providing a cost-efficient learning system to build low-level size-invariant planners for a variety of interactive navigation problems. We evaluate on static and dynamic configurations of MazeBase grid-worlds, with randomly generated environments of several different sizes, and on a StarCraft navigation scenario, with more complex dynamics, and pixels as input. Planning is a key component for artificial agents in a variety of domains. However, a limit of classical planning algorithms is that one needs to know how to search for an optimal -or at least reasonable -solution for each instantiation of every possible type of plan. As the environment dynamics and states complexity increase, this makes writing planners difficult, cumbersome, or simply entirely impractical. This is among the reasons why ""learning to plan"" has been an active research area to address these shortcomings BID17 BID8 . To be useful in practice we propose that methods that enable to learn planners should have at least two properties: they should be traces free, i.e. not require traces from an optimal planner, and they should generalize, i.e. learn planners that are able to function on plans of the same type but of unseen instance and/or planning horizons.In Reinforcement Learning (RL), learning to plan can be framed as the problem of finding a policy that maximises the expected return from the environment, where such policy is a greedy function that selects actions that will visit states with a higher value for the agent. This in turns shifts the problem to the one of obtaining good estimates of state values. One of the most commonly used algorithms to solve this problem is Value Iteration (VI), which estimates the state values by collecting and propagating the observed rewards until a fixed point is reached. A policy -or a plan -can then be constructed by rolling out the obtained value function on the desired state-action pairs.When the environment can be represented as an occupancy map, a 2D grid, it is possible to approximate this planning algorithm using a deep convolutional neural network (CNN) to propagate the rewards on the grid cells. This enables one to differentiate directly through the planner steps and perform end-to-end learning of the value function. BID25 train such models -Value Iteration Networks (VIN) -with a supervised loss on the trace from a search/planning algorithm, with the goal to find the parameters that can solve the shortest path task in such environments by iteratively learning the value function using the convnet. However, this baseline requires good target value estimates, violating our wished trace free property, and limiting its usage in interactive, dynamic settings. Furthermore, it doesn't take advantage of the model structure to generalise to harder instances of the task.In this work we extend the formalization used in VIN to more accurately represent the structure of grid-world-like scenarios, enabling Value Iteration network modules to be naturally used within the reinforcement learning framework beyond the scope of the initial work, while also removing some of the limitations and underlying assumptions constraining the original architecture. We show that our models can not only learn to plan and navigate in dynamic environments, but that their hierarchical structure provides a way to generalize to navigation tasks where the required planning horizon and the size of the map are much larger than the ones seen at training time. Our main contributions include: (1) introducing VProp and MVProp, network planning modules which successfully learn to solve pathfinding tasks via reinforcement learning using minimal parametrization, (2) demonstrating the ability to generalize to large unseen maps when training exclusively on much smaller ones, and (3) showing that our modules can learn to plan in environments with more complex dynamics than a static grid world, both in terms of transition function and observation complexity. Architectures that try to solve the large but structured space of navigation tasks have much to benefit from employing planners that can be learnt from data, however these need to be sample efficient to quickly adapt to local environment dynamics so that they can provide a flexible planning horizon without the need to collect new data. Our work shows that such planners can be successfully learnt via Reinforcement Learning when the dynamics of the task are taken into account, and that great generalization capabilities can be expected when these models are applied to 2D path-planning tasks. Furthermore, we have demonstrated that our methods can even generalize when the environment has dynamic, noisy, and adversarial elements, or with high-dimensional observation spaces, enabling them to be employed in relatively complex tasks. A major issue that still prevents these planners from being deployed on harder tasks is computational cost, since the depth increases with the length of the path that agents must solve, however architectures employing VI modules as low level planners have been successfully tackling complex interactive tasks (Section 1.1), thus we expect our methods to provide a way for such type of work to train end-to-end via reinforcement learning, even for pathfinding tasks found in different graph-like structures (for which we have at least the relevant convolutional operators). Finally, interesting venues where VProp and MVProp may be applied are mobile robotics and visual tracking BID3 , where our work could be used to learn arbitrary propagation functions, and model a wide range of potential functions.A More general graph structures VIN and VProp are, in their most general formulation, applicable to any graph-structured input. They ultimately belong to the general class of graph convolutional neural networks, and several variations of the value iteration modules specifically tailored to non-regular graph structures have been proposed (see e.g., BID13 ). The three equations for VProp (Sections 3.2, 3.1) are applicable to any graph structure, as long as there is a one-to-one mapping between nodes in the neighborhood of the current position and actions (which is usually the case in navigation problems). Our work focuses on the simplest possible parametrization that is relevant in many navigation and pathfinding scenarios, while for more general graph structures, even assuming a deterministic model, the term p i,j v DISPLAYFORM0 to account for the edge weight between i, j and i , j . For regular graph structures such as 2D grids, this can be included in the embedding function Φ, which outputs not just one parameter for each position but as many parameters as the neighborhood size. Other possibilities include using an attention mechanism for graph-convolutional neural networks in place of p (see e.g., Tamar et al. (2016, section 4.4) ). in such case, our method differs from the original VIN purely by the parametrization of the reward and the focus on the deterministic models, which we believe are relevant in navigation problems.",We present planners based on convnets that are sample-efficient and that generalize to larger instances of navigation and pathfinding problems.,MazeBase ; at least two ; VIN ; CNN ; StarCraft ; One ; -Value Iteration Networks ; Φ ; Value Iteration ; Tamar et al,environments ; domains ; they ; the reinforcement learning framework ; such policy ; scenarios ; regular graph structures ; the dynamics ; our method ; high-dimensional observation spaces,MazeBase ; at least two ; VIN ; CNN ; StarCraft ; One ; -Value Iteration Networks ; Φ ; Value Iteration ; Tamar et al,"Value Propagation (VProp) is a set of parameter-efficient differentiable planning modules built on Value Iteration, which can successfully be trained using reinforcement learning to solve unseen tasks and can learn to navigate in dynamic environments. The modules enable learning to plan when the environment also includes stochastic elements, enabling a cost-efficient learning system to build low-level size-invariant planners for a variety of interactive navigation problems. They evaluate on static and dynamic configurations of MazeBase grid-worlds, with randomly generated environments of several different sizes, and on a StarCraft navigation scenario with more complex dynamics, pixels as input",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"The backpropagation algorithm is the de-facto standard for credit assignment in artificial neural networks due to its empirical results. Since its conception, variants of the backpropagation algorithm have emerged. More specifically, variants that leverage function changes in the backpropagation equations to satisfy their specific requirements. Feedback Alignment is one such example, which replaces the weight transpose matrix in the backpropagation equations with a random matrix in search of a more biologically plausible credit assignment algorithm. In this work, we show that function changes in the  backpropagation procedure is equivalent to adding an implicit learning rate to an artificial neural network. Furthermore, we learn activation function derivatives in the backpropagation equations to demonstrate early convergence in these artificial neural networks. Our work reports competitive performances with early convergence on MNIST and CIFAR10 on sufficiently large deep neural network architectures. Credit assignment BID10 is the task of identifying neurons and weights that are responsible for a desired prediction. Currently, the backpropagation (BP) algorithm BID9 ) is the de-facto standard for credit assignment in artificial neural networks. The backpropagation algorithm assigns credit by computing partial derivatives for weights and neurons with respect to the networks cost function.Variants of the backpropagation procedure have emerged since its conception. More specifically variants that exploit function changes in the backpropagation procedure. Feedback Alignment BID5 is considered a biologically plausible alternative to vanilla backpropagation. Feedback alignment is a variant of the backpropagation algorithm that uses a random weight matrix instead of the weight transpose matrix in the backpropagation equation. Despite not scaling to the ImageNet dataset BID1 , the algorithm relaxes BP weight symmetry requirements and demonstrate comparable learning capabilities to that of BP on small datasets Similarly, BID0 produced unique backpropagation equations to train an artificial neural network by learning parts of the backpropagation equations. BID0 report early convergence on unique backpropagation equations for CIFAR10. In this work, we demonstrate that function changes in the backpropagation equations particularly activation function derivatives is equivalent to adding an implicit learning rate in stochastic gradient descent.",We demonstrate that function changes in the backpropagation is equivalent to an implicit learning rate,Feedback Alignment ; one ; ImageNet ; BP,competitive performances ; sufficiently large deep neural network architectures ; variants ; their specific requirements ; that ; the ImageNet ; activation function derivatives ; the algorithm ; a desired prediction ; credit,Feedback Alignment ; one ; ImageNet ; BP,"The backpropagation algorithm (BP) algorithm BID9) is the de-facto standard for credit assignment in artificial neural networks due to its empirical results. Variants of the backpropation procedure have emerged since its conception, including variants that leverage function changes in the backpagation equations to satisfy their specific requirements. Feedback Alignment BID5 is considered a biologically plausible alternative to vanilla backpropaganda. BID0, which employs a random weight matrix instead of the weight transpose matrix, demonstrates early convergence on sufficiently large deep neural network architectures.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Knowledge-grounded dialogue is a task of generating an informative response based on both discourse context and external knowledge. As we focus on better modeling the knowledge selection in the multi-turn knowledge-grounded dialogue, we propose a sequential latent variable model as the first approach to this matter. The model named sequential knowledge transformer (SKT) can keep track of the prior and posterior distribution over knowledge; as a result, it can not only reduce the ambiguity caused from the diversity in knowledge selection of conversation but also better leverage the response information for proper choice of knowledge. Our experimental results show that the proposed model improves the knowledge selection accuracy and subsequently the performance of utterance generation. We achieve the new state-of-the-art performance on Wizard of Wikipedia (Dinan et al., 2019) as one of the most large-scale and challenging benchmarks. We further validate the effectiveness of our model over existing conversation methods in another knowledge-based dialogue Holl-E dataset (Moghe et al., 2018). Knowledge-grounded dialogue is a task of generating an informative response based on both discourse context and selected external knowledge (Ghazvininejad et al., 2018) . For example, it is more descriptive and engaging to respond ""I've always been more of a fan of the American football team from Pittsburgh, the Steelers!"" than ""Nice, I like football too."". As it has been one of the key milestone tasks in conversational research (Zhang et al., 2018) , a majority of previous works have studied how to effectively combine given knowledge and dialogue context to generate an utterance (Zhang et al., 2018; Li et al., 2019b; Parthasarathi & Pineau, 2018; Madotto et al., 2018; Gopalakrishnan et al., 2019) . Recently, Dinan et al. (2019) proposed to tackle the knowledge-grounded dialogue by decomposing it into two sub-problems: first selecting knowledge from a large pool of candidates and generating a response based on the selected knowledge and context. In this work, we investigate the issue of knowledge selection in the multi-turn knowledge-grounded dialogue, since practically the selection of pertinent topics is critical to better engage humans in conversation, and technically the utterance generation becomes easier with a more powerful and consistent knowledge selector in the system. Especially, we focus on developing a sequential latent variable model for knowledge selection, which has not been discussed in previous research. We believe it brings several advantages for more engaging and accurate knowledge-based chit-chat. First, it can correctly deal with the diversity in knowledge selection of conversation. Since one can choose any knowledge to carry on the conversation, there can be one-to-many relations between dialogue context and knowledge selection. Such multimodality by nature makes the training of a dialogue system much more difficult in a data-driven way. However, if we can sequentially model the history of knowledge selection in previous turns, we can reduce the scope of probable knowledge candidates at current turn. Second, the sequential latent model can better leverage the response information, which makes knowledge selection even more accurate. It is naturally easy to select the knowledge in the pool once the response is known, because the response is generated based on the selected knowledge. Our sequential model can keep track of prior and posterior distribution over knowledge, which are sequentially updated considering the responses in previous turns, and thus we can better predict the knowledge by sampling from the posterior. Third, the latent model works even when the knowledge selection labels for previous dialogue are not available, which is common (Dinan et al., 2019) . Table 1 : Accuracy of knowledge selection with and without knowing the response. We test with GRU (Cho et al., 2014) , Transformer (Vaswani et al., 2017) and BERT (Devlin et al., 2019) as the sentence encoder. For human evaluation, we randomly sample 20 dialogues and ask human annotators to select the most likely knowledge sentence from the pool. Finally, the contributions of this work are as follows. 1. We propose a novel model named sequential knowledge transformer (SKT). To the best of our knowledge, our model is the first attempt to leverage a sequential latent variable model for knowledge selection, which subsequently improves knowledge-grounded chit-chat. 2. Our experimental results show that the proposed model improves not only the knowledge selection accuracy but also the performance of utterance generation. As a result, we achieve the new state-of-the-art performance on Wizard of Wikipedia (Dinan et al., 2019 ) and a knowledge-annotated version of Holl-E (Moghe et al., 2018) dataset. This work investigated the issue of knowledge selection in multi-turn knowledge-grounded dialogue, and proposed a sequential latent variable model, for the first time, named sequential knowledge transformer (SKT). Our method achieved the new state-of-the-art performance on the Wizard of Wikipedia benchmark (Dinan et al., 2019) and a knowledge-annotated version of Holl-E dataset (Moghe et al., 2018) . There are several promising future directions beyond this work. First, we can explore other inference models such as sequential Monte Carlo methods using filtering variational objectives (Maddison et al., 2017a) . Second, we can study the interpretability of knowledge selection such as measuring the uncertainty of attention (Heo et al., 2018) .",Our approach is the first attempt to leverage a sequential latent variable model for knowledge selection in the multi-turn knowledge-grounded dialogue. It achieves the new state-of-the-art performance on Wizard of Wikipedia benchmark.,Cho et al. ; Dinan et al. ; Ghazvininejad ; the Wizard of Wikipedia ; Pittsburgh ; Steelers ; Third ; two ; Second ; Monte Carlo,the response ; human evaluation ; the prior and posterior distribution ; previous dialogue ; multi-turn knowledge-grounded dialogue ; Wikipedia ; the American football team ; the diversity ; the uncertainty ; human annotators,Cho et al. ; Dinan et al. ; Ghazvininejad ; the Wizard of Wikipedia ; Pittsburgh ; Steelers ; Third ; two ; Second ; Monte Carlo,"Knowledge-grounded dialogue is a task of generating an informative response based on both discourse context and external knowledge. The sequential latent variable model (SKT) can keep track of prior and posterior distribution over knowledge and can help reduce ambiguity caused from the diversity in knowledge selection of conversation and better leverage response information for proper choice of knowledge. We achieve the new state-of-the-art performance on Wizard of Wikipedia (Dinan et al., 2019) as one of the most large-scale and challenging benchmarks. We further validate the effectiveness of our model over existing conversation methods in another knowledge-based dialogue Holl-E dataset",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Each training step for a variational autoencoder (VAE) requires us to sample from the approximate posterior, so we usually choose simple (e.g. factorised) approximate posteriors in which sampling is an efficient computation that fully exploits GPU parallelism.   However, such simple approximate posteriors are often insufficient, as they eliminate statistical dependencies in the posterior.   While it is possible to use normalizing flow approximate posteriors for continuous latents, there is nothing analogous for discrete latents. The most natural approach to model discrete dependencies is an autoregressive distribution, but sampling from such distributions is inherently sequential and thus slow.   We develop a fast, parallel sampling procedure for autoregressive distributions based on fixed-point iterations which enables efficient and accurate variational inference in discrete state-space models.   To optimize the variational bound, we considered two ways to evaluate probabilities: inserting the relaxed samples directly into the pmf for the discrete distribution, or converting to continuous logistic latent variables and interpreting the K-step fixed-point iterations as a normalizing flow.   We found that converting to continuous latent variables gave considerable additional scope for mismatch between the true and approximate posteriors, which resulted in biased inferences, we thus used the former approach.   We tested our approach on the neuroscience problem of inferring discrete spiking activity from noisy calcium-imaging data, and found that it gave accurate connectivity estimates in an order of magnitude less time. We have described an approach to sampling from a discrete autoregressive distribution using a parallel, flow-like procedure, derived by considering fixed-point iterations that converge to a sample from the underlying autoregressive process. We applied this procedure to speed up sampling from autoregressive approximate posteriors in the variational inference training loop. This allowed us to rapidly learn autoregressive posteriors in the context of neural data analysis, allowing us to realise the benefits of autoregressive approximate posteriors for single and multi cell data in reasonable timescales.It is important to remember that while we can sample using K fixed-point iterations, we can only evaluate the probability of a sample once it has converged. This mismatch introduces a level of approximation in addition to those that are typical when relaxing discrete distributions BID8 BID11 ), but we can deal with the additional approximation error in the same way: by evaluating the model using samples drawn from the underlying discrete, autoregressive approximate posterior.Past work has used similar properties of the underlying generative model, to speed up messagepassing based inference algorithms BID6 BID4 . It is likely that their approach will be preferable when exact inference is possible albeit costly due to large tree-width/timecourses, whereas our approach will be preferable when exact inference is not possible due to longrange temporal dependencies.Finally, our work suggests two directions for future work. First, while it is possible to use normalizing flows to define approximate posteriors for continuous state-space models, it may be difficult to know exactly which normalizing flow will prove most effective. In this context, our procedure of using fixed-point iterations may be a useful starting point. Second, we showed that while it may be possible to convert a discrete latent variable model to an equivalent model with continuous latents, this typically introduces considerable scope for mismatch between the prior and approximate posterior. However, the actual approximate posterior is relatively simple, a mixture of truncated Logistics, and as such, it may be possible to design approximate posteriors or even whole relaxation schemes that more closely match the true posterior, and indeed this may underlie the gains shown by BID17 .",We give a fast normalising-flow like sampling procedure for discrete latent variable models.,VAE ; GPU ; two ; First ; Second ; Logistics,"the benefits ; This mismatch ; normalizing flow approximate posteriors ; probabilities ; a fast, parallel sampling procedure ; considerable scope ; this procedure ; truncated Logistics ; the same way ; the underlying discrete",VAE ; GPU ; two ; First ; Second ; Logistics,"The most natural approach to sampling from a variational autoencoder (VAE) is simple (e.g. factorised) approximate posteriors, which eliminate statistical dependencies in the posterior. However, there is nothing analogous for discrete latents.   We develop a fast, parallel sampling procedure for autoregressive distributions based on fixed-point iterations which enables efficient and accurate variational inference in discrete state-space models. The variational bound is optimized using two ways to evaluate probabilities: inserting relaxed samples directly into the pmf for the discrete distribution, or converting to continuous logistic latent variables and K-step fixed",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Recent improvements in large-scale language models have driven progress on automatic generation of syntactically and semantically consistent text for many real-world applications. Many of these advances leverage the availability of large corpora. While training on such corpora encourages the model to understand long-range dependencies in text, it can also result in the models internalizing the social biases present in the corpora. This paper aims to quantify and reduce biases exhibited by language models. Given a conditioning context (e.g. a writing prompt) and a language model, we analyze if (and how) the sentiment of the generated text is affected by changes in values of sensitive attributes (e.g. country names, occupations, genders, etc.) in the conditioning context, a.k.a. counterfactual evaluation. We quantify these biases by adapting individual and group fairness metrics from the fair machine learning literature. Extensive evaluation on two different corpora (news articles and Wikipedia) shows that state-of-the-art Transformer-based language models exhibit biases learned from data. We propose embedding-similarity and sentiment-similarity regularization methods that improve both individual and group fairness metrics without sacrificing perplexity and semantic similarity---a positive step toward development and deployment of fairer language models for real-world applications. Text representation learning methods (word and sentence encoders) trained on large unlabeled corpora are widely used in the development of natural language processing systems (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018; Devlin et al., 2018) . Progress in this area has led to consistent improvements of model performances on many downstream tasks. However, recent studies have found that both context-free and context-dependent word embedding models contain human-like semantic biases, including gender and race (Bolukbasi et al., 2016; Caliskan et al., 2017; Zhao et al., 2019) . Zhao et al. (2018a) provide an insight into this phenomenon by showing that web corpora contain biases (e.g., gender) which are inherited by models trained on these datasets. In this work, we focus on language models which have been shown to exhibit systematic biases (Lu et al., 2018; Bordia & Bowman, 2019; Qian et al., 2019) . We train a Transformer-based language model (Vaswani et al., 2017; on two large corpora: Wikipedia articles from Wikitext-103 (Merity et al., 2016) and news articles from the English-language news corpus from . 1 We analyze systematic variations in sentiment scores of the text generated by the language model given a conditioning context, under different instantiations of control variables (e.g. country names, occupations, and person names) in the context. In a counterfactual experiment, we find that sentiment scores for the text generated by this language model vary substantially as we change the control variables in the context. We propose two approaches to reduce counterfactual sentiment biases based on the concept of embedding similarity or sentiment similarity. In the first method, we encourage hidden states of the conditioning context to be similar irrespective of the instantiations of the control variables in the context. In the second method, we regularize the difference between sentiment scores of various instantiations of the control variables. Experiments with counterfactual conditioning demonstrate that both of these methods reduce sentiment biases while retaining the generation capability of the language model, as measured by perplexity and semantic similarity. While specifying optimal model fairness behavior is difficult, our method provides a framework to address various fairness specifications and an important step toward the deployment of fairer language models. Our main contributions in this paper are: • We demonstrate systematic counterfactual sentiment biases in large-scale language models. • We present methods to quantify these biases by adopting individual and group fairness metrics from the fair machine learning literature. • We propose embedding and sentiment similarity-based methods for training language models to be invariant to certain transformations of their inputs. • We empirically demonstrate the efficacy of these methods to reduce counterfactual sentiment biases of language models. We use a sentiment classifier as a proxy to measure biases in this paper. We note that the classifier itself is not perfect and might exhibit some biases. We leave investigations of an unbiased evaluator to future work. As large-scale language models are increasingly deployed for real-world applications, developing methods for assessing and mitigating bias with respect to sensitive attributes may be an increasingly important area of inquiry for facilitating pro-social outcomes. Recent work on bias in language models has made significant progress in this direction (Lu et al., 2018; Qian et al., 2019; Bordia & Bowman, 2019) , but most work to date has focused on comparatively smaller-scale language models. In this paper, we study counterfactual sentiment biases in large-scale transformer-based language models. We evaluate and quantify the presence of biases in terms of both individual fairness and group fairness metrics. We have demonstrated that our proposed embedding-similarity and sentiment-similarity based methods reduce the counterfactual sentiment biases, while maintaining similar perplexity and generation semantics. While specifying optimal model fairness behavior is difficult, our method provides a framework to address various fairness specifications and an important step toward the deployment of fairer language models. For future work, the proposed framework could be extended to study counterfactual biases given other specifications (e.g. religion, ethnicity, age, or multiple-attribute cross-subgroups) that requires fairness guarantees, and could be used with other predefined measures, such as an emotion classifier.",We reduce sentiment biases based on counterfactual evaluation of text generation using language models.,Zhao ; Lu et al. ; Devlin ; Zhao et ; al. ; Bolukbasi et al. ; first ; Wikipedia ; Caliskan ; English,this direction ; a sentiment classifier ; we ; large-scale transformer-based language models ; data ; real-world applications ; various fairness specifications ; hidden states ; a framework ; sensitive attributes,Zhao ; Lu et al. ; Devlin ; Zhao et ; al. ; Bolukbasi et al. ; first ; Wikipedia ; Caliskan ; English,"The advancement in large-scale language models has driven progress on automatic generation of syntactically and semantically consistent text for real-world applications. However, many of these advances leverage the availability of large corpora. While training on such corpora encourages the model to understand long-range dependencies in text, it can also lead to the models internalizing social biases and biases. In this paper, we examine how the sentiment of the generated text is affected by changes in values of sensitive attributes in the conditioning context, a.k.a. counterfactual evaluation. We incorporate individual and group fairness metrics from the fair machine learning literature",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Knowledge Distillation (KD) is a widely used technique in recent deep learning research to obtain small and simple models whose performance is on a par with their large and complex counterparts. Standard Knowledge Distillation tends to be time-consuming because of the training time spent to obtain a teacher model that would then provide guidance for the student model. It might be possible to cut short the time by training a teacher model on the fly, but it is not trivial to have such a high-capacity teacher that gives quality guidance to student models this way. To improve this, we present a novel framework of Knowledge Distillation exploiting dark knowledge from the whole training set. In this framework, we propose a simple and effective implementation named Distillation by Utilizing Peer Samples (DUPS) in one generation. We verify our algorithm on numerous experiments. Compared with standard training on modern architectures, DUPS achieves an average improvement of 1%-2% on various tasks with nearly zero extra cost. Considering some typical Knowledge Distillation methods which are much more time-consuming, we also get comparable or even better performance using DUPS. Recent years have witnessed continuous development of deep neural network models. A general trend is that improvements in model performance are usually coupled with more complex architecture designs and higher cost of computation. In order to obtain more compact models with higher quality, the idea of Knowledge Distillation (KD) first emerged in the form of knowledge transfer between models (Buciluǎ et al., 2006) . KD takes advantage of the ""dark knowledge"" by transferring it from teacher models to student models so as to facilitate the latter's training process (Hinton et al., 2015) . Student models, with the availability of softened output vectors from teacher models in KD, have access to richer information in comparison to directly learning from hard labels provided by training set. KD significantly improves smaller models' performance, and thus it further allows model compression. Although great progress has been made in this area, much more training cost is incurred due to involved time-consuming mid-output (e.g. feature maps) alignment when training student models, on top of extra training of a huge teacher model. It is ad meaningful objective of finding more efficient KD methods. Recent works by (Furlanello et al., 2018) and (Lan et al., 2018b) show that a stronger teacher model is not the necessary condition for improving the student model. Their research shows that it is possible that the student model's performance can be significantly improved by an identically structured teacher model. Although the techniques remain inefficient due to the cost of multi-generation (at least one extra) training of teacher models, these works give important hints that cheaper teachers with considerable effectiveness may exist. Recently (Yang et al., 2018) extend these works, trying to obtain continuously improved teachers by introducing the cyclic learning rate technique in one-generation training. They propose Snapshot Distillation (SD), which uses models obtained from earlier checkpoints as teachers and skips the process of separately training a teacher model. Inspired by recent interesting ideas of dataset distillation for objectives on other research areas, we propose a novel approach for KD in this paper. Instead of relying on the assitance of a separate teacher model or checkpoint, we exploit hidden knowledge in the dataset to generate a surrogate teacher. Specifically, we first define a more general framework of knowledge distillation utilizing the whole dataset to generate extra supervision signals, rather than using a single sample alone. Then we propose a very simple yet effective implementation of one-generation KD, called Distillation by Utilizing Peer Samples (DUPS). In DUPS, each sample borrows continuously boosted secondary information from a random subset of peer samples belonging to the same category on the fly. We perform extensive experiments on CIFAR100 dataset, with various modern architectures such as PreActResNet, WideResNet, and ResNeXt, demonstrating that our proposed DUPS gains significant improvement compared to standard SGD training with nearly zero extra computation cost. DUPS also outperforms recent one-generation KD method SnapShot Distillation (Yang et al., 2018) on most architectures. Moreover, we validate our algorithm on more practical tasks, include ImageNet classification, transfer learning, and language model. Experiments show that DUPS is generally effective across different tasks. In summary, our main contributions include: 1) To the best of our knowledge, we are the first to propose an extension framework of Knowledge Distillation utilizing the whole training set other than a single sample. 2) Under this framework we implement a general on-the-fly algorithm DUPS which achieves significant improvement at almost no extra cost. The rest of the paper is organized as follows. Section 2 presents prior works related to this paper. Section 3 introduces our methodology of the general knowledge distillation framework. Section 4 demonstrates our experimental results and provides some discussions. Section 5 concludes the paper. Here we give a short discussion about how and why DUPS brings benefits. We first demonstrate some empirical characteristics of DUPS observed in our experiments. We plot the learning curve of the whole training procedure of PreActResNet18 as Fig. 1 . For better demonstration purpose, we divide the training process into only 4 stages for DUPS training, with each stage consisting of 40 epochs. We observe that SGD and DUPS display almost the same standard of performance in the first stage as expected. While at the 41th epoch, both training and test accuracy of DUPS get a sharp rise due to involving teacher signal generated in the 40th epoch. Then both training and test accuracy drop slightly for a few epochs, and then return to the trend of slowly rising for the remaining epochs until next stage. A similar phenomenon also appears at the beginning of next stage, although the magnitude of accuracy improvement becomes much more smaller. As the model reaches the beginning of final stage, we no longer see increases in accuracy since training is nearly saturated. We notice that since the first sharp rising, DUPS continuously outperforms SGD by a stable gap for the following training epochs until convergence. We also investigate the influence of different choices of hyper-parameters specific to DUPS. The most important two are the number of stages and number of random peer samples. We run a grid search method to validate different combinations of these two variables. We use update intervals, or equivalently number of epochs per stage, instead of number of stages for clarity in this experiments. In Fig. 2 we see that performance of DUPS does not seem to be very sensitive to most combinations of the hyperparameters. When the number of peer samples increases to 5 or more, model accuracy tends to be over 77.3%. Even when the number of peer samples is low, a good choice of the value of update interval can boost the model performance significantly. For example, when number of peer samples is 1 and update interval is set to be between 20 to 40, DUPS still delivers satisfying results which is comparable to its best performance. Low accuracy of the model only happens consistently when the value of update interval is large. If update interval is set to 80, the model, teacher-student knowledge transfer only takes place once during the whole training process. Consequently, the opportunity to distill the knowledge obtained from the dataset is too rare for the model to benefit from DUPS. In this paper, we have introduced a general framework for one-generation KD: We incorporate the information contained within the dataset into teacher-student optimization. We have also proposed an effective implementation of this general framework named DUPS. With extensive experiments, this simple yet effective algorithm is verified to be effective in improving model performance in tasks like image classification, transfer learning and language modeling with almost no additional cost in training resources. The demonstrated success of DUPS imply that utilizing dataset information during training potentially allow us to gain even more benefits.",We present a novel framework of Knowledge Distillation utilizing peer samples as the teacher,Yang et al. ; SGD ; Hinton ; Snapshot Distillation ; al. ; Lan et al. ; the beginning of next stage ; nearly zero ; at least one ; Fig,The demonstrated success ; secondary information ; a general framework ; model compression ; the knowledge ; clarity ; Utilizing ; summary ; ad meaningful objective ; earlier checkpoints,Yang et al. ; SGD ; Hinton ; Snapshot Distillation ; al. ; Lan et al. ; the beginning of next stage ; nearly zero ; at least one ; Fig,"Knowledge Distillation (KD) is a widely used technique in deep learning research to obtain small and simple models whose performance is on a par with their large and complex counterparts. However, it is not trivial to have such a high-capacity teacher that provides quality guidance to student models. To improve this, we present a novel framework of Knowledge Distillation exploiting dark knowledge from the whole training set. In this framework, we introduce a simple and effective implementation named Distillation by Utilizing Peer Samples (DUPS) in one generation. The results are comparable to standard training on modern architectures with nearly zero extra cost.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Despite the success of Generative Adversarial Networks (GANs) in image synthesis, there lacks enough understanding on what networks have learned inside the deep generative representations and how photo-realistic images are able to be composed from random noises. In this work, we show that highly-structured semantic hierarchy emerges from the generative representations as the variation factors for synthesizing scenes. By probing the layer-wise representations with a broad set of visual concepts at different abstraction levels, we are able to quantify the causality between the activations and the semantics occurring in the output image. Such a quantification identifies the human-understandable variation factors learned by GANs to compose scenes. The qualitative and quantitative results suggest that the generative representations learned by GAN are specialized to synthesize different hierarchical semantics: the early layers tend to determine the spatial layout and configuration, the middle layers control the categorical objects, and the later layers finally render the scene attributes as well as color scheme. Identifying such a set of manipulatable latent semantics facilitates semantic scene manipulation. Success of deep neural networks stems from the representation learning, which identifies the explanatory factors underlying the high-dimensional observed data (Bengio et al. (2013) ). Prior work has shown that many concept detectors spontaneously emerge inside the deep representations trained for classification task. For example, Gonzalez-Garcia et al. (2018) shows that networks for object recognition are able to detect semantic object parts, and Bau et al. (2017) confirms that deep representations from classifying images learn to detect different categorical concepts at different layers. Analyzing the deep representations and their emergent structures gives insight into the generalization ability of deep features (Morcos et al. (2018) ) as well as the feature transferability across different tasks (Yosinski et al. (2014) ). But current efforts on interpreting deep representations mainly focus on discriminative models (Zhou et al. (2015) ; Gonzalez-Garcia et al. (2018) ; Zeiler and Fergus (2014) ; Agrawal et al. (2014) ; Bau et al. (2017) ). Recent advance of Generative Adversarial Networks (GANs) (Goodfellow et al. (2014) ; Karras et al. (2018a; ; Brock et al. (2019) ) is capable of transforming random noises into high-quality images, however, the nature of the learned generative representations and how a photo-realistic image is being composed over different layers of the generator in GAN remain much less explored. It is known that the internal units of Convolutional Neural Networks (CNNs) emerge as object detectors when trained to categorize scenes (Zhou et al. (2015) ). Representing and detecting informative categorical objects provides an ideal solution for classifying scenes, such as sofa and TV are representative of living room while bed and lamp are of bedroom. However, synthesizing a scene demands far more knowledge for the generative models to learn. Specifically, in order to produce highly-diverse scene images, the deep representations might be required to not only generate every individual object relevant to a specific scene category, but also decide the underlying room layout as well as render various scene attributes, e.g., the lighting condition and color scheme. Very recent work on interpreting GANs Bau et al. (2019) visualized that the internal filters at intermediate layers are specialized for generating some certain objects, but studying scene synthesis from object aspect only is far from fully understanding how GAN is able to compose a photo-realistic image, which contains multiple variation factors from layout level, category level, to attribute level. The original StyleGAN work (Karras et al. (2018b) ) pointed out that the layer-wise latent codes actually control the synthesis from coarse to fine, but how these variation factors are composed together and how to quantify such semantic information are still uncertain. Differently, this work gives a much deeper interpretation on the hierarchical generative representations in the sense that we match these layer-wise variation factors with human-understandable scene variations at multiple abstraction levels, including layout, category (object), attribute, and color scheme. Starting with the state-of-the-art StyleGAN models (Karras et al. (2018b) ) as the example, we reveal that highly-structured semantic hierarchy emerges from the deep generative representations with layer-wise stochasticity trained for synthesizing scenes, even without any external supervision. Layerwise representations are first probed with a broad set of visual concepts at different abstraction levels. By quantifying the causality between the layer-wise activations and the semantics occurring in the output image, we are able to identify the most relevant variation factors across different layers of a GAN model with layer-wise latent codes: the early layers specify the spatial layout, the middle layers compose the category-guided objects, and the later layers render the attributes and color scheme of the entire scene. We further show that identifying such a set of manipulatable latent variation factors from layouts, objects, to scene attributes and color schemes facilitates the semantic image manipulation with large diversity. The proposed manipulation technique is further generalized to other GANs such as BigGAN (Brock et al. (2019) ) and ProgressiveGAN (Karras et al. (2018a) ). Disentanglement of Semantics. Some variation factors we detect in the generative representation are more disentangled with each other than other semantics. Compared to the perceptual path length and linear separability described in Karras et al. (2018b) and the cosine similarity proposed in Shen et al. (2019) , our work offers a new metric for disentanglement analysis. In particular, we move the latent code along one semantic direction and then check how the semantic scores of other factors change accordingly. As shown in Fig.8(a) , when we modify the spatial layout, all scene attributes are barely affected, suggesting that GAN learns to disentangle layout-level semantic from attribute-level. However, there are also some scene attributes (from same abstraction level) entangling with each other. Taking Fig.8(c) as an example, when modulating ""indoor lighting"", ""natural lighting"" also varies. This is also aligned with human perception, further demonstrating the effectiveness of our proposed quantification metric. Application to Other GANs. We further apply our method for two other GAN structures, i.e., PGGAN (Karras et al. (2018a) ) and BigGAN (Brock et al. (2019) ). These two models are trained on LSUN dataset (Yu et al. (2015) ) and Places dataset ) respectively. Compared to StyleGAN, PGGAN feeds the latent vector only to the very first convolutional layer and hence does not support layer-wise analysis. But the proposed re-scoring method can still be applied to help identify manipulatable semantics, as shown in Fig.9(a ) . BigGAN is the state-of-the-art conditional GAN model that concatenates the latent vector with a class-guided embedding code before feeding it to the generator, and it also allows layer-wise analysis like StyleGAN. Fig.9(b ) gives analysis results on BigGAN from attribute level, where we can tell that scene attribute can be best modified at upper layers compared to lower layers or all layers. Meanwhile , the quantitative curve shows consistent result with the discovery on StyleGAN as in Fig.3(a) . These results demonstrate the generalization ability of our approach as well as the emergence of manipulatable factors in other GANs. In this paper, we show the emergence of highly-structured variation factors inside the deep generative representations learned by GANs with layer-wise stochasticity. In particular, the GAN model spontaneously learns to set up layout at early layers, generate categorical objects at middle layers, and render scene attribute and color scheme at later layers when trained to synthesize scenes. A re-scoring method is proposed to quantitatively identify the manipulatable semantic concepts within a well-trained model, enabling photo-realistic scene manipulation.",We show that highly-structured semantic hierarchy emerges in the deep generative representations as a result for synthesizing scenes.,Agrawal ; Zeiler ; Morcos ; ProgressiveGAN ; linear ; first ; one ; Karras ; BigGAN ; LSUN,multiple abstraction levels ; visual concepts ; the scene ; various scene attributes ; Bengio et al ; manipulatable semantics ; upper layers ; the attributes ; this paper ; scene attributes,Agrawal ; Zeiler ; Morcos ; ProgressiveGAN ; linear ; first ; one ; Karras ; BigGAN ; LSUN,"Generative Adversarial Networks (GANs) are successful in image synthesis due to their knowledge of deep generative representations and their emergent structures. In this work, we show that highly-structured semantic hierarchy emerges from the generative representation as the variation factors for synthesizing scenes. By probing the layer-wise representations with a broad set of visual concepts at different abstraction levels, we can identify the causality between the activations and semantics occurring in the output image, and identify the human-understandable variation factors learned by GANs to compose scenes. Deep neural networks are specialized to synthesize different hierarchical semantics.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"In this paper, we design a generic framework for learning a robust text classification model that achieves accuracy comparable to standard full models under test-time
 budget constraints. We take a different approach from existing methods and learn to dynamically delete a large fraction of unimportant words by a low-complexity selector such that the high-complexity classifier only needs to process a small fraction of important words. In addition, we propose a new data aggregation method to train the classifier, allowing it to make accurate predictions even on fragmented sequence of words. Our end-to-end method achieves state-of-the-art performance while its computational complexity scales linearly with the small fraction of important words in the whole corpus. Besides, a single deep neural network classifier trained by our framework can be dynamically tuned to different budget levels at inference time. Recent advances in deep neural networks (DNN) has improved the performance of natural language processing tasks such as document classification, question answering, and sentiment analysis BID29 BID20 BID22 BID31 . These approaches process the entire text and construct representations of words and phrases in order to perform target tasks. While these models do realize high accuracy, their computational-time scales linearly with the size of the documents, which can be slow for documents containing many sentences. In this context, various approaches based on modifying the existing RNN or LSTM architecture have been proposed BID21 ; BID31 to speed-up processing. However, processing is still fundamentally sequential, which in turn requires loading entire documents to process, limiting compute gains. We proposed a budgeted learning framework for learning a robust classifier under test-time budget constraints. We demonstrated that training classifiers with data aggregation work well with low-complexity selectors based on word-embedding or bag-of-word model and achieve good performance with fragmented input. The future work includes applying the proposed framework to other text reading tasks and improving the data aggregation strategy by applying learning to search approaches BID4 .","Modular framework for document classification and data aggregation technique for making the framework robust to various distortion, and noise and focus only on the important words.",RNN,compute gains ; DNN ; accurate predictions ; word ; the data aggregation strategy ; good performance ; a single deep neural network classifier ; These approaches ; words ; test-time budget constraints,RNN,"In this paper, we design a generic framework for learning a robust text classification model that achieves accuracy comparable to standard full models under test-time budget constraints. In addition, we introduce a new data aggregation method to train the classifier, allowing it to make accurate predictions even on fragmented sequence of words. The end-to-end method achieves state-of-the-art performance while its computational complexity scales linearly with the small fraction of important words in the whole corpus. Additionally, a single deep neural network classifier trained by our framework can be dynamically tuned to different budget levels at inference time. Recent advances in deep neural networks",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Parameter pruning is a promising approach for CNN compression and acceleration by eliminating redundant model parameters with tolerable performance loss. Despite its effectiveness, existing regularization-based parameter pruning methods usually drive weights towards zero with large and constant regularization factors, which neglects the fact that the expressiveness of CNNs is fragile and needs a more gentle way of regularization for the networks to adapt during pruning. To solve this problem, we propose a new regularization-based pruning method (named IncReg) to incrementally assign different regularization factors to different weight groups based on their relative importance, whose effectiveness is proved on popular CNNs compared with state-of-the-art methods. Recently, deep Convolutional Neural Networks (CNNs) have made a remarkable success in computer vision tasks by leveraging large-scale networks learning from big amount of data. However, CNNs usually lead to massive computation and storage consumption, thus hindering their deployment on mobile and embedded devices. To solve this problem, many research works focus on compressing the scale of CNNs. Parameter pruning is a promising approach for CNN compression and acceleration, which aims at eliminating redundant model parameters at tolerable performance loss. To avoid hardware-unfriendly irregular sparsity, structured pruning is proposed for CNN acceleration BID0 BID22 . In the im2col implementation BID1 BID3 of convolution, weight tensors are expanded into matrices, so there are generally two kinds of structured sparsity, i.e. row sparsity (or filter-wise sparsity) and column sparsity (or shape-wise sparsity) BID24 BID23 .There are mainly two categories of structured pruning. One is importance-based methods, which prune weights in groups based on some established importance criteria BID17 BID19 BID23 . The other is regularization-based methods, which add group regularization terms to learn structured sparsity BID24 BID16 BID8 . Existing group regularization approaches mainly focus on the regularization form (e.g. Group LASSO BID26 ) to learn structured sparsity, while ignoring the influence of regularization factor. In particular , they tend to use a large and constant regularization factor for all weight groups in the network BID24 BID16 , which has two problems. Firstly, this 'one-size-fit-all' regularization scheme has a hidden assumption that all weights in different groups are equally important, which however does not hold true, since weights with larger magnitude tend to be more important than those with smaller magnitude. Secondly, few works have noticed that the expressiveness of CNNs is so fragile BID25 during pruning that it cannot withstand a large penalty term from beginning, especially for large pruning ratios and compact networks (like ResNet BID7 ). AFP BID6 was proposed to solve the first problem, while ignored the second one. In this paper , we propose a new regularization-based method named IncReg to incrementally learn structured sparsity. We propose a new structured pruning method based on an incremental way of regularization, which helps CNNs to transfer their expressiveness to the rest parts during pruning by increasing the regularization factors of unimportant weight groups little by little. Our method is proved to be comparably effective on popular CNNs compared with state-of-the-art methods, especially in face of large pruning ratios and compact networks.",we propose a new regularization-based pruning method (named IncReg) to incrementally assign different regularization factors to different weight groups based on their relative importance.,Secondly ; two ; IncReg ; Firstly ; first ; One ; Convolutional Neural Networks ; e.g. Group LASSO ; zero ; second,larger magnitude ; the network ; which ; few works ; this problem ; whose effectiveness ; the influence ; importance-based methods ; the regularization form ; the expressiveness,Secondly ; two ; IncReg ; Firstly ; first ; One ; Convolutional Neural Networks ; e.g. Group LASSO ; zero ; second,"Parameter pruning is a promising approach for CNN compression and acceleration by eliminating redundant model parameters with tolerable performance loss. However, existing regularization-based parameter pruning methods often drive weights towards zero with large and constant regularization factors, which neglects the fact that the expressiveness of CNNs is fragile and needs a more gentle way of regularization for the networks to adapt during pruning. In order to avoid hardware-unfriendly irregular sparsity, structured pruning has been proposed for CNN acceleration BID0 BID22. In the im2col implementation BID1 BID3 of convolution, weight tensors",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"We present a neural architecture search algorithm to construct compact reinforcement learning (RL) policies, by combining ENAS and ES in a highly scalable and intuitive way. By defining the combinatorial search space of NAS to be the set of different edge-partitionings (colorings) into same-weight classes, we represent compact architectures via efficient learned edge-partitionings. For several RL tasks, we manage to learn colorings translating to effective policies parameterized by as few as 17 weight parameters, providing >90 % compression over vanilla policies and 6x compression over state-of-the-art compact policies based on Toeplitz matrices, while still maintaining good reward. We believe that our work is one of the first attempts to propose a rigorous approach to training structured neural network architectures for RL problems that are of interest especially in mobile robotics with limited storage and computational resources. Consider a fixed Markov Decision Process (MDP) M and an agent aiming to maximize its total expected/discounted reward obtained in the environment E governed by M. An agent is looking for a sequence of actions a 0 , ..., a T −1 leading to a series of steps maximizing this reward. One of the approaches is to construct a policy π θ : S → A, parameterized by vector θ, which is a mapping from states to actions. Policy π θ determines actions chosen in states visited by an agent. Such a reinforcement learning (RL) policy is usually encoded as a neural network, in which scenario parameters θ correspond to weights and biases of a neural network. Reinforcement learning policies π θ often consist of thousands or millions of parameters (e.g. when they involve vision as part of the state vector) and therefore training them becomes a challenging high-dimensional optimization problem. Deploying such high-dimensional policies on hardware raises additional concerns in resource constrained settings (e.g. limited storage), emerging in particular in mobile robotics (Gage, 2002) . The main question we tackle in this paper is the following: Are high dimensional architectures necessary for encoding efficient policies and if not, how compact can they be in in practice? We show that finding such compact representations is a nontrivial optimization problem despite recent observations that some hardcoded structured families (Choromanski et al., 2018) provide certain levels of compactification and good accuracy at the same time. We model the problem of finding compact presentations by using a joint objective between the combinatorial nature of the network's parameter sharing profile and the reward maximization of RL optimization. We leverage recent advances in the ENAS (Efficient Neural Architecture Search) literature and theory of pointer networks (Vinyals et al., 2015; Pham et al., 2018; Zoph & Le, 2017) to optimize over the combinatorial component of this objective and state of the art evolution strategies (ES) methods (Choromanski et al., 2018; Salimans et al., 2017; Mania et al., 2018a) to optimize over the RL objective. We propose to define the combinatorial search space to be the the set of different edge-partitioning (colorings) into same-weight classes and construct policies with learned weight-sharing mechanisms. We call networks encoding our policies: chromatic networks. We are inspired by two recent papers: (Choromanski et al., 2018) and (Gaier & Ha, 2019) . In the former one, policies based on Toeplitz matrices were shown to match their unstructured counterparts accuracy-wise, while leading to the substantial reduction of the number of parameters from Figure 1 : On the left: matrix encoding linear Toeplitz policy at time t for the RL task with 6-dimensional state vector and 4-dimensional action vector. On the right: that policy in the vectorized form. As we see, a policy defined by a matrix with 24 entries is effectively encoded by a 9-dimensional vector. thousands (Salimans et al., 2017) to hundreds (Choromanski et al., 2018) . Instead of quadratic (in sizes of hidden layers), those policies use only linear number of parameters. The Toeplitz structure can be thought of as a parameter sharing mechanism, where edge weights along each diagonal of the matrix are the same (see: Fig. 1 ). However, this is a rigid pattern that is not learned. We show in this paper that weight sharing patterns can be effectively learned, which further reduces the number of distinct parameters. For instance, using architectures of the same sizes as those in (Choromanski et al., 2018) , we can train effective policies for OpenAI Gym tasks with as few as 17 distinct weights. The latter paper proposes an extremal approach, where weights are chosen randomly instead of being learned, but the topologies of connections are trained and thus are ultimately strongly biased towards RL tasks under consideration. It was shown in (Gaier & Ha, 2019 ) that such weight agnostic neural networks (WANNs) can encode effective policies for several nontrivial RL problems. WANNs replace conceptually simple feedforward networks with general graph topologies using NEAT algorithm (Stanley & Miikkulainen, 2002) providing topological operators to build the network. Our approach is a middle ground, where the topology is still a feedforward neural network, but the weights are partitioned into groups that are being learned in a combinatorial fashion using reinforcement learning. While (Chen et al., 2015) shares weights randomly via hashing, we learn a good partitioning mechanisms for weight sharing. Our key observation is that ENAS and ES can naturally be combined in a highly scalable but conceptually simple way. To give context, vanilla NAS (Zoph & Le, 2017) for classical supervised learning setting (SL) requires a large population of 450 GPU-workers (child models) all training one-by-one, which results in many GPU-hours of training. ENAS (Pham et al., 2018) uses weight sharing across multiple workers to reduce the time, although it can reduce computational resources at the cost of the variance of the controller's gradient. Our method solves both issues (fast training time and low controller gradient variance) by leveraging a large population of much-cheaper CPU workers (300) increasing the effective batch-size of the controller, while also training the workers simultaneously via ES. This setup is not possible in SL, as single CPUs cannot train large image-based classifiers in practice. Furthermore, this magnitude of scaling by numerous workers can be difficult with policy gradient or Q-learning methods as they can be limited by GPU overhead due to exact-gradient computation. We believe that our work is one of the first attempts to propose a flexible, rigorous approach to training compact neural network architectures for RL problems. Those may be of particular importance in mobile robotics (Gage, 2002) where computational and storage resources are very limited. We also believe that this paper opens several new research directions regarding structured policies for robotics. We presented a principled and flexible algorithm for learning structured neural network architectures for RL policies and encoded by compact sets of parameters. Our architectures, called chromatic networks, rely on partitionings of a small sets of weights learned via ENAS methods. Furthermore, we have also provided a scalable way of performing NAS techniques with RL policies which is not limited to weight-sharing, but can potentially also be used to construct several other combinatorial structures in a flexible fashion, such as node deletions and edge removals. We showed that chromatic networks provide more aggressive compression than their state-of-the-art counterparts while preserving efficiency of the learned policies. We believe that our work opens new research directions, especially from using other combinatorial objects. Detailed analysis of obtained partitionings (see: Appendix C) also shows that learned structured matrices are very different from previously used state-of-the-art (in particular they are characterized by high displacement rank), yet it is not known what their properties are. It would be also important to understand how transferable those learned partitionings are across different RL tasks (see: Appendix D). We set LSTM hidden layer size to be 64, with 1 hidden layer. The learning rate was 0.001, and the entropy penalty strength was 0.3. We used a moving average weight of 0.99 for the critic, and used a temperature of 1.0 for softmax, with the training algorithm as REINFORCE.","We show that ENAS with ES-optimization in RL is highly scalable, and use it to compactify neural network policies by weight sharing.",Choromanski ; Fig ; thousands or millions ; REINFORCE ; OpenAI Gym ; Stanley & Miikkulainen ; Gage ; hundreds ; SL ; thousands,a rigid pattern ; ENAS methods ; they ; The learning rate ; e.g. limited storage ; a small sets ; two recent papers ; recent advances ; a moving average weight ; new research directions,Choromanski ; Fig ; thousands or millions ; REINFORCE ; OpenAI Gym ; Stanley & Miikkulainen ; Gage ; hundreds ; SL ; thousands,"We present a neural architecture search algorithm to construct compact reinforcement learning (RL) policies by combining ENAS and ES in a highly scalable and intuitive way. By defining the combinatorial search space of NAS to be the set of different edge-partitionings (colorings) into same-weight classes, we construct compact architectures via efficient learned edge-parts. For several RL tasks, we manage to learn colorings translating to effective policies parameterized by as few as 17 weight parameters, providing >90 % compression over vanilla policies and 6x compression over state-of-the-art compact policies based on Toeplitz mat",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Gradient-based meta-learning techniques are both widely applicable and proficient at solving challenging few-shot learning and fast adaptation problems. However, they have practical difficulties when operating on high-dimensional parameter spaces in extreme low-data regimes. We show that it is possible to bypass these limitations by learning a data-dependent latent generative representation of model parameters, and performing gradient-based meta-learning in this low-dimensional latent space. The resulting approach, latent embedding optimization (LEO), decouples the gradient-based adaptation procedure from the underlying high-dimensional space of model parameters. Our evaluation shows that LEO can achieve state-of-the-art performance on the competitive miniImageNet and tieredImageNet few-shot classification tasks. Further analysis indicates LEO is able to capture uncertainty in the data, and can perform adaptation more effectively by optimizing in latent space. Humans have a remarkable ability to quickly grasp new concepts from a very small number of examples or a limited amount of experience, leveraging prior knowledge and context. In contrast, traditional deep learning approaches BID24 BID39 treat each task independently and hence are often data inefficient -despite providing significant performance improvements across the board, such as for image classification BID41 BID14 , reinforcement learning BID29 BID40 , and machine translation BID3 BID44 . Just as humans can efficiently learn new tasks, it is desirable for learning algorithms to quickly adapt to and incorporate new and unseen information.Few-shot learning tasks challenge models to learn a new concept or behaviour with very few examples or limited experience BID6 BID23 . One approach to address this class of problems is meta-learning, a broad family of techniques focused on learning how to learn or to quickly adapt to new information. More specifically, optimization-based meta-learning approaches BID34 BID7 aim to find a single set of model parameters that can be adapted with a few steps of gradient descent to individual tasks. However, using only a few samples (typically 1 or 5) to compute gradients in a high-dimensional parameter space could make generalization difficult, especially under the constraint of a shared starting point for task-specific adaptation.In this work we propose a new approach, named Latent Embedding Optimization (LEO), which learns a low-dimensional latent embedding of model parameters and performs optimization-based meta-learning in this space. Intuitively, the approach provides two advantages. First, the initial parameters for a new task are conditioned on the training data, which enables a task-specific starting point for adaptation. By incorporating a relation network into the encoder, this initialization can better consider the joint relationship between all of the input data. Second, by optimizing in the lower-dimensional latent space, the approach can adapt the behaviour of the model more effectively. Further, by allowing this process to be stochastic, the ambiguities present in the few-shot data regime can be expressed.We demonstrate that LEO achieves state-of-the-art results on both the miniImageNet and tieredImageNet datasets, and run an ablation study and further analysis to show that both conditional parameter generation and optimization in latent space are critical for the success of the method. Source code for our experiments is available at https://github.com/deepmind/leo. We have introduced Latent Embedding Optimization (LEO), a meta-learning technique which uses a parameter generative model to capture the diverse range of parameters useful for a distribution over tasks, and demonstrated a new state-of-the-art result on the challenging 5-way 1-and 5-shot miniImageNet and tieredImageNet classification problems. LEO achieves this by learning a lowdimensional data-dependent latent embedding, and performing gradient-based adaptation in this space, which means that it allows for a task-specific parameter initialization and can perform adaptation more effectively.Future work could focus on replacing the pre-trained feature extractor with one learned jointly through meta-learning, or using LEO for tasks in reinforcement learning or with sequential data.",Latent Embedding Optimization (LEO) is a novel gradient-based meta-learner with state-of-the-art performance on the challenging 5-way 1-shot and 5-shot miniImageNet and tieredImageNet classification tasks.,LEO ; One ; Latent Embedding Optimization ; two ; First ; Second,a shared starting point ; the underlying high-dimensional space ; adaptation ; techniques ; examples ; Source code ; a meta-learning technique ; the approach ; tasks ; a new task,LEO ; One ; Latent Embedding Optimization ; two ; First ; Second,"Gradient-based meta-learning techniques are widely applicable and proficient at solving challenging few-shot learning and fast adaptation problems. However, they face practical limitations when operating on high-dimensional parameter spaces. LEO's latent embedding optimization (LEO) combines the gradient-based adaptation procedure from the underlying high-dimension space of model parameters, and achieves state-of-the-art performance on competitive miniImageNet and tieredImageNet few-shots classification tasks. Further analysis indicates LEO is able to capture uncertainty in the data, and can perform adaptation more effectively by optimizing in latent space. Humans can efficiently learn new",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Routing models, a form of conditional computation where examples are routed through a subset of components in a larger network, have shown promising results in recent works. Surprisingly, routing models to date have lacked important properties, such as architectural diversity and large numbers of routing decisions. Both architectural diversity and routing depth can increase the representational power of a routing network. In this work, we address both of these deficiencies. We discuss the significance of architectural diversity in routing models, and explain the tradeoffs between capacity and optimization when increasing routing depth. In our experiments, we find that adding architectural diversity to routing models significantly improves performance, cutting the error rates of a strong baseline by 35% on an Omniglot setup. However, when scaling up routing depth, we find that modern routing techniques struggle with optimization. We conclude by discussing both the positive and negative results, and suggest directions for future research. Modern neural networks process each input in the exact same way. This static paradigm is rigid compared to how brains process sensory inputs. Brains can utilize different subnetworks to process different categories of objects, such as face-specific processing in the fusiform face area BID27 BID17 . While static neural networks are empirically effective, it remains an open question whether neural networks with input-dependent processing can improve performance. Input-dependent processing holds the promise of offering better parameter efficiency and reduced computation due to the specialization of processing.Input-dependent processing has been underexplored in comparison with the wealth of work on static networks. Much of the work exploring input-dependent processing has taken the form of per-example routing within a network BID44 BID12 BID35 BID41 , which is form of conditional computation BID3 . In per-example routing, different examples are processed by different subcomponents, or experts BID23 , inside a larger model, or supernetwork BID12 . Only a subset of experts in the supernetwork are active for any given example. This paradigm enables the experts, each of which has its own set of parameters, to specialize to subsets of the input domain. The process of routing each example, which determines the experts that are used, is learned jointly with the parameters of the experts.Routing models to date have been relatively small, homogeneous networks. Typically, the same architectural unit (e.g., a fully connected layer of the same width) is used for every expert. The experts differ only in the parameters. Intuitively, the diversity of input examples is best handled by a diversity of architectural units with varying properties, implying that the usage of homogeneous experts is limiting. Furthermore, the number of routing decisions made in prior routing network works has typically been five or fewer. More routing decisions increase the number of distinct paths in the network, which may increase representational power. Making static networks deeper reliably improves performance, so we suspect that the representational power of routing networks is limited when only a few routing decisions are made.In this work, we address these two deficiencies in routing models. Since we aim to increase the representational capacities of routing models, we first introduce a simple trick that reduces overfitting.We then show how routing models with architectural diversity represent a broad family of models that generalize a number of powerful models. We also discuss the tradeoffs of scaling up the number of routing decisions with respect to optimization difficulty. In our experiments, we demonstrate that architecturally diverse routing models beat the best baselines with a 35% improvement in error rate on an Omniglot setup. By ablating the architectural diversity, we show that diversity plays a key role in achieving strong performance. We then scale up the number of decisions in routing models on CIFAR-10 and demonstrate that while competitive performance can be achieved, the accuracy drops as the number of decisions increase due to optimization challenges. Finally, we discuss our both our positive and negative findings and suggest future research directions for routing models. In this work, we introduced diversity to routing models and experimented with increasing routing depth. We believe that these two ideas are both intuitive and simple for researchers to implement. In our experiments, we found that architectural diversity can have a big impact in final performance. However, the impact of routing depth remains uncertain due to optimization difficulties faced by current methods.While routing models are a promising direction of research, practitioners still prefer static models due to their simplicity and reliable performance. For the use of routing models to become widespread, there must be a successful application of routing models on a domain where static models struggle. We believe that large scale problems fit this criteria, since they play into the theoretical scaling strengths of routing models. While architectural diversity will help improve routing models on large scale tasks, the routing depth optimization problem will continue to impede success. We encourage researchers to develop methods that will enable routing methods to effectively scale on large scale tasks. We remain optimistic that routing models will play an important role in the neural networks of the future.","Per-example routing models benefit from architectural diversity, but still struggle to scale to a large number of routing decisions.",Omniglot ; five ; two ; first,a key role ; form ; the exact same way ; optimization difficulties ; this criteria ; current methods ; the specialization ; parameters ; different categories ; future research directions,Omniglot ; five ; two ; first,"Routing models, a form of conditional computation where examples are routed through a subset of components in a larger network, have shown promising results in recent works. However, they lacked important properties, such as architectural diversity and routing depth. In this work, we discuss the significance of architectural diversity in routing models, and explain the tradeoffs between capacity and optimization.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Summarization of long sequences into a concise statement is a core problem in natural language processing, requiring non-trivial understanding of the input. Based on the promising results of graph neural networks on highly structured data, we develop a framework to extend existing sequence encoders with a graph component that can reason about long-distance relationships in weakly structured data such as text. In an extensive evaluation, we show that the resulting hybrid sequence-graph models outperform both pure sequence models as well as pure graph models on a range of summarization tasks. Summarization, the task of condensing a large and complex input into a smaller representation that retains the core semantics of the input, is a classical task for natural language processing systems. Automatic summarization requires a machine learning component to identify important entities and relationships between them, while ignoring redundancies and common concepts.Current approaches to summarization are based on the sequence-to-sequence paradigm over the words of some text, with a sequence encoder -typically a recurrent neural network, but sometimes a 1D-CNN BID34 or using self-attention BID32 -processing the input and a sequence decoder generating the output. Recent successful implementations of this paradigm have substantially improved performance by focusing on the decoder, extending it with an attention mechanism over the input sequence and copying facilities BID32 . However, while standard encoders (e.g. bidirectional LSTMs) theoretically have the ability to handle arbitrary long-distance relationships, in practice they often fail to correctly handle long texts and are easily distracted by simple noise BID19 .In this work, we focus on an improvement of sequence encoders that is compatible with a wide range of decoder choices. To mitigate the long-distance relationship problem, we draw inspiration from recent work on highly-structured objects BID23 BID20 BID14 BID12 . In this line of work, highly-structured data such as entity relationships, molecules and programs is modelled using graphs. Graph neural networks are then successfully applied to directly learn from these graph representations. Here, we propose to extend this idea to weakly-structured data such as natural language. Using existing tools, we can annotate (accepting some noise) such data with additional relationships (e.g. co-references) to obtain a graph. However , the sequential aspect of the input data is still rich in meaning, and thus we propose a hybrid model in which a standard sequence encoder generates rich input for a graph neural network. In our experiments, the resulting combination outperforms baselines that use pure sequence or pure graph-based representations.Briefly, the contributions of our work are: 1. A framework that extends standard sequence encoder models with a graph component that leverages additional structure in sequence data. 2. Application of this extension to a range of existing sequence models and an extensive evaluation on three summarization tasks from the literature. 3. We release all used code and data at https://github.com/CoderPat/structured-neural-summarization. add a parameter to this dynamic parameter list BILSTM → LSTM: adds a new parameter to the specified parameter BILSTM+GNN → LSTM:creates a new instance of the dynamic type specified BILSTM+GNN → LSTM+POINTER: add a parameter to a list of parameters Figure 1 : An example from the dataset for the METHODDOC source code summarization task along with the outputs of a baseline and our models. In the METHODNAMING dataset, this method appears as a sample requiring to predict the name Add as a subtoken sequence of length 1. We presented a framework for extending sequence encoders with a graph component that can leverage rich additional structure. In an evaluation on three different summarization tasks, we have shown that this augmentation improves the performance of a range of different sequence models across all tasks. We are excited about this initial progress and look forward to deeper integration of mixed sequence-graph modeling in a wide range of tasks across both formal and natural languages. The key insight, which we believe to be widely applicable, is that inductive biases induced by explicit relationship modeling are a simple way to boost the practical performance of existing deep learning systems. We use the datasets and splits of BID4 provided by their website. Upon scanning all methods in the dataset, the size of the corpora can be seen in Table 4 . More information can be found at BID4 .",One simple trick to improve sequence models: Compose them with a graph model,three ; BILSTM+GNN → ; BILSTM+GNN → LSTM+POINTER ; METHODNAMING ; Add,decoder choices ; the name Add ; Current approaches ; the datasets ; this initial progress ; the size ; standard encoders ; a sample ; the specified parameter ; a recurrent neural network,three ; BILSTM+GNN → ; BILSTM+GNN → LSTM+POINTER ; METHODNAMING ; Add,"Natural language processing requires non-trivial understanding of input. Graph neural networks on highly structured data develop a framework to extend existing sequence encoders with a graph component that can reason about long-distance relationships in weakly structured data such as text. The resulting hybrid sequence-graph models outperform both pure sequence models as well as pure graph models on a range of summarization tasks. The task of condensing a large and complex input into a smaller representation is a classical task for natural language processing systems. Automatic summarization requires a machine learning component to identify important entities and relationships between them, while ignoring redundancies and common concepts",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Many processes can be concisely represented as a sequence of events leading from a starting state to an end state. Given raw ingredients, and a finished cake, an experienced chef can surmise the recipe. Building upon this intuition,  we propose a new class of visual generative models: goal-conditioned predictors (GCP). Prior work on video generation largely focuses on prediction models that only observe frames from the beginning of the video. GCP instead treats videos as start-goal transformations, making video generation easier by conditioning on the more informative context provided by the first and final frames.  Not only do existing forward prediction approaches synthesize better and longer videos when modified to become goal-conditioned,  but GCP models can also utilize structures that are not linear in time, to accomplish hierarchical prediction . To this end, we study both auto-regressive GCP models and novel tree-structured GCP models that generate frames recursively, splitting the video iteratively into finer and finer segments delineated by subgoals . In experiments across simulated and real datasets, our GCP methods generate high-quality sequences over long horizons .  Tree-structured GCPs are also substantially easier to parallelize than auto-regressive GCPs, making training  and  inference  very  efficient, and allowing the model to train on sequences that are thousands of frames in length.Finally, we demonstrate the utility of GCP approaches for imitation learning in the setting without access to expert actions . Videos are on the supplementary website: https://sites.google.com/view/video-gcp Many phenomena, both natural and artificial, are naturally characterized as transformations -the most salient information about them is contained in the start and end states, given which it is possible to fill in intermediate states from prior experience. For example, ending up in San Francisco after starting in Oakland entails getting into a car and crossing the Bay Bridge. Similarly, to an expert engineer observing a bridge, the task of reverse-engineering how it was built is well-defined and tractable. In contrast, consider the task of predicting forward in time, having observed only the steel and concrete that went into making the bridge. Such forward prediction tasks are severely underconstrained, leading to high uncertainties that compound with time, making it impossible to make meaningful predictions after only a few stages of iterative forward prediction (see Fig. 1 ). This is aggravated in highdimensional settings such as forward video prediction, which despite being the most widely studied setting for video synthesis, struggles to produce coherent video longer than a few seconds. We propose to condition video synthesis instead on the substantially more informative context of the start and the goal frame. We term such models goal-conditioned predictors (GCP). Much like the engineer observing the bridge, GCPs treat long videos as start-goal transformations and reverseengineer the full video, conditioned on the first and final frames. The simplest instantiation of GCPs modifies existing forward prediction approaches to also observe the final frame. More broadly, once we consider conditioning on the goal frame, we can devise new types of GCP models that more efficiently leverage the hierarchical structure present in real-world event sequences ( Fig. 1, right) . Just as coarse-to-fine image synthesis (Karras et al., 2017) generates a high-resolution image by iteratively adding details to a low-resolution image, we can synthesize a temporally downsampled video in the form of sequences of keyframes, and fill it in iteratively. We propose to In our experiments, all GCP variants successfully generate longer and higher-quality video than has been demonstrated with standard auto-regressive video prediction models, which only utilize the starting frames for context. Furthermore, we show that tree-structured GCPs are more parallelizable than auto-regressive models, leading to very fast training and inference. We show that we can train tree-structured GCPs on videos consisting of thousands of frames. We also study the applications of GCPs, demonstrating that they can be utilized to enable prediction-based control in simulated imitation learning scenarios. In these settings, the GCP models can be trained without access to demonstrator actions, and can synthesize visual plans directly from start and goal images, which can then be tracked using an inverse model. We presented goal-conditioned predictors (GCPs) -predictive models that generate video sequences between a given start and goal frame. GCPs must learn to understand the mechanics of the environment that they are trained in, in order to accurately predict the intermediate events that must take place in order to bring about the goal images from the start images. GCP models not only allow for substantially more accurate video prediction than conventional models that are conditioned only on the beginning context, but also allow for novel model architectures. Specifically, we explore how, in addition to more conventional auto-regressive GCPs, we can devise tree-structured GCP models that predict video sequences hierarchically, starting with the coarsest level subgoals and recursively subdividing until a full sequence is produced. Our experimental results show that GCPs can make more accurate predictions. We also demonstrate that they can be utilized in an imitation learning scenario, where they can learn behaviors from video demonstrations without example actions. Imitation from observations, without actions, is applicable in a wide range of realistic scenarios. For example, a robot could learn the mechanics of cooking from watching videos on YouTube (Damen et al., 2018) , and then use this model to learn how to cook on its own. We hope that the imitation framework presented in our work can be a step in towards effectively leveraging such data for robotic control. A DATA PROCESSING For the Human 3.6 dataset, we downsample the original videos to 64 by 64 resolution. We obtain videos of length of roughly 800 to 1600 frames, which we randomly crop in time to 500-frame sequences. We split the Human 3.6 into training, validation and test set by correspondingly 95%, 5% and 5% of the data. On the TAP dataset, we use 48949 videos for training, 200 for validation and 200 for testing.",We propose a new class of visual generative models: goal-conditioned predictors. We show experimentally that conditioning on the goal allows to reduce uncertainty and produce predictions over much longer horizons.,al. ; Karras ; thousands ; Oakland ; Videos ; San Francisco ; a few seconds ; Fig ; the Bay Bridge ; first,more accurate predictions ; the intermediate events ; time ; Imitation ; sequences ; them ; start ; standard auto-regressive video prediction models ; visual plans ; finer and finer segments,al. ; Karras ; thousands ; Oakland ; Videos ; San Francisco ; a few seconds ; Fig ; the Bay Bridge ; first,"We propose a new class of visual generative models: goal-conditioned predictors (GCP). Prior work on video generation focused on prediction models that only observe frames from the beginning of the video. However, GCP focuses on the start-goal transformations, making video generation easier by conditioning on the more informative context provided by the first and final frames. GCP models can also utilize structures that are linear in time, to accomplish hierarchical prediction. In experiments across simulated and real datasets, we demonstrate the utility of GCP approaches for imitation learning in the setting without access to expert actions. These forward prediction tasks are severely undercon",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Deep neural networks (DNNs) have been shown to over-fit a dataset when being trained with noisy labels for a long enough time. To overcome this problem, we present a simple and effective method self-ensemble label filtering (SELF) to progressively filter out the wrong labels during training. Our method improves the task performance by gradually allowing supervision only from the potentially non-noisy (clean) labels and stops learning on the filtered noisy labels. For the filtering, we form running averages of predictions over the entire training dataset using the network output at different training epochs. We show that these ensemble estimates yield more accurate identification of inconsistent predictions throughout training than the single estimates of the network at the most recent training epoch. While filtered samples are removed entirely from the supervised training loss, we dynamically leverage them via semi-supervised learning in the unsupervised loss. We demonstrate the positive effect of such an approach on various image classification tasks under both symmetric and asymmetric label noise and at different noise ratios. It substantially outperforms all previous works on noise-aware learning across different datasets and can be applied to a broad set of network architectures. The acquisition of large quantities of a high-quality human annotation is a frequent bottleneck in applying DNNs. There are two cheap but imperfect alternatives to collect annotation at large scale: crowdsourcing from non-experts and web annotations, particularly for image data where the tags and online query keywords are treated as valid labels. Both these alternatives typically introduce noisy (wrong) labels. While Rolnick et al. (2017) empirically demonstrated that DNNs can be surprisingly robust to label noise under certain conditions, Zhang et al. (2017) has shown that DNNs have the capacity to memorize the data and will do so eventually when being confronted with too many noisy labels. Consequently, training DNNs with traditional learning procedures on noisy data strongly deteriorates their ability to generalize -a severe problem. Hence, limiting the influence of label noise is of great practical importance. A common approach to mitigate the negative influence of noisy labels is to eliminate them from the training data and train deep learning models just with the clean labels (Frénay & Verleysen, 2013) . Employing semi-supervised learning can even counteract the noisy labels (Laine & Aila, 2016; Luo et al., 2018) . However, the decision which labels are noisy and which are not is decisive for learning robust models. Otherwise, unfiltered noisy labels still influence the (supervised) loss and affect the task performance as in these previous works. They use the entire label set to compute the loss and severely lack a mechanism to identify and filter out the erroneous labels from the labels set. In this paper, we propose a self-ensemble label filtering (SELF) framework that identifies potentially noisy labels during training and keeps the network from receiving supervision from the filtered noisy labels. This allows DNNs to gradually focus on learning from undoubtedly correct samples even with an extreme level of noise in the labels (e.g., 80% noise ratio) and leads to improved performance as the supervision become less noisy. The key contribution of our work is progressive filtering, i.e., leverage the knowledge provided in the network's output over different training iterations to form a consensus of predictions (self-ensemble predictions) to progressively identify and filter out the noisy labels from the labeled data. When learning under label noise, the network receives noisy updates and hence fluctuates strongly. Such conduct of training would impede to learn stable neural representations and further mislead the consensus of the predictions. Therefore, it is essential to incorporate a model with stable training behavior to obtain better estimates from the consensus. Concretely, we employ the semi-supervised technique as a backbone to our framework to stabilize the learning process of the model. Correctly, we maintain the running average model, such as proposed by Tarvainen & Valpola (2017) , a.k.a. the Mean-Teacher model. This model ensemble learning provides a more stable supervisory signal than the noisy model snapshots and provides a stable ground for progressive filtering to filter out potential noisy labels. Note that this is different from just a mere combination of semi-supervised techniques with a noisy label filtering method. We call our approach self-ensemble label filtering (SELF) -that establishes model ensemble learning as a backbone to form a solid consensus of the self-ensemble predictions to filter out the noisy labels progressively. Our framework allows to compute supervised loss on cleaner subsets rather than the entire noisy labeled data as in previous works. It further leverages the entire dataset, including the filtered out erroneous samples in the unsupervised loss. To best of our knowledge, we are the first to identify and propose self-ensemble as a principled technique against learning under noisy labels. Our motivation stems from the observation that DNNs start to learn from easy samples in initial phases and gradually adapt to hard ones during training. When trained on wrongly labeled data, DNNs learn from clean labels at ease and receive inconsistent error signals from the noisy labels before over-fitting to the dataset. The network's prediction is likely to be consistent on clean samples and inconsistent or oscillates strongly on wrongly labeled samples over different training iterations. Based on this observation, we record the outputs of a single network made on different training epochs and treat them as an ensemble of predictions obtained from different individual networks. We call these ensembles that are evolved from a single network self-ensemble predictions. Subsequently, we identify the correctly labeled samples via the agreement between the provided label set and our running average of self-ensemble predictions. The samples of ensemble predictions that agree with the provided labels are likely to be consistent and treated as clean samples. In summary, our SELF framework stabilizes the training process and improves the generalization ability of DNNs. We evaluate the proposed technique on image classification tasks using CI-FAR10, CIFAR100 & ImageNet. We demonstrate that SELF consistently outperforms the existing approaches on asymmetric and symmetric noise at all noise levels, as shown in Fig. 1 . Besides, SELF remains robust towards the choice of the network architecture. Our work is transferable to other tasks without the need to modify the architecture or the primary learning objective. Figure 2: Overview of the self-ensemble label filtering (SELF) framework. The model starts in iteration 0 with training from the noisy label set. During training, the model maintains a selfensemble, a running average of itself (Tarvainen & Valpola, 2017) to provide a stable learning signal. Also, the model collects a self-ensemble prediction (moving-average) for the subsequent filtering. Once the best model is found, these predictions identify and filter out noisy labels using the original label set L 0 . The model performs this progressive filtering until there is no more better model. For details see Algorithm 1. 2 SELF-ENSEMBLE LABEL FILTERING 2.1 OVERVIEW Fig. 2 shows an overview of our proposed approach. In the beginning, we assume that the labels of the training set are noisy. The model attempts to identify correct labels progressively using selfforming ensembles of models and predictions. Since wrong labels cause strong fluctuations in the model's predictions, using ensembles is a natural way to counteract noisy labels. Concretely, in each iteration, the model learns from a detected set of potentially correct labels and maintains a running average of model snapshots (realized by the Mean Teacher model Tarvainen & Valpola (2017) ). This ensemble model is evaluated on the entire dataset and provides an additional learning signal for training the single models. Additionally, our framework maintains the runningaverage of the model's predictions for the filtering process. The model is trained until we find the best model w.r.t. the performance on the validation set (e.g., by early-stopping). The set of correct labels is detected based on the strategy defined in Sec. 2.2. In the next iteration, we again use all data and the new filtered label set as input for the model training. The iterative training procedure stops when no better model can be found. In the following, we give more details about the combination of this training and filtering procedure. We propose a simple and easy to implement a framework to train robust deep learning models under incorrect or noisy labels. We filter out the training samples that are hard to learn (possibly noisy labeled samples) by leveraging ensemble of predictions of the single network's output over different training epochs. Subsequently, we allow clean supervision from the non-hard samples and further leverage additional unsupervised loss from the entire dataset. We show that our framework results in DNN models with superior generalization performance on CIFAR-10, CIFAR-100 & ImageNet and outperforms all previous works under symmetric (uniform) and asymmetric noises. Furthermore, our models remain robust despite the increasing noise ratio and change in network architectures.",We propose a self-ensemble framework to train more robust deep learning models under noisy labeled datasets.,Frénay & Verleysen ; w.r.t ; Zhang et al ; Luo et al. ; first ; Fig ; noisy data ; Laine & Aila ; Sec ; Tarvainen & Valpola,Our motivation ; the filtering process ; the training data ; clean supervision ; which labels ; the runningaverage ; the Mean-Teacher model ; the knowledge ; additional unsupervised loss ; The iterative training procedure,Frénay & Verleysen ; w.r.t ; Zhang et al ; Luo et al. ; first ; Fig ; noisy data ; Laine & Aila ; Sec ; Tarvainen & Valpola,"Deep neural networks (DNNs) have been shown to over-fit a dataset when trained with noisy labels for a long enough time. To overcome this problem, we use self-ensemble label filtering (SELF) to gradually filter out the wrong labels during training. This approach improves task performance by gradually allowing supervision only from potentially non-noisy (clean) labels and stops learning on the filtered noisy labels. In the unsupervised loss, we employ semi-supervised learning to augment the task performance. The positive effect of this approach on image classification tasks under symmetric and asymmetric label noise and at different noise",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Neural networks are vulnerable to small adversarial perturbations. Existing literature largely focused on understanding and mitigating the vulnerability of learned models. In this paper, we demonstrate an intriguing phenomenon about the most popular robust training method in the literature, adversarial training: Adversarial robustness, unlike clean accuracy, is sensitive to the input data distribution. Even a semantics-preserving transformations on the input data distribution can cause a significantly different robustness for the adversarial trained model that is both trained and evaluated on the new distribution. Our discovery of such sensitivity on data distribution is based on a study which disentangles the behaviors of clean accuracy and robust accuracy of the Bayes classifier. Empirical investigations further confirm our finding. We construct semantically-identical variants for MNIST and CIFAR10 respectively, and show that standardly trained models achieve comparable clean accuracies on them, but adversarially trained models achieve significantly different robustness accuracies. This counter-intuitive phenomenon indicates that input data distribution alone can affect the adversarial robustness of trained neural networks, not necessarily the tasks themselves. Lastly, we discuss the practical implications on evaluating adversarial robustness, and make initial attempts to understand this complex phenomenon. Neural networks have been demonstrated to be vulnerable to adversarial examples BID22 BID3 . Since the first discovery of adversarial examples, great progress has been made in constructing stronger adversarial attacks BID12 BID18 BID17 BID6 . In contrast, defenses fell behind in the arms race BID5 BID1 . Recently a line of works have been focusing on understanding the difficulty in achieving adversarial robustness from the perspective of data distribution. In particular, BID24 demonstrated the inevitable tradeoff between robustness and clean accuracy in some particular examples. BID20 showed that the sample complexity of ""learning to be robust"" learning could be significantly higher than that of ""learning to be accurate"".In this paper, we contribute to this growing literature from a new angle, by studying the relationship between adversarial robustness and the input data distribution. We focus on the adversarial training method, arguably the most popular defense method so far due to its simplicity, effectiveness and scalability BID12 BID13 BID15 BID17 BID8 . Our main contribution is the finding that adversarial robustness is highly sensitive to the input data distribution:A semantically-lossless shift on the data distribution could result in a drastically different robustness for adversarially trained models.Note that this is different from the transferability of a fixed model that is trained on one data distribution but tested on another distribution. Even retraining the model on the new data distribution may give us a completely different adversarial robustness on the same new distribution. This is also in sharp contrast to the clean accuracy of standard training, which, as we show in later sections, is insensitive to such shifts. To our best knowledge, our paper is the first work in the literature that demonstrates such sensitivity.Our investigation is motivated by the empirical observations on the MNIST dataset and the CIFAR10 dataset. In particular , while comparable SOTA clean accuracies (the difference is less than 3%) are achieved by MNIST and CIFAR10 BID10 , CIFAR10 suffers from much lower achievable robustness than MNIST in practice. 1 Results of this paper consist of two parts. First in theory , we start with analyzing the difference between the regular Bayes error and the robust error, and show that the regular Bayes error is invariant to invertible transformations of the data distribution, but the robust error is not. We further prove that if the input data is uniformly distributed, then the perfect decision boundary cannot be robust. However, we also manage to find a robust model for the binarized MNIST dataset (semantically almost identical to MNIST, later described in Section 3). The certification method by BID26 guarantees that this model achieves at most 3% robust error. Such a sharp contrast suggests the important role of the data distribution in adversarial robustness, and leads to our second contribution on the empirical side: we design a series of augmented MNIST and CIFAR10 datasets to demonstrate the sensitivity of adversarial robustness to the input data distribution.Our finding of such sensitivity raises the question of how to properly evaluate adversarial robustness. In particular, the sensitivity of adversarial robustness suggests that certain datasets may not be sufficiently representative when benchmarking different robust learning algorithms. It also raises serious concerns about the deployment of believed-to-be-robust training algorithm in a real product. In a standard development procedure , various models (for example different network architectures) would be prototyped and measured on the existing data. However, the sensitivity of adversarial robustness makes the truthfulness of the performance estimations questionable, as one would expect future data to be slightly shifted. We illustrate the practical implications in Section 4 with two practical examples: 1) the robust accuracy of PGD trained model is sensitive to gamma values of gamma-corrected CIFAR10 images. This indicates that image datasets collected under different light conditions may have different robustness properties; 2) both as a ""harder"" version of MNIST, the fashion-MNIST BID27 and edge-fashion-MNIST (an edge detection variant described in Section 4.2) exhibit completely different robustness characteristics. This demonstrates that different datasets may give completely different evaluations for the same algorithm.Finally, our finding opens up a new angle and provides novel insights to the adversarial vulnerability problem, complementing several recent works on the issue of data distributions' influences on robustness. BID24 hypothesize that there is an intrinsic tradeoff between clean accuracy and adversarial robustness. Our studies complement this result, showing that there are different levels of tradeoffs depending on the characteristics of input data distribution, under the same learning settings (training algorithm, model and training set size). BID20 show that different data distributions could have drastically different properties of adversarially robust generalization, theoretically on Bernoulli vs mixtures of Gaussians, and empirically on standard benchmark datasets. From the sensitivity perspective, we demonstrate that being from completely different distributions (e.g. binary vs Gaussian or MNIST vs CIFAR10) may not be the essential reason for having large robustness difference. Gradual semantics-preserving transformations of data distribution can also cause large changes to datasets' achievable robustness. We make initial attempts in Section 5 to further understand this sensitivity. We investigated perturbable volume and inter-class distance as the natural causes of the sensitivity; model capacity and sample complexity as the natural remedies. However, the complexity of the problem has so far defied our efforts to give a definitive answer. In this paper we provided theoretical analyses to show the significance of input data distribution in adversarial robustness, which further motivated our systematic experiments on MNIST and CI-FAR10 variants. We discovered that, counter-intuitively, robustness of adversarial trained models are sensitive to semantically-preserving transformations on data. We demonstrated the practical implications of our finding that the existence of such sensitivity questions the reliability in evaluating robust learning algorithms on particular datasets. Finally, we made initial attempts to understand this sensitivity. DISPLAYFORM0 Then we apply Markov's inequality, for all real number t > 0: DISPLAYFORM1 Finally, we observe that the longest (in terms of 2 norm) such ∞ attacks vector to HP 2 are parallel to the normal vector 1 to HP 2 . They have 2 distance √ d. The set these attacks cover is characterized by {x DISPLAYFORM2 Let t = 2 d, we have: DISPLAYFORM3 In the case of zero-one loss, RR DISPLAYFORM4 A.2 PROOF FOR THEOREM 2.1Proof. (First Inequality for Cube) The proof here follows that of BID16 , but we track of the tight constants so as to give tighter adversarial robustness calculations.Let Φ be one dimensional standard normal cumulative distribution function and let µ d denote d dimensional Gaussian measures. Consider the map T : DISPLAYFORM5 T pushes forward µ d defined on R d into a probability measure P on (0, 1) d : DISPLAYFORM6 for A ⊂ (0, 1) d . Next we have the following Gaussian isoperimetric/concentration inequality BID16 : DISPLAYFORM7 Now for A ⊂ (0, 1) d , we have: DISPLAYFORM8 where the first inequality follows from that T has Lipschitz constant DISPLAYFORM9 , and thus T −1 has Lipschitz constant √ 2π; and the second one follows from Gaussian isoperimetric inequality. DISPLAYFORM10 Additionally, the inequality Φ(x) ≥ 1 − e x 2 2 implies the last inequality in the theorem.","Robustness performance of PGD trained models are sensitive to semantics-preserving transformation of image datasets, which implies the trickiness of evaluation of robust learning algorithms in practice.",SOTA ; Bernoulli ; one ; Markov ; Gaussians ; Gaussian ; MNIST ; zero-one ; PGD ; two,two practical examples ; the Bayes classifier ; this complex phenomenon ; The certification method ; the perspective ; terms ; The set ; the adversarial vulnerability problem ; one dimensional standard normal cumulative distribution function ; Our studies,SOTA ; Bernoulli ; one ; Markov ; Gaussians ; Gaussian ; MNIST ; zero-one ; PGD ; two,"Neural networks are vulnerable to small adversarial perturbations due to their sensitivity to the input data distribution. An intriguing phenomenon about the most popular robust training method in the literature, adversarial training, is that adversarial robustness, unlike clean accuracy, is highly sensitive to input data distributions. The discovery of such sensitivity on data distribution is based on a study which disentangles the behaviors of clean accuracy and robust accuracy of the Bayes classifier. Empirical investigations further confirm our finding. We construct semantically-identical variants for MNIST and CIFAR10 respectively, and show that standardly trained models achieve",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Learning long-term dependencies is a key long-standing challenge of recurrent neural networks (RNNs). Hierarchical recurrent neural networks (HRNNs) have been considered a promising approach as long-term dependencies are resolved through shortcuts up and down the hierarchy. Yet, the memory requirements of Truncated Backpropagation Through Time (TBPTT) still prevent training them on very long sequences. In this paper, we empirically show that in (deep) HRNNs, propagating gradients back from higher to lower levels can be replaced by locally computable losses, without harming the learning capability of the network, over a wide range of tasks. This decoupling by local losses reduces the memory requirements of training by a factor exponential in the depth of the hierarchy in comparison to standard TBPTT. Recurrent neural networks (RNNs) model sequential data by observing one sequence element at a time and updating their internal (hidden) state towards being useful for making future predictions. RNNs are theoretically appealing due to their Turing-completeness Siegelmann and Sontag (1995) , and, crucially, have been tremendously successful in complex real-world tasks, including machine translation Cho et al. (2014) ; Sutskever et al. (2014) , language modelling Mikolov et al. (2010) , and reinforcement learning Mnih et al. (2016) . Still, training RNNs in practice is one of the main open problems in deep learning, as the following issues prevail. (1) Learning long-term dependencies is extremely difficult because it requires that the gradients (i.e. the error signal) have to be propagated over many steps, which easily causes them to vanish or explode Hochreiter (1991) ; Bengio et al. (1994) ; Hochreiter (1998) (2) Truncated Backpropagation Through Time (TBPTT) Williams and Peng (1990) , the standard training algorithm for RNNs, requires memory that grows linearly in the length of the sequences on which the network is trained. This is because all past hidden states must be stored. Therefore, the memory requirements of training RNNs with large hidden states on long sequences become prohibitively large. (3) In TBPTT, parameters cannot be updated until the full forward and backward passes have been completed. This phenomenon is known as the parameter update lock Jaderberg et al. (2017) . As a consequence, the frequency at which parameters can be updated is inversely proportional to the length of the time-dependencies that can be learned, which makes learning exceedingly slow for long sequences. The problem of vanishing/exploding gradients has been alleviated by a plethora of approaches ranging from specific RNN architectures Hochreiter and Schmidhuber (1997) ; Cho et al. (2014) to optimization techniques aiming at easing gradient flow Martens and Sutskever (2011) ; Pascanu et al. (2013) . A candidate for effectively resolving the vanishing/exploding gradient problem is hierarchical RNNs (HRNNs) Schmidhuber (1992) ; El Hihi and Bengio (1996) ; Koutnik et al. (2014) ; Sordoni et al. (2015) ; Chung et al. (2016) . In HRNNs, the network itself is split into a hierarchy of levels, which are updated at decreasing frequencies. As higher levels of the hierarchy are updated less frequently, these architectures have short (potentially logarithmic) gradient paths that greatly reduce the vanishing/exploding gradients issue. In this paper, we show that in HRNNs, the lower levels of the hierarchy can be decoupled from the higher levels, in the sense that the gradient flow from higher to lower levels can effectively be replaced by locally computable losses. Also, we demonstrate that in consequence, the decoupled HRNNs admit training with memory decreased by a factor exponentially in the depth of the hierarchy compared to HRNNs with standard TBPTT. The local losses stem from decoder networks which are trained to decode past inputs to each level from the hidden state that is sent up the hierarchy, thereby forcing this hidden state to contain all relevant information. We experimentally show that in a diverse set of tasks which rely on long-term dependencies and include deep hierarchies, the performance of the decoupled HRNN with local losses is indistinguishable from the standard HRNN. In summary, we introduce a RNN architecture with short gradient paths that can be trained memoryefficiently, thereby addressing issues (1) and (2). In the bigger picture, we believe that our approach of replacing gradient flow in HRNNs by locally computable losses may eventually help to attempt solving issue (3) as well. In this paper, we have shown that in hierarchical RNNs the gradient flow from higher to lower levels can be effectively replaced by locally computable losses. This allows memory savings up to an exponential factor in the depth of the hierarchy. In particular, we first explained how not propagating gradients from higher to lower levels permits these memory savings. Then, we introduced auxiliary losses that encourage information to flow up the hierarchy. Finally, we demonstrated experimentally that the memory-efficient HRNNs with our auxiliary loss perform on par with the memory-heavy HRNNs and strongly outperform HRNNs given the same memory budget on a wide range of tasks, including deeper hierarchies. High capacity RNNs, like Differentiable Plasticity Miconi et al. (2018) , or Neural Turing Machines Graves et al. (2014) have been shown to be useful and even achieve state-of-the-art in many tasks. However, due to the memory cost of TBPTT, training such models is often impractical for long sequences. We think that combining these models with our techniques in future work could open the possibility for using high capacity RNNs for tasks involving long-term dependencies that have been out of reach so far. Still, the problem of the parameter update lock remains. While this is the most under-explored of the three big problems when training RNNs (vanishing/exploding gradients and memory requirements being the other two), resolving it is just as important in order to be able to learn long-term dependencies. We believe that the techniques laid out in this work (i.e. replacing gradients in HRNNs by locally computable losses) can be a stepping stone towards solving the parameter update lock. We leave this for future work.",We replace some gradients paths in hierarchical RNN's by an auxiliary loss. We show that this can reduce the memory cost while preserving performance.,Bengio ; El Hihi ; Mikolov et al ; two ; Koutnik ; Sordoni ; Hochreiter ; Pascanu ; three ; Martens,state ; gradient flow ; we ; these models ; one sequence element ; training RNNs ; deep hierarchies ; the higher levels ; a diverse set ; the learning capability,Bengio ; El Hihi ; Mikolov et al ; two ; Koutnik ; Sordoni ; Hochreiter ; Pascanu ; three ; Martens,"Learning long-term dependencies is a key challenge of recurrent neural networks (RNNs). Hierarchical recurrent networks (HRNNs) have been considered a promising approach as they resolve the problems through shortcuts up and down the hierarchy. However, the memory requirements of Truncated Backpropagation Through Time (TBPTT) prevent training RNNs on very long sequences. In this paper, we show that in (deep) HRNNs, propagating gradients back from higher to lower levels can be replaced by locally computable losses without harming the learning capability of the network. Recurrent neural networks model sequential data",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"We study the following three fundamental problems about ridge regression: (1) what is the structure of the estimator? (2) how to correctly use cross-validation to choose the regularization parameter? and (3) how to accelerate computation without losing too much accuracy? We consider the three problems in a unified large-data linear model. We give a precise representation of ridge regression as a covariance matrix-dependent linear combination of the true parameter and the noise. 
 We study the bias of $K$-fold cross-validation for choosing the regularization parameter, and propose a simple bias-correction. We analyze the accuracy of primal and dual sketching for ridge regression, showing they are surprisingly accurate. Our results are illustrated by simulations and by analyzing empirical data. Ridge or 2 -regularized regression is a widely used method for prediction and estimation when the data dimension p is large compared to the number of datapoints n. This is especially so in problems with many good features, where sparsity assumptions may not be justified. A great deal is known about ridge regression. It is Bayes optimal for any quadratic loss in a Bayesian linear model where the parameters and noise are Gaussian. The asymptotic properties of ridge have been widely studied (e.g., Tulino & Verdú, 2004; Serdobolskii, 2007; Couillet & Debbah, 2011; Dicker, 2016; Dobriban & Wager, 2018, etc) . For choosing the regularization parameter in practice, cross-validation (CV) is widely used. In addition, there is an exact shortcut (e.g., Hastie et al., 2009, p. 243) , which has good consistency properties (Hastie et al., 2019) . There is also a lot of work on fast approximate algorithms for ridge, e.g., using sketching methods (e.g., el Alaoui & Mahoney, 2015; Chen et al., 2015; Wang et al., 2018; Chowdhury et al., 2018, among","We study the structure of ridge regression in a high-dimensional asymptotic framework, and get insights about cross-validation and sketching.",Chen et al. ; el Alaoui & Mahoney ; Bayesian ; Couillet & Debbah ; Chowdhury ; Tulino & Verdú ; Serdobolskii ; CV ; Dicker ; Ridge,Debbah ; the bias ; ridge regression ; an exact shortcut ; work ; p. ; the three problems ; Our results ; the structure ; practice,Chen et al. ; el Alaoui & Mahoney ; Bayesian ; Couillet & Debbah ; Chowdhury ; Tulino & Verdú ; Serdobolskii ; CV ; Dicker ; Ridge,"In a unified large-data linear model, ridge regression is a covariance matrix-dependent linear combination of the true parameter and the noise. The bias of $K$-fold cross-validation for choosing the regularization parameter is explained by simulations and by analyzing empirical data. The accuracy of primal and dual sketching for ridge regression are surprisingly accurate. The asymptotic properties of ridge have been widely studied and fast approximate algorithms for ridge are available.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Group convolutional neural networks (G-CNNs) can be used to improve classical CNNs by equipping them with the geometric structure of groups. Central in the success of G-CNNs is the lifting of feature maps to higher dimensional disentangled representations, in which data characteristics are effectively learned, geometric data-augmentations are made obsolete, and predictable behavior under geometric transformations (equivariance) is guaranteed via group theory. Currently, however, the practical implementations of G-CNNs are limited to either discrete groups (that leave the grid intact) or continuous compact groups such as rotations (that enable the use of Fourier theory). In this paper we lift these limitations and propose a modular framework for the design and implementation of G-CNNs for arbitrary Lie groups. In our approach the differential structure of Lie groups is used to expand convolution kernels in a generic basis of B-splines that is defined on the Lie algebra. This leads to a flexible framework that enables localized, atrous, and deformable convolutions in G-CNNs by means of respectively localized, sparse and non-uniform B-spline expansions. The impact and potential of our approach is studied on two benchmark datasets: cancer detection in histopathology slides (PCam dataset) in which rotation equivariance plays a key role and facial landmark localization (CelebA dataset) in which scale equivariance is important. In both cases, G-CNN architectures outperform their classical 2D counterparts and the added value of atrous and localized group convolutions is studied in detail. Group convolutional neural networks (G-CNNs) are a class of neural networks that are equipped with the geometry of groups. This enables them to profit from the structure and symmetries in signal data such as images (Cohen & Welling, 2016) . A key feature of G-CNNs is that they are equivariant with respect to transformations described by the group, i.e., they guarantee predictable behavior under such transformations and are insensitive to both local and global transformations on the input data. Classical CNNs are a special case of G-CNNs that are equivariant to translations and, in contrast to unconstrained NNs, they make advantage of (and preserve) the basic structure of signal data throughout the network (LeCun et al., 1990) . By considering larger groups (i.e. considering not just translation equivariance) additional geometric structure can be utilized in order to improve performance and data efficiency (see G-CNN literature in Sec. 2). Part of the success of G-CNNs can be attributed to the lifting of feature maps to higher dimensional objects that are generated by matching kernels under a range of poses (transformations in the group). This leads to a disentanglement with respect to the pose and together with the group structure this enables a flexible way of learning high level representations in terms of low-level activated neurons observed in specific configurations, which we conceptually illustrate in Fig. 1 . From a neuro-psychological viewpoint, this resembles a hierarchical composition from low-to high-level features akin to the recognition-by-components model by Biederman (1987) , a viewpoint which is also adopted in work on capsule networks (Hinton et al., 2011; Sabour et al., 2017) . In particular in ) the group theoretical connection is made explicit with equivariant capsules that provide a sparse index/value representation of feature maps on groups (Gens & Domingos, 2014) . In G-CNNs feature maps are lifted to the high-dimensional domain of the group G in which features are disentangled with respect to pose/transformation parameters. G-convolution kernels then learn to recognize high-level features in terms of patterns of relative transformations, described by the group structure. This is conceptually illustrated for the detection of faces, which in the SE(2) case are considered as a pattern of lines in relative positions and orientations, or in the R 2 R + case as blobs/circles in relative positions and scales. Representing low-level features via features maps on groups, as is done in G-CNNs, is also motivated by the findings of Hubel & Wiesel (1959) and Bosking et al. (1997) on the organization of orientation sensitive simple cells in the primary visual cortex V1. These findings are mathematically modeled by sub-Riemannian geometry on Lie groups (Petitot, 2003; Citti & Sarti, 2006; Duits et al., 2014) and led to effective algorithms in image analysis (Franken & Duits, 2009; Bekkers et al., 2015b; Favali et al., 2016; Duits et al., 2018; Baspinar, 2018) . In recent work Montobbio et al. (2019) show that such advanced V1 modeling geometries emerge in specific CNN architectures and in Ecker et al. (2019) the relation between group structure and the organization of V1 is explicitly employed to effectively recover actual V1 neuronal activities from stimuli by means of G-CNNs. Figure 2: The Log-map allows us to map elements from curved manifolds such as the 2-sphere to a flat Euclidean tangent space. For Lie groups the Logmap is analytic, globally defined, and it provides us with a flexible tool to define group convolution kernels via Bsplines. In our Lie group context the 2-sphere is treated as a quotient group SO(3)/SO(2). Technical details are given in Sec. 3 and App. B. that are the semi-direct product of the translation group with a Lie group H. As such, only a few core definitions about the Lie group H (group product, inverse, Log, and action on R d ) need to be implemented in order to build full G-CNNs that are locally equivariant to the transformations in H. The impact and potential of our approach is studied on two datasets in which respectively rotation and scale equivariance plays a key role: cancer detection in histopathology slides (PCam dataset) and facial landmark localization (CelebA dataset). In both cases G-CNNs out-perform their classical 2D counterparts and the added value of atrous and localized G-convolutions is studied in detail. This paper presents a flexible framework for building G-CNNs for arbitrary Lie groups. The proposed B-spline basis functions, which are used to represent convolution kernels, have unique properties that cannot be achieved by classical Fourier based basis functions. Such properties include the construction of localized, atrous, and deformable convolution kernels. We experimentally demonstrated the added value of localized and atrous group convolutions on two different applications, considering two different groups. In particular in experiments with scale-translation G-CNNs, kernel localization was important. The B-spline basis functions can be considered as smooth pixels on Lie groups and they enable us to design G-CNNs using familiar notions from classical CNN design (localized, atrous, and deformable convolutions). Future work will focus on exploring these options further in new applications that could benefit from equivariance constraints, for which the tools now are available for a large class of transformation groups via the proposed Lie group B-splines. (Kantorovich & Akilov, 1982 , Ch 9, Thm 5), or (Duits, 2005, Thm 1) , that if K is linear and bounded it is an integral operator.",The paper describes a flexible framework for building CNNs that are equivariant to a large class of transformations groups.,App ; Sabour ; Riemannian ; Montobbio et al ; Kantorovich & Akilov ; Biederman ; Franken & Duits ; Petitot ; Citti & Sarti ; Sec,G-CNNs feature maps ; higher dimensional disentangled representations ; these limitations ; a range ; the Lie ; R d ; scale ; blobs/circles ; two different applications ; Hubel,App ; Sabour ; Riemannian ; Montobbio et al ; Kantorovich & Akilov ; Biederman ; Franken & Duits ; Petitot ; Citti & Sarti ; Sec,"Group convolutional neural networks (G-CNNs) can be used to improve classical CNNs by equipping them with the geometric structure of groups. This enables them to profit from the structure and symmetries in signal data such as images and facial landmark localization (CelebA dataset). In our approach, the differential structure of Lie groups is used to expand convolution kernels in a generic basis of B-splines defined on the Lie algebra. The impact and potential of our approach is studied on two benchmark datasets: cancer detection in histopathology slides (PCam dataset) in which rotation equivariance plays a key",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"For multi-valued functions---such as when the conditional distribution on targets given the inputs is multi-modal---standard regression approaches are not always desirable because they provide the conditional mean. Modal regression approaches aim to instead find the conditional mode, but are restricted to nonparametric approaches. Such approaches can be difficult to scale, and make it difficult to benefit from parametric function approximation, like neural networks, which can learn complex relationships between inputs and targets. In this work, we propose a parametric modal regression algorithm, by using the implicit function theorem to develop an objective for learning a joint parameterized function over inputs and targets. We empirically demonstrate on several synthetic problems that our method (i) can learn multi-valued functions and produce the conditional modes, (ii) scales well to high-dimensional inputs and (iii) is even more effective for certain unimodal problems, particularly for high frequency data where the joint function over inputs and targets can better capture the complex relationship between them. We conclude by showing that our method provides small improvements on two regression datasets that have asymmetric distributions over the targets. The goal in regression is to find the relationship between the input (observation) variable X ∈ X and the output (response) Y ∈ Y variable, given samples of (X, Y ). The underlying premise is that there exists an unknown underlying function g * : X → Y that maps the input space X to the output space Y. We only observe a noise-contaminated value of that function: sample (x, y) has y = g * (x) + η for some noise η. If the goal is to minimize expected squared error, it is well known that E[Y |x] is the optimal predictor (Bishop, 2006) . It is common to use Generalized Linear Models (Nelder & Wedderburn, 1972) , which attempt to estimate E[Y |x] for different uni-modal distribution choices for p(y|x), such as Gaussian (l 2 regression) and Poisson (Poisson regression). For multi-modal distributions, however, predicting E[Y |x] may not be desirable, as it may correspond to rarely observed y that simply fall between two modes. Further, this predictor does not provide any useful information about the multiple modes. Modal regression is designed for this problem, and though not widely used in the general machine learning community, has been actively studied in statistics. Most of the methods are non-parametric, and assume a single mode jae Lee (1989) ; Lee & Kim (1998) ; Kemp & Silva (2012) ; Yu & Aristodemou (2012) ; Yao & Li (2014) ; Lv et al. (2014) ; Feng et al. (2017) . The basic idea is to adjust target values towards their closest empirical conditional modes, based on a kernel density estimator. These methods rely on the chosen kernel and may have issues scaling to high-dimensional data due to issues in computing similarities in high-dimensional spaces. There is some recent work using quantile regression to estimate conditional modes (Ota et al., 2018) , and though promising for a parametric approach, is restricted to linear quantile regression. A parametric approach for modal regression would enable these estimators to benefit from the advances in learning functions with neural networks. The most straightforward way to do so is to learn a mixture distribution, such as with conditional mixture models with parameters learning by a neural network (Powell, 1987; Bishop, 1994; Williams, 1996; Husmeier, 1997; Husmeier & Taylor, 1998; Zen & Senior, 2014; Ellefsen et al., 2019) . The conditional modes can typically be extracted from such models. Such a strategy, however, might be trying to solve a harder problem than is strictly needed. The actual goal is to simply identify the conditional modes, without accurately representing the full conditional distribution. Training procedures for the conditional distribution can be more complex. Methods like EM can be slow (Vlassis & Krose, 1999) and some approaches have opted to avoid this altogether by discretizing the target and learning a discrete distribution (Weigend & Srivastava, 1995; Feindt, 2004) . Further, the mixture requires particular probabilistic choices to be made, including the number of components, which may not be correctly specified: they might be more or less than the true number of conditional modes. In this paper, we propose a new parametric modal regression approach, by developing an objective to learn a parameterized function f (x, y) on both input feature and target/output. We use the Implicit Function Theorem (Munkres, 1991) , which states that if we know the input-output relation in the form of an implicit function, then a general multi-valued function, under certain gradient conditions, can locally be converted to a single-valued function. We learn a function f (x, y) that approximates such local functions, by enforcing the gradient conditions. We empirically demonstrate that our method can effectively learning the conditional modes on several synthetic problems, and that for those same problems, scales well when the input is made high-dimensional. We also show an interesting benefit that the joint representation learned over x and y appears to improve prediction performance even for uni-modal problem, for high frequency functions where the function values changes quickly between nearby x. Finally, we show that our method provides small improvements on two regression datasets that have asymmetric distributions over the targets. The proposed approach to multi-valued prediction is flexible, allowing for a variable number of conditional modes to be discovered for each x, and we believe it is a promising direction for further improvements in parametric modal regression. The paper introduces a simple and powerful implicit function learning approach for modal regression. We show that it can handle datasets where the conditional distribution p(y|x) is multimodal, and is particularly useful when the underlying true mapping has a large bandwidth limit. We also illustrate that our algorithm achieves competitive performance on large real world datasets with different underlying target distributions. We would like to conclude with the following future directions. First, it would be interesting to establish connections to KDE-based modal regression methods, which have a nice theoretical interpretation (Feng et al., 2017) . The connection may yield finite sample analysis for our implicit function learning algorithm. Second, like many supervised learning algorithms, our algorithm may also overfit to noise. Popular regularization technique such as random dropout (Srivastava et al., 2014) may be tested for very noisy data. Third, in online learning setting, the efficiency of doing prediction by arg min y f θ (x, y) 2 + ( ∂f θ (x,y) ∂y + 1) 2 becomes a concern. One possible solution is to borrow ideas from cross-entropy method as used in reinforcement learning (Lim et al., 2018; Simmons-Edler et al., 2019) . For example, we can use a separate NN to suggest a set of initial values of y for searching optimums by gradient methods. Last, it is worth investigating alternative constraints on the Jacobian instead of restricting the diagonal values to −1.",We introduce a simple and novel modal regression algorithm which is easy to scale to large problems.,Husmeier ; Yu & Aristodemou ; al. ; Second ; Srivastava ; Ellefsen ; One ; Feng et al. ; Williams ; Gaussian,some approaches ; Modal regression approaches ; our method ; some recent work ; two regression datasets ; online learning setting ; ideas ; the Implicit Function Theorem ; alternative constraints ; certain gradient conditions,Husmeier ; Yu & Aristodemou ; al. ; Second ; Srivastava ; Ellefsen ; One ; Feng et al. ; Williams ; Gaussian,"Modal regression approaches aim to find the conditional mode, but are restricted to nonparametric approaches. These approaches can be difficult to scale, and make it difficult to benefit from parametric function approximation, like neural networks, which learn complex relationships between inputs and targets. In this work, we develop a parametric modal regression algorithm, using the implicit function theorem, to learn multi-valued functions and produce conditional modes. This approach scales well to high-dimensional inputs and high frequency data, and is even more effective for unimodal problems. The underlying premise is that there exists an unknown underlying function g * : X → Y",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"We present a simple proof for the benefit of depth in multi-layer feedforward network with rectifed activation (``""depth separation""). Specifically we present a sequence of classification problems f_i such that (a) for any fixed depth rectified network we can find an index m such that problems with index > m require exponential network width to fully represent the function f_m; and (b) for any problem f_m in the family, we present a concrete neural network with linear depth and bounded width that fully represents it.

 While there are several previous work showing similar results, our proof uses substantially simpler tools and techniques, and should be accessible to undergraduate students in computer science and people with similar backgrounds. We present a simple, geometric proof of the benefit of depth in deep neural networks. We prove that there exist a set of functions indexed by m, each of which can be efficiently represented by a depth m rectified MLP network requiring O(m) parameters. However, for any bounded depth rectified MLP network, there is a function f m in this set that representing it will require an exponential number of parameters in m. More formally, let G d be the set of multi-layer perceptron (MLP) networks with rectified activation and d hidden layers, and let g Θ be such an MLP with parameters Θ. We will prove the following theorem: Theorem 1 (Depth Separation). There exists a set of functions f 1 , f 2 , ..., f i : R 2 → {−1, 1} such that: While this is not a novel result, a main characteristic of our proof is its simplicity. In contrast to previous work, our proof uses only basic algebra, geometry and simple combinatorial arguments. As such, it can be easily read and understood by newcomers and practitioners, or taught in an undergraduate class, without requiring extensive background. Tailoring to these crowds, our presentation style is more verbose then is usual in papers of this kind, attempting to spell out all steps explicitly. We also opted to trade generality for proof simplicity, remaining in input space R 2 rather than the more general R n , thus allowing us to work with lines rather than hyperplanes. Beyond being easy to visualize, it also results in simple proofs of the different lemmas.",ReLU MLP depth seperation proof with gemoteric arguments,linear depth ; O(m ; Θ.,a set ; an undergraduate class ; width ; geometry ; this kind ; lines ; students ; practitioners ; it ; contrast,linear depth ; O(m ; Θ.,"We present a simple proof for the benefit of depth in multi-layer feedforward network with rectifed activation (``depth separation""). For any fixed depth rectified network with index m, we present a concrete neural network with linear depth and bounded width that fully represents the function f_m. While there are several previous work showing similar results, our proof uses substantially simpler tools and techniques, and should be accessible to undergraduate students in computer science and people with similar backgrounds. Theorem 1 (Depth Separation). There exists a set of functions indexed by m, each of which can be efficiently represented by a depth m rectified ML",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"We investigate the robustness properties of image recognition models equipped with two features inspired by human vision, an explicit episodic memory and a shape bias, at the ImageNet scale. As reported in previous work, we show that an explicit episodic memory improves the robustness of image recognition models against small-norm adversarial perturbations under some threat models. It does not, however, improve the robustness against more natural, and typically larger, perturbations. Learning more robust features during training appears to be necessary for robustness in this second sense. We show that features derived from a model that was encouraged to learn global, shape-based representations (Geirhos et al., 2019) do not only improve the robustness against natural perturbations, but when used in conjunction with an episodic memory, they also provide additional robustness against adversarial perturbations. Finally, we address three important design choices for the episodic memory: memory size, dimensionality of the memories and the retrieval method. We show that to make the episodic memory more compact, it is preferable to reduce the number of memories by clustering them, instead of reducing their dimensionality. ImageNet-trained deep neural networks (DNNs) are state of the art models for a range of computer vision tasks and are currently also the best models of the human visual system and primate visual systems more generally (Schrimpf et al., 2018 ). Yet, they have serious deficiencies as models of human and primate visual systems: 1) they are extremely sensitive to small adversarial perturbations imperceptible to the human eye (Szegedy et al., 2013) , 2) they are much more sensitive than humans to larger, more natural perturbations (Geirhos et al., 2018) , 3) they rely heavily on local texture information in making their predictions, whereas humans rely much more on global shape information (Geirhos et al., 2019; , 4) a fine-grained, image-by-image analysis suggests that images that ImageNet-trained DNNs find hard to recognize do not match well with the images that humans find hard to recognize . Here, we add a fifth under-appreciated deficiency: 5) human visual recognition has a strong episodic component lacking in DNNs. When we recognize a coffee mug, for instance, we do not just recognize it as a mug, but as this particular mug that we have seen before or as a novel mug that we have not seen before. This sense of familiarity/novelty comes automatically, involuntarily, even when we are not explicitly trying to judge the familiarity/novelty of an object we are seeing. More controlled psychological experiments also confirm this observation: humans have a phenomenally good longterm recognition memory with a massive capacity even in difficult one-shot settings (Standing, 1973; Brady et al., 2008) . Standard deep vision models, on the other hand, cannot perform this kind of familiarity/novelty computation naturally or automatically, since this information is available to a trained model only indirectly and implicitly in its parameters. What does it take to address these deficiencies and what are the potential benefits, if any, of doing so other than making the models more human-like in their behavior? In this paper, we address these questions. We show that a minimal model incorporating an explicit key-value based episodic memory does not only make it psychologically more realistic, but also reduces the sensitivity to small adversarial perturbations. It does not, however, reduce the sensitivity to larger, more natural perturbations and it does not address the heavy local texture reliance issue. In the episodic memory, using features from DNNs that were trained to learn more global shape-based representations (Geirhos et al., 2019) addresses these remaining issues and moreover provides additional robustness against adversarial perturbations. Together, these results suggest that two basic ideas motivated and inspired by human vision, a strong episodic memory and a shape bias, can make image recognition models more robust to both natural and adversarial perturbations at the ImageNet scale.","systematic study of large-scale cache-based image recognition models, focusing particularly on their robustness properties",two ; ImageNet ; second ; Geirhos ; al. ; three ; fifth ; one ; Brady,instance ; It ; that ; Szegedy et al ; they ; the sensitivity ; these remaining issues ; an object ; both natural and adversarial perturbations ; a phenomenally good longterm recognition memory,two ; ImageNet ; second ; Geirhos ; al. ; three ; fifth ; one ; Brady,"The robustness properties of image recognition models equipped with two features inspired by human vision, an explicit episodic memory and a shape bias, at the ImageNet scale, improve the robustness against small-norm adversarial perturbations under some threat models. Learning more robust features during training appears to be necessary for robustness in the second sense, where features derived from a model that was encouraged to learn global, shape-based representations (Geirhos et al., 2019) enhance robustness. These features enhance the performance against natural perturbation, and when used in conjunction with a memory, they also provide additional robustness",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Machine learning algorithms for generating molecular structures offer a promising new approach to drug discovery. We cast molecular optimization as a translation problem, where the goal is to map an input compound to a target compound with improved biochemical properties. Remarkably, we observe that when generated molecules are iteratively fed back into the translator, molecular compound attributes improve with each step. We show that this finding is invariant to the choice of translation model, making this a ""black box"" algorithm. We call this method Black Box Recursive Translation (BBRT), a new inference method for molecular property optimization. This simple, powerful technique operates strictly on the inputs and outputs of any translation model. We obtain new state-of-the-art results for molecular property optimization tasks using our simple drop-in replacement with well-known sequence and graph-based models. Our method provides a significant boost in performance relative to its non-recursive peers with just a simple ""``for"" loop. Further, BBRT is highly interpretable, allowing users to map the evolution of newly discovered compounds from known starting points. Automated molecular design using generative models offers the promise of rapidly discovering new compounds with desirable properties. Chemical space is large, discrete, and unstructured, which together, present important challenges to the success of any molecular optimization campaign. Approximately 10 8 compounds have been synthesized (Kim et al., 2015) while the range of potential drug-like candidates is estimated to between 10 23 and 10 80 (Polishchuk et al., 2013) . Consequently, new methods for intelligent search are paramount. A recently introduced paradigm for compound generation treats molecular optimization as a translation task where the goal is to map an input compound to a target compound with favorable properties (Jin et al., 2019b) . This framework has presented impressive results for constrained molecular property optimization where generated compounds are restricted to be structurally similar to the source molecule. We extend this framework to unconstrained molecular optimization by treating inference, vis-à-vis decoding strategies, as a first-class citizen. We observe that generated molecules can be repeatedly fed back into the model to generate even better compounds. This finding is invariant to the choice of translation model, making this a ""black box"" algorithm. This invariance is particularly attractive considering the recent emphasis on new molecular representations (Gómez-Bombarelli et al., 2018; Jin et al., 2018; Dai et al., 2018; Li et al., 2018; Kusner et al., 2017; Krenn et al., 2019) . Using our simple drop-in replacement, our method can leverage these recently introduced molecular representations in a translation setting for better optimization. We introduce Black Box Recursive Translation (BBRT), a new inference method for molecular property optimization. Surprisingly, by applying BBRT to well-known sequence-and graph-based models in the literature, we can produce new state-of-the-art results on property optimization benchmark tasks. Through an exhaustive exploration of various decoding strategies, we demonstrate the empirical benefits of using BBRT. We introduce simple ranking methods to decide which outputs are fed back into the model and find ranking to be an appealing approach to secondary property optimization. Finally, we demonstrate how BBRT is an extensible tool for interpretable and user-centric molecular design applications.",We introduce a black box algorithm for repeated optimization of compounds using a translation framework.,Black Box Recursive Translation ; Kim et al. ; al. ; Jin et al. ; first ; Jin ; Dai et al. ; Li ; Kusner ; Krenn et al.,the model ; generated compounds ; when generated molecules ; the recent emphasis ; this method ; outputs ; favorable properties ; the range ; a translation task ; molecular compound attributes,Black Box Recursive Translation ; Kim et al. ; al. ; Jin et al. ; first ; Jin ; Dai et al. ; Li ; Kusner ; Krenn et al.,"Machine learning algorithms for generating molecular structures offer a promising new approach to drug discovery. The goal is to map an input compound to a target compound with improved biochemical properties, and molecular compound attributes improve with each step. This finding is invariant to the choice of translation model, making this a ""black box"" algorithm. The Black Box Recursive Translation (BBRT) is a simple, powerful inference method for molecular property optimization, focusing on inputs and outputs of any translation model. We obtain state-of-the-art results for molecular properties optimization tasks using simple drop-in replacement with well-known sequence and graph-based",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Adam-typed optimizers, as a class of adaptive moment estimation methods with the exponential moving average scheme, have been successfully used in many applications of deep learning. Such methods are appealing for capability on large-scale sparse datasets. On top of that, they are computationally efficient and insensitive to the hyper-parameter settings. In this paper, we present a new framework for adapting Adam-typed methods, namely AdamT. Instead of applying a simple exponential weighted average, AdamT also includes the trend information when updating the parameters with the adaptive step size and gradients. The newly added term is expected to efficiently capture the non-horizontal moving patterns on the cost surface, and thus converge more rapidly. We show empirically the importance of the trend component, where AdamT outperforms the conventional Adam method constantly in both convex and non-convex settings. Employing first order optimization methods, such as stochastic gradient descent (SGD), is a key of solving large-scale problems. The classic gradient descent algorithm is widely used to update the model parameters, denoted by x, x t+1 = x t − η∇f (x t ), where the gradient is denoted by ∇f (x t ) and the step size by η. While the method has shown its efficiency for many contemporary tasks, the adaptive variants of SGD outperform the vanilla SGD methods on their rapid training time. Specifically, the step size η is substituted by an adaptive step size η/ √ v t , and v t is generated from the squared gradient [∇f (x t )] 2 . Several variants of the popular adaptive optimizers can be summarized into such common format. These optimizers share gradients calculation and parameters updating functions, but specify different moving average schemes for calculating the parameter-wise adaptive learning rate v t . For example, AdaGrad (Duchi et al., 2011) takes the arithmetic average of historical squared gradients [∇f (x t )] 2 . Compared with the conventional momentum method, it adapts the learning rate to each parameter to suit the sparse data structure, and thus gains a rapid convergence speed (Ruder, 2016) . Later, Tieleman & Hinton (2012) proposed RMSProp to reduce the aggressiveness of the decay rate in AdaGrad. The method modifies v t to the exponentially decayed squared gradients. Similar implementations could also be found in ADADELTA (Zeiler, 2012) . Instead of the squared gradients, the method applies squared parameter updates to define the adaptive learning rate. As a result, each update guarantees the same hypothetical units as the parameter. Later, Adam (Kingma & Ba, 2015) modifies RMSProp with the idea from momentum methods (Qian, 1999) . Except for the second moment moving average, the new rule also replaces the gradient ∇f (x t ) at the end of the Equation (1) to the first-moment estimation. The method has practically shown its superiority regarding the converge speed and memory requirement. While the aforementioned methods are the most famous frameworks, there are also many variants for each of them. The examples include NAdam (Dozat, 2016) , AMSGrad (Reddi et al., 2018) and Adafom (Chen et al., 2019) . So far, the adaptive methods with exponential moving average gradients have gained great attention with huge success in many deep learning tasks. However, it remains unsolved whether the simple exponential smoothing results or the level information is sufficient in capturing the landscape of the cost surface. When clear upward or downward pattern could be recognized within the moving routine, it is suggested to add a trend term on top of the single level information. In this paper, we modify the Adam rule with trend-corrected exponential smoothing schemes, namely AdamT, to obtain the local minima with a faster speed. To the best of our knowledge, our research is the first to apply the trend-corrected features on gradients scaling and parameters updating. It shall be emphasized that our framework is universally implementable for all adaptive update methods that apply the exponential average term, including but not restricted to ADADELTA, RMSProp, AdaMAX and other well-recognized methods. For the sake of conciseness, in this specific paper, we focus on Adam regarding rule modification and performance comparison. Our contributions in this paper could be summarized in three-fold: 1. We propose the notion of trend corrected exponential smoothing to modify the conventional application of exponential moving average in optimizers with adaptive gradients. Our AdamT method collaborates the trend information into the update rule of Adam. 2. We show the conditions for the method to converge in convex settings. The regret bound is in consistent to Adam at O( √ T ). 3. We demonstrate AdamT's convergence in both convex and non-convex settings. The performance is compared with Adam, where AdamT shows clear superiority on both the training set and the test set, especially for non-convex problems. For the remainder of the paper, we present the fundamental idea of Adam and Holt's linear methods in Section 2. In Section 3 and 4, we detail the update rules and experimental analysis, respectively. In addition, Section 5 reviews recent developments of Adam-typed optimizers. While many of them focus more on non-convex optimizations, there is a potential to incorporate our methods with such frameworks and this extension is expected for future settings. In this work, we have modified the scheme to calculate the adaptive step size from exponential moving average to trend-corrected exponential smoothing. Empirical results demonstrate that our method, AdamT, works well in practice and constantly beats the baseline method Adam. We leave some potentials for future developments. First, although we focused primarily on ADAM for theoretical and experimental analysis, we believe that similar ideas could also extend to other adaptive gradient methods, such as RMSProp (Tieleman & Hinton, 2012) and AMSGrad (Reddi et al., 2018) . Also, this work, the same as the original ADAM method, relies on the theoretical assumption of convex problems settings. We have demonstrated its computational ability on the non-convex settings, and it is possible to extend the theoretical framework to non-convex scenarios. Some potential candidates in the latest research are listed in Section 5. To find how the expectation of the trend estimates b m t relates to the expectation of the difference between the level estimates at successive timesteps ( m t − m t−1 ), we take the expectation for both sides of the above equation: where ζ can be considered as a small constant, since the factor (γ 1 φ 1 ) t−i will be tiny if the associated expectation E[( )] is stationary, the constant ζ will be zero. To further simplify the above equation, we apply the formula for the sum of geometric sequence: This suggests that we can use the term ] to correct the bias and close the discrepancy between the above two expectations at the presence of the damping factor φ 1 .","We present a new framework for adapting Adam-typed methods, namely AdamT, to include the trend information when updating the parameters with the adaptive step size and gradients.",RMSProp ; Reddi ; AdaGrad ; two ; al. ; Chen et al. ; Tieleman & Hinton ; AMSGrad ; zero ; Dozat,namely AdamT ; Adam-typed methods ; a key ; the converge speed and memory requirement ; many contemporary tasks ; other adaptive gradient methods ; the theoretical framework ; performance comparison ; both sides ; the conventional application,RMSProp ; Reddi ; AdaGrad ; two ; al. ; Chen et al. ; Tieleman & Hinton ; AMSGrad ; zero ; Dozat,"Adam-typed optimizers, as a class of adaptive moment estimation methods with the exponential moving average scheme, have been successfully used in deep learning. They are computationally efficient and insensitive to hyper-parameter settings. In this paper, we present a new framework for adapting Adam-typing methods, namely AdamT, which includes the trend information when updating the parameters with the adaptive step size and gradients. The newly added term is expected to efficiently capture non-horizontal moving patterns on the cost surface, and converge more rapidly. The trend component, where AdamT outperforms the conventional Adam method constantly in both convex",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to down-stream tasks, several model compression techniques on pre-trained language representations have been proposed (Sun et al., 2019; Sanh, 2019). However, surprisingly,  the simple baseline of just pre-training and fine-tuning compact models has been overlooked. In this paper, we first show that pre-training remains important in the context of smaller architectures, and fine-tuning pre-trained compact models can be competitive to more elaborate methods proposed in concurrent work. Starting with pre-trained compact models, we then explore transferring task knowledge from large fine-tuned models through standard knowledge distillation. The resulting simple, yet effective and general algorithm, Pre-trained Distillation, brings further improvements. Through extensive experiments, we more generally explore the interaction between pre-training and distillation under two variables that have been under-studied: model size and properties of unlabeled task data. One surprising observation is that they have a compound effect even when sequentially applied on the same data. To accelerate future research, we will make our 24 pre-trained miniature BERT models publicly available. Self-supervised learning on a general-domain text corpus followed by end-task learning is the twostaged training approach that enabled deep-and-wide Transformer-based networks (Vaswani et al., 2017) to advance language understanding (Devlin et al., 2018; Yang et al., 2019b; Sun et al., 2019b; . However, state-of-the-art models have hundreds of millions of parameters, incurring a high computational cost. Our goal is to realize their gains under a restricted memory and latency budget. We seek a training method that is well-performing, general and simple and can leverage additional resources such as unlabeled task data. Before considering compression techniques, we start with the following research question: Could we directly train small models using the same two-staged approach? In other words, we explore the idea of applying language model (LM) pre-training and task fine-tuning to compact architectures directly. This simple baseline has so far been overlooked by the NLP community, potentially based on an underlying assumption that the limited capacity of compact models is capitalized better when focusing on the end task rather than a general language model objective. Concurrent work to ours proposes variations of the standard pre-training+fine-tuning procedure, but with limited generality (Sun et al., 2019a; Sanh, 2019) . We make the surprising finding that pre-training+fine-tuning in its original formulation is a competitive method for building compact models. For further gains, we additionally leverage knowledge distillation (Hinton et al., 2015) , the standard technique for model compression. A compact student is trained to recover the predictions of a highly accurate teacher. In addition to the posited regularization effect of these soft labels (Hinton et al., 2015) , distillation provides a means of producing pseudo-labels for unlabeled data. By regarding LM pre-training of compact models as a student initialization strategy, we can take advantage of both methods. The resulting algorithm is a sequence of three standard training operations: masked LM (MLM) pre-training (Devlin et al., 2018) , task-specific distillation, and optional fine-tuning. From here on, we will refer to it as Pre-trained Distillation (PD) ( Figure 1 ). As we will show in Get loss L ← − y P Ω (y|x) log P θ (y|x) In a controlled study following data and model architecture settings in concurrent work (Section 4), we show that Pre-trained Distillation outperforms or is competitive with more elaborate approaches which use either more sophisticated distillation of task knowledge (Sun et al., 2019a) or more sophisticated pre-training from unlabeled text (Sanh, 2019) . The former distill task knowledge from intermediate teacher activations, starting with a heuristically initialized student. The latter fine-tune a compact model that is pre-trained on unlabeled text with the help of a larger LM teacher. One of the most noteworthy contributions of our paper are the extensive experiments that examine how Pre-trained Distillation and its baselines perform under various conditions. We investigate two axes that have been under-studied in previous work: model size and amount/quality of unlabeled data. While experimenting with 24 models of various sizes (4m to 110m parameters) and depth/width trade-offs, we observe that pre-trained students can leverage depth much better than width; in contrast, this property is not visible for randomly-initialized models. For the second axis, we vary the amount of unlabeled data, as well as its similarity to the labeled set. Interestingly, Pretrained Distillation is more robust to these variations in the transfer set than standard distillation. Finally, in order to gain insight into the interaction between LM pre-training and task-specific distillation, we sequentially apply these operations on the same dataset. In this experiment, chaining the two operations performs better than any one of them applied in isolation, despite the fact that a single dataset was used for both steps. This compounding effect is surprising, indicating that pre-training and distillation are learning complementary aspects of the data. Given the effectiveness of LM pre-training on compact architectures, we will make our 24 pretrained miniature BERT models publicly available in order to accelerate future research.",Studies how self-supervised learning and knowledge distillation interact in the context of building compact models.,hundreds of millions ; Hinton ; Devlin et al. ; NLP ; Transformer ; Yang et al. ; P θ ; two ; Sun ; L ← −,randomly-initialized models ; the most noteworthy contributions ; pre-trained compact models ; Pretrained Distillation ; the same data ; et al ; either more sophisticated distillation ; both methods ; which ; Pre-trained Distillation,hundreds of millions ; Hinton ; Devlin et al. ; NLP ; Transformer ; Yang et al. ; P θ ; two ; Sun ; L ← −,"Pre-trained language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training and fine-tuning compact models. However, this simple baseline has been overlooked by the NLP community due to the limited capacity of compact models, which can be capitalized better when focusing on the end task rather than a general language objective. To accelerate future research, we will make our 24 pre-trained miniature BERT models publicly available. This is a training method that is well-performing, general and simple and can leverage additional resources such as unlabeled task data.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"In standard generative adversarial network (SGAN), the discriminator estimates the probability that the input data is real. The generator is trained to increase the probability that fake data is real. We argue that it should also simultaneously decrease the probability that real data is real because 1) this would account for a priori knowledge that half of the data in the mini-batch is fake, 2) this would be observed with divergence minimization, and 3) in optimal settings, SGAN would be equivalent to integral probability metric (IPM) GANs. 

 We show that this property can be induced by using a relativistic discriminator which estimate the probability that the given real data is more realistic than a randomly sampled fake data. We also present a variant in which the discriminator estimate the probability that the given real data is more realistic than fake data, on average. We generalize both approaches to non-standard GAN loss functions and we refer to them respectively as Relativistic GANs (RGANs) and Relativistic average GANs (RaGANs). We show that IPM-based GANs are a subset of RGANs which use the identity function. 

 Empirically, we observe that 1) RGANs and RaGANs are significantly more stable and generate higher quality data samples than their non-relativistic counterparts, 2) Standard RaGAN with gradient penalty generate data of better quality than WGAN-GP while only requiring a single discriminator update per generator update (reducing the time taken for reaching the state-of-the-art by 400%), and 3) RaGANs are able to generate plausible high resolutions images (256x256) from a very small sample (N=2011), while GAN and LSGAN cannot; these images are of significantly better quality than the ones generated by WGAN-GP and SGAN with spectral normalization.

 The code is freely available on https://github.com/AlexiaJM/RelativisticGAN. Generative adversarial networks (GANs) BID7 form a broad class of generative models in which a game is played between two competing neural networks, the discriminator D and the generator G. D is trained to discriminate real from fake data, while G is trained to generate fake data that D will mistakenly recognize as real. In the original GAN by BID4 , which we refer to as Standard GAN (SGAN), D is a classifier, thus it is predicting the probability that the input data is real. When D is optimal, the loss function of SGAN is approximately equal to the Jensen-Shannon divergence (JSD) BID4 . SGAN has two variants for the generator loss functions: saturating and non-saturating. In practice, the former has been found to be very unstable, while the latter has been found to more stable BID4 . Under certain conditions, proved that, if real and fake data are perfectly classified, the saturating loss has zero gradient and the non-saturating loss has non-zero, but volatile gradient. In practice, this means that the discriminator in SGAN often cannot be trained to optimality or with a too high learning rate; otherwise, gradients may vanish and, if so, training will stop. This problem is generally more noticeable in high-dimensional setting (e.g., high resolution images and discriminator architectures with high expressive power) given that there are enough degrees of freedom available to perfectly classify the training set.To improve on SGAN, many GAN variants have been suggested using different loss functions and discriminators that are not classifiers (e.g., LSGAN BID14 , WGAN ). Although these approaches have partially succeeded in improving stability and data quality, the large-scale study by BID13 suggests that these approaches do not consistently improve on SGAN. Additionally, some of the most successful approaches, such as WGAN-GP BID5 , are much more computationally demanding than SGAN.Many of the recent successful GANs variants have been based on Integral probability metrics (IPMs) BID18 ) (e.g., WGAN , WGAN-GP, Fisher GAN , Sobolev GAN ). In IPM-based GANs, the discriminator is real-valued and constrained to a specific class of function which regularize the discriminator. See for a review of the different IPMs.These IPM constraints have been shown to be beneficial even in non-IPM based GANs. Spectral normalization BID15 improves the stability of various GANs and it consists in making the discriminator Lipschitz-1, which is the constraint of WGAN. Similarly, the gradient penalty of WGAN-GP also provides improve the stability of SGAN BID3 . Although this shows that certain IPM constraints improve the stability of GANs, it does not explain why IPM-based GANs generally provide increased stability over other metrics/divergences in GANs (e.g., JSD for SGAN, f -divergences for f -GANs BID19 ).Note that although powerful, IPM-based GANs tend to more computationally demanding than other GANs. Certain IPM-based GANs use a gradient penalty (e.g. WGAN-GP, Sobolev GAN) which is very computationally costly and most IPM-based GANs need more than one discriminator update per generator update (WGAN-GP requires at least 5 BID5 ). Assuming equal training time for D and G, every additional discriminator update increase training time by a significant 50%.In this paper , we argue that non-IPM-based GANs are missing a key ingredient, a relativistic discriminator, which IPM-based GANs already possess. We show that a relativistic discriminator is necessary to make GANs analogous to divergence minimization and produce sensible predictions based on the a priori knowledge that half of the samples in the mini-batch are fake. We provide empirical evidence showing that GANs with a relativistic discriminator are more stable and produce data of higher quality. In this paper, we proposed the relativistic discriminator as a way to fix and improve on standard GAN. We further generalized this approach to any GAN loss and introduced a generally more stable variant called RaD. Our results suggest that relativism significantly improve data quality and stability of GANs at no computational cost. Furthermore, using a relativistic discriminator with other tools of the trade (spectral norm, gradient penalty, etc.) may lead to better state-of-the-art.Future research is needed to fully understand the mathematical implications of adding relativism to GANs. Furthermore, our experiments were limited to certain loss functions using only one seed, due to computational constraints. More experiments are required to determine which relativistic GAN loss function is best over a wide-range of datasets and hyper-parameters. We greatly encourage researchers and machine learning enthusiasts with greater computing power to experiment further with our approach. Table 3 : An illustrative example of the discriminator's output in standard GAN as traditionally defined (P (x r is real) = sigmoid(C(x r ))) versus the Relativistic average Discriminator (RaD) (P (x r is real|C(x f )) = sigmoid(C(x r ) − C(x f ))). Breads represent real images, while dogs represent fake images. Scenario Absolute probability Relative probability (Standard GAN) (Relativistic average Standard GAN)Real image looks real and fake images look fake DISPLAYFORM0 Real image looks real but fake images look similarly real on average DISPLAYFORM1 Real image looks fake but fake images look more fake on average DISPLAYFORM2 P (x r is bread|C(x f )) = .88",Improving the quality and stability of GANs using a relativistic discriminator; IPM GANs (such as WGAN-GP) are a special case.,more than one ; Sobolev GAN ; LSGAN ; Fisher GAN ; between two ; G. D ; SGAN.Many ; WGAN ; real|C(x ; Jensen,"other tools ; the a priori knowledge ; a relativistic discriminator ; the mathematical implications ; gradient penalty ; significantly better quality ; other GANs ; function ; powerful, IPM-based GANs ; both approaches",more than one ; Sobolev GAN ; LSGAN ; Fisher GAN ; between two ; G. D ; SGAN.Many ; WGAN ; real|C(x ; Jensen,"In standard generative adversarial network (SGAN), the discriminator estimates the probability that the input data is real, while the generator is trained to increase the probability of real data. In optimal settings, SGAN would be equivalent to integral probability metric (IPM) GANs. We show that this property can be induced by relativistic discriminator estimation of the likelihood that the given real data is more realistic than randomly sampled fake data. We also show that IPM-based GAN's are a subset of RGANs which use the identity function.  Empirically, we observe that RGAN and RaGAN",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Recent advances in learning from demonstrations (LfD) with deep neural networks have enabled learning complex robot skills that involve high dimensional perception such as raw image inputs. 
 LfD algorithms generally assume learning from single task demonstrations. In practice, however, it is more efficient for a teacher to demonstrate a multitude of tasks without careful task set up, labeling, and engineering. Unfortunately in such cases, traditional imitation learning techniques fail to represent the multi-modal nature of the data, and often result in sub-optimal behavior. In this paper we present an LfD approach for learning multiple modes of behavior from visual data. Our approach is based on a stochastic deep neural network (SNN), which represents the underlying intention in the demonstration as a stochastic activation in the network. We present an efficient algorithm for training SNNs, and for learning with vision inputs, we also propose an architecture that associates the intention with a stochastic attention module.
 We demonstrate our method on real robot visual object reaching tasks, and show that
 it can reliably learn the multiple behavior modes in the demonstration data. Video results are available at https://vimeo.com/240212286/fd401241b9. A key problem in robotic control is to simplify the problem of programming a complex behavior. Traditional control engineering approaches, which rely on accurate manual modeling of the system environment, are very challenging to apply in modern robotic applications where most sensory inputs come from images and other high-dimensional signals such as tactile feedback.In contrast, imitation learning, or learning from demonstration (LfD) approaches BID31 aim to directly learn a control policy from mentor or expert demonstrations. The key advantages of LfD are simplicity and data-efficiency, and indeed, LfD has been successfully used for learning complex robot skills such as locomotion BID32 , driving BID27 BID30 , flying BID0 , and manipulation BID21 BID4 BID25 . Recently, advances in deep representation learning BID12 have facilitated LfD methods with high dimensional perception, such as mapping raw images directly to controls BID9 . These advances are capable of learning generalizable skills BID18 , and offer a promising approach for modern industrial challenges such as pick and place tasks BID5 .One challenge in LfD, however, is learning different modes of the same task. For example, consider learning to pick up an object from a pile. The demonstrator can choose to pick up a different object each time, yet we expect LfD to understand that these are similar demonstrations of the same pick-up skill, only with a different intention in mind. Moreover , we want the learned robot behavior to display a similar multi-modal 1 nature.Standard approaches for LfD with image inputs, such as learning with deep neural networks (NNs) BID27 BID9 BID18 , are not suitable for learning multimodal behaviors. In their essence, NNs learn a deterministic mapping from observation to control, which cannot represent the inherently multi-modal latent intention in the demonstrations. In practice , this manifests as an 'averaging' of the different modes in the data BID3 , leading to an undesirable policy.A straightforward approach for tackling the multi-modal problem in LfD is to add a label for each mode in the data. Thus, in the pick-up task above, the demonstrator would also explicitly specify the object she intends to pick-up beforehand. Such an approach has several practical shortcomings: it requires the demonstrator to record more data, and requires the possible intentions to be specified in advance, making it difficult to use the same recorded data for different tasks. More importantly , such a solution is conceptually flawed -it solves an algorithmic challenge by placing additional burden on the client.In this work, we propose an approach for LfD with multi-modal demonstrations that does not require any additional data labels. Our method is based on a stochastic neural network model, which represents the latent intention as a random activation in the network. We propose a novel and efficient learning algorithm for training stochastic networks, and present a network architecture suitable for LfD with raw image inputs, where the intention takes the form of a stochastic attention over features in the image.We show that our method can reliably reproduce behavior with multiple intentions in real-robot object reaching tasks. Moreover, in scenarios where multiple intentions exist in the demonstration data, the stochastic neural networks perform better than their deterministic counterparts. We presented an approach for learning from demonstrations that contain multiple modes of performing the same task. Our method is based on stochastic neural networks, and represents the mode Figure 2 : Comparison of IDS and SNN algorithms. We plot three different errors during training (on the training data), for the same model trained using IDS and SNN algorithm. Left: the respective training loss for each method. Since the max in IDS upper bounds the softmax in SNN, the loss plot for IDS lower bounds SNN. Middle: the IDS loss on the training data, for both models. Since the SNN is trained on a different loss function (softmax), its performance is worse. This shows an important point: if, at test time, we use optimistic sampling to sample z from best samples during training, we should expect IDS to perform better than SNN. Right: the average log-likelihood loss during training. The SNN wins here, since the softmax encourages to increase the likelihood of 'incorrect' z values. This provides additional motivation for using optimistic sampling.of performing the task by a stochastic vector -the intention, which is given as input to a feedforward neural network. We presented a simple and efficient algorithm for training our models, and a particular implementation suitable for vision-based inputs. As we demonstrated in real-robot experiments, our method can reliably learn to reproduce the different modes in the demonstration data, and outperforms standard approaches in cases where such different modes exist.In future work we intend to investigate the extension of this approach to more complex manipulation tasks such as grasping and assembly, and domains with a very large number of objects in the scene. An interesting point in our model is tying the features to the intention by an attention mechanism, and we intend to further investigate recurrent attention mechanisms (Xu et al., 2015) that could offer better generalization at inference time. F (Q, θ) =E Q log P (u 1:T , z|x 1:T ; θ) Q(z|x 1:T , u 1:T ) ≤E P (·|x 1:T ,u 1:T ;θ) [log P (y 1:T |x 1:T ,θ)] DISPLAYFORM0 where F is the Kullback Liebler divergence between P (u 1:T |z, x 1:T ; θ) and Q(z|u 1:T , x 1:T ) given as follows:F (Q, θ) = −D KL (Q||P (·|x 1:T , u 1:T ; θ)) + log P (y 1:T |x 1:T ,θ). Most importantly, it has also been shown in Theorem 2 of BID23 that if Q and θ form a pair of local maximizer to F , then θ is also a local maximum of the original likelihood maximization problem. To maximize F w.r.t Q, one has the closed form solution based on Bayes theorem: Q * (z|u 1:T , x 1:T ; θ old ) =P (z|y 1:T , x 1:T , θ) DISPLAYFORM1 Here, {z 1 , . . . , z N } is a sequence of latent random variables sampled i.i.d. from the distribution P (z).Given parameter θ, denoted by θ old , immediately the posterior distribution Q that maximizes F is given by: Q * (z|x 1:T , u 1:T ) = P (z|x 1:T , u 1:T ; θ old ). In this case, the above loss function is equivalent to the complete data log-likelihood * (θ, θ old ) := E P (·|u 1:T ,x 1:T ;θold) log P (x 1:T , z|u 1:T ; θ) P (z|x 1:T , u 1:T ; θ old ) , which is a lower bound of the log likelihood. Furthermore , if θ = θ old , then clearly * (θ old , θ old ) is equal to the log-likelihood log P (y 1:T |x 1:T ,θ old ).Tang & Salakhutdinov (2013) present a generalized EM algorithm to train a SNN. In the E-step, the following approximate posterior distribution is used:Q(z|u 1:T , x 1:T ; θ old ) :=r(z; x 1:T ,y 1:T , θ old )P (z), wherer (z; x 1:T ,y 1:T , θ old ) = r(z; x 1:T , u 1: DISPLAYFORM2 r(z i ; x 1:T , u 1:T , θ old ) is the the importance sampling weight.Recall that for our distribution model, r(z; x 1:T , u 1:T , θ old ) ∝ exp(−d(f (x, z; θ), u)), therefore we obtain that the importance weights correspond to a soft-max over the prediction error.In the M-step, the θ parameters are updated with the gradient vector with respect to the following optimization: θ ∈ arg max θ∈Θˆ (θ, θ old ), wherê DISPLAYFORM3 (z i ; x 1:T ,y 1:T , θ old ) log P (y 1:T , z i |x 1:T , θ)is the empirical expected log likelihood, andQ is the posterior distribution from the E-step. Here we drop the last term in F because in our case Q that does not depend on θ. Correspondingly, the gradient estimate is given by: DISPLAYFORM4 (z i )∇ θ log r(z i ; x 1:T , u 1:T , θ), the equality is due to the facts that log P (y 1:T , z|x 1:T , θ) = log r(z; x 1:T ,y 1:T , θ) + log P (z) and distribution P (z) is independent of θ.To better understand this estimator, we will analyze the bias and variance of the gradient estimator. Based on the construction of importance sampling weight, immediately the gradient estimator is consistent. Furthermore, under certain regular assumptions, the bias is O(N −1/2 ). (This means the gradient estimator is asymptotically unbiased.) Furthermore, the variance of this estimator is given by DISPLAYFORM5 where the integrand is given by v(z; θ) =r(z; x 1:T ,y 1:T , θ old ) · (∇ θ log r(z; x 1:T , u 1:T , θ)) 2 ≥ 0.",multi-modal imitation learning from unstructured demonstrations using stochastic neural network modeling intention.,Kullback Liebler ; r(z ; IDS ; P ; max θ∈Θˆ ; v(z ; F ; log r(z ; EM ; KL,immediately the posterior distribution Q ; this work ; The key advantages ; the θ parameters ; the construction ; respect ; the demonstrations ; training ; modern industrial challenges ; a generalized EM algorithm,Kullback Liebler ; r(z ; IDS ; P ; max θ∈Θˆ ; v(z ; F ; log r(z ; EM ; KL,"LfD (learning from demonstrations) with deep neural networks has enabled learning complex robot skills that involve high dimensional perception such as raw image inputs. However, traditional imitation learning techniques fail to represent the multi-modal nature of the data, and often result in sub-optimal behavior. In this paper, we present an LfD approach for learning multiple modes of behavior from visual data, based on stochastic deep neural network (SNN), which represents the underlying intention in the demonstration, and is associated with vision inputs. In order to simplify the problem of programming a complex behavior, we propose an architecture that associates the",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"The recent rise in popularity of few-shot learning algorithms has enabled models to quickly adapt to new tasks based on only a few training samples. Previous few-shot learning works have mainly focused on classification and reinforcement learning. 
 In this paper, we propose a few-shot meta-learning system that focuses exclusively on regression tasks. Our model is based on the idea that the degree of freedom of the unknown function can be significantly reduced if it is represented as a linear combination of a set of appropriate basis functions. This enables a few labelled samples to approximate the function. We design a Feature Extractor network to encode basis functions for a task distribution, and a  Weights Generator to generate the weight vector for a novel task. We show that our model outperforms the current state of the art meta-learning methods in various regression tasks. Regression deals with the problem of learning a model relating a set of inputs to a set of outputs. The learned model can be thought as function y = F (x) that will give a prediction y ∈ R dy given input x ∈ R dx where d y and d x are dimensions of the output and input respectively. Typically, a regression model is trained on a large number of data points to be able to provide accurate predictions of new inputs. Recently, there have been a surge in popularity on few-shot learning methods (Vinyals et al., 2016; BID7 BID4 . Few-shot learning methods require only a few examples from each task to be able to quickly adapt and perform well on a new task. The fewshot learning model in essence is learning to learn i.e. the model learns to quickly adapt itself to new tasks rather than just learning to give the correct prediction for a particular input sample.In this work, we propose a few shot learning model that targets few-shot regression tasks. We evaluate our model on the sinusoidal regression tasks and compare our model's performance to several meta-learning algorithms. We further introduce two more regression tasks, namely the 1D heat equation task modeled by partial differential equations and the 2D Gaussian distribution task. We propose a few-shot meta learning system that focuses exclusively on regression tasks. Our model is based on the idea of linear representation of basis functions. We design a Feature extractor network to encode the basis functions for the entire task distribution. We design a Weight generator network to generate the weights from the K training samples of a novel task drawn from the same task distribution. We show that our model has competitive performance in in various few short regression tasks. A TECHNICAL DETAILS",We propose a few-shot learning model that is tailored specifically for regression tasks,Feature Extractor ; Weights Generator ; F ; two ; Gaussian ; Feature ; Weight,the basis functions ; our model's performance ; the idea ; the weights ; This ; a  Weights Generator ; a particular input sample ; each task ; the correct prediction ; freedom,Feature Extractor ; Weights Generator ; F ; two ; Gaussian ; Feature ; Weight,"The recent rise in popularity of few-shot learning algorithms has enabled models to quickly adapt to new tasks based on only a few training samples. This approach outperforms the current state of the art meta-learning methods in regression tasks. Our model is based on the idea that the degree of freedom of unknown function can be significantly reduced if it is represented as a linear combination of a set of appropriate basis functions. We design a Feature Extractor network to encode basis functions for the entire task distribution, and a Weight generator network to generate weights from K training samples drawn from the same task distribution. We show that our model outperforms current state",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent. Recently, the Vision-and-Language (VLN) navigation task BID3 , which requires the agent to follow natural language instructions to navigate through a photo-realistic unknown environment, has received significant attention BID46 BID19 ). In the VLN task, an agent is placed in an unknown realistic environment and is required to follow natural language instructions to navigate from its starting location to a target location. In contrast to some existing navigation tasks BID24 BID53 BID33 , we address the class of tasks where the agent does not have an explicit representation of the target (e.g., location in a map or image representation of the goal) to know if the goal has been reached or not BID31 BID22 BID18 BID6 . Instead, the agent needs to be aware of its navigation status through the association between the sequence of observed visual inputs to instructions.Consider an example as shown in FIG0 , given the instruction ""Exit the bedroom and go towards the table. Go to the stairs on the left of the couch. Wait on the third step."", the agent first needs to locate which instruction is needed for the next movement, which in turn requires the agent to be aware of (i.e., to explicitly represent or have an attentional focus on) which instructions were completed or ongoing in the previous steps. For instance, the action ""Go to the stairs"" should be carried out once the agent has exited the room and moved towards the table. However, there exists inherent ambiguity for ""go towards the table"". Intuitively, the agent is expected to ""Go to the stairs"" after completing ""go towards the table"". But, it is not clear what defines the completeness of ""Go towards the table"". The completeness of an ongoing action often depends on the availability of the next action. Since the transition between past and next part of the instructions is a soft boundary, in order to determine when to transit and to follow the instruction correctly the agent is required to keep track of both grounded instructions. On the other hand, assessing the progress made towards the goal has indeed been shown to be important for goal-directed tasks in humans decision-making BID8 BID12 BID9 . While a number of approaches have been proposed for VLN BID3 BID46 BID19 , previous approaches generally are not aware of which instruction is next nor progress towards the goal; indeed, we qualitatively show that even the attentional mechanism of the baseline does not successfully track this information through time.In this paper, we propose an agent endowed with the following abilities: (1) identify which direction to go by finding the part of the instruction that corresponds to the observed images-visual grounding, (2) identify which part of the instruction has been completed or ongoing and which part is potentially needed for the next action selection-textual grounding, and (3) ensure that the grounded instruction can correctly be used to estimate the progress made towards the goal, and apply regularization to ensure this -progress monitoring. Therefore, we introduce the self-monitoring agent consisting of two complementary modules: visual-textual co-grounding and progress monitor.More specifically, we achieve both visual and textual grounding simultaneously by incorporating the full history of grounded instruction, observed images, and selected actions into the agent. We leverage the structural bias between the words in instructions used for action selection and progress made towards the goal and propose a new objective function for the agent to measure how well it can estimate the completeness of instruction-following. We then demonstrate that by conditioning on the positions and weights of grounded instruction as input, the agent can be self-monitoring of its progress and further ensure that the textual grounding accurately reflects the progress made.Overall, we propose a novel self-monitoring agent for VLN and make the following contributions: (1) We introduce the visual-textual co-grounding module, which performs grounding interdependently across both visual and textual modalities. We show that it can outperform the baseline method by a large margin. (2) We propose to equip the self-monitoring agent with a progress monitor, and for navigation tasks involving instructions instantiate this by introducing a new objective function for training. We demonstrate that, unlike the baseline method, the position of grounded instruction can follow both past and future instructions, thereby tracking progress to the goal. (3) With the proposed self-monitoring agent, we set the new state-of-the-art performance on both seen and unseen environments on the standard benchmark. With 8% absolute improvement in success rate on the unseen test set, we are ranked #1 on the challenge leaderboard. We introduce a self-monitoring agent which consists of two complementary modules: visual-textual co-grounding module and progress monitor. The visual-textual co-grounding module locates the instruction completed in the past, the instruction needed in the next action, and the moving direction from surrounding images. The progress monitor regularizes and ensures the grounded instruction correctly reflects the progress towards the goal by explicitly estimating the completeness of instruction-following. This estimation is conditioned on the positions and weights of grounded instruction. Our approach sets a new state-of-the-art performance on the standard Room-to-Room dataset on both seen and unseen environments. While we present one instantiation of self-monitoring for a decision-making agent, we believe that this concept can be applied to other domains as well. BID46 , and Speaker-Follower BID19 . *: with data augmentation. TAB4 . We can see that our proposed method outperformed existing approaches with a large margin on both validation unseen and test sets. Our method with greedy decoding for action selection improved the SR by 9% and 8% on validation unseen and test set. When using progress inference for action selection, the performance on the test set significantly improved by 5% compared to using greedy decoding, yielding 13% improvement over the best existing approach. Network architecture. The embedding dimension for encoding the navigation instruction is 256. We use a dropout layer with ratio 0.5 after the embedding layer. We then encode the instruction using a regular LSTM, and the hidden state is 512 dimensional. The MLP g used for projecting the raw image feature is BN − → F C − → BN − → Dropout − → ReLU . The FC layer projects the 2176-d input vector to a 1024-d vector, and the dropout ratio is set to be 0.5. The hidden state of the LSTM used for carrying the textual and visual information through time in Eq. 1 is 512. We set the maximum length of instruction to be 80, thus the dimension of the attention weights of textual grounding α t is also 80. The dimension of the learnable matrices from Eq. 2 to 5 are: DISPLAYFORM0 DISPLAYFORM1 closest previous trajectory, so that when a single agent traverses through all recorded trajectories, the overhead for switching from one trajectory to another can be reduced significantly. The final selected trajectory from beam search is then lastly logged to the trajectory. This therefore yields exactly the same success rate and navigation error, as the metrics are computed according to the last viewpoint from a trajectory.",We propose a self-monitoring agent for the Vision-and-Language Navigation task.,VLN ; two ; Code ; third ; first ; SR ; FC ; Eq,both validation ; the standard benchmark ; natural language instructions ; validation ; the association ; the next movement ; a large margin ; a new objective function ; the baseline method ; its starting location,VLN ; two ; Code ; third ; first ; SR ; FC ; Eq,"The Vision-and-Language Navigation (VLN) task involves an agent following navigational instruction in photo-realistic unknown environments. This challenging task requires the agent to be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and progress monitor to ensure the grounded instruction correctly reflects the navigation progress.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Activation is a nonlinearity function that plays a predominant role in the convergence and performance of deep neural networks. While Rectified Linear Unit (ReLU) is the most successful activation function, its derivatives have shown superior performance on benchmark datasets. In this work, we explore the polynomials as activation functions (order ≥ 2) that can approximate continuous real valued function within a given interval. Leveraging this property, the main idea is to learn the nonlinearity, accepting that the ensuing function may not be monotonic. While having the ability to learn more suitable nonlinearity, we cannot ignore the fact that it is a challenge to achieve stable performance due to exploding gradients - which is prominent with the increase in order. To handle this issue, we introduce dynamic input scaling, output scaling, and lower learning rate for the polynomial weights. Moreover, lower learning rate will control the abrupt fluctuations of the polynomials between weight updates. In experiments on three public datasets, our proposed method matches the performance of prior activation functions, thus providing insight into a network’s nonlinearity preference. Deep learning methods have achieved excellent results in visual understanding, visual recognition, speech, and natural language processing tasks (Krizhevsky et al. (2012) , Lee et al. (2014) , Goodfellow et al. (2014) , Hochreiter & Schmidhuber (1997) , Oord et al. (2016) , Vaswani et al. (2017) ). The convolutional neural networks (CNNs) first introduced in LeCun et al. (1999) , is the foundation for numerous vision tasks. While recurrent neural networks, wavenet and the recent transformers with attention mechanism are the core algorithms used in speech and natural language processing. The commonality is the importance of deeper architectures that has both theoretical and empirical evidence (Serre et al. (2007) , Simonyan & Zisserman (2015) , Lee et al. (2014) ). One essential component for deep neural networks is the activation function that enables nonlinearity. While ReLUs are the most used nonlinearity, sigmoid and hyperbolic tangent are the traditional functions. Several derivatives of ReLU are presented in recent years that further improve the performance and minimize vanishing gradients issue (Maas et al. (2013) , He et al. (2015a) , Clevert et al. (2015) , Ramachandran et al. (2019) ). While most are fixed functions, the negative slope for Leaky ReLUs can be adjusted during the network design, and remains constant while training. Parametric ReLU adaptively changes the negative slope during training using a trainable parameter and demonstrate a significant boost in performance (He et al. (2015a) ). A relatively new activation function, Swish, is derived by an automated search techniques (Ramachandran et al. (2019) ). While the parameter β enables learning, the performance difference reported in the study between parametric and non-parametric versions is minimal. To this end, rather than using a fixed or heavily constrained nonlinearity, we believe that the nonlinearity learned by the deep networks can provide more insight on how they can be designed. In this work, we focus on the use of polynomials as nonlinearity functions. We demonstrate the stability of polynomial of orders 2 to 9 by introducing scaling functions and initialization scheme that approximates well known activation functions. Experiments on three public datasets show that our method competes with state-of-the-art activation functions on a variety of deep architectures. Despite their imperfections, our method allows each layer to find their preferred nonlinearity during training. Finally, we show the learned nonlinearities that are both monotonic and non-monotonic. We proposed a polynomial activation function that learns the nonlinearity using trainable coefficients. Our contribution is stabilizing the networks with polynomial activation as a nonlinearity by introducing scaling, initilization technique and applying a lower learning rate for the polynomial weights, which provides more insight about the nonlinearity prefered by networks. The resulting nonlinearities are both monotonic and non-monotonic in nature. In our MNIST experiments, we showed the stability of our method with orders 2 to 9 and achieved superior perfromance when compared to ReLUs, LReLUs, PReLUs, ELUs, GELUs, SELUs and Swish. In our CIFAR experiments, the performance by replacing ReLUs with polynomial activations using DenseNet, Residual Networks and Wide Residual Networks is on par with eight state-of-the-art activation functions. While the increase of parameters is negligible, our method is computationally expensive. We believe that by designing networks with simpler activations like ReLU for the initial layers, followed by layers with polynomial activations can further improve accuracies.",We propose polynomial as activation functions.,LReLUs ; Parametric ReLU ; Goodfellow et al ; Residual Networks ; Oord ; Vaswani ; Leaky ReLUs ; Ramachandran ; al. ; Wide Residual Networks,superior perfromance ; their preferred nonlinearity ; the polynomials ; par ; the increase ; the performance ; orders ; the parameter ; polynomial ; this property,LReLUs ; Parametric ReLU ; Goodfellow et al ; Residual Networks ; Oord ; Vaswani ; Leaky ReLUs ; Ramachandran ; al. ; Wide Residual Networks,"Activation is a nonlinearity function that plays a predominant role in the convergence and performance of deep neural networks. The Rectified Linear Unit (ReLU) is the most successful activation function, its derivatives have shown superior performance on benchmark datasets. In experiments on three public datasets, we introduce dynamic input scaling, output scaling, and lower learning rate for polynomial weights. The main idea is to learn the nonlinearities, accepting that the ensuing function may not be monotonic. However, it is a challenge to achieve stable performance due to exploding gradients due to vanishing gradients. To handle this issue, dynamic input",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline. Pretrained word embeddings BID30 BID32 are a staple tool for NLP. These models provide continuous representations for word types, typically learned from cooccurrence statistics on unlabeled data, and improve generalization of downstream models across many domains. Recently, a number of models have been proposed for contextualized word embeddings. Instead of using a single, fixed vector per word type, these models run a pretrained encoder network over the sentence to produce contextual embeddings of each token. The encoder, usually an LSTM BID18 or a Transformer BID47 , can be trained on objectives like machine translation BID29 or language modeling BID33 BID19 , for which large amounts of data are available. The activations of this network-a collection of one vector per token-fit the same interface as conventional word embeddings, and can be used as a drop-in replacement input to any model. Applied to popular models, this technique has yielded significant improvements to the state-of-the-art on several tasks, including constituency parsing BID22 , semantic role labeling BID44 , and coreference , and has outperformed competing techniques BID8 ) that produce fixed-length representations for entire sentences.Our goal in this work is to understand where these contextual representations improve over conventional word embeddings. Recent work has explored many token-level properties of these representations, such as their ability to capture part-of-speech tags BID4 BID3 BID42 , morphology BID2 b) , or word-sense disambiguation BID33 . BID34 extends this to constituent phrases, and present a heuristic for unsuper- Figure 1 : Probing model architecture ( § 3.1). All parameters inside the dashed line are fixed, while we train the span pooling and MLP classifiers to extract information from the contextual vectors. The example shown is for semantic role labeling, where s(1) = [1, 2) corresponds to the predicate (""eat""), while s (2) = [2, 5) is the argument (""strawberry ice cream""), and we predict label A1 as positive and others as negative. For entity and constituent labeling, only a single span is used.vised pronominal coreference. We expand on this even further and introduce a suite of edge probing tasks covering a broad range of syntactic, semantic, local, and long-range phenomena. In particular, we focus on asking what information is encoded at each position, and how well it encodes structural information about that word's role in the sentence. Is this information primarily syntactic in nature, or do the representations also encode higher-level semantic relationships? Is this information local, or do the encoders also capture long-range structure?We approach these questions with a probing model (Figure 1 ) that sees only the contextual embeddings from a fixed, pretrained encoder. The model can access only embeddings within given spans, such as a predicate-argument pair, and must predict properties, such as semantic roles, which typically require whole-sentence context. We use data derived from traditional structured NLP tasks: tagging, parsing, semantic roles, and coreference. Common corpora such as OntoNotes BID49 provide a wealth of annotations for well-studied concepts which are both linguistically motivated and known to be useful intermediates for high-level language understanding. We refer to our technique as ""edge probing"", as we decompose each structured task into a set of graph edges ( § 2) which we can predict independently using a common classifier architecture ( § 3.1) 2 . We probe four popular contextual representation models ( § 3.2): CoVe BID29 , ELMo BID33 , OpenAI GPT , and BERT .We focus on these models because their pretrained weights and code are available, since these are most likely to be used by researchers. We compare to word-level baselines to separate the contribution of context from lexical priors, and experiment with augmented baselines to better understand the role of pretraining and the ability of encoders to capture long-range dependencies. We introduce a suite of ""edge probing"" tasks designed to probe the sub-sentential structure of contextualized word embeddings. These tasks are derived from core NLP tasks and encompass a range of syntactic and semantic phenomena. We use these tasks to explore how contextual embeddings improve on their lexical (context-independent) baselines. We focus on four recent models for contextualized word embeddings-CoVe, ELMo, OpenAI GPT, and BERT.Based on our analysis, we find evidence suggesting the following trends. First, in general, contextualized embeddings improve over their non-contextualized counterparts largely on syntactic tasks (e.g. constituent labeling) in comparison to semantic tasks (e.g. coreference), suggesting that these embeddings encode syntax more so than higher-level semantics. Second, the performance of ELMo cannot be fully explained by a model with access to local context, suggesting that the contextualized representations do encode distant linguistic information, which can help disambiguate longer-range dependency relations and higher-level syntactic structures.We release our data processing and model code, and hope that this can be a useful tool to facilitate understanding of, and improvements in, contextualized word embedding models.","We probe for sentence structure in ELMo and related contextual embedding models. We find existing models efficiently encode syntax and show evidence of long-range dependencies, but only offer small improvements on semantic tasks.",BERT.Based ; OpenAI GPT ; BERT ; four ; Devlin et al. ; NLP ; OntoNotes ; corpora ; one ; al.,experiment ; high-level language understanding ; These models ; encoders ; The encoder ; conventional word embeddings ; properties ; nature ; a predicate-argument pair ; generalization,BERT.Based ; OpenAI GPT ; BERT ; four ; Devlin et al. ; NLP ; OntoNotes ; corpora ; one ; al.,"Contextualized representation models such as ELMo and BERT (Devlin et al., 2018a) have achieved state-of-the-art results on a diverse array of downstream NLP tasks. Based on recent token-level probing work, we introduce a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. These models encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline. Pretrained word embeddings BID30 BID32 are a staple tool",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Generating and scheduling activities is particularly challenging
 when considering both consumptive resources and
 complex resource interactions such as time-dependent resource
 usage.We present three methods of determining valid
 temporal placement intervals for an activity in a temporally
 grounded plan in the presence of such constraints. We introduce
 the Max Duration and Probe algorithms which are
 sound, but incomplete, and the Linear algorithm which is
 sound and complete for linear rate resource consumption.
 We apply these techniques to the problem of scheduling
 awakes for a planetary rover where the awake durations
 are affected by existing activities. We demonstrate how the
 Probe algorithm performs competitively with the Linear algorithm
 given an advantageous problem space and well-defined
 heuristics. We show that the Probe and Linear algorithms
 outperform the Max Duration algorithm empirically.
 We then empirically present the runtime differences between
 the three algorithms. The Probe algorithm is currently base-lined
 for use in the onboard scheduler for NASA’s next planetary
 rover, the Mars 2020 rover. In many space missions, consumptive resources such as energy or data volume limit the number of activities that can be scheduled. These consumptive resources are oftentimes replenished periodically or gradually over time. For example, data is downlinked-replenishing data capacity-or energy is generated by solar panels or radioisotope thermoelectric generator (RTG) power supplies. The scheduler must therefore schedule activities while staying aware of resource replenishment in order to ensure that the resource state does not violate constraints (e.g. energy below a specified level or data buffers overflow). We focus on awake and asleep scheduling for a planetary rover, but our techniques generalize scheduling in the presence of complex consumptive resource activities.We focus on the onboard scheduler for NASA's next planetary rover, the Mars 2020 (M2020) rover (Jet Propulsion Laboratory 2018a) . Since the heart of our paper is awake and asleep scheduling, we concentrate on energy as the limit- ing consumptive resource. The M2020 rover's power source is a Multi-Mission Radioisotope Thermoelectric Generator (MMRTG) (Jet Propulsion Laboratory 2018b). The MM-RTG constantly generates energy for the rover's battery, but the CPU's awake and ""idle"" state (i.e. no other tasks) consumes more energy than the MMRTG provides. Therefore, the rover can only increase its energy, measured as battery state of charge (SOC), when the rover is asleep. The rover, however, must stay awake to not only execute activities, but also (re)-invoke the scheduler to generate a schedule. The M2020 onboard scheduler is responsible for generating and scheduling these awake periods.In order to generate and schedule awakes, the scheduler must compute valid start times for awakes and activities jointly to ensure that there is sufficient energy for both the awake and the activities. Each activity, however, requires varying awake sizes depending on existing awake periods and the activity's scheduled start time. If the activity is close to an existing awake, it may be necessary to extend an existing awake rather than generating a new awake as this would require the rover to shutdown and wakeup in quick succession ( Figure 1 ) which may lead to issues if the shutdown runs longer than nominally expected. Due to its varying duration, an awake's energy consumption and valid start times are challenging to determine.The remainder of the paper is organized as follows. First, we describe the timeline representation, which is also used by the M2020 onboard scheduler. We discuss calculating valid start time intervals-intervals in which starting the activity would not violate any constraints-and define the problem in relation to the timeline framework. Second, we discuss a general case-by-case approach to handling automatically generated awakes and the challenges specific cases pose. Third, we present three specific approaches to handling these challenges when generating and scheduling awakes: a) an over-conservative approach that always uses the maximum awake period potentially required by the activity when calculating valid intervals; b) a ""probing"" approach that only considers a single point in time rather than the entire interval; and c) a linear algebra approach that calculates exact valid intervals given the linear rate of energy replenishment and consumption. The ""probing"" approach is currently base-lined for the M2020 onboard scheduler. Fourth, we present empirical analysis to compare their de- Figure 1: When scheduling activity B, the scheduler should extend the existing awake rather than creating a new one to account for the possibility that the shutdown runs longer than nominally expected. W is a wakeup and S is a shutdown.grees of completeness and runtime performance. Lastly, we reference related works, describe future works, and discuss conclusions. Generating and scheduling activities in the presence of consumptive regenerative resources is especially challenging when a driving factor of feasibility of placement is dependent on interactions with the existing schedule. Scheduling activities and their awake periods is particularly challenging in the context of M2020 because the awake's duration is dependent on existing awakes. We presented three algorithms-Max Duration, Probe, and Linear-for scheduling awakes and analyzed their completeness and runtime. Despite being a locally sound and complete algorithm, the Linear algorithm was not always able to outperform in the global problem space. We demonstrated how a simple and incomplete algorithm can perform both suboptimally, as seen with the Max Duration algorithm, and also close to optimal, as seen with the Probe algorithm, dependent on the heuristic and input parameters. We showed that the Probe algorithm is a fair alternative to a more complete algorithm, especially considering its ease of implementation and runtime improvement.",This paper describes and analyzes three methods to schedule non-fixed duration activities in the presence of consumptive resources.,Probe ; RTG ; SOC ; MM ; NASA ; MMRTG ; CPU ; Fourth ; Linear ; W,"three algorithms-Max Duration ; completeness and runtime performance ; the runtime differences ; algorithm ; downlinked-replenishing data capacity ; a specified level or data buffers ; energy replenishment ; automatically generated awakes ; the linear rate ; NASA’s next planetary
 rover",Probe ; RTG ; SOC ; MM ; NASA ; MMRTG ; CPU ; Fourth ; Linear ; W,"Generating and scheduling activities is challenging when considering both consumptive resources and time-dependent resource interactions. The Max Duration and Linear algorithms perform competitively in the presence of complex consumptive resource activities. The Probe algorithm performs competitively with the Linear algorithm given an advantageous problem space and well-defined heuristics. The Linear algorithm performs better in the context of complex resource interactions such as awake and asleep scheduling, and the Probe algorithm outperforms the Max Duration algorithm empirically.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Global feature pooling is a modern variant of feature pooling providing better interpretatability and regularization. Although alternative pooling methods exist (eg. max, lp norm, stochastic), the averaging operation is still the dominating global pooling scheme in popular models. As fine-grained recognition requires learning subtle, discriminative features, we consider the question: is average pooling the optimal strategy? We first ask: ``is there a difference between features learned by global average and max pooling?'' Visualization and quantitative analysis show that max pooling encourages learning features of different spatial scales. We then ask ``is there a single global feature pooling variant that's most suitable for fine-grained recognition?'' A thorough evaluation of nine representative pooling algorithms finds that: max pooling outperforms average pooling consistently across models, datasets, and image resolutions; it does so by reducing the generalization gap; and generalized pooling's performance increases almost monotonically as it changes from average to max. We finally ask: ``what's the best way to combine two heterogeneous pooling schemes?'' Common strategies struggle because of potential gradient conflict but the ``freeze-and-train'' trick works best. We also find that post-global batch normalization helps with faster convergence and improves model performance consistently. Deeply rooted in the works of complex cells in the visual cortex (Hubel & Wiesel, 1962) and locally orderless images (Koenderink & Van Doorn, 1999) , feature pooling has been an indispensable component of visual recognition in both traditional bag-of-words (BOW) frameworks (Csurka et al., 2004; Lazebnik et al., 2006) using hand-crafted features (e.g. SIFT (Lowe, 2004) , HOG (Dalal & Triggs, 2005) ), and modern convolutional neural networks (CNNs) (LeCun et al., 1998; Krizhevsky et al., 2012) . A recent variant of this technique, called ""global feature pooling"" (Lin et al., 2013) , distinguishes itself by defining its pooling kernel the same size as input feature map. The pooling output is a scalar value indicating the existence of certain features (or patterns). Benefits of global pooling are two-fold: allowing better interpretation of the underlying filters as feature detectors, and serving as a strong network regularizer to reduce overfitting. Global pooling is thus used in most, if not all, recent state-of-the-art deep models He et al., 2016; Szegedy et al., 2017; Huang et al., 2017; Hu et al., 2018) in visual recognition. Unless otherwise noted, all the pooling methods discussed in this paper are used as the global pooling layer. Feature pooling is also of special interests to Fine-grained Visual Categorization (FGVC) (Rosch et al., 1976; Nilsback & Zisserman, 2010; Farrell et al., 2011) , where objects are classified into subcategories rather than basic categories. Carefully designed pooling schemes can learn helpful discriminative features and yield better performance without requiring more conv-layers in the network. Wang et al. (2018) provided a good example that combines three pooling operations: average, max and cross-channel pooling to learn to capture class-specific discriminative patches. Another major research direction is higher-order pooling: Lin et al. (2015) proposed to apply bilinear pooling (also know as second-order pooling) to capture pairwise correlations between the feature channels and model part-feature interactions; Gao et al. (2016) proposed compact bilinear pooling that applies random maclaurin projection and tensor sketch projection to approximate the outer product operation, greatly reducing parameters without sacrificing accuracy; Works along this line of research include low-rank bilinear pooling (Kim et al., 2016) , grassmann pooling (Wei et al., 2018) , kernel pooling (Cui et al., 2017) , and Alpha-pooling Simon et al. (2017) , etc. Although higher-order pooling methods output a vector rather than a scalar, they're still relevant as they reside in the same location as the global pooling layer. The most common pooling operations are average, max and striding. Striding always takes the activation at a fixed location, thus is never applied as global pooling. An abundant set of pooling flavors exist for both traditional and modern feature extractors. Stochastic pooling randomly chooses an activation according to a multinomial distribution decided by activation strength in the pooling region. Fractional max pooling (Graham, 2014) can be adapted to fractional sized pooling regions. Spatial pyramid pooling (He et al., 2015) outputs the combination of multiple max pooling with different sized pooling kernels. S3Pool (Zhai et al., 2017) , or stochastic spatial sampling Pooling, randomly picks a sub-region to apply max pooling to. Detail-preserving pooling (Saeedan et al., 2018) computes the output as the linear combination of input feature pixels whose weight is proportional to differences of the input intensities. Translation invariant pooling (Zhang, 2019) borrowed the idea of anti-alias by low-pass filtering from signal processing. A major pooling family, generalized pooling, aims to find a smooth transition between average and max pooling: k-max pooling (Kalchbrenner et al., 2014) outputs the average of the k highest activations of the feature map; l p norm pooling generalizes pooling to the p-norm of the input feature map (Boureau et al., 2010) ; soft pooling (Boureau et al., 2010) , or softmax pooling, outputs the sum of feature map weighted by softmax output; mixed pooling (Lee et al., 2016 ) computes a weighted sum of the max and average pooling; gated pooling (Lee et al., 2016 ) is similar to mixed pooling but the weight is learned instead. To the best of our knowledge, these pooling operations remain largely unexplored in the global pooling scenario. An interesting observation is that all highly-ranked classification models ""happen"" to choose the same averaging operation in their global pooling layer. Is this an arbitrary choice or actually the optimal strategy? How does average pooling compare against the other pooling schemes (e.g. max) in general image classification and also fine-grained visual recognition? Research (Boureau et al., 2010; Murray & Perronnin, 2014; Scherer et al., 2010; Hu et al., 2018; has shown that the selection of feature pooling affects the algorithm's performance, whether using hand-crafted features or deep features. Specially, Murray & Perronnin (2014) showed max pooling has superior performance in the traditional recognition framework because of its better pattern discriminability, and the same conclusion was made by an experimental evaluation of Scherer et al. (2010) using LeNet-5 (LeCun et al., 1998) on the Caltech 101 (Fei-Fei et al., 2007) and NORB (Jarrett et al., 2009 ) dataset. Boureau et al. (2010) provided a theoretical proof that ""max pooling is particularly well suited to the separation of features that are very sparse."" However, in squeeze and excitation networks (Hu et al., 2018) , global max pooling is reported to achieve 0.29% higher top-1 error and 0.05% higher top-5 error than average pooling. Similar results were reported by using VGG (Simonyan & Zisserman, 2015) and GoogleNet (Szegedy et al., 2016) . It seems max pooling is less preferred as a global pooling scheme than before. These intriguing contrasts call for a careful examination of both pooling schemes. Our investigation begins with the two most common global average and max pooling. Specially, we're interested to know what features have both pooling methods helped learned. Feature map visualization indicates that max pooling produces sparser final conv-layer feature maps. This is further verified quantitatively by two perceptually-consistent sparsity metrics: discrete entropy and thresholded l 0 norm. Visualization of final conv-layer filters further helps us conclude empirically that: global average pooling encourages object-level features while global max pooling focuses more on part-level features. As class-specific features often reside in localized object parts in finegrained datasets, it's equal to say global max pooling find more discriminative features, well aligned with previous findings (Murray & Perronnin, 2014; . The second question to answer is that ""is there a single optimal pooling operation on different finegrained datasets across different models?"" We evaluate nine representative pooling schemes, which are: average, max, k-max, l p norm, soft, logavgexp, mixed, gated, and stochastic pooling, in the experiment section. We make several observations: max pooling outperforms average pooling across datasets, input resolutions, and models. The reason behind this phenomenon, besides their feature differences, is relevant to the fact that max pooling generalizes better. Most pooling methods we evaluated performs better than average pooling, with k-max (k = 2) and mixed pooling (α = 0.5) being the top two. Our k-max pooling model, when trained properly, beats all previous higher-order pooling methods using the same backbone. The fact that no single pooling works best for all models leads to the need for learnable pooling, where the pooling function is not chosen by heuristic, but optimized via gradient descent. However, our finding that model performance decrease and generalization gap increases in an almost monotonic way when generalized pooling changes from max to average casts a shadow upon the learnable generalized pooling. A pooling is better not because it minimizes training loss, but because it better regularizes the model. Throughout our experiment, post-global batch normalization is applied as another key ingredient achieving consistent performance improvement and faster convergence. Finally, we explore the integration of heterogeneous pooling. Since different features can be learned by average or max pooling, our assumption is that learning a model with heterogeneous poolings will lead to better performance, but what's the best way to integrate them? We review and evaluate three common strategies, but found their improvement upon single pooling is limied. Our hypothesis is that different pooling methods interfere and cancel each other out when learned together. We instead propose to apply the ""freeze-and-train"" trick. The intuition is that the frozen branch won't degrade during training and the gradients will be well separated. The resulting architecture only adds a tiny amount of parameters to a backbone network, but consistently outperforms single pooling models. In this paper, we focus on the global pooling layer in popular classification models as applied to the task of fine-grained recognition. By visualizing the final conv-layer filters and feature maps, we discover that max pooling produces much sparser feature maps and helps the network learn part-level features. Average pooling, on the other hand, encourages object-level features to be learned. We evaluated nine representative global pooling schemes for fine-grained recognition. K-max (k = 2) pooling outperformed all other global pooling schemes and is actually better than all higher-order pooling models. We made several observations from pooling benchmark experiments: (1) max pooling performs better than average pooling across datasets, models, and input resolution; (2) max pooling generalizes better than average pooling; and (3) model performance displays an approximately monotonically increasing characteristic when generalized pooling changes from average to max. Based on these observations, we discussed the potential risk of learning a generalized pooling: namely that minimizing training loss may lead to average pooling and thus be prone to overfitting. We highlight the importance of post-global batch normalization -which is absent from most, if not all, popular state-of-the-art models -in helping to attain faster convergence and in consistently improving model performance. We evaluated several strategies for heterogeneous pooling integration. The freeze-and-train trick performs best among all end-to-end learnable models. For future work, we suggest consideration of models learned from scratch alongside those fine-tuned from pretrained weights. In addition, experiments should be explored on a broader set of data, not just on fine-grained datasets, in order to affirm whether the findings presented here generalize to more general-purpose datasets such as ImageNet and/or MS-COCO.",A benchmark of nine representative global pooling schemes reveals some interesting findings.,Caltech ; Boureau et al. ; Krizhevsky et al. ; Fei-Fei ; Szegedy et al. ; Visual Categorization ; Saeedan ; Graham ; Detail ; Lin et al.,an almost monotonic way ; more discriminative features ; generalized pooling's performance ; the integration ; the same size ; FGVC ; Szegedy ; this ; Similar results ; the works,Caltech ; Boureau et al. ; Krizhevsky et al. ; Fei-Fei ; Szegedy et al. ; Visual Categorization ; Saeedan ; Graham ; Detail ; Lin et al.,"In popular models, the averaging operation is the dominant global pooling operation. However, in fine-grained recognition, we consider the question: is average pooling the optimal strategy? Visualization and quantitative analysis show that max pooling encourages learning features of different spatial scales. We then ask: 'is there a single global feature pooling variant that's most suitable for fine-Grained recognition?'' A thorough evaluation of nine representative pooling algorithms finds that, in general, it outperforms average poolsing consistently across models, datasets, and image resolutions, reducing the generalization gap, and generalized pooling performs almost monoton",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"A disentangled representation of a data set should be capable of recovering the underlying factors that generated it. One question that arises is whether using Euclidean space for latent variable models can produce a disentangled representation when the underlying generating factors have a certain geometrical structure. Take for example the images of a car seen from different angles. The angle has a periodic structure but a 1-dimensional representation would fail to capture this topology. How can we address this problem? The submissions presented for the first stage of the  NeurIPS2019 Disentanglement Challenge consist of a Diffusion Variational Autoencoder ($\Delta$VAE) with a hyperspherical latent space which can for example recover periodic true factors. The training of the $\Delta$VAE is enhanced by incorporating a modified version of the Evidence Lower Bound (ELBO) for tailoring the encoding capacity of the posterior approximate. Variational Autoencoders (VAEs) proposed by BID4 are an unsupervised learning method that can estimate the underlying generative model that produced a data set in terms of the so-called latent variables. In the context of VAEs, a disentangled representation is obtained when the latent variables represent the true independent underlying factors, which usually have a semantic meaning, that generated the data set.VAEs assume that a data set X = {x i } N i=1 consists of N independent and identically distributed data points belonging to a set X. A set Z of unobserved latent variables is proposed and the main goal is to maximize the log-likelihood via variational inference using an approximate to the posterior distribution Q X|z with parameters a, b calculated by neural networks. A prior distribution P Z is selected before training such that the training of the VAE is carried out by maximizing for each data point the Evidence Lower Bound (ELBO) w.r.t. the neural network weights that calculate a, b given by DISPLAYFORM0 To accomplish the disentanglement of latent variables BID3 proposed to weight the contribution of both terms in the ELBO by using a parameter β ∈ R + to change the capacity of encoding of the posterior distribution. The idea of changing the capacity of the encoding distribution was further explored in BID0 where the KullbackLeibler divergence term is pushed towards a certain value C ∈ R + in each training step. The combination of both approaches led to a to the following training objective to be maximized, DISPLAYFORM1 The value of β is fixed before training and C is increased linearly each epoch of training from a minimum value C min to C max . We refer to this procedure as capacity annealing.In some cases the underlying factors that generated a data set have a certain geometrical/topological structure that cannot be captured with the traditional Euclidean latent variables as has been mentioned in and in . This problem is referred to as manifold mismatch.For the NeurIPS2019 Disentanglement challenge, datasets for local evaluation are provided based on the paper by BID5 . It is important to note that in such datasets there is at least one underlying factor that has a periodic structure. Take for example the Cars3D dataset consisting of images of cars. In particular, one factor of variation is the azimuthal angle of rotation of the car. The geometrical structure of this factor is circular and thus it is better represented with a periodical latent variable.The Diffusion Variational Autoencoders ∆VAE presented by Pérez Rey et al. FORMULA0 provide a versatile method that can be used to implement arbitrary closed manifolds for a latent space, in particular, hyperspheres.",Description of submission to NeurIPS2019 Disentanglement Challenge based on hyperspherical variational autoencoders,at least one ; max ; first ; Q X|z ; the Evidence Lower Bound (ELBO ; N ; KullbackLeibler ; VAE ; P Z ; Euclidean,"a, b ; the underlying generating factors ; this topology ; unobserved latent variables ; $\Delta$VAE ; local evaluation ; at least one underlying factor ; ∈ R + ; the azimuthal angle ; The idea",at least one ; max ; first ; Q X|z ; the Evidence Lower Bound (ELBO ; N ; KullbackLeibler ; VAE ; P Z ; Euclidean,"In the context of latent variable models, a disentangled representation is obtained when the underlying generating factors have a certain geometrical structure and a 1-dimensional representation would fail to capture this topology. Diffusion Variational Autoencoder ($\Delta$VAE) is an unsupervised learning method that can estimate the underlying generative model that produced a data set in terms of latent variables. A prior distribution P Z is selected before training such that the training of the VAE is carried out by maximizing for each data point the Evidence Lower Bound (ELBO) w.r.t. the neural network weights",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Due to the success of residual networks (resnets) and related architectures, shortcut connections have quickly become standard tools for building convolutional neural networks. The explanations in the literature for the apparent effectiveness of shortcuts are varied and often contradictory. We hypothesize that shortcuts work primarily because they act as linear counterparts to nonlinear layers. We test this hypothesis by using several variations on the standard residual block, with different types of linear connections, to build small (100k--1.2M parameter) image classification networks. Our experiments show that other kinds of linear connections can be even more effective than the identity shortcuts. Our results also suggest that the best type of linear connection for a given application may depend on both network width and depth. Deep convolutional neural networks have become the dominant force for many image classification tasks; see BID10 ; BID17 ; BID19 . Their ability to assimilate low-, medium-, and high-level features in an end-to-end multi-layer fashion has led to myriad groundbreaking advances in the field. In recent years, residual networks (resnets) have emerged as one of the best performing neural network archetypes in the literature; see BID6 . Through the use of identity shortcut connections, resnets have overcome the challenging technical obstacles of vanishing gradients and the apparent degradation that otherwise comes with training very deep networks. Resnets have achieved state-of-the-art performance on several image classification datasets using very deep neural networks, sometimes with over 1000 layers.Although shortcut connections appeared in the early neural network literature, e.g., BID1 ; BID14 ; BID16 , their importance became more clear in 2015 with the emergence of the HighwayNets of BID18 and resnets. The former involved gated shortcut connections that regulate the flow of information across the network, while the latter used identity shortcut connections, which are parameterless. Resnets are also presumed to be easier to train and seem to perform better in practice. In their first resnet paper, He et al. argued that identity maps let gradients flow back, enabling the training of very deep networks, and that it's easier for a layer to learn when initialized near an identity map than near a zero map (with small random weights); see also .However , in a flurry of recent activity, most notably from BID25 ; BID4 ; BID22 ; BID13 and BID23 , arguments have emerged that the effectiveness of resnets is not due to their depth, where practitioners were training networks of hundreds or thousands of layers, but rather that deep resnets are effectively creating ensembles of shallower networks, and the layers are more likely to refine and reinforce existing features than engineer new ones. These arguments assert that the achievement of resnets is less about extreme depth and more about their ability to ease backpropagation with moderate depth. Indeed , in many cases wider residual networks that were only 10-50 layers deep were shown to perform better and train in less time than very deep ones (over 100 layers). See BID25 .More recently still, others have presented many clever and creative ways to train very deep networks using variations on the shortcut theme; see for example BID8 ; BID11 ; BID26 ; ; BID0 ; BID2 ; BID27 ; BID6 ; BID12 ; BID24 ; Savarese (2016); BID19 , and BID21 . In summary, shortcut connections clearly help in practice, but there are many different, and sometimes conflicting hypotheses as to why.In this paper we investigate a new hypothesis about shortcut connections, namely, that their power lies not in the identity mapping itself, but rather just in combining linear and nonlinear functions at each layer. The tests where identity shortcuts were observed to perform better than general linear connections were all done in very deep (100 or more layers) networks. The recent evidence that wider, shallower, resnet networks can outperform deeper ones suggests that it is worth investigating whether identity connections are better than general linear connections in such networks.We first describe some of the intuition about why this might be the case. We then investigate this idea with careful experiments using relatively small networks constructed of five different types of blocks. These blocks are all variations on the idea of residual blocks (resblocks), but where the identity shortcut is replaced with a more general linear function. We call these blocks, consisting of both a linear and a nonlinear part, tandem blocks and the resulting networks tandem networks. Residual networks and several similar architectures are special cases of tandem networks.The networks we use in our experiments are relatively small (100k-1.2M parameter) image classification networks constructed from these various tandem blocks. The small networks are appropriate because the goal of the experiments is not to challenge state-of-the-art results produced by much larger models, but rather to compare the five architectures in a variety of settings in order to gain insight into their relative strengths and weaknesses. Whereas many other authors pursue extreme network depth as a goal in itself, here we limit our focus to comparing performance (in this case, classification accuracy) of different architectures.Our experiments suggest that general linear layers, which have learnable parameters, perform at least as well as the identity shortcut of resnets. This is true even when some width is sacrificed to keep the total number of parameters the same. Our results further suggest that the best specific type of linear connection to use in the blocks of a tandem network depends on several factors, including both network width and depth. We generalized residual blocks (which use identity shortcut connections) to tandem blocks (which can learn any linear connection, not just the identity). We found that general linear connections with learnable weights, have the same benefits as the identity maps in residual blocks, and they actually increase performance compared to identity maps. We also showed that linear connections do not learn identity maps, even when initialized with identity weight matrices. These results seem to confirm that the success of residual networks and related architectures is not due to special properties of identity maps, but rather is simply a result of using linear maps to complement nonlinear ones.The additional flexibility gained by replacing identity maps with convolutions led to better results in every one of our experiments. This was not due to extra parameters, as we adjusted layer widths to keep parameter counts as close to equal as possible. Instead, general linear convolutions appear to do a better job than identity maps of working together with nonlinear convolutions.Our results further suggest that tandem blocks with a single nonlinear convolution tend to outperform those with two, but blocks that use 3 × 3 convolutions for their linear connections may be better in wide networks than those with 1 × 1s.Finally, we note that there are many more possible types of tandem block than those we have considered here, and many more applications in which to test them.","We generalize residual blocks to tandem blocks, which use arbitrary linear maps instead of shortcuts, and improve performance over ResNets.",linear ; recent years ; first ; al. ; zero ; hundreds or thousands ; Savarese ; five ; two,weaknesses ; several similar architectures ; low- ; performance ; the early neural network literature ; He ; These blocks ; The recent evidence ; blocks ; the best performing neural network archetypes,linear ; recent years ; first ; al. ; zero ; hundreds or thousands ; Savarese ; five ; two,"The success of residual networks (resnets) and related architectures, shortcut connections have quickly become standard tools for building convolutional neural networks. The explanations for the apparent effectiveness of shortcuts are varied and contradictory. However, we test several variations on the standard residual block, with different types of linear connections, to build small (100k--1.2M parameter) image classification networks. These results suggest that the best type of linear connection for a given application may depend on both network width and depth. Deep Convolutional Neural networks have become the dominant force for many image classification tasks; see BID10 ; BID17 ;",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Pointwise localization allows more precise localization and accurate interpretability, compared to bounding box, in applications where objects are highly unstructured such as in medical domain. In this work, we focus on  weakly supervised localization (WSL) where a model is trained to classify an image and localize regions of interest at pixel-level using only global image annotation. Typical convolutional attentions maps are prune to high false positive regions. To alleviate this issue, we propose a new deep learning method for WSL, composed of a localizer and a classifier, where the localizer is constrained to determine relevant and irrelevant regions using conditional entropy (CE) with the aim to reduce false positive regions. Experimental results on a public medical dataset and two natural datasets, using Dice index, show that, compared to state of the art WSL methods, our proposal can provide significant improvements in terms of image-level classification and pixel-level localization (low false positive) with robustness to overfitting. A public reproducible PyTorch implementation is provided. Pointwise localization is an important task for image understanding, as it provides crucial clues to challenging visual recognition problems, such as semantic segmentation, besides being an essential and precise visual interpretability tool. Deep learning methods, and particularly convolutional neural networks (CNNs), are driving recent progress in these tasks. Nevertheless, despite their remarkable performance, their training requires large amounts of labeled data, which is time consuming and prone to observer variability. To overcome this limitation, weakly supervised learning (WSL) has emerged recently as a surrogate for extensive annotations of training data (Zhou, 2017) . WSL involves scenarios where training is performed with inexact or uncertain supervision. In the context of pointwise localization or semantic segmentation, weak supervision typically comes in the form of image level tags (Kervadec et al., 2019; Kim et al., 2017; Pathak et al., 2015; Teh et al., 2016; Wei et al., 2017) , scribbles (Lin et al., 2016; Tang et al., 2018) or bounding boxes (Khoreva et al., 2017) . Current state-of-the-art WSL methods rely heavily on pixelwise activation maps produced by a CNN classifier at the image level, thereby localizing regions of interest (Zhou et al., 2016) . Furthermore, this can be used as an interpretation of the model's decision (Zhang & Zhu, 2018) . The recent literature abounds of WSL works that relax the need of dense and prohibitively time consuming pixel-level annotations (Rony et al., 2019) . Bottom-up methods rely on the input signal to locate regions of interest, including spatial pooling techniques over activation maps (Durand et al., 2017; Oquab et al., 2015; Sun et al., 2016; Zhang et al., 2018b; Zhou et al., 2016) , multi-instance learning (Ilse et al., 2018) and attend-and-erase based methods (Kim et al., 2017; Li et al., 2018; Pathak et al., 2015; Singh & Lee, 2017; Wei et al., 2017) . While these methods provide pointwise localization, the models in (Bilen & Vedaldi, 2016; Kantorov et al., 2016; Shen et al., 2018; Tang et al., 2017; Wan et al., 2018 ) predict a bounding box instead, i.e., perform weakly supervised object detection. Inspired by human visual attention, top-down methods rely on the input signal and a selective backward signal to determine the corresponding region of interest. This includes special feedback layers (Cao et al., 2015) , backpropagation error (Zhang et al., 2018a) and Grad-CAM (Chattopadhyay et al., 2018; Selvaraju et al., 2017) . In many applications, such as in medical imaging, region localization may require high precision such as cells, boundaries, and organs localization; regions that have an unstructured shape, and different scale that a bounding box may not be able to localize precisely. In such cases, a pointwise localization can be more suitable. The illustrative example in Fig.1 (bottom row) shows a typical case where using a bounding box to localize the glands is clearly problematic. This motivates us to consider predicting a mask instead of a bounding box. Consequently, our latter choice of evaluation datasets is constrained by the availability of both global image annotation for training and pixel-level annotation for evaluation. In this work, we focus on the case where there is one object of interest in the image. Often, within an agnostic-class setup, input image contains the object of interest among other irrelevant parts (noise, background). Most the aforementioned WSL methods do not consider such prior, and feed the entire image to the model. In such scenario, (Wan et al., 2018) argue that there is an inconsistency between the classification loss and the task of WSL; and that typically the optimization may reach sub-optimal solutions with considerable randomness in them, leading to high false positive localization. False positive localization is aggravated when a class appears in different and random shape/structure, or may have relatively similar texture/color to the irrelevant parts driving the model to confuse between both parts. False positive regions can be problematic in critical domains such as medical applications where interpretability plays a central role in trusting and understanding an algorithm's prediction. To address this important issue, and motivated by the importance of using prior knowledge in learning to alleviate overfitting when training using few samples (Belharbi et al., 2017; Krupka & Tishby, 2007; Mitchell, 1980; Yu et al., 2007) , we propose to use the aforementioned prior in order to favorite models with low false positive localization. To this end, we constrain the model to learn to localize both relevant and irrelevant regions simultaneously in an end-to-end manner within a WSL scenario, where only image-level labels are used for training. We model the relevant (discriminative) regions as the complement of the irrelevant (non-discriminative) regions (Fig.1) . Our model is composed of two sub-models: (1) a localizer that aims to localize both types of regions by predicting a latent mask, (2) and a classifier that aims to classify the visible content of the input image through the latent mask. The localizer is driven through CE (Cover & Thomas, 2006) to simultaneously identify (1) relevant regions where the classifier has high confidence with respect to the image label, (2) and irrelevant regions where the classifier is being unable to decide which image label to assign. This modeling allows the discriminative regions to pop out and be used to assign the corresponding image label, while suppressing non-discriminative areas, leading to more reliable predictions. In order to localize complete discriminative regions, we extend our proposal by training the localizer to recursively erase discriminative parts during training only. To this end, we propose a consistent recursive erasing algorithm that we incorporate within the backpropagation. At each recursion, and within the backpropagation, the algorithm localizes the most discriminative region; stores it; then erases it from the input image. At the end of the final recursion, the model has gathered a large extent of the object of interest that is fed next to the classifier. Thus, our model is driven to localize complete relevant regions while discarding irrelevant regions, resulting in more reliable region localization. Moreover, since the discriminative parts are allowed to be extended over different instances, our proposal handles multi-instances intrinsically. The main contribution of this paper is a new deep learning framework for WSL at pixel level. The framework is composed of two sequential sub-networks where the first one localizes regions of interest, whereas the second classifies them. Based on CE, the end-to-end training of the framework allows to incorporate prior knowledge that, an image is more likely to contain relevant and irrelevant regions. Throughout the CE measured at the classifier level, the localizer is driven to localize relevant regions (with low CE) and irrelevant regions (with high CE). Such localization is achieved with the main goal of providing a more interpretable and reliable regions of interest with low false positive localization. This paper also contributes a consistent recursive erasing algorithm that is incorporated within backpropagation, along with a practical implementation in order to obtain complete discriminative regions. Finally, we conduct an extensive series of experiments on three public image datasets (medical and natural), where the results show the effectiveness of the proposed approach in terms of pointwise localization (measured with Dice index) while maintaining competitive accuracy for image-level classification. In this work, we present a novel approach for WSL at pixel-level where we impose learning relevant and irrelevant regions within the model with the aim to reduce false positive localization. Evaluated on three datasets, and compared to state of the art WSL methods, our approach shows its effectiveness in accurately localizing regions of interest with low false positive while maintaining a competitive classification error. This makes our approach more reliable in term of interpetability. As future work, we consider extending our approach to handle multiple classes within the image. Different constraints can be applied over the predicted mask, such as texture properties, shape, or other region constraints. Predicting bounding boxes instead of heat maps is considered as well since they can be more suitable in some applications where pixel-level accuracy is not required. Our recursive erasing algorithm can be further improved by using a memory-like mechanism that provides spatial information to prevent forgetting the previously spotted regions and promote localizing the entire region (Sec.B.3).",A deep learning method for weakly-supervised pointwise localization that learns using image-level label only. It relies on conditional entropy to localize relevant and irrelevant regions aiming to minimize false positive regions.,Grad-CAM ; Zhou et al. ; Teh ; Mitchell ; first ; Yu et al. ; Tang ; Oquab et al. ; al. ; Krupka & Tishby,activation maps ; different instances ; it ; heat maps ; a practical implementation ; the entire region ; Tang et al ; more reliable region localization ; Cover ; their remarkable performance,Grad-CAM ; Zhou et al. ; Teh ; Mitchell ; first ; Yu et al. ; Tang ; Oquab et al. ; al. ; Krupka & Tishby,"Weakly supervised localization (WSL) is an important task for image understanding, where a model is trained to classify an image and localize regions of interest at pixel-level using only global image annotation. However, typical convolutional attentions maps are prune to high false positive regions. A public reproducible PyTorch implementation is provided. The current state-of-the-art WSL methods rely heavily on pixelwise activation maps produced by a CNN classifier at the image level, leading to significant improvements in image-level classification and pixel localization (low false positive) with robustness to overfitting. The recent",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"In this paper, we investigate mapping the hyponymy relation of
 wordnet to feature vectors.
   We aim to model lexical knowledge in such a way that it can be used as
  input in generic machine-learning models, such as phrase entailment
  predictors.
   We propose two models. The first one leverages an existing mapping of
  words to feature vectors (fasttext), and attempts to classify
  such vectors as within or outside of each class. The second model is fully supervised,
  using solely wordnet as a ground truth. It maps each concept to an
  interval or a disjunction thereof.
   On the first model, we approach, but not quite attain state of the
  art performance. The second model can achieve near-perfect accuracy.
 Distributional encoding of word meanings from the large corpora BID8 BID12 have been found to be useful for a number of NLP tasks. These approaches are based on a probabilistic language model by BID2 of word sequences, where each word w is represented as a feature vector f (w) (a compact representation of a word, as a vector of floating point values).This means that one learns word representations (vectors) and probabilities of word sequences at the same time.While the major goal of distributional approaches is to identify distributional patterns of words and word sequences, they have even found use in tasks that require modeling more fine-grained relations between words than co-occurrence in word sequences. Folklore has it that simple manipulations of distributional word embedding vectors is inadequate for problems involving detection of other kinds of relations between words rather than their co-occurrences. In particular , distributional word embeddings are not easy to map onto ontological relations and vice-versa. We consider in this paper the hyponymy relation, also called the is-a relation, which is one of the most fundamental ontological relations.Possible sources of ground truth for hyponymy are WORDNET Fellbaum (1998), FRAMENET Baker et al. (1998) , and JEUXDEMOTS 1 BID6 . These resources have been designed to include various kinds of lexical relations between words, phrases, etc. However these resources have a fundamentally symbolic representation, which can not be readily used as input to neural NLP models. Several authors have proposed to encode hyponymy relations in feature vectors BID14 BID13 BID0 BID10 . However, there does not seem to be a common consensus on the underlying properties of such encodings. In this paper, we aim to fill this gap and clearly characterize the properties that such an embedding should have. We additionally propose two baseline models approaching these properties: a simple mapping of FASTTEXT embeddings to the WORDNET hyponymy relation, and a (fully supervised) encoding of this relation in feature vectors. We found that defining the problem of representing HYPONYMY in a feature vector is not easy. Difficulties include 1. the sparseness of data, 2. whether one wants to base inclusion on an underlying (possibly relaxed) inclusion in the space of vectors, and 3. determining what one should generalize.Our investigation of WORDNET over fastText demonstrates that WORDNET classes are not cleanly linearly separated in fastText, but they are sufficiently well separated to give a useful recall for an approximate inclusion property. Despite this, and because the negative cases vastly outnumber the positive cases, the rate of false negatives is still too high to give any reasonable precision. One could try to use more complex models, but the sparsity of the data would make such models extremely sensitive to overfitting.Our second model takes a wholly different approach: we construct intervals directly from the HY-PONYMY relation. The main advantage of this method is its simplicity and high-accuracy. Even with a single dimension it rivals other models. A possible disadvantage is that the multi-dimensional version of this model requires disjunctions to be performed. Such operations are not necessarily available in models which need to make use of the HYPONYMY relation. At this stage, we make no attempt to match the size of intervals to the probability of a word. We aim to address this issue in future work.Finally, one could see our study as a criticism of WORDNET as a natural representative of HY-PONYMY. Because it is almost structured like a tree, one can suspect that it in fact misses many hyponymy relations. This would also explain why our simple fastText-based model predicts more relations than present in WORDNET. One could think of using other resources, such as JEUXDE-MOTS. Our preliminary investigations suggest that these seem to suffer from similar flaws -we leave complete analysis to further work.",We investigate mapping the hyponymy relation of wordnet to feature vectors,HY-PONYMY ; HYPONYMY ; one ; Folklore ; second ; NLP ; WORDNET ; FRAMENET Baker ; WORDNET Fellbaum ; first,more relations ; Such operations ; a number ; similar flaws ; distributional approaches ; embedding vectors ; The main advantage ; this ; the HY-PONYMY relation ; each word w,HY-PONYMY ; HYPONYMY ; one ; Folklore ; second ; NLP ; WORDNET ; FRAMENET Baker ; WORDNET Fellbaum ; first,"In this paper, we explore the hyponymy relation of wordnet to feature vectors (fasttext) and how it can be used as an input in generic machine-learning models, such as phrase entailmentment and predictors. The first model is fully supervised, using solely wordnet as a ground truth. It maps each concept to an interval or a disjunction thereof. The second model achieves near-perfect accuracy. Distributional encoding of word meanings from the large corpora BID8 BID12 have been found to be useful for a number of NLP tasks.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Are neural networks biased toward simple functions?
 Does depth always help learn more complex features?
 Is training the last layer of a network as good as training all layers?
 These questions seem unrelated at face value, but in this work we give all of them a common treatment from the spectral perspective.
 We will study the spectra of the *Conjugate Kernel, CK,* (also called the *Neural Network-Gaussian Process Kernel*), and the *Neural Tangent Kernel, NTK*.
 Roughly, the CK and the NTK tell us respectively ``""what a network looks like at initialization"" and ""``what a network looks like during and after training.""
 Their spectra then encode valuable information about the initial distribution and the training and generalization properties of neural networks.
 By analyzing the eigenvalues, we lend novel insights into the questions put forth at the beginning, and we verify these insights by extensive experiments of neural networks.
 We believe the computational tools we develop here for analyzing the spectra of CK and NTK serve as a solid foundation for future studies of deep neural networks.
 We have open-sourced the code for it and for generating the plots in this paper at github.com/jxVmnLgedVwv6mNcGCBy/NNspectra. Understanding the behavior of neural networks and why they generalize has been a central pursuit of the theoretical deep learning community. Recently, Valle-Pérez et al. (2018) observed that neural networks have a certain ""simplicity bias"" and proposed this as a solution to the generalization question. One of the ways with which they argued that this bias exists is the following experiment: they drew a large sample of boolean functions by randomly initializing neural networks and thresholding the output. They observed that there is a bias toward some ""simple"" functions which get sampled disproportionately more often. However, their experiments were only done for relu networks. Can one expect this ""simplicity bias"" to hold universally, for any architecture? A priori, this seems difficult, as the nonlinear nature seems to present an obstacle in reasoning about the distribution of random networks. However, this question turns out to be more easily treated if we allow the width to go to infinity. A long line of works starting with Neal (1995) and extended recently by ; ; Yang (2019) have shown that randomly initialized, infinite-width networks are distributed as Gaussian processes. These Gaussian processes also describe finite width random networks well (Valle-Pérez et al., 2018) . We will refer to the corresponding kernels as the Conjugate Kernels (CK), following the terminology of Daniely et al. (2016) . Given the CK K, the simplicity bias of a wide neural network can be read off quickly from the spectrum of K: If the largest eigenvalue of K accounts for most of tr K, then a typical random network looks like a function from the top eigenspace of K. In this paper, we will use this spectral perspective to probe not only the simplicity bias, but more generally, questions regarding how hyperparameters affect the generalization of neural networks. Via the usual connection between Gaussian processes and linear models with features, the CK can be thought of as the kernel matrix associated to training only the last layer of a wide randomly initialized network. It is a remarkable recent advance (Jacot et al., 2018; Allen-Zhu et al., 2018a; c; Du et al., 2018) that, under a certain regime, a wide neural network of any depth evolves like a linear model even when training all parameters. The associated kernel is call the Neural Tangent Kernel, which is typically different from CK. While its theory was initially derived in the infinite width setting, Lee et al. (2019) confirmed with extensive experiment that this limit is predictive of finite width neural networks as well. Thus, just as the CK reveals information about what a network looks like at Next, we examine how hyperparameters affect the performance of neural networks through the lens of NTK and its spectrum. To do so, we first need to understand the simpler question of how a kernel affects the accuracy of the function learned by kernel regression. A coarse-grained theory, concerned with big-O asymptotics, exists from classical kernel literature (Yao et al., 2007; Raskutti et al., 2013; Lin and Rosasco; Schölkopf and Smola, 2002) . However, the fine-grained details, required for discerning the effect of hyperparameters, have been much less studied. We make a first attempt at a heuristic, fractional variance (i.e. what fraction of the trace of the kernel does an eigenspace contribute), for understanding how a minute change in kernel effects a change in performance. Intuitively, if an eigenspace has very large fractional variance, so that it accounts for most of the trace, then a ground truth function from this eigenspace should be very easy to learn. Using this heuristic, we make two predictions about neural networks, motivated by observations in the spectra of NTK and CK, and verify them with extensive experiments. • Deeper networks learn more complex features, but excess depth can be detrimental as well. Spectrally, depth can increase fractional variance of an eigenspace, but past an optimal depth, it will also decrease it. (Section 5) Thus, deeper is not always better. • Training all layers is better than training just the last layer when it comes to more complex features, but the opposite is true for simpler features. Spectrally, fractional variances of more ""complex"" eigenspaces for the NTK are larger than the correponding quantities of the CK. (Section 6) Finally, we use our spectral theory to predict the maximal nondiverging learning rate (""max learning rate"") of SGD (Section 7). In general, we will not only verify our theory with experiments on the theoretically interesting distributions, i.e. uniform measures over the boolean cube and the sphere, or the standard Gaussian, but also confirm these findings on real data like MNIST and CIFAR10 1 . For space concerns, we review relevant literature along the flow of the main text, and relegate a more complete discussion of the related research landscape in Appendix A. In this work, we have taken a first step at studying how hyperparameters change the initial distribution and the generalization properties of neural networks through the lens of neural kernels and their spectra. We obtained interesting insights by computing kernel eigenvalues over the boolean cube and relating them to generalization through the fractional variance heuristic. While it inspired valid predictions that are backed up by experiments, fractional variance is clearly just a rough indicator. We hope future work can refine on this idea to produce a much more precise prediction of test loss. Nevertheless, we believe the spectral perspective is the right line of research that will not only shed light on mysteries in deep learning but also inform design choices in practice. Boolean cube theory predicts max learning rate for real datasets Figure 5 : Spectral theory of CK and NTK over boolean cube predicts max learning rate for SGD over real datasets MNIST and CIFAR10 as well as over boolean cube 128 , the sphere √ 128S 128−1 , and the standard Gaussian N (0, I 128 ). In all three plots, for different depth, nonlinearity, σ 2 w , σ 2 b of the MLP, we obtain its maximal nondiverging learning rate (""max learning rate"") via binary search. We center and normalize each image of MNIST and CIFAR10 to the sphere, where d = 28 2 = 784 for MNIST and d = 3 × 32 2 = 3072 for CIFAR10. See Appendix E.2 for more details. (a) We empirically find max learning rate for training only the last layer of an MLP. Theoretically, we predict 1/Φ(0) where Φ corresponds to the CK of the MLP. We see that our theoretical prediction is highly accurate. Note that the Gaussian and Sphere points in the scatter plot coincide with and hide behind the BoolCube points. (b) and (c) We empirically find max learning rate for training all layers. Theoretically, we predict 1/Φ(0) where Φ corresponds to the NTK of the MLP. The points are identical between (b) and (c), but the color coding is different. Note that the Gaussian points in the scatter plots coincide with and hide behind the Sphere points. In (b) we see that our theoretical prediction when training all layers is not as accurate as when we train only the last layer, but it is still highly correlated with the empirical max learning rate. It in general underpredicts, so that half of the theoretical learning rate should always have SGD converge. This is expected, since the NTK limit of training dynamics is only exact in the large width limit, and larger learning rate just means the training dynamics diverges from the NTK regime, but not necessarily that the training diverges. In (c), we see that deeper networks tend to accept higher learning rate than our theoretical prediction. If we were to preprocess MNIST and CIFAR10 differently, then our theory is less accurate at predicting the max learning rate; see Fig. 9","Eigenvalues of Conjugate (aka NNGP) and Neural Tangent Kernel can be computed in closed form over the Boolean cube and reveal the effects of hyperparameters on neural network inductive bias, training, and generalization.",Yao et al. ; Du et al. ; Daniely et al ; Jacot ; half ; first ; the CK K ; SGD ; al. ; MLP,the large width limit ; the infinite width setting ; our theoretical prediction ; this ; an MLP ; deep learning ; The associated kernel ; (i.e. what fraction ; a bias ; tr K,Yao et al. ; Du et al. ; Daniely et al ; Jacot ; half ; first ; the CK K ; SGD ; al. ; MLP,"The spectra of the *Conjugate Kernel, CK,*, and the *Neural Tangent Kernel, NTK, provide insights into the behavior of neural networks and their generalization properties. These spectra encode valuable information about the initial distribution and training and generalization attributes of networks. The computational tools we develop for analyzing the spectra and generating the plots in this paper are open-sourced and can be used for future studies of deep neural networks. By analyzing the eigenvalues, we provide novel insights into neural networks' behavior and generalize, and we verify these insights by extensive experiments. We also verify",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"We introduce CBF, an exploration method that works in the absence of rewards or end of episode signal. CBF is based on intrinsic reward derived from the error of a dynamics model operating in feature space. It was inspired by (Pathak et al., 2017), is easy to implement, and can achieve results such as passing four levels of Super Mario Bros, navigating VizDoom mazes and passing two levels of SpaceInvaders. We investigated the effect of combining the method with several auxiliary tasks, but find inconsistent improvements over the CBF baseline.
 Modern reinforcement learning methods work well for tasks with dense reward functions, but in many environments of interest the reward function may be sparse, require considerable human effort to specify, be misspecified, or be prohibitively costly to evaluate. In general it is much easier to find environments that we could train an agent to act in than it is to find sensible reward functions to train it with. It is therefore desirable to find ways to learn interesting behaviors from environments without specified reward functions. BID19 introduced an exploration strategy that leads to sophisticated behavior in several games in the absence of any extrinsic reward. The strategy involves 1. Learning features using an inverse dynamics prediction task, 2. training a forward dynamics model in the feature space, and 3. using the error of the forward model as an intrinsic reward for an exploration agent.Inspired by this result we wondered if it was possible to improve the method by using a different task for learning the features in step 1. To our surprise we found that the choice of feature-learning task didn't matter much. In fact when skipping step 1 altogether, we often obtained comparable or better results. As a result we obtained a method that is simple to implement, and shows purposeful behavior on a range of games, including passing over four levels of Super Mario Bros without any extrinsic rewards or end of episode signal (see video here). Previous work reported making significant progress on the first level of this game. In addition we report the results of using our method on VizDoom maze environment, and a range of Atari games. Our experiments have shown that any of the joint training methods can work well for exploration. The fact that a method as simple as CBF performs so well, however, suggests that the success of the method of BID19 comes to a great extent from a feature-bootstrapping effect, and the utility of an auxiliary task, if any, is to stabilize this process.Some immediate future research directions include trying CBF on environments with continuous action spaces, and investigating feature-bootstrapping for count-based exploration methods such as BID17 . We would also like to research exploration of environments with greater amounts of stochasticity. 9 APPENDIX: EXPERIMENTAL DETAILS PREPROCESSING We followed the standard preprocessing for Atari games (see wrappers used in the DQN implementation in BID10 ) for all of our experiments, except for not using the automatic ""press fire"" in the beginning of the episode wrapper. For Mario and VizDoom we downscaled the observations to 84 by 84 pixels, converted them to grayscale, stacked sequences of four frames as four channels of the observation, and used a frame skip of four. We also used an action wrapper replicating the action space used in BID19 .",A simple intrinsic motivation method using forward dynamics model error in feature space of the policy.,CBF ; al. ; four ; Super Mario Bros ; VizDoom ; two ; SpaceInvaders ; first ; Atari ; Mario,video ; It ; several auxiliary tasks ; our surprise ; many environments ; all ; the episode wrapper ; our method ; four frames ; Our experiments,CBF ; al. ; four ; Super Mario Bros ; VizDoom ; two ; SpaceInvaders ; first ; Atari ; Mario,"We introduce CBF, an exploration method that works in the absence of rewards or end of episode signal, based on intrinsic reward derived from the error of a dynamics model operating in feature space. It is easy to implement, and can achieve results such as passing four levels of Super Mario Bros, navigating VizDoom mazes, and passing two levels of SpaceInvaders. However, it has inconsistent improvements over the CBF baseline. Modern reinforcement learning methods work well for tasks with dense reward functions, but in many environments of interest the reward function may be sparse, require considerable human effort to specify, be misspecified, or prohib",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Equivariance is a nice property to have as it produces much more parameter efficient neural architectures and preserves the structure of the input through the feature mapping. Even though some combinations of transformations might never appear (e.g. an upright face with a horizontal nose), current equivariant architectures consider the set of all possible transformations in a transformation group when learning feature representations. Contrarily, the human visual system is able to attend to the set of relevant transformations occurring in the environment and utilizes this information to assist and improve object recognition. Based on this observation, we modify conventional equivariant feature mappings such that they are able to attend to the set of co-occurring transformations in data and generalize this notion to act on groups consisting of multiple symmetries. We show that our proposed co-attentive equivariant neural networks consistently outperform conventional rotation equivariant and rotation & reflection equivariant neural networks on rotated MNIST and CIFAR-10. Thorough experimentation in the fields of psychology and neuroscience has provided support to the intuition that our visual perception and cognition systems are able to identify familiar objects despite modifications in size, location, background, viewpoint and lighting (Bruce & Humphreys, 1994) . Interestingly, we are not just able to recognize such modified objects, but are able to characterize which modifications have been applied to them as well. As an example, when we see a picture of a cat, we are not just able to tell that there is a cat in it, but also its position, its size, facts about the lighting conditions of the picture, and so forth. Such observations suggest that the human visual system is equivariant to a large transformation group containing translation, rotation, scaling, among others. In other words, the mental representation obtained by seeing a transformed version of an object, is equivalent to that of seeing the original object and transforming it mentally next. These fascinating abilities exhibited by biological visual systems have inspired a large field of research towards the development of neural architectures able to replicate them. Among these, the most popular and successful approach is the Convolutional Neural Network (CNN) (LeCun et al., 1989) , which incorporates equivariance to translation via convolution. Unfortunately, in counterpart to the human visual system, CNNs do not exhibit equivariance to other transformations encountered in visual data (e.g. rotations). Interestingly, however, if an ordinary CNN happens to learn rotated copies of the same filter, the stack of feature maps becomes equivariant to rotations even though individual feature maps are not (Cohen & Welling, 2016) . Since ordinary CNNs must learn such rotated copies independently, they effectively utilize an important number of network parameters suboptimally to this end (see Fig. 3 in Krizhevsky et al. (2012) ). Based on the idea that equivariance in CNNs can be extended to larger transformation groups by stacking convolutional feature maps, several approaches have emerged to extend equivariance to, e.g. planar rotations (Dieleman et al., 2016; Marcos et al., 2017; Weiler et al., 2018; Li et al., 2018) , spherical rotations (Cohen et al., 2018; Worrall & Brostow, 2018; Cohen et al., 2019) , scaling (Marcos et al., 2018; Worrall & Welling, 2019) and general transformation groups (Cohen & Welling, 2016) , such that transformed copies of a single entity are not required to be learned independently. Figure 1: Our visual system infers object identities according to their size, location and orientation in a scene. In this blurred picture, observers describe the scene as containing a car and a pedestrian in the street. However, the pedestrian is in fact the same shape as the car, except for a 90 • rotation. The atypicality of this orientation for a car within the context defined by the street scene causes the car to be recognized as a pedestrian. Extracted from Oliva & Torralba (2007) . Although incorporating equivariance to arbitrary transformation groups is conceptually and theoretically similar 1 , evidence from real-world experiences motivating their integration might strongly differ. Several studies in neuroscience and psychology have shown that our visual system does not react equally to all transformations we encounter in visual data. Take, for instance, translation and rotation. Although we easily recognize objects independently of their position of appearance, a large corpus of experimental research has shown that this is not always the case for in-plane rotations. Yin (1969) showed that mono-oriented objects, i.e. complex objects such as faces which are customarily seen in one orientation, are much more difficult to be accurately recognized when presented upsidedown. This behaviour has been reproduced, among others, for magazine covers (Dallett et al., 1968) , symbols (Henle, 1942) and even familiar faces (e.g. from classmates) (Brooks & Goldstein, 1963) . Intriguingly, Schwarzer (2000) found that this effect exacerbates with age (adults suffer from this effect much more than children), but, adults are much faster and accurate in detecting mono-oriented objects in usual orientations. Based on these studies, we draw the following conclusions: • The human visual system does not perform (fully) equivariant feature transformations to visual data. Consequently, it does not react equally to all possible input transformations encountered in visual data, even if they belong to the same transformation group (e.g. in-plane rotations). • The human visual system does not just encode familiarity to objects but seems to learn through experience the poses in which these objects customarily appear in the environment to assist and improve object recognition (Freire et al., 2000; Riesenhuber et al., 2004; Sinha et al., 2006) . Complementary studies (Tarr & Pinker, 1989; Oliva & Torralba, 2007) suggest that our visual system encodes orientation atypicality relative to the context rather than on an absolute manner (Fig. 1) . Motivated by the aforementioned observations we state the co-occurrence envelope hypothesis: The Co-occurrence Envelope Hypothesis. By allowing equivariant feature mappings to detect transformations that co-occur in the data and focus learning on the set formed by these co-occurrent transformations (i.e. the co-occurrence envelope of the data), one is able to induce learning of more representative feature representations of the data, and, resultantly, enhance the descriptive power of neural networks utilizing them. We refer to one such feature mapping as co-attentive equivariant. Identifying the co-occurrence envelope. Consider a rotation equivariant network receiving two copies of the same face (Fig. 2a) . A conventional rotation equivariant network is required to perform inference and learning on the set of all possible orientations of the visual patterns constituting a face regardless of the input orientation (Fig. 2b) . However, by virtue of its rotation equivariance, it is able to recognize rotated faces even if it is trained on upright faces only. A possible strategy to simplify the task at hand could be to restrict the network to react exclusively to upright faces (Fig. 2c) . In this case, the set of relevant visual pattern orientations becomes much smaller, at the expense of disrupting equivariance to the rotation group. Resultantly, the network would risk becoming unable to detect faces in any other orientation than those it is trained on. A better strategy results from restricting the set of relevant pattern orientations by defining them relative to one another (e.g. mouth Figure 2: Effect of multiple attention strategies for the prioritization of relevant pattern orientations in rotation equivariant networks for the task of face recognition. Given that all attention strategies are learned exclusively from upright faces, we show the set of relevant directions for the recognition of faces in two orientations (Fig. 2a) obtained by: no attention (Fig. 2b) , attending to the pattern orientations of appearance independently (Fig. 2c) and, attending to the pattern orientations of appearance relative to one another (Fig. 2d ). Built upon Figure 1 from Schwarzer (2000) . orientation w.r.t. the eyes) as opposed to absolutely (e.g. upright mouth) (Fig. 2d ). In such a way, we are able to exploit information about orientation co-occurrences in the data without disrupting equivariance. The set of co-occurrent orientations in Fig. 2d corresponds to the co-occurrence envelope of the samples in Fig. 2a for the transformation group defined by rotations. In this work, we introduce co-attentive equivariant feature mappings and apply them on existing equivariant neural architectures. To this end, we leverage the concept of attention (Bahdanau et al., 2014) and modify existing mathematical frameworks for equivariance, such that co-occurrent transformations can be detected. It is critical not to disrupt equivariance in the attention procedure as to preserve it across the entire network. To this end, we introduce cyclic equivariant self-attention, a novel attention mechanism able to preserve equivariance to a large set of transformation groups. Experiments and results. We explore the effects of co-attentive equivariant feature mappings for single and multiple symmetry groups. Specifically, we replace conventional rotation equivariant mappings in p4-CNNs (Cohen & Welling, 2016) and DRENs (Li et al., 2018) with co-attentive ones. We show that co-attentive rotation equivariant neural networks consistently outperform their conventional counterparts in fully (rotated MNIST) and partially (CIFAR-10) rotational settings. Subsequently, we generalize cyclic equivariant self-attention to multiple similarity groups and apply it on p4m-CNNs (Cohen & Welling, 2016 ) (equivariant to rotation and mirror reflections). Our results are in line with those obtained for single symmetry groups and support our stated hypothesis. Our results show that co-attentive equivariant feature mappings can be utilized to enhance conventional equivariant ones. Interestingly, co-attentive equivariant mappings are beneficial both in partially and fully rotational settings. We attribute this to the fact that a set of co-occurring orientations between patterns can be easily defined (and exploited) in both settings. It is important to note that we utilized attention independently over each spatial position u on the codomain of the corresponding group convolution. Resultantly, we were restricted to mappings of the form xA, which, in turn, constraint our attention mechanism to have a circulant structure in order to preserve equivariance (since group actions acting in the codomain of the group convolution involve cyclic permutations and cyclic self-attention is applied in the codomain of the group convolution). In future work, we want to extend the idea presented here to act on the entire group simultaneously (i.e. along u as well). By doing so, we lift our current restriction to mappings of the form xA and therefore, may be able to develop attention instances with enhanced descriptive power. Following the same line of though, we want to explore incorporating attention in the convolution operation itself. Resultantly, one is not restricted to act exclusively on the codomain of the convolution, but instead, is able to impose structure in the domain of the mapping as well. Naturally, such an approach could lead to enhanced descriptiveness of the incorporated attention mechanism. Moreover, we want to utilize and extend more complex attention strategies (e.g. Bahdanau et al. (2014); Luong et al. (2015) ; Vaswani et al. (2017) ; Mishra et al. (2017) ) such that they can be applied to large transformation groups without disrupting equivariance. As outlined earlier in Section 3.1, this becomes very challenging from a computational perspective as well, as it requires extensive usage of the corresponding attention mechanism. Resultantly, an efficient implementation thereof is mandatory. Furthermore, we want to extend co-attentive equivariant feature mappings to continuous (e.g. Worrall et al. (2017) ) and 3D space (e.g. Cohen et al. (2018) ; Worrall & Brostow (2018) ; Cohen et al. (2019) ) groups, and for applications other than visual data (e.g. speech recognition). Published as a conference paper at ICLR 2020 Finally, we believe that our approach could be refined and extended to a first step towards dealing with the enumeration problem of large groups (Gens & Domingos, 2014) , such that functions acting on the group (e.g. group convolution) are approximated by evaluating them on the set of cooccurring transformations as opposed to on the entire group. Such approximations are expected to be very accurate, as non-co-occurrent transformations are rare. This could be though of as sharping up co-occurrent attention to co-occurrent restriction. We have introduced the concept of co-attentive equivariant feature mapping and applied it in the context of equivariant neural networks. By attending to the co-occurrence envelope of the data, we are able to improve the performance of conventional equivariant ones on fully (rotated MNIST) and partially (CIFAR-10) rotational settings. We developed cyclic equivariant self-attention, an attention mechanism able to attend to the co-occurrence envelope of the data without disrupting equivariance to a large set of transformation groups (i.e. all transformation groups G, whose action in the codomain of a G-equivariant feature mapping produce cyclic permutations). Our obtained results support the proposed co-occurrence envelope hypothesis. A OBTAINING CO-OCCURRENT ATTENTION VIA EQUATION 5 Figure 4: Synchronous movement of feature mappings and attention masks as a function of input rotation in the group p4 (r max = 4). In this section, we provide a meticulous description on how co-occurrent attention is obtained via the method presented in the paper. Intuitively, a direct approach to address the problem illustrated in the introduction (Section 1) and Figure 2 requires an attention mechanism that acts simultaneously on r and λ (see Eq. 3). However, we illustrate how the depicted problem can be simplified such that attention along r is sufficient by taking advantage of the equivariance property of the network. Let p be the input of a roto-translational convolution f R : Z 2 × Θ × Λ 0 → Z 2 × Θ × Λ 1 as defined in Eq. 3, and Θ be the set of rotations by θ r degrees: Θ = {θ r = r 2π rmax } rmax r=1 . Let f R (p)(u) ∈ R rmax×Λ1 be the matrix consisting of the r max oriented responses for each λ ∈ Λ 1 learned representation at a certain position u. Since the vectors f R (p)(u, λ) ∈ R rmax , λ ∈ Λ 1 permute cyclically as a result of the rotation equviariance property of f R , it is mandatory to ensure equivariance to cyclic permutations for each f R (p)(u, λ) during the course of the attention procedure (see Section 3). At first sight, one is inclined to think that there is no connection between multiple vectors f R (p)(u, λ) in f R (p)(u), and, therefore, in order to exploit co-occurences, one must impose additional constraints along the λ axis. However, there is indeed an implicit restriction in f R (p)(u) along λ resulting from the rotation equivariance property of the mapping f R , which we can take advantage from to simplify the problem at hand. Consider, for instance, the input θ i p, a θ i -rotated version of p. By virtue of the equivariance property of f R , we have (locally) that f R (θ i p) = P i (f R (p)). Furthermore, we know that this property must hold for all the learned feature representations f R (p)(u, λ),∀λ ∈ Λ 1 . Resultantly, we have that: In other words, if one of the learned mappings f R (p)(u, r, λ) experiences a permutation P i along r, all the learned representations f R (p)(u, r, λ), ∀λ ∈ Λ 1 must experience the exact same permutation P i as well. Resultantly, the equivariance property of the mapping f R ensures that all the Λ 1 learned feature representations f R (p)(u, λ) ""move synchronously"" as a function of input rotation θ i . Likewise, if we apply a cyclic equivariant attention mechanism A C λ independently on top of each λ learned representation f R (p)(u, λ), we obtain that the relation A C λ (f R (θ i p))(u, r, λ) = P i (A C λ (f R (p))(u, r, λ)) ∀λ ∈ Λ 1 (13) must hold as well. Similarly to the case illustrated in Eq. 12 and given that A C λ is equivariant to cyclic permutations on the domain, we obtain that all the Λ 1 learned attention masks A C λ ""move synchronously"" as a function of input rotation θ i as well (see Fig. 4 ). From Eq. 13 and Figure 4 , one can clearly see that by utilizing A C λ independently along r and taking advantage from the fact that all Λ 1 learned feature representations are tied with one another via f R , one is able to prioritize learning of feature representations that co-occur together as opposed to the much looser formulation in Eq. 12, where feedback is obtained from all orientations.",We utilize attention to restrict equivariant neural networks to the set or co-occurring transformations in data.,Luong ; Dallett et al. ; Li ; Worrall ; Gens & Domingos ; Cohen et al ; Worrall & Welling ; Schwarzer ; Cohen et al. ; Mishra et al,transformation groups ; modifications ; these ; its rotation equivariance ; network parameters ; occurences ; all the learned feature representations ; our attention mechanism ; the much looser formulation ; attention instances,Luong ; Dallett et al. ; Li ; Worrall ; Gens & Domingos ; Cohen et al ; Worrall & Welling ; Schwarzer ; Cohen et al. ; Mishra et al,"Equivariance is a useful property for neural architectures, preserving the structure of input through feature mapping. The human visual system is able to attend to all possible transformations in a transformation group when learning feature representations and utilizes this information to assist and improve object recognition. We modify conventional equivariant feature mappings such that they can attend to the set of co-occurring transformations in data and generalize this notion to groups consisting of multiple symmetries. Our proposed co-attentive neural networks consistently outperform conventional rotation equivariants and rotation & reflection equivant neural networks on rotated MNIST and CIFAR-10",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Supervised learning problems---particularly those involving social data---are often subjective. That is, human readers, looking at the same data, might come to legitimate but completely different conclusions based on their personal experiences. Yet in machine learning settings feedback from multiple human annotators is often reduced to a single ``ground truth'' label, thus hiding the true, potentially rich and diverse interpretations of the data found across the social spectrum. We explore the rewards and challenges of discovering and learning representative distributions of the labeling opinions of a large human population. A major, critical cost to this approach is the number of humans needed to provide enough labels not only to obtain representative samples but also to train a machine to predict representative distributions on unlabeled data. We propose aggregating label distributions over, not just individuals, but also data items, in order to maximize the costs of humans in the loop. We test different aggregation approaches on state-of-the-art deep learning models. Our results suggest that careful label aggregation methods can greatly reduce the number of samples needed to obtain representative distributions. This paper explores the problem of label aggregation in domains that are highly subjective, i.e., where different annotators may disagree for perfectly legitimate reasons. Such settings are common, if underacknowledged. Though increasingly, mass media provides stories about the unintended consequences of ignoring this diversity in machine learning.For example, Beauty.ai sponsored a worldwide beauty contest, judged by a machine learning algorithm. Though light-skinned entrants made up the majority of entrants, they nonetheless won a disproportionate number of contests. BID0 Tay, a Twitter-based learning agent, developed by Microsoft, was taught to tweet that the Holocaust was made up 2 (though the Holocaust factually existed, the same cybersocial dynamics of training bias found in subjective domains led to this outcome). ProPublica discovered that Northpointe risk assessment software-used to help judges determine sentence length for convicts-recommended longer sentences for African-American men than other groups, even when controlled for confounding factors. BID2 X:2 Fig. 1 . In this example, data items (black dots) are labeled by five human annotators each (left), where color indicates label choice, yielding an empirical label distribution y i for each data item i. By clustering similarly labeled objects, we pool together (right) the labels of all data items assigned to the same cluster k into a single, much larger sample θ k for all items in the cluster. Our research suggests that, in some cases, this larger sample (or a mixture of cluster samples) is a better representation of the true population distribution of beliefs about each data item in the cluster and can lead to better predictive supervised learning.Learning a distribution of beliefs about a data item, rather than a single ""ground truth"" label, poses unique challenges. It increases the dimensionality of the learning problem so that more data items may be needed. It also may require more labels per item to get a representative sample of the human populations' beliefs. And for most problems, labels are relatively expensive to obtain. Though crowdsourcing platforms have made this task convenient, they are frequently a resource bottleneck in supervised learning loops.Our main contribution is a method for minimizing the number of labels needed to learn to predict socially representative label distributions. It is based on the hypothesis that the sources are subjectivity are limited, and so the number of distinct distributions of beliefs over all data items is likewise limited. In other words, the label distributions are samples from a relatively small number of true, but hidden, distributions. See Figure 1 . These hidden distributions can be seen as latent classes representing population-level beliefs about the labels. According to this hypothesis, we can use unsupervised clustering algorithms to pool together the labels of data items with similar distributions into higher resolution distributions of beliefs shared commonly among all data items in the same cluster.In particular, we: (1) explore subjectivity as the problem of learning representative distributions from a target population of responses to target questions, BID1 propose clustering as a sensible means for pooling together labels from similar data items, to reduce the number of labels needed (3) test what we call our clustering hypothesis, that the label distributions of subjective data are clustered around a small number of underlying, true distributions (4) study how different label aggregation strategies and representations affect the performance of state-of-the art deep learning predictors.It would seem that bias is an inherent part of any information reduction process, such as those found in statistical learning BID29 . So it seems naive to expect that machines can learn unbiased models through unsupervised learning alone, or even for any supervised learning that assumes a singular, correct answer to most problems. We hope that this research sparks a broader debate about the best practices for machine learning with humans in the loop.The rest of this paper is organized as follows. Section 2 describes our experimental workflow, Section 3 presents our results, Section 4 discusses our study, Section 5 presents related work, and Section 6 is the conclusion. Figure 2 describes the basic experimental workflow in this study. We discuss each phase below. Note that there are two testing phases, one for determining how well each aggregation method fits the data and another for how well supervised learning algorithms trained by each aggregation strategy perform. Since these test phases share some methods, we discuss them together at the end of the section. Fig. 2 . The basic experimental workflow involves obtaining crowdsourced labels for raw data (yielding empirical label distributions for each data item), trying various strategies for aggregating and pooling those labels (including no aggregation), and finally testing how each method affects the accuracy of machine learning prediction. Note there are two testing phases: one for how well each aggregation strategy fits the data and one for machine learning performance. We also list important terms, keywords, and abbreviations associated with each phase of the workflow.",We study the problem of learning to predict the underlying diversity of beliefs present in supervised learning domains.,Holocaust ; item i. ; Microsoft ; five ; ProPublica ; Fig ; African-American ; Northpointe ; Twitter ; two,distinct distributions ; similar distributions ; the costs ; the human populations' beliefs ; the unintended consequences ; the workflow ; those ; contests ; other words ; the performance,Holocaust ; item i. ; Microsoft ; five ; ProPublica ; Fig ; African-American ; Northpointe ; Twitter ; two,"Supervised learning problems are often subjective, where human readers come to legitimate but completely different conclusions based on their personal experiences. In machine learning settings feedback from multiple human annotators is often reduced to a single ``ground truth'' label, hiding the true, potentially rich and diverse interpretations of the data found across the social spectrum. This approach reduces the number of labels needed to obtain representative samples and train a machine to predict representative distributions on unlabeled data. We propose aggregating label distributions over, not just individuals, but also data items, in order to maximize the costs of humans in the loop. We test different aggregation approaches on state",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Over the last few years exciting work in deep generative models has produced models able to suggest new organic molecules by generating strings, trees, and graphs representing their structure. While such models are able to generate molecules with desirable properties, their utility in practice is limited due to the difficulty in knowing how to synthesize these molecules. We therefore propose a new molecule generation model, mirroring a more realistic real-world process, where reactants are selected and combined to form more complex molecules. More specifically, our generative model proposes a bag of initial reactants (selected from a pool of commercially-available molecules) and uses a reaction model to predict how they react together to generate new molecules. Modeling the entire process of constructing a molecule during generation offers a number of advantages. First, we show that such a model has the ability to generate a wide, diverse set of valid and unique molecules due to the useful inductive biases of modeling reactions. Second, modeling synthesis routes rather than final molecules offers practical advantages to chemists who are not only interested in new molecules but also suggestions on stable and safe synthetic routes. Third, we demonstrate the capabilities of our model to also solve one-step retrosynthesis problems, predicting a set of reactants that can produce a target product.",A deep generative model for organic molecules that first generates reactant building blocks before combining these using a reaction predictor.,the last few years ; First ; Second ; Third ; one,"the entire process ; a number ; trees ; a reaction model ; molecules ; our model ; valid and unique molecules ; practice ; a molecule ; a wide, diverse set",the last few years ; First ; Second ; Third ; one,"Deep generative models are able to suggest new organic molecules by generating strings, trees, and graphs representing their structure. However, their utility in practice is limited due to the difficulty in synthesizing these molecules. Our generative model proposes a bag of initial reactants and a reaction model to predict how they react together to generate new molecules. The model's ability to generate a wide, diverse set of valid and unique molecules due to its useful inductive biases. It also solves one-step retrosynthesis problems, predicting a target product.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"While much recent work has targeted learning deep discrete latent variable models with variational inference, this setting remains challenging, and it is often necessary to make use of potentially high-variance gradient estimators in optimizing the ELBO. As an alternative, we propose to optimize a non-ELBO objective derived from the Bethe free energy approximation to an MRF's partition function. This objective gives rise to a saddle-point learning problem, which we train inference networks to approximately optimize. The derived objective requires no sampling, and can be efficiently computed for many MRFs of interest. We evaluate the proposed approach in learning high-order neural HMMs on text, and find that it often outperforms other approximate inference schemes in terms of true held-out log likelihood. At the same time, we find that all the approximate inference-based approaches to learning high-order neural HMMs we consider underperform learning with exact inference by a significant margin. There has been much recent interest in learning deep generative models with discrete latent variables BID29 BID28 BID12 BID25 BID15 Lee et al., 2018, inter alia) , especially in the case where these latent variables have structure -that is, where the interdependence between the discrete latents is modeled. Most recent work has focused on learning these models with variational inference BID14 , and in particular with variational autoencoders (VAEs) BID17 BID32 .Variational inference has a number of convenient properties, including that it involves the maximization of the evidence lower-bound (ELBO), a lower bound on the log marginal likelihood of the data. At the same time, when learning models with discrete latent variables variational inference may require the use of potentially high-variance gradient estimators, which are obtained during learning by sampling from the variational posterior; see Appendix A for an empirical investigation into the variance of various popular estimators when learning neural text HMMs with VAEs.In this paper we investigate learning discrete latent variable models with an alternative objective to the ELBO. In particular , we propose to approximate the intractable log marginal likelihood with an objective deriving from the Bethe free energy BID1 , a quantity which is intimately related to loopy belief propagation (LBP) BID30 BID45 BID9 BID9 , and which is the basis for ""outer approximations"" to the marginal polytope BID39 . The Bethe free energy is attractive because if all the factors in the factor graph associated with the model have low degree, it can often be evaluated efficiently, without any need for approximation by sampling (see Section 2). Of course, requiring all factors in the factor graph to be of low degree severely limits the expressiveness of directed graphical models. It does not, however , limit the expressiveness of markov random fields (MRFs) (i.e., undirected graphical models) as severely, since we can simply have an extremely loopy MRF, with arbitrary pairwise factors; see FIG1 (c) and Section 2.2.We accordingly propose to learn deep, undirected graphical models with latent variables, using a saddlepoint objective that makes use of the Bethe free energy approximation to the model's partition functions. We further amortize inference by using ""inference networks"" BID36 BID17 BID13 BID38 in optimizing the saddle-point objective. Unlike the ELBO, our objective will not form a lower bound on the log marginal likelihood, but an approximation to it. At the same time (and unlike other recent work on MRFs with a variational flavor BID21 BID23 ), this objective can be optimized efficiently, without sampling, and in our experiments in learning neural HMMs on text it outperforms other approximate inference methods in terms of held out log likelihood. We emphasize, however , that despite the improvement observed when training with the proposed objective, in our experiments all approximate inference methods were found to significantly underperform learning with exact inference; see Section 4.3. We begin with the results obtained by maximizing the true log marginal likelihood of the training data under both the directed (""Full"" in Table 1 ) and undirected models (""Pairwise MRF"" in Table 1 ), by backpropagating gradients through the relevant dynamic programs. These results establish how well our models perform under exact inference, and are shown in the last row of each subtable in Table 1 . We see that perplexities are roughly comparable between the directed and undirected models when trained with exact inference.We now consider the remaining directed HMM results of Table 1 , where the models are trained with approximate inference. In the first row of each ""Full"" subtable there, we show the result of maximizing the ELBO using a mean field-style posterior approximation and the REINFORCE BID43 gradient estimator, with an input-dependent baseline to reduce variance BID29 . The results are quite poor, with this approximate inference scheme leading to a gain of almost 200 points in perplexity over exact inference. Using the tighter IWAE BID3 objectives improves performance slightly in all cases, though the most dramatic performance improvement comes from using a first-order HMM posterior in maximizing the ELBO, which can be sampled from exactly using quantities calculated with the forward algorithm BID31 BID4 BID34 BID49 . While these results are encouraging, note that in general we may not have an exact dynamic program for sampling from a lower-order structured model, and that moreover we still appear to incur a perplexity penalty of more than 100 points over exact inference; see Appendix A for an empirical comparison of the variance of these estimators.Moving to the MRF results, the second row of each ""Pairwise MRF"" subtable in Table 1 contains the results of optimizing F as a saddle point problem. While this approach too underperforms exact inference by approximately 100 points in perplexity, somewhat remarkably it manages to consistently outperform the best approximate inference results for the directed models by a fair margin. The first row of each ""Pairwise MRF"" subtable in Table 1 attempts to determine whether the jump in perplexity when moving to the F objective is due to the approximate inference or to the approximate objective, by minimizing the F objective using the exact marginals, as calculated by a dynamic program. (Note that this is not equivalent to the negative log marginal likelihood, since the factor graphs are loopy). Interestingly, we see that this performs almost as well as the exact objective, suggesting that, at least for HMM models, the F objective is reasonable, and approximate inference remains the problem.Despite these encouraging results, we note that there are several drawbacks to the proposed approach. In particular, we find that in practice F indeed can over-or under-estimate perplexity. Moreover, while ELBO values are not perfectly correlated with their corresponding true perplexities, values of F seem even less correlated, which necessitates finding correlated proxies of perplexity that may be monitored during training. Finally, we note that explicitly calculating the projection onto the nullspace of A may be prohibitive for some models (e.g., large RBMs BID35 ), and so other approaches to tackling the constrained optimization problem are likely necessary. We have presented an objective for learning latent-variable MRFs based on the Bethe approximation to the partition function, which can often be efficiently evaluated and requires no sampling. This objective leads to slightly better held-out perplexities than other approximate inference methods when learning neural HMMs. Future work will examine scaling the proposed method to larger, non-sequential MRFs, and whether F -like objectives can be made to better correlate with the true perplexity.",Learning deep latent variable MRFs with a saddle-point objective derived from the Bethe partition function approximation.,ELBO ; F ; inter alia ; the last row ; first ; second ; Appendix ; Pairwise MRF ; Bethe ; HMM,the jump ; the most dramatic performance improvement ; underperform ; a dynamic program ; a number ; Appendix A ; the models ; all approximate inference methods ; the remaining directed HMM results ; discrete latent variables,ELBO ; F ; inter alia ; the last row ; first ; second ; Appendix ; Pairwise MRF ; Bethe ; HMM,"Learning deep discrete latent variable models with variational inference requires high-variance gradient estimators in optimizing the ELBO. A non-ELBO objective derived from the Bethe free energy approximation to an MRF partition function can be efficiently computed for many MRFs of interest. In this paper, we evaluate the proposed approach in learning high-order neural HMMs on text, and find that it often outperforms other approximate inference schemes in terms of true held-out log likelihood. However, all the approximate inference-based approaches to learning neural text HMMs underperform learning with exact inference by a significant margin.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Bitcoin is a virtual coinage system that enables users to trade virtually free of a central trusted authority. All transactions on the Bitcoin blockchain are publicly available for viewing, yet as Bitcoin is built mainly for security it’s original structure does not allow for direct analysis of address transactions. 
 Existing analysis methods of the Bitcoin blockchain can be complicated, computationally expensive or inaccurate. We propose a computationally efficient model to analyze bitcoin blockchain addresses and allow for their use with existing machine learning algorithms. We compare our approach against Multi Level Sequence Learners (MLSLs), one of the best performing models on bitcoin address data. Bitcoin(Nakamoto) is a virtual coinage system that functions much like a standard currency, enabling users to provide virtual payment for goods and services free of a central trusted authority. Bitcoin relies on the transmission of digital information, utilizing cryptographic methods to ensure secure, unique transactions. Individuals and businesses transact with the coin electronically on a peerto-peer network utilizing a shared transaction ledger (the Blockchain). It caught wide attention beginning in 2011, and various altcoins a general name for all other cryptocurrencies post-Bitcoin soon appeared It has placed itself as the most widespread and commonly used cryptocurrency with no signs of slowing down (Chan et al., 2017) . Representing over 81% of the total market of cryptocurrencies(coi), Its market capitalization is estimated to be approximately $177.8 Billioncoi accounting for about 90% of the total market capitalization of Virtual Currencies(Houben & Snyers). Bitcoin uses public key cryptography to generate secure addresses for users where each address is a public key, and use of the bitcoins stored in it requires signing with a private key. These address identifiers are used by their owners to hold bitcoin pseudonymously. A typical Bitcoin transaction consists of two sets: a set of source addresses and a set of destination addresses. Coins in the source addresses are collected and then sent in differing amounts to the destination addresses. (Houben & Snyers) While bitcoin address data is publicly available, it is not straightforward to analyze address transaction data since it is not aggregated in one block/place. It is apparent that address2vec is a significant improvement over a baseline approach, although not as accurate as MLSLs we believe further tuning of the model's architecture can yield a more accurate iteration of address2vec, especially making the model end to end differentiable, we currently use separate phases. We also plan to test address2vec on different bitcoin behavior tasks, measuring the similarity of various users and their relationships by measuring their vector distances and predicting market rates of bitcoin through analyzing most recent addresses on the blockchain.",a 2vec model for cryptocurrency transaction graphs,Bitcoin ; Multi Level Sequence Learners ; Blockchain ; post-Bitcoin ; Chan ; al. ; Virtual Currencies(Houben & Snyers ; two ; Houben & Snyers ; one,a significant improvement ; Coins ; Existing analysis methods ; the model's architecture ; the best performing models ; services ; A typical Bitcoin transaction ; each address ; their owners ; a computationally efficient model,Bitcoin ; Multi Level Sequence Learners ; Blockchain ; post-Bitcoin ; Chan ; al. ; Virtual Currencies(Houben & Snyers ; two ; Houben & Snyers ; one,"Bitcoin is a virtual coinage system that enables users to trade virtually free of a central trusted authority. However, it's original structure does not allow for direct analysis of address transactions. We propose a computationally efficient model to analyze bitcoin blockchain addresses and allow for their use with existing machine learning algorithms. We compare our approach against Multi Level Sequence Learners (MLSLs), one of the best performing models on bitcoin address data. Bitcoin(Nakamoto) operates much like a standard currency, enabling users to provide virtual payment for goods and services without a central trust authority. It relies on the transmission of digital information, utilizing cryptographic methods",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"In this study we focus on first-order meta-learning algorithms that aim to learn a parameter initialization of a network which can quickly adapt to new concepts, given a few examples. We investigate two approaches to enhance generalization and speed of learning of such algorithms, particularly expanding on the Reptile (Nichol et al., 2018) algorithm. We introduce a novel regularization technique called meta-step gradient pruning and also investigate the effects of increasing the depth of network architectures in first-order meta-learning. We present an empirical evaluation of both approaches, where we match benchmark few-shot image classification results with 10 times fewer iterations using Mini-ImageNet dataset and with the use of deeper networks, we attain accuracies that surpass the current benchmarks of few-shot image classification using Omniglot dataset. A common drawback consistently seen in traditional machine learning algorithms is the need for large amounts of training data in order to learn a given task BID5 , whereas the ability to grasp new concepts with just a few examples is clearly seen in the way people learn BID6 . This offers many challenges in fast adaption of machine learning in new fields and hence there is a growing interest in algorithms that can learn with limited data availability BID9 .In the development of learning methods that can be trained effectively on sparse data, the process of learning-to-learn is seen as a crucial step BID0 . This is often termed as meta-learning (Schaul & Schmidhuber, 2010) , where a variety of techniques have been presented. In our study, we specifically focus on approaches that learn an initialization of a network, trained on a dataset of tasks. Model-agnostic meta-learning (MAML) BID1 presented this exact approach and its applications of few-shot image classification, where a task was defined as correct classification of a test image out of N object classes, after training on a set of K examples per each class. Furthermore, MAML presented its first-order variant, where the second order derivatives were eliminated during computation while preserving results of the benchmarks. The approach avoided the computational expense of second order derivatives by treating them as constants. Firstorder meta-learning was further investigated in the Reptile algorithm BID7 , where the implementation was simplified eliminating the need for a test set in the tasks. Our study uses Reptile as the algorithm of choice to incorporate the techniques presented to improve generalization of first-order meta-learning.Even though first-order meta-learning has shown to attain fast generalization of concepts given limited data, empirical evaluations on few-shot image classification tasks BID7 show potential to improve the outcomes, especially on inputs with richer features such as real world images. Also drawbacks are seen in slower convergence, requiring a large number of iterations during the training phase. In this study, we investigate techniques used to obtain higher task generalization in models such as regularization BID11 and deeper networks BID3 and we present ways of adapting those in first-order meta-learning.The contributions of our study are as follows.• Introduction of meta-step gradient pruning, a novel approach to regularize parameter updates in first-order meta-learning.• Empirical evaluation of meta-step gradient pruning, achieving benchmark few-shot image classification accuracies with 10 times fewer iterations.• Empirical evaluation of deeper networks in the meta-learning setting, achieving results that surpass the current benchmarks in few-shot image classification. Our proposed novel approach of meta-step gradient pruning demonstrated enhanced generalization effects on the outcomes of first-order meta-learning. The reduced gaps between train and test set accuracies, during training of Omniglot and Mini-ImageNet few-shot classification tasks showed that the parameter initialization has learned to generalize better on the train set.We were able to almost match the benchmark results of first-order MAML and Reptile implementations with 10 times fewer iterations using our algorithm. This further emphasized the improved generalization, helping the parameters to converge the loss on few-shot classification. This increase in speed is vital in tasks such as Mini-Imagenet, because performing first-order meta-learning on real world noisy images is computationally expensive and time-consuming.With our approach of introducing deeper networks to the inner-loop in Omniglot few-shot classification, we showed results surpassing the current benchmarks of both first-order MAML and Reptile algorithms. The expanded parameter space with deeper models shows higher generalization as expected, but it makes the implementation more computationally expensive. This was identified as one drawback of this approach when applying to richer input data such as Mini-ImageNet tasks.Enhanced and fast generalization is utmost important when learning with limited data. Looking forward, we see the importance of elaborated theoretical analysis of meta-step gradient pruning and more techniques of regularization during meta-learning. Also in the future we plan to investigate on the application of first-order meta-learning in other applications such as reinforcement learning.",The study introduces two approaches to enhance generalization of first-order meta-learning and presents empirical evaluation on few-shot image classification.,first ; two ; Reptile ; Mini ; Omniglot ; Schaul & Schmidhuber ; MAML ; second ; iterations.• Empirical ; one,elaborated theoretical analysis ; real world noisy images ; large amounts ; other applications ; methods ; an initialization ; the benchmark results ; a novel approach ; the train set ; just a few examples,first ; two ; Reptile ; Mini ; Omniglot ; Schaul & Schmidhuber ; MAML ; second ; iterations.• Empirical ; one,"First-order meta-learning algorithms aim to learn a parameter initialization of a network which can quickly adapt to new concepts. The Reptile (Nichol et al., 2018) algorithm introduced a novel regularization technique called meta-step gradient pruning and the effects of increasing the depth of network architectures. In our study, we compare benchmark few-shot image classification results with 10 times fewer iterations using Mini-ImageNet dataset and with deeper networks, we attain accuracies that surpass the current benchmarks of few-shots image classification using Omniglot dataset. However, there are many challenges in fast adaption of machine learning in new",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Three-dimensional geometric data offer an excellent domain for studying representation learning and generative modeling. In this paper, we look at geometric data represented as point clouds. We introduce a deep autoencoder (AE) network with excellent reconstruction quality and generalization ability. The learned representations outperform the state of the art in 3D recognition tasks and enable basic shape editing applications via simple algebraic manipulations, such as semantic part editing, shape analogies and shape interpolation. We also perform a thorough study of different generative models including GANs operating on the raw point clouds, significantly improved GANs trained in the fixed latent space our AEs and, Gaussian mixture models (GMM). Interestingly, GMMs trained in the latent space of our AEs produce samples of the best fidelity and diversity.
 To perform our quantitative evaluation of generative models, we propose simple measures of fidelity and diversity based on optimally matching between sets point clouds. Three-dimensional (3D) representations of real-life objects are a core tool for vision, robotics, medicine, augmented and virtual reality applications. Recent encodings like view-based projections, volumetric grids and graphs, complement more traditional shape representations such as 3D meshes, level set functions, curve-based CAD models and constructive solid geometry BID1 . These encodings, while effective in their respective domains (e.g. acquisition or rendering), are often poor in semantics. For example, naïvely interpolating between two different cars in a view-based representation does not yield a representation of an ""intermediate"" car. Furthermore, these raw, high-dimensional representations are typically not well suited for the design of generative models via classic statistical methods. As such, editing and designing new objects with such representations frequently involves the construction and manipulation of complex, object-specific parametric models that link the semantics to the representation. This may require significant expertise and effort.Recent advances in deep learning bring the promise of a data-driven approach. In domains where data is plentiful, deep learning tools have eliminated the need for hand-crafting features and models. Deep learning architectures like autoencoders (AEs) BID22 Kingma & Welling, 2013) and Generative Adversarial Networks (GANs) BID10 BID20 Denton et al., 2015; BID6 are successful at learning complex data representations and generating realistic samples from complex underlying distributions. Recently, deep learning architectures for view-based projections BID30 BID14 , volumetric grids BID18 BID32 BID12 and graphs BID4 BID13 Defferrard et al., 2016; Yi et al., 2016b) have appeared in the 3D machine learning literature.In this paper we focus on point clouds, a relatively unexplored 3D modality. Point clouds provide a homogeneous, expressive and compact representation of surface geometry, easily amenable to geometric operations. These properties make them attractive from a learning point of view. In addition, they come up as the output of common range-scanning acquisition pipelines used in devices like the Kinect and iPhone's recent face identification feature. Only a handful of deep architectures for 3D point clouds exist in the literature: PointNet BID17 successfully tackled classification and segmentation tasks; BID14 used point-clouds as an intermediate step in their pipeline; Fan et al. (2016) used pointclouds as the underlying representation to extract 3D information from 2D images. We provide the first results that use deep architectures with the focus of learning representations and generative models for point clouds.Generative models have garnered increased attention recently in the deep learning community with the introduction of GANs BID10 ). An issue with GAN-based generative pipelines is that training them is notoriously hard and unstable BID24 . More importantly, there is no universally accepted way to evaluate generative models. In evaluating generative models one is interested in both fidelity, i.e. how much the generated points resemble the actual data, and coverage, i.e. what fraction of the data distribution a generated sample represents. The latter is especially important given the tendency of certain GANs to exhibit mode collapse. We provide simple methods to deal with both issues (training and evaluation) in our target domain. Our specific contributions are:• We design a new AE architecture-inspired by recent architectures used for classification BID17 -that is capable of learning compact representations of point clouds with excellent reconstruction quality even on unseen samples. The learned representations are (i) good for classification via simple methods (SVM), improving on the state of the art BID31 ; (ii) suitable for meaningful interpolations and semantic operations.• We create the first set of generative models which ( i) can generate point clouds measurably similar to the training data and held-out test data; (ii) provide good coverage of the training and test dataset. We argue that jointly learning the representation and training the GAN is unnecessary for our modality. We propose a workflow that first learns a representation by training an AE with a compact bottleneck layer, then trains a plain GAN in that fixed latent representation. Intuitively, training a GAN inside a compact, low-dimensional representation is easier. We point to theory BID0 that supports this idea, and verify it empirically. Latent GANs are much easier to train than monolithic (raw) GANs and achieve superior reconstruction with much better coverage. Somewhat surprisingly, GMMs trained in the latent space of fixed AEs achieve the best performance across the board.• We show that multi-class GANs work almost on par with dedicated GANs trained per-objectcategory, as long as they are trained in the latent space.• To support our qualitative evaluation, we perform a careful study of various old and new metrics, in terms of their applicability ( i) as objectives for learning good representations; (ii) for the evaluation of generated samples. We find that a commonly used point cloud metric, Chamfer distance, fails to discriminate certain pathological cases from good examples. We also propose fidelity and coverage metrics for our generative models, based on an optimal matching between two different samples, e.g. a set of point clouds generated by the model and a held-out test set.The rest of this paper is organized as follows: Section 2 outlines the necessary background and building blocks for our work and introduces our evaluation metrics. Section 3 introduces our models for latent representations and generation of point clouds. In Section 4, we evaluate all of our models both quantitatively and qualitatively, and analyze their behaviour. Further results and evaluation can be found in the appendix. The code for all our models is publicly available 1 .2 BACKGROUND DISPLAYFORM0 Autoencoders. Autoencoders (AE -inset) are deep architectures that aim to reproduce their input. They are especially useful, when they contain a narrow bottleneck layer between input and output. Upon successful training, the bottleneck layer corresponds to a lowdimensional representation, a code for the dataset. The Encoder (E) learns to compress a data point x into its latent representation, z. The Decoder (D) can then reproduce x from its encoded version z. DISPLAYFORM1 Generative Adversarial Networks. GANs are state-of-the-art generative models. The basic architecture (inset) is based on a adversarial game between a generator (G) and a discriminator (D). The generator aims to synthesize samples that look indistinguishable from real data (drawn from x ∼ p data ) by passing a randomly drawn sample z ∼ p z through the generator function G. The discriminator tries to tell synthesized from real samples. The most commonly used losses for the discriminator and generator networks are: DISPLAYFORM2 DISPLAYFORM3 where θ (D) , θ (G) are the parameters for the discriminator and the generator network respectively. In addition to the classical GAN formulation, we also use the improved Wasserstein GAN BID11 , which has shown improved stability during training.Challenges specific to point cloud geometry. Point clouds as an input modality present a unique set of challenges when building a network architecture. As an example, the convolution operator -now ubiquitous in image-processing pipelines -requires the signal (in our case, geometry) to be defined on top of an underlying grid-like structure. Such a structure is not available in raw point clouds, which renders them significantly more difficult to encode than e.g. images or voxel grids. Recent classification work on point clouds (PointNet -Qi et al. (2016a) ) bypasses this issue by circumventing 2D convolutions. Another issue with point clouds as a representation is that they are unordered -any permutation of a point set still describes the same shape. This complicates comparisons between two point sets, typically needed to define a loss function. This unorderedness of point clouds also creates the need for making the encoded feature permutation invariant.Point-set distances. Two permutation-invariant metrics for comparing unordered point sets have been proposed in the literature (Fan et al., 2016) . On the one hand, the Earth Mover's distance (EMD) BID21 is the solution of a transportation problem which attempts to transform one set to the other. For two equally sized subsets S 1 ⊆ R 3 , S 2 ⊆ R 3 , their EMD is defined by d EM D (S 1 , S 2 ) = min φ:S1→S2 x∈S1x − φ(x) 2 where φ is a bijection. Interpreted as a loss, EMD is differentiable almost everywhere. On the other hand, the Chamfer (pseudo)-distance (CD) measures the squared distance between each point in one set to its nearest neighbor in the other set: DISPLAYFORM4 It is still differentiable but more computationally efficient.Evaluation Metrics for representations and generative models. In the remainder of the paper, we will frequently need to compare a given set (distribution) of points clouds, whether reconstructed or synthesized, to its ground truth counterpart. For example, one might want to assess the quality of a representation model, in terms of how well it matches the training set or a held-out test set. Such a comparison might be done to evaluate the faithfulness and/or diversity of a generative model, and measure potential mode-collapse. To measure how well a point-cloud distribution A matches a ground truth distribution G, we use the following metrics:Coverage. For each point-cloud in A we find its closest neighbor in G; closeness can be computed using either CD or EMD, thus yielding two different metrics, COV-CD and COV-EMD. Coverage is measured as the fraction of the point-clouds in G that were matched to point-clouds in A. A high coverage score typically indicates that most of G is roughly represented within A. We presented a novel set of architectures for 3D point-cloud representation learning and generation. Our results show good generalization to unseen data and our representations encode meaningful semantics. In particular our generative models are able to produce faithful samples and cover most of the ground truth distribution without memorizing a few examples. Interestingly, we see that the best-performing generative model in our experiments is a GMM trained in the fixed latent space of an AE. While, this might not be a universal result, it suggests that simple classic tools should not be dismissed. A thorough investigation on the conditions under which simple latent GMMs are as powerful as adversarially trained models would be of significant interest.",Deep autoencoders to learn a good representation for geometric 3D point-cloud data; Generative models for point clouds.,Yi et al. ; Chamfer ; two ; Kinect ; ∼ p z ; the Earth Mover's ; al. ; EMD ; SVM ; iPhone,its nearest neighbor ; bypasses ; new objects ; Such a comparison ; them ; the deep learning community ; the classical GAN formulation ; a code ; the parameters ; effort,Yi et al. ; Chamfer ; two ; Kinect ; ∼ p z ; the Earth Mover's ; al. ; EMD ; SVM ; iPhone,"Three-dimensional geometric data offer an excellent domain for studying representation learning and generative modeling. In this paper, we introduce a deep autoencoder (AE) network with excellent reconstruction quality and generalization ability. The learned representations outperform the state of the art in 3D recognition tasks and enable basic shape editing applications via simple algebraic manipulations, such as semantic part editing, shape analogies and shape interpolation. We also perform a thorough study of different generative models, including GANs operating on raw point clouds, Gaussian mixture models (GMM) trained in the latent space of AEs and Gaussian",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Heuristic search research often deals with finding algorithms for offline planning which aim to minimize the number of expanded nodes or planning time. In online planning, algorithms for real-time search or deadline-aware search have been considered before. However, in this paper, we are interested in the problem of {\em situated temporal planning} in which an agent's plan can depend on exogenous events in the external world, and thus it becomes important to take the passage of time into account during the planning process.  
 Previous work on situated temporal planning has proposed simple pruning strategies, as well as complex schemes for a simplified version of the associated metareasoning problem. 
 In this paper, we propose a simple metareasoning technique,  called the crude greedy scheme, which can be applied in a situated temporal planner. Our empirical evaluation shows that the crude greedy scheme outperforms standard heuristic search based on cost-to-go estimates. For many years, research in heuristic search has focused on the objective of minimizing the number of nodes expanded during search (e.g BID7 ). While this is the right objective under various scenarios, there are various scenarios where it is not. For example, if we still want an optimal plan but want to minimize search time, selective max BID10 or Rational Lazy A˚ ) can be used. Other work has dealt with finding a boundedly suboptimal plan as quickly as possible BID20 , or with finding any solution as quickly as possible BID21 . Departing from this paradigm even more, in motion planning the setting is that edge-cost evaluations are the most expensive operation, requiring different search algorithms BID14 BID11 .While the settings and objectives mentioned above are quite different from each other, they are all forms of offline planning. Addressing online planning raises a new set of objectives and scenarios. For example , in real-time search, an agent must interleave planning and execution, requiring still different search algorithms BID13 BID18 BID6 BID5 . Deadline-aware search BID9 must find a plan within some deadline. The BUGSY planner BID1 attempts to optimize the utility of a plan, which depends on both plan quality and search time.In this paper we are concerned with a recent setting, called situated temporal planning BID2 . Situated temporal planning addresses a problem where planning happens online, in the presence of external temporal constraints such as deadlines. In situated temporal planning, a plan must be found quickly enough that it is possible to execute that plan after planning completes. Situated temporal planning is inspired by the planning problem a robot faces when it has to replan BID3 , but the problem statement is independent of this motivation.The first planner to address situated temporal planning BID2 ) used temporal reasoning BID8 prune search nodes for which it is provably too late to start execution. It also used estimates of remaining search time BID9 together with information from the temporal relaxed planning graph BID4 ) to estimate whether a given search node is likely to be timely, meaning that it is likely to lead to a solution which will be executable when planning finishes. It also used dual open lists : one only for timely nodes, and another one for all nodes (including nodes for which it is likely too late to start execution). However, the planner still used standard heuristic search algorithms (GBFS or Weighted A˚) with these open lists, while noting that this is the wrong thing to do, and leaving for future work finding the right search control rules.Inspired by this problem, a recent paper BID19 proposed a rational metareasoning BID17 approach for a simplified version of the problem faced by the situated planner. The problem was simplified in several ways: first, the paper addressed a one-shot version of the metareasoning problem, and second, the paper assumed distributions on the remaining search time and on the deadline for each node are known. The paper then formulated the metareasoning problem as an MDP, with the objective of maximizing the probability of finding a timely plan, and showed that it is intractable. It also gave a greedy decision rule, which worked well in an empirical evaluation with various types of distributions.In this paper, we explore using such a metareasoning approach as an integrated part of a situated temporal planner.This involves addressing the two simplifications described above. The naive way of addressing the first simplification -the one-shot nature of the greedy rule -is to apply it at every expansion decision the underlying heuristic search algorithm makes, in order to choose which node from the open list to expand. The problem with this approach is that the number of nodes on the open list grows very quickly (typically exponentially), and so even a linear time metareasoning algorithm would incur too much overhead. Thus, we introduce an even simpler decision rule, which we call the crude greedy scheme, which does not require access to the distributions, but only to their estimated means. Additionally, the crude greedy scheme allows us to compute one number for each node,Q, and expand nodes with a highQvalue first. This allows us to use a regular open list, although one that is not sorted according to cost-to-go estimates, as in standard heuristic search. In fact, as we will see, cost-to-go estimates play no role in the ordering criterion at all.An empirical evaluation on a set of problems from the Robocup Logistics League (RCLL) domain BID16 BID15 shows that using the crude greedy scheme in the situated temporal planner BID2 leads to a timely solution of significantly more problems than using standard heuristic search, even with pruning late nodes and dual open lists. Next, we briefly survey the main results of the metareasoning paper BID19 , and then describe how we derive the crude greedy decision rule, and conclude with an empirical evaluation that demonstrates its efficacy. In this paper, we have provided the first practical metareasoning approach for situated temporal planning. We showed empirically that this approach outperforms standard heuristic search based on cost-to-go estimates. Nevertheless, the temporal relaxed planning graph BID4 ) serves an important purpose here, allowing us to estimate both remaining planning time and the deadline for a node. Thus, we believe our results suggest that cost-to-go estimates are not as important for situated temporal planning as they are for minimizing the number of expanded nodes or planning time as in classical heuristic search.The metareasoning scheme we provided is a crude version of the greedy scheme of BID19 . We introduced approximations in order to make the metareasoning sufficiently fast and in order to utilize only readily available information generated during the search. We also proposed a more refined and better theoretically justified version of the algorithm ('improved greedy'), but making the improved version applicable in the planner is a non-trivial challenge that forms part of our future research.Ongoing Work: Crude version of the improved greedy schemeThe improved greedy scheme is better justified, but has an additional term where we need the complete distribution (f 1 pt, t d q is needed, rather than just the expectation ErD i s).We would like to replace this distribution with a small number of parameters than can be easier to obtain. Basically the same considerations apply here as well, except that the the term involving f 1 i requires access to the full distributions m i , D i . Given specific distribution types, it may be possible to compute this term as a function of ErD i s and e i . However, this part of the work is still in progress and at present we are not sure what parameters we can obtain during the search that would support the improved scheme.",Metareasoning in a Situated Temporal Planner,two ; one ; BUGSY ; MDP ; the Robocup Logistics League ; Weighted ; first ; second ; Rational Lazy ; ErD,the metareasoning paper ; one number ; Our empirical evaluation ; both remaining planning time ; any solution ; scenarios ; order ; the probability ; the underlying heuristic search algorithm ; the objective,two ; one ; BUGSY ; MDP ; the Robocup Logistics League ; Weighted ; first ; second ; Rational Lazy ; ErD,"Heuristic search research focuses on offline planning algorithms which minimize the number of expanded nodes or planning time. In online planning, algorithms for real-time search or deadline-aware search have been considered before. However, in situated temporal planning, an agent's plan can depend on exogenous events in the external world, and it becomes important to take the passage of time into account during the planning process. The BUGSY planner BID1 proposes simple pruning strategies and complex schemes for a simplified version of the associated metareasoning problem. The crude greedy scheme outperforms standard heuristic search based on cost-to-go estimates",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Partial differential equations (PDEs)  play a prominent role in many disciplines such as applied mathematics, physics, chemistry, material science, computer science, etc. PDEs are commonly derived based on physical laws or empirical observations. However, the governing equations for many complex systems in modern applications are still not fully known. With the rapid development of sensors, computational power, and data storage in the past decade, huge quantities of data can be easily collected and efficiently stored. Such vast quantity of data offers new opportunities for data-driven discovery of hidden physical laws. Inspired by the latest development of neural network designs in deep learning, we propose a new feed-forward deep network, called PDE-Net, to fulfill two objectives at the same time: to accurately predict dynamics of complex systems and to uncover the underlying hidden PDE models. The basic idea of the proposed PDE-Net is to learn differential operators by learning convolution kernels (filters), and apply neural networks or other machine learning methods to approximate the unknown nonlinear responses. Comparing with existing approaches, which either assume the form of the nonlinear response is known or fix certain finite difference approximations of differential operators, our approach has the most flexibility by learning both differential operators and the nonlinear responses. A special feature of the proposed PDE-Net is that all filters are properly constrained, which enables us to easily identify the governing PDE models while still maintaining the expressive and predictive power of the network. These constrains are carefully designed by fully exploiting the relation between the orders of differential operators and the orders of sum rules of filters (an important concept originated from wavelet theory). We also discuss relations of the PDE-Net with some existing networks in computer vision such as Network-In-Network (NIN) and Residual Neural Network (ResNet). Numerical experiments show that the PDE-Net has the potential to uncover the hidden PDE of the observed dynamics, and predict the dynamical behavior for a relatively long time, even in a noisy environment. Differential equations, especially partial differential equations(PDEs), play a prominent role in many disciplines to describe the governing physical laws underlying a given system of interest. Traditionally, PDEs are derived based on simple physical principles such as conservation laws, minimum energy principles, or based on empirical observations. Important examples include the NavierStokes equations in fluid dynamics, the Maxwell's equations for electromagnetic propagation, and the Schrödinger's equations in quantum mechanics. However, many complex systems in modern applications (such as many problems in climate science, neuroscience, finance, etc.) still have eluded mechanisms, and the governing equations of these systems are only partially known. With the rapid development of sensors, computational power, and data storage in the last decade, huge quantities of data can be easily collected and efficiently stored . Such vast quantity of data offers new opportunities for data-driven discovery of potentially new physical laws. Then, one may ask the following interesting and intriguing question: can we learn a PDE model (if there exists one) from a given data set and perform accurate and efficient predictions using the learned model?One of earlier attempts on data-driven discovery of hidden physical laws is by BID0 and BID19 . Their main idea is to compare numerical differentiations of the experimental data with analytic derivatives of candidate functions, and apply the symbolic regression and the evolutionary algorithm to determining the nonlinear dynamical system. Recently , BID1 , BID18 , BID17 and BID21 propose an alternative approach using sparse regression. They construct a dictionary of simple functions and partial derivatives that are likely to appear in the unknown governing equations. Then, they take advantage of sparsity promoting techniques to select candidates that most accurately represent the data. When the form of the nonlinear response of a PDE is known, except for some scalar parameters, presented a framework to learn these unknown parameters by introducing regularity between two consecutive time step using Gaussian process. More recently, introduced a new class of universal function approximators called the physics informed neural networks which is capable of discovering nonlinear PDEs parameterized by scalars.These recent work greatly advanced the progress of the problem. However, symbolic regression is expensive and does not scale very well to large systems. The sparse regression method requires to fix certain numerical approximations of the spatial differentiations in the dictionary beforehand, which limits the expressive and predictive power of the dictionary. Although the framework presented by ; is able to learn hidden physical laws using less data than the approach of sparse regression, the explicit form of the PDEs are assumed to be known except for a few scalar learnable parameters. Therefore, extracting governing equations from data in a less restrictive setting remains a great challenge.The main objective of this paper is to accurately predict the dynamics of complex systems and to uncover the underlying hidden PDE models (should they exist) at the same time, with minimal prior knowledge on the systems. Our inspiration comes from the latest development of deep learning techniques in computer vision. An interesting fact is that some popular networks in computer vision, such as ResNet BID9 b) , have close relationship with PDEs BID4 BID6 BID8 BID20 BID13 . Furthermore, the deeper is the network , the more expressive power the network possesses, which may enable us to learn more complex dynamics arose from fields other than computer vision. However, existing deep networks designed in deep learning mostly emphasis on expressive power and prediction accuracy. These networks are not transparent enough to be able to reveal the underlying PDE models, although they may perfectly fit the observed data and perform accurate predictions. Therefore, we need to carefully design the network by combining knowledge from deep learning and applied mathematics so that we can learn the governing PDEs of the dynamics and make accurate predictions at the same time. Note that our work is closely related to BID4 where the authors designed their network based on discretization of quasilinear parabolic equations. However, it is not clear if the dynamics of image denoising has to be governed by PDEs, nor did the authors attempt to recover the PDE (should there exists one).In this paper, we design a deep feed-forward network , named PDE-Net, based on the following generic nonlinear evolution PDE DISPLAYFORM0 The objective of the PDE-Net is to learn the form of the nonlinear response F and to perform accurate predictions. Unlike the existing work, the proposed network only requires minor knowledge on the form of the nonlinear response function F , and requires no knowledge on the involved differential operators (except for their maximum possible order) and their associated discrete approximations. The nonlinear response function F can be learned using neural networks or other machine learning methods, while discrete approximations of the differential operators are learned using convolution kernels (i.e. filters) jointly with the learning of the response function F . If we have a prior knowledge on the form of the response function F , we can easily adjust the network architecture by taking advantage of the additional information. This may simplify the training and improve the results. We will also discuss relations of the PDE-Net to some existing networks in computer vision such as Network-In-Network (NIN) and ResNet. Details are given in Section 2.In Section 3 and Section 4, we conduct numerical experiments on a linear PDE (convection-diffusion equation) and a nonlinear PDE (convection-diffusion equation with a nonlinear source). We generate data set for each PDE using high precision numerical methods and add Gaussian noise to mimic real situations. Our numerical results show that the PDE-Net can uncover the hidden equations of the observed dynamics, and can predict the dynamical behavior for a relatively long time, even in a noisy environment.A particular novelty of our approach is that we impose appropriate constraints on the learnable filters in order to easily identify the governing PDE models while still maintaining the expressive and pre-dictive power of the network. This makes our approach different from existing deep convolutional networks which mostly emphasis on the prediction accuracy of the networks, as well as all the existing approaches of learning PDEs from data which assume either the form of the response function is known or have fixed approximations of the differential operators. In other words, our proposed approach not only has vast flexibility in fitting observed dynamics and is able to accurately predict its future behavior, but is also able to reveal the hidden equations driving the observed dynamics. The constraints on the filters are motivated by the earlier work of BID2 ; where general relations between wavelet frame transforms and differential operators were established. In particular, it was observed in that we can relate filters and finite difference approximation of differential operators by examining the orders of sum rules of the filters (an important concept in wavelet theory and closely related to vanishing moments of wavelet functions). These constraints on the filters may also be useful in network designs for machine learning tasks in computer vision.2 PDE-NET: A FLEXIBLE DEEP ARCHTECTURE TO LEARN PDES FROM DATA Given a series of measurements of some physical quantities {u(t, ·) : DISPLAYFORM1 : Ω → R, we want to discover the governing PDEs of the data. We assume that the observed data are associated with a PDE that takes the following general form: DISPLAYFORM2 (1) Our objective is to design a feed-forward network, named the PDE-Net, that approximates the PDE (1) in the way that: 1) we can predict the dynamical behavior of the equation for as long time as possible ; 2) we are able to reveal the form of the response function F and the differential operators involved. There are two main components of the PDE-Net that are combined together in the same network : one is automatic determination on the differential operators involved in the PDE and their discrete approximations; the other is to approximate the nonlinear response function F . In this section, we start with discussions on the relation between convolutions and differentiations in discrete setting. This section presents numerical results of training the PDE-Net using the data set described in the previous subsection. We will specifically observe how the learned PDE-Net performs in terms of prediction of dynamical behavior and identification of the underlying PDE model. Furthermore, we will investigate the effects of some of the hyper-parameters (e.g. size of the filters, number of δt-blocks) on the learned PDE-Net. This section presents numerical results of the trained PDE-Net using the data set described in Section 4.1. We will observe how the trained PDE-Net performs in terms of prediction of dynamical behavior and identification of the underlying PDE model. In this paper, we designed a deep feed-forward network, called the PDE-Net, to discover the hidden PDE model from the observed dynamics and to predict the dynamical behavior. The PDE-Net consists of two major components which are jointly trained: to approximate differential operations by convolutions with properly constrained filters, and to approximate the nonlinear response by deep neural networks or other machine learning methods. The PDE-Net is suitable for learning PDEs as general as in (1). However, if we have a prior knowledge on the form of the response function F , we can easily adjust the network architecture by taking advantage of the additional information. This may simplify the training and improve the results. As an example, we considered a linear variable-coefficient convection-diffusion equation. The results show that the PDE-Net can uncover the hidden equation of the observed dynamics, and predict the dynamical behavior for a relatively long time, even in a noisy environment. Furthermore, having deep structure (i.e. multiple δt-blocks) and larger learnable filters can improve the PDE-Net in terms of stability and can prolong reliable predictions. As part of the future work, we will try the proposed framework on real data sets. One of the important directions is to uncover hidden variables which cannot be measured by sensors directly, such as in data assimilation. Another interesting direction which is worth exploring is to learn stable and consistent numerical schemes for a given PDE model based on the architecture of the PDE-Net.","This paper proposes a new feed-forward network, call PDE-Net, to learn PDEs from data.",NIN ; Schrödinger ; two ; Gaussian ; quantum mechanics ; the last decade ; PDE ; F ; Maxwell ; Residual Neural Network,vast flexibility ; as long time ; the hidden PDE ; network designs ; number ; analytic derivatives ; regularity ; simple physical principles ; Another interesting direction ; the same network,NIN ; Schrödinger ; two ; Gaussian ; quantum mechanics ; the last decade ; PDE ; F ; Maxwell ; Residual Neural Network,"Partial differential equations (PDEs) play a prominent role in many disciplines, such as applied mathematics, physics, chemistry, material science, computer science, etc. PDEs are derived from physical laws or empirical observations. However, the governing equations for complex systems in modern applications are still not fully known. With the rapid development of sensors, computational power, and data storage, huge quantities of data can be easily collected and efficiently stored. The PDE-Net is designed to learn differential operators by learning convolution kernels (filters), and apply neural networks or machine learning methods to approximate unknown nonlinear responses. The basic idea",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Designing a convolution for a spherical neural network requires a delicate tradeoff between efficiency and rotation equivariance. DeepSphere, a method based on a graph representation of the discretized sphere, strikes a controllable balance between these two desiderata. This contribution is twofold. First, we study both theoretically and empirically how equivariance is affected by the underlying graph with respect to the number of pixels and neighbors. Second, we evaluate DeepSphere on relevant problems. Experiments show state-of-the-art performance and demonstrates the efficiency and flexibility of this formulation. Perhaps surprisingly, comparison with previous work suggests that anisotropic filters might be an unnecessary price to pay. Spherical data is found in many applications (figure 1). Planetary data (such as meteorological or geological measurements) and brain activity are example of intrinsically spherical data. The observation of the universe, LIDAR scans, and the digitalization of 3D objects are examples of projections due to observation. Labels or variables are often to be inferred from them. Examples are the inference of cosmological parameters from the distribution of mass in the universe , the segmentation of omnidirectional images (Khasanova & Frossard, 2017) , and the segmentation of cyclones from Earth observation (Mudigonda et al., 2017) . 2 A rigid full-sphere sampling is not ideal: brain activity is only measured on the scalp, the Milky Way's galactic plane masks observations, climate scientists desire a variable resolution, and the position of weather stations is arbitrary and changes over time. (e) Graphs can faithfully and efficiently represent sampled spherical data by placing vertices where it matters. As neural networks (NNs) have proved to be great tools for inference, variants have been developed to handle spherical data. Exploiting the locally Euclidean property of the sphere, early attempts used standard 2D convolutions on a grid sampling of the sphere (Boomsma & Frellsen, 2017; Su & Grauman, 2017; Coors et al., 2018) . While simple and efficient, those convolutions are not equivariant to rotations. On the other side of this tradeoff, Cohen et al. (2018) and Esteves et al. (2018) proposed to 2 METHOD DeepSphere leverages graph convolutions to achieve the following properties: (i) computational efficiency, (ii) sampling flexibility, and (iii) rotation equivariance (section 3). The main idea is to model the sampled sphere as a graph of connected pixels: the length of the shortest path between two pixels is an approximation of the geodesic distance between them. We use the graph CNN formulation introduced in (Defferrard et al., 2016 ) and a pooling strategy that exploits hierarchical samplings of the sphere. (ii) that a larger architecture can compensate for the lack of generality. We indeed observed that more feature maps and depth led to higher performance (section C.3). This work showed that DeepSphere strikes an interesting, and we think currently optimal, balance between desiderata for a spherical CNN. A single parameter, the number of neighbors k a pixel is connected to in the graph, controls the tradeoff between cost and equivariance (which is linked to performance). As computational cost and memory consumption scales linearly with the number of pixels, DeepSphere scales to spherical maps made of millions of pixels, a required resolution to faithfully represent cosmological and climate data. Also relevant in scientific applications is the flexibility offered by a graph representation (for partial coverage, missing data, and non-uniform samplings). Finally, the implementation of the graph convolution is straightforward, and the ubiquity of graph neural networks -pushing for their first-class support in DL frameworks -will make implementations even easier and more efficient. A potential drawback of graph Laplacian-based approaches is the isotropy of graph filters, reducing in principle the expressive power of the NN. Experiments from Cohen et al. (2019) and Boscaini et al. (2016) indeed suggest that more general convolutions achieve better performance. Our experiments on 3D shapes (section 4.1) and climate (section 4.3) however show that DeepSphere's isotropic filters do not hinder performance. Possible explanations for this discrepancy are that NNs somehow compensate for the lack of anisotropic filters, or that some tasks can be solved with isotropic filters. The distortions induced by the icosahedral projection in (Cohen et al., 2019) or the leakage of curvature information in (Boscaini et al., 2016) might also alter performance. Developing graph convolutions on irregular samplings that respect the geometry of the sphere is another research direction of importance. Practitioners currently interpolate their measurements (coming from arbitrarily positioned weather stations, satellites or telescopes) to regular samplings. This practice either results in a waste of resolution or computational and storage resources. Our ultimate goal is for practitioners to be able to work directly on their measurements, however distributed.",A graph-based spherical CNN that strikes an interesting balance of trade-offs for a wide variety of applications.,al. ; CNN ; Khasanova & Frossard ; Graphs ; DL ; Cohen ; Mudigonda ; Boscaini ; two ; Euclidean,"A potential drawback ; spherical maps ; Experiments ; the expressive power ; the universe, LIDAR scans ; the universe ; desiderata ; the geodesic distance ; This work ; comparison",al. ; CNN ; Khasanova & Frossard ; Graphs ; DL ; Cohen ; Mudigonda ; Boscaini ; two ; Euclidean,"DeepSphere, a neural network based on a graph representation of the discretized sphere, achieves a controllable balance between efficiency and rotation equivariance. The main contribution is twofold. Firstly, we study both theoretically and empirically how the underlying graph is affected by the number of pixels and neighbors. Secondly, we evaluate the performance of DeepSphere on relevant problems, demonstrating the efficiency and flexibility of this formulation. Spherical data is found in many applications. Planetary data and brain activity are examples of intrinsically spherical data. The observation of the universe, LIDAR scans, and the digitalization of 3D objects are examples",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"The embedding layers transforming input words into real vectors are the key components of deep neural networks used in natural language processing. However, when the vocabulary is large, the corresponding weight matrices can be enormous, which precludes their deployment in a limited resource setting. We introduce a novel way of parametrizing embedding layers based on the Tensor Train (TT) decomposition, which allows compressing the model significantly at the cost of a negligible drop or even a slight gain in performance.   We evaluate our method on a wide range of benchmarks in natural language processing and analyze the trade-off between performance and compression ratios for a wide range of architectures, from MLPs to LSTMs and Transformers. Deep neural networks (DNNs) typically used in natural language processing (NLP) employ large embeddings layers, which map the input words into continuous representations and usually have the form of lookup tables. Despite such simplicity and, arguably because of it, the resulting models are cumbersome, which may cause problems in training and deploying them in a limited resource setting. Thus, the compression of large neural networks and the development of novel lightweight architectures have become essential problems in NLP research. One way to reduce the number of parameters in the trained model is to imply a specific structure on its weight matrices (e.g., assume that they are low-rank or can be well approximated by low-rank tensor networks). Such approaches are successful at compressing the pre-trained models, but they do not facilitate the training itself. Furthermore, they usually require an additional fine-tuning stage to recover the performance of the original model. In this paper, we introduce a new, parameter efficient embedding layer, termed TT-embedding, which can be plugged in into any model and trained end-to-end. The benefits of our compressed TT-layer are twofold. Firstly, instead of storing huge embedding matrix, we store a sequence of much smaller 2-dimensional and 3-dimensional tensors, necessary for reconstructing the required embeddings, which allows compressing the model significantly at the cost of a negligible performance drop. Secondly, the overall number of parameters can be relatively small (and constant) during the whole training stage, which allows to use larger batches or train efficiently in a case of limited resources. To validate the efficiency of the proposed approach, we have tested it on several popular NLP tasks. In our experiments, we have observed that the standard embeddings can be replaced by TT-embeddings with the compression ratio of 1 − 3 orders without any significant drop (and sometimes even with a slight gain) of the metric of interest. Specifically, we report the following compression ratios of the embedding layers: 441 on the IMDB dataset with 0.2% absolute increase in classification accuracy; 15 on the WMT 2014 En-De dataset with 0.3 drop in the BLEU score. Additionally, we have also evaluated our algorithm on a task of binary classification based on a large number of categorical features. More concretely, we applied TT-embedding to the click through rate (CTR) prediction problem, a crucial task in the field of digital advertising. Neural networks, typically used for solving this problem, while being rather elementary, include a large number of embedding layers of significant size. As a result, a majority of model parameters that represent these layers, may occupy hundreds of gigabytes of space. We show that TT-embedding not only considerably reduces the number of parameters in such models, but also sometimes improves their accuracy. We propose a novel embedding layer, the TT-embedding, for compressing huge lookup tables used for encoding categorical features of significant cardinality, such as the index of a token in natural language processing tasks. The proposed approach, based on the TT-decomposition, experimentally proved to be effective, as it heavily decreases the number of training parameters at the cost of a small deterioration in performance. In addition, our method can be easily integrated into any deep learning framework and trained via backpropagation, while capitalizing on reduced memory requirements and increased training batch size. Our experimental results suggest several appealing directions for future work. First of all, TTembeddings impose a concrete tensorial low-rank structure on the embedding matrix, which was shown to improve the generalization ability of the networks acting as a regularizer. The properties and conditions of applicability of this regularizer are subject to more rigorous analysis. Secondly, unlike standard embedding, we can introduce non-linearity into TT-cores to improve their expressive power (Khrulkov et al., 2019) . Additionally, it is important to understand how the order of tokens in the vocabulary affects the properties of the networks with TT-embedding. We hypothesize that there exists the optimal order of tokens which better exploits the particular structure of TT-embedding and leads to a boost in performance and/or compression ratio. Finally, the idea of applying higher-order tensor decompositions to reduce the number of parameters in neural nets is complementary to more traditional methods such as pruning (Han et al., 2015) and quantization (Hubara et al., 2017; Xu et al., 2018) . Thus, it would be interesting to make a thorough comparison of all these methods and investigate whether their combination may lead to even stronger compression.",Embedding layers are factorized with Tensor Train decomposition to reduce their memory footprint.,al. ; Secondly ; BLEU ; First ; Han ; Firstly ; CTR ; Hubara et al. ; hundreds ; NLP,linearity ; increased training batch size ; MLPs ; tokens ; the development ; low-rank tensor networks ; the index ; this problem ; significant cardinality ; the input words,al. ; Secondly ; BLEU ; First ; Han ; Firstly ; CTR ; Hubara et al. ; hundreds ; NLP,"The embedding layers transforming input words into real vectors are crucial components of deep neural networks used in natural language processing. However, when the vocabulary is large, the corresponding weight matrices can be enormous, which precludes their deployment in a limited resource setting. The Tensor Train (TT) decomposition allows compressing the model significantly at the cost of a negligible drop or even a slight gain in performance. We evaluate performance and compression ratios for a wide range of architectures, from MLPs to LSTMs and Transformers. Deep neural networks (DNNs) typically used in NLP employ large embeddings layers, which map",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"The problem of accelerating drug discovery relies heavily on automatic tools to optimize precursor molecules to afford them with better biochemical properties. Our work in this paper substantially extends prior state-of-the-art on graph-to-graph translation methods for molecular optimization. In particular, we realize coherent multi-resolution representations by interweaving the encoding of substructure components with the atom-level encoding of the original molecular graph. Moreover, our graph decoder is fully autoregressive, and interleaves each step of adding a new substructure with the process of resolving its attachment to the emerging molecule. We evaluate our model on multiple molecular optimization tasks and show that our model significantly outperforms previous state-of-the-art baselines. Molecular optimization seeks to modify compounds in order to improve their biochemical properties. This task can be formulated as a graph-to-graph translation problem analogous to machine translation. Given a corpus of molecular pairs {(X, Y )}, where Y is a paraphrase of X with better chemical properties, the model is trained to translate an input molecular graph into its better form. The task is difficult since the space of potential candidates is vast, and molecular properties can be complex functions of structural features. Moreover, graph generation is computationally challenging due to complex dependencies involved in the joint distribution over nodes and edges. Similar to machine translation, success in this task is predicated on the inductive biases built into the encoder-decoder architecture, in particular the process of generating molecular graphs. Prior work (Jin et al., 2019) proposed a junction tree encoder-decoder that utilized valid chemical substructures (e.g., aromatic rings) as building blocks to generate graphs. Each molecule was represented as a junction tree over chemical substructures in addition to the original atom-level graph. While successful, the approach remains limited in several ways. The tree and graph encoding were carried out separately, and decoding proceeded in strictly successive steps: first generating the junction tree for the new molecule, and then attaching its substructures together. This means the predicted attachments do not impact the subsequent substructure choices (see Figure 1a) . Moreover, the attachment prediction process is non-autoregressive, thus it can predict inconsistent substructure attachments across different nodes in the junction tree (see Figure 1b ). We propose a multi-resolution, hierarchically coupled encoder-decoder for graph generation. Our auto-regressive decoder interleaves the prediction of substructure components with their attachments to the molecule being generated. In particular, a target graph is unraveled as a sequence of triplet predictions (where to expand the graph, new substructure type, its attachment). This enables us to model strong dependencies between successive attachments and substructure choices. The encoder is designed to represent molecules at different resolutions in order to match the proposed decoding process. Specifically, the encoding of each molecule proceeds across three levels, with each layer capturing essential information for its corresponding decoding step. The graph convolution of atoms at the lowest level supports the prediction of attachments and the convolution over substructures at the highest level supports the prediction of successive substructures. Compared to prior work, our decoding process is much more efficient because it decomposes each generation step into a hierarchy of smaller steps in order to avoid combinatorial explosion. We also extend the method to handle conditional translation where desired criteria are fed as input to the translation process. This enables our method to handle different combinations of criteria at test time. Since their tree and graph decoders are isolated, the model can generate invalid junction trees which cannot be assembled into any molecule. This problem can be solved when we interleave the tree and graph decoding steps, allowing the predicted attachments to guide the substructure prediction; b) Their non-autoregressive graph decoder often predicts inconsistent local substructure attachments during training. To this end, we propose an autoregressive decoder that interleaves the prediction of substructures with their attachments. We evaluate our new model on multiple molecular optimization tasks. Our baselines include previous state-of-the-art graph generation methods (You et al., 2018a; Liu et al., 2018; Jin et al., 2019) and an atom-based translation model we implemented for a more comprehensive comparison. Our model significantly outperforms these methods in discovering molecules with desired properties, yielding 3.3% and 8.1% improvement on QED and DRD2 optimization tasks. During decoding, our model runs 6.3 times faster than previous substructure-based generation methods. We further conduct ablation studies to validate the advantage of our hierarchical decoding and multi-resolution encoding. Finally, we show that conditional translation can succeed (generalize) even when trained on molecular pairs with only 1.6% of them having desired target property combination. In this paper, we developed a hierarchical graph-to-graph translation model that generates molecular graphs using chemical substructures as building blocks. In contrast to previous work, our model is fully autoregressive and learns coherent multi-resolution representations. The experimental results show that our method outperforms previous models under various settings. A ADDITIONAL FIGURES The message passing network MPN ψ (H, {x u }, {x uv }) over graph H is defined as: Algorithm 3 LSTM MPN with T message passing iterations wu } w∈N (u)\v for all edges (u, v) ∈ H simultaneously. end for Return node representations end function Attention Layer Our attention layer is a bilinear attention function with parameter θ = {A θ }: Figure 7: Illustration of AtomG2G decoding process. Atoms marked with red circles are frontier nodes in the queue Q. In each step, the model picks the first node v t from Q and predict whether there will be new atoms attached to v t . If so, it predicts the atom type of new node u t (atom prediction). Then the model predicts the bond type between u t and other nodes in Q sequentially for |Q| steps (bond prediction, |Q| = 2). Finally, it adds the new atom to the queue Q. AtomG2G Architecture AtomG2G is an atom-based translation method that is directly comparable to HierG2G. Here molecules are represented solely as molecular graphs rather than a hierarchical graph with substructures. Table 3 : Training set size and substructure vocabulary size for each dataset.","We propose a multi-resolution, hierarchically coupled encoder-decoder for graph-to-graph translation.",Jin et al. ; first ; three ; fed ; al. ; Liu et al. ; Jin ; QED ; MPN,multi-resolution encoding ; training ; better chemical properties ; previous models ; the encoder-decoder architecture ; a junction tree ; their tree ; our decoding process ; test time ; the space,Jin et al. ; first ; three ; fed ; al. ; Liu et al. ; Jin ; QED ; MPN,"The problem of accelerating drug discovery relies heavily on automatic tools to optimize precursor molecules to afford them with better biochemical properties. Our graph-to-graph translation method is fully autoregressive, and interleaves each step of adding a new substructure with the process of resolving its attachment to the emerging molecule. Molecular optimization aims to modify compounds in order to improve their biochemical properties, and this task can be formulated as a graph-tgraph translation problem analogous to machine translation. The task is computationally challenging due to complex dependencies involved in joint distribution over nodes and edges. However, success in this task is predicated on inductive biases",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"This work seeks the possibility of generating the human face from voice solely based on the audio-visual data without any human-labeled annotations. To this end, we propose a multi-modal learning framework that links the inference stage and generation stage. First, the inference networks are trained to match the speaker identity between the two different modalities. Then the pre-trained inference networks cooperate with the generation network by giving conditional information about the voice. Utilizing audio-visual cues together to recognize a person's identity has been studied in various fields from neuroscience (Hasan et al., 2016; Tsantani et al., 2019) to practical machine learning applications (Nagrani et al., 2018b; a; Wen et al., 2019a; Shon et al., 2019) . For example, some neurological studies have found that in some cortical areas, humans recognize familiar individuals by combining signals from several modalities, such as faces and voices (Hasan et al., 2016) . In conjunction with the neurological studies, it is also a well known fact that a human speech production system is directly related to the shape of the vocal tract (Mermelstein, 1967; Teager & Teager, 1990) . Inspired by the aforementioned scientific evidence, we would like to ask three related questions from the perspective of machine learning: 1) Is it possible to match the identity of faces and voices? (inference) 2) If so, is it possible to generate a face image from a speech signal? (generation) 3) Can we find the relationship between the two modalities only using cross-modal self-supervision with the data ""in-the-wild""? To answer these questions, we design a two-step approach where the inference and generation stages are trained sequentially. First, the two inference networks for each modality (speech encoder and face encoder) are trained to extract the useful features and to compute the cross-modal identity matching probability. Then the trained inference networks are transferred to the generation stage to pass the information about the speech, which helps the generation network to output the face image from the conditioned speech. We believe, however, that it is impossible to perfectly reconstruct all the attributes in the image of a person's face through the characteristics of the voice alone. This is due to factors that are clearly unrelated to one's voice, such as lighting, glasses, and orientation, that also exist in the natural face image. To reflect the diverse characteristics presented in the face images ""in-the-wild"", we therefore model the generation process by incorporating two latent factors into the neural network. More specifically, we adopted conditional generative adversarial networks (cGANs) (Mirza & Osindero, 2014; so that the generator network can produce a face image that is dependent not only on the paired speech condition, but also on the stochastic variable. This allows the latent factors that contribute to the overall facial attributes to be disentangled into two factors: one that is relevant to the voice and the other that is irrelevant. Adopting cGANs negligently still leaves a few problems. For example, the condition in a cGANs framework is typically provided as embedded conditional vectors through the embedding look-up table for one-hot encoded labels (Brock et al., 2019; . The raw signals such as speech, however, cannot be taken directly from the embedding look-up table, so an encoder module is required. Therefore, the trained speech encoder from the inference step is reused to output a pseudo conditional label that is used to extract meaningful information relevant to the corresponding face. Then the generator and the discriminator are trained in an adversarial way by utilizing the pseudo-embedded conditional vectors obtained from the trained speech encoder in the first step. Another problem with applying the conventional cGANs for generating faces from voice arises from the fact that the distinction between different speakers can be quite subtle, which calls for a need for a more effective conditioning method. To mitigate this problem, we propose a new loss function, relativistic identity cGANs (relidGANs) loss, with modification of the relativistic GANs (JolicoeurMartineau, 2019), allowing us to generate the face with a more distinct identity. Each step will be described in greater detail in Section 3. Our contributions can be summarized as follows: 1. We propose simple but effective end-to-end inference networks trained on audio-visual data without any labels in a self-supervised manner that perform a cross-modal identity matching task. 2. A cGANs-based generation framework is proposed to generate the face from speech, to be seamlessly integrated with the trained networks from inference stage. 3. A new loss function, so called a relidGANs loss, is designed to preserve a more consistent identity between the voices and the generated images. 4. An extensive analysis is conducted on both inference and generation tasks to validate our proposed approaches. In this work, we proposed a cross-modal inference and generation framework that can be trained in a fully self-supervised way. We trained cGANs by transferring the trained networks from the inference stage so that the speech could be successfully encoded as a pseudo conditional embedding. We also proposed relidGANs loss to train the discriminator to penalize negatively paired face and speech so that the generator could produce face images with more distinguished identity between different speakers. As a future work, we would like to address a data bias problem (e.g., ethnicity, gender, age, etc.) that exists in many datasets. This is a significant problem as many publicly available datasets have biased demographic statistics, consequently affecting the results of many algorithms (Buolamwini & Gebru, 2018) . We believe that this can be solved with the use of a better data sampling strategy in an unsupervised manner such as (Amini et al., 2019) . In addition, we would like to expand the proposed methods to various multi-modal datasets by generalizing the proposed concept to other modalities. Real Generated Top-5 nearest face images",This paper proposes a method of end-to-end multi-modal generation of human face from speech based on a self-supervised learning framework.,Brock et al. ; Nagrani ; Shon ; three ; JolicoeurMartineau ; al. ; Mermelstein ; Buolamwini & Gebru ; Teager & Teager ; Hasan et al.,a self-supervised manner ; a new loss function ; practical machine learning applications ; a relidGANs loss ; the shape ; the relationship ; the first step ; one-hot encoded labels ; the pseudo-embedded conditional vectors ; some cortical areas,Brock et al. ; Nagrani ; Shon ; three ; JolicoeurMartineau ; al. ; Mermelstein ; Buolamwini & Gebru ; Teager & Teager ; Hasan et al.,"The multi-modal learning framework is designed to link the inference stage and generation stage. The pre-trained inference networks are trained to match the speaker identity between the two different modalities and cooperate with the generation network by giving conditional information about the voice. The ability to recognize familiar individuals using audio-visual cues has been studied in various fields from neuroscience (Hasan et al., 2016; Tsantani and Shon, 2019) to practical machine learning applications (Nagrani et al, 2018b; a; Wen et al. 2019a; Shon et al.' 2019). In addition to the scientific evidence",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Recent advances in computing technology and sensor design have made it easier to collect longitudinal or time series data from patients, resulting in a gigantic amount of available medical data. Most of the medical time series lack annotations or even when the annotations are available they could be subjective and prone to human errors. Earlier works have developed natural language processing techniques to extract concept annotations and/or clinical narratives from doctor notes. However, these approaches are slow and do not use the accompanying medical time series data. To address this issue, we introduce the problem of concept annotation for the medical time series data, i.e., the task of predicting and localizing medical concepts by using the time series data as input. We propose Relational Multi-Instance Learning (RMIL) - a deep Multi Instance Learning framework based on recurrent neural networks, which uses pooling functions and attention mechanisms for the concept annotation tasks. Empirical results on medical datasets show that our proposed models outperform various multi-instance learning models. Clinicians have limited time (e.g., only a few minutes BID31 ) to study and treat each patient. However, they are overloaded with a lot of patient data from multiple sources and in various formats, such as patient medical history and doctor's notes in free-flowing text, vitals and monitoring data which are captured as time series, and prescriptions and drugs which appear as medical codes including ICD-9 (Organization & Corporation, 1998) , LOINC codes BID21 , etc. This rich information should be summarized and available to clinicians in easily digestible format for faster diagnosis and treatment. Graphical visualizations BID54 are a popular approach to show patient data to doctors. However, recent studies have shown that graphical visualisations are not always helpful for clinicians' decision-making BID46 BID63 . Text summaries on the other hand are widely embraced and are usually adopted in practice BID58 . Most existing systems use natural language processing techniques (Afantenos et al., 2005; BID23 to generate summaries from doctor notes which include test results, discharge reports, observational notes, etc. While these systems are useful, they only use one source of data, i.e., doctor's notes which might have noisy and erroneous entries, for text summarization. On the other hand, electronic health records have other sources of patient data such as vital signs, monitoring sensors, and lab results in the form of multivariate time-series, which can be more accurate and may contain rich information about patient's conditions. Few existing patient summarization systems actually extract information directly from these time series for concept prediction and/or summarization. Generating simple text summaries such as trends from time series has been investigated before BID61 but is marginally useful since these trends are not mapped to the medical concepts which clinicians can quickly comprehend. Recent works BID52 BID16 BID48 BID14 have successfully shown that clinical events and outcomes can be predicted using medical codes or clinical time series data. However, directly obtaining medical concept annotations and summaries from the time series data is still an open question.In this work, we introduce the concept annotation task as the problem of predicting and localizing the medical concepts by modeling the related medical time series data. FIG0 illustrates a concept annotation example where medical time series data such as heart rate, pH and blood gas pressure are given, and the goal is to predict the time series of concepts such as intubation, extubation and resuscitate. To solve concept annotation problem, we formulate it as a Multi-Instance Learning (MIL) problem BID20 and propose a deep learning based framework called Relational MultiInstance Learning (RMIL). RMIL uses Recurrent Neural Networks (RNNs) to model multivariate time series data, leverages instance relations via attention mechanisms, and provides concept predictions using pooling functions.The main contributions of our work are the following. We present a unified view of the MIL approaches for time series data using RNN models with different pooling functions and attention mechanisms. We show that our RMIL model is capable of learning a good classifier for concept detection (bag label predictions) and concept localization tasks (instance label prediction), even though it is only trained using bag labels. We demonstrate that RMIL obtains promising results on real-world medical datasets and outperforms popular MIL approaches.The rest of the paper is structured as follows. In the following section, we briefly discuss the related works. Afterwards, we describe MIL framework and describe how RNN can be combined with multi-instance learning framework to obtain our proposed RMIL. In Sections 4 and 5, we present experimental results and conclusions respectively. In the appendix, we demonstrate anomaly detection as another application of our RMIL framework. We can study the interpretability of concept localization by looking at the localization results of our RMIL models, even though the model is trained without the labels for localization. FIG3 shows the ground truth annotations of two respiratory concepts -intubation and extubation concepts, and the prediction probabilities of these concepts obtained by our RMIL attention-based LSTM models. From figure 2(a ) we can make the following observations, ( i) intubation usually happens before extubation for the same patient, (ii) intubation and extubation could happen on the same day, and (iii) intubation and extubation occur commonly within the first 24 hours of admission. From the figure 2(b), we see that our RMIL attention based LSTM predicts that the probability of intubation happening on the first day of admission is higher (draker gray means higher probability of concept occurrence) and the probability of extubation happening within first day is lower. This indicates that the model has correctly learnt that intubation should appear before extubation. This also implicitly implies that the RMIL attention-based LSTM models have correctly learnt the instance-level relationships from the medical time series data with only bag-level labels.",We propose a deep Multi Instance Learning framework based on recurrent neural networks which uses pooling functions and attention mechanisms for the concept annotation tasks.,two ; al. ; Multi-Instance Learning ; Recurrent Neural Networks ; the same day ; LOINC ; Relational MultiInstance Learning ; the first day ; RMIL ; Clinicians,Sections ; concept detection ; medical concepts ; another application ; various formats ; Corporation ; The main contributions ; blood gas pressure ; concept predictions ; pooling functions,two ; al. ; Multi-Instance Learning ; Recurrent Neural Networks ; the same day ; LOINC ; Relational MultiInstance Learning ; the first day ; RMIL ; Clinicians,"Modern advances in computing technology and sensor design have made it easier to collect longitudinal or time series data from patients, resulting in a huge amount of available medical data. However, most medical time series lack annotations or clinical narratives. Natural language processing techniques to extract concept annotations and clinical narratives from doctor notes are slow and do not use the accompanying medical timeseries data. To address this issue, we propose Relational Multi-Instance Learning (RMIL) - a deep Multi Instance Learning framework based on recurrent neural networks, which uses pooling functions and attention mechanisms for concept annotation tasks. The proposed models outperform various multi-instance learning models",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Imitation learning, followed by reinforcement learning algorithms, is a promising paradigm to solve complex control tasks sample-efficiently. However, learning from demonstrations often suffers from the covariate shift problem, which results
 in cascading errors of the learned policy. We introduce a notion of conservatively extrapolated value functions, which provably lead to policies with self-correction. We design an algorithm Value Iteration with Negative Sampling (VINS) that practically learns such value functions with conservative extrapolation. We show that VINS can correct mistakes of the behavioral cloning policy on simulated robotics benchmark tasks. We also propose the algorithm of using VINS to initialize a reinforcement learning algorithm, which is shown to outperform prior works in sample efficiency. Reinforcement learning (RL) algorithms, especially with sparse rewards, often require a large amount of trial-and-errors. Imitation learning from a small number of demonstrations followed by RL finetuning is a promising paradigm to improve the sample efficiency (Rajeswaran et al., 2017; Večerík et al., 2017; Hester et al., 2018; Nair et al., 2018; Gao et al., 2018) . The key technical challenge of learning from demonstrations is the covariate shift: the distribution of the states visited by the demonstrations often has a low-dimensional support; however, knowledge learned from this distribution may not necessarily transfer to other distributions of interests. This phenomenon applies to both learning the policy and the value function. The policy learned from behavioral cloning has compounding errors after we execute the policy for multiple steps and reach unseen states (Bagnell, 2015; Ross & Bagnell, 2010) . The value function learned from the demonstrations can also extrapolate falsely to unseen states. See Figure 1a for an illustration of the false extrapolation in a toy environment. We develop an algorithm that learns a value function that extrapolates to unseen states more conservatively, as an approach to attack the optimistic extrapolation problem (Fujimoto et al., 2018a) . Consider a state s in the demonstration and its nearby states that is not in the demonstration. The key intuition is thats should have a lower value than s, because otherwises likely should have been visited by the demonstrations in the first place. If a value function has this property for most of the pair (s,s) of this type, the corresponding policy will tend to correct its errors by driving back to the demonstration states because the demonstration states have locally higher values. We formalize the intuition in Section 4 by defining the so-called conservatively-extrapolated value function, which is guaranteed to induce a policy that stays close to the demonstrations states (Theorem 4.4). In Section 5, we design a practical algorithm for learning the conservatively-extrapolated value function by a negative sampling technique inspired by work on learning embeddings Mikolov et al. (2013) ; Gutmann & Hyvärinen (2012) . We also learn a dynamical model by standard supervised learning so that we compute actions by maximizing the values of the predicted next states. This algorithm does not use any additional environment interactions, and we show that it empirically helps correct errors of the behavioral cloning policy. (a) The value function learned from the standard Bellman equation (or supervised learning) on the demonstration states. The value function falsely extrapolates to the unseen states. For example, the top left corner has erroneously the largest value. As a result, once the policy induced by the value function makes a mistake, the error will compound. (b) The conservatively-extrapolated value function (defined in equation (4.2)) learned with negative sampling (VINS, Algorithm 2 in Section 5) . The values at unseen states tend to be lower than their nearby states in the demonstrations, and therefore the corresponding policy tend to correct itself towards the demonstration trajectories. Figure 1: A toy environment where the agent aims to walk from a starting state (the yellow entry) to a goal state (the green entry). The reward is sparse: R(s, a) = −1 unless s is at the goal (which is also the terminal state.) The colors of the entries show the learned value functions. Entries in black edges are states in demonstrations. The cyan arrows show the best actions according to the value functions. When additional environment interactions are available, we use the learned value function and the dynamical model to initialize an RL algorithm. This approach relieves the inefficiency in the prior work (Hester et al., 2018; Nair et al., 2018; Rajeswaran et al., 2017 ) that the randomly-initialized Q functions require a significant amount of time and samples to be warmed up, even though the initial policy already has a non-trivial success rate. Empirically, the proposed algorithm outperforms the prior work in the number of environment interactions needed to achieve near-optimal success rate. In summary, our main contributions are: 1) we formalize the notion of values functions with conservative extrapolation which are proved to induce policies that stay close to demonstration states and achieve near-optimal performances, 2) we propose the algorithm Value Iteration with Negative Sampling (VINS) that outperforms behavioral cloning on three simulated robotics benchmark tasks with sparse rewards, and 3) we show that initializing an RL algorithm from VINS outperforms prior work in sample efficiency on the same set of benchmark tasks. We devise a new algorithm, VINS, that can learn self-correctable by learning value function and dynamical model from demonstrations. The key idea is a theoretical formulation of conservativelyextrapolated value functions that provably leads to self-correction. The empirical results show a promising performance of VINS and an algorithm that initializes RL with VINS. It's a fascinating direction to study other algorithms that may learn conservatively-extrapolated value functions in 11 The standard error in the paper means the standard error of average success rate over 10 (100 for Reach 10) different random seeds by the same algorithm, that is, the standard deviation of 10 numbers over √ 10 (or 10, respectively). 12 The curve for Nair et al.'s only starts after a few thousands steps because the code we use https: //github.com/jangirrishabh/Overcoming-exploration-from-demos only evaluates after the first epoch.","We introduce a notion of conservatively-extrapolated value functions, which provably lead to policies that can self-correct to stay close to the demonstration states, and learn them with a novel negative sampling technique.",Fujimoto ; Mikolov et al ; Gao ; first ; Večerík et al. ; Bagnell ; Reach ; RL ; Ross & Bagnell ; three,time ; states ; prior works ; the number ; the states ; multiple steps ; a goal state ; the top left corner ; the yellow entry ; a large amount,Fujimoto ; Mikolov et al ; Gao ; first ; Večerík et al. ; Bagnell ; Reach ; RL ; Ross & Bagnell ; three,"Imitation learning, followed by reinforcement learning algorithms, is a promising paradigm to solve complex control tasks sample-efficiently. However, learning from demonstrations often suffers from the covariate shift problem, which results in cascading errors of the learned policy. We introduce a concept of conservatively extrapolated value functions, which provably lead to policies with self-correction. We develop an algorithm Value Iteration with Negative Sampling (VINS) that practically learns such value functions with conservative extrapolation. VINS can correct mistakes of behavioral cloning policy on simulated robotics benchmark tasks. We also introduce a reinforcement learning algorithm, which outperforms",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Making deep convolutional neural networks more accurate typically comes at the cost of increased computational and memory resources. In this paper, we reduce this cost by exploiting the fact that the importance of features computed by convolutional layers is highly input-dependent, and propose feature boosting and suppression (FBS), a new method to predictively amplify salient convolutional channels and skip unimportant ones at run-time. FBS introduces small auxiliary connections to existing convolutional layers. In contrast to channel pruning methods which permanently remove channels, it preserves the full network structures and accelerates convolution by dynamically skipping unimportant input and output channels. FBS-augmented networks are trained with conventional stochastic gradient descent, making it readily available for many state-of-the-art CNNs. We compare FBS to a range of existing channel pruning and dynamic execution schemes and demonstrate large improvements on ImageNet classification. Experiments show that FBS can respectively provide 5× and 2× savings in compute on VGG-16 and ResNet-18, both with less than 0.6% top-5 accuracy loss. State-of-the-art vision and image-based tasks such as image classification BID19 BID33 BID11 , object detection BID31 and segmentation BID26 are all built upon deep convolutional neural networks (CNNs). While CNN architectures have evolved to become more efficient, the general trend has been to use larger models with greater memory utilization, bandwidth and compute requirements to achieve higher accuracy. The formidable amount of computational resources used by CNNs present a great challenge in the deployment of CNNs in both cost-sensitive cloud services and low-powered edge computing applications.One common approach to reduce the memory, bandwidth and compute costs is to prune over-parameterized CNNs. If performed in a coarse-grain manner this approach is known as channel pruning BID37 BID25 BID35 . Channel pruning evaluates channel saliency measures and removes all input and output connections from unimportant channels-generating a smaller dense model. A saliency-based pruning method, however, has threefold disadvantages. Firstly, by removing channels, the capabilities of CNNs are permanently lost, and the resulting CNN may never regain its accuracy for difficult inputs for which the removed channels were responsible. Secondly, despite the fact that channel pruning may drastically shrink model size, without careful design, computational resources cannot be effectively reduced in a CNN without a detrimental impact on its accuracy. Finally, the saliency of a neuron is not static, which can be illustrated by the feature visualization in FIG0 . Here, a CNN is shown a set of input images, certain channel neurons in a convolutional output may get highly excited, whereas another set of images elicit little response from the same channels. This is in line with our understanding of CNNs that neurons in a convolutional layer specialize in recognizing distinct features, and the relative importance of a neuron depends heavily on the inputs.The above shortcomings prompt the question: why should we prune by static importance, if the importance is highly input-dependent? Surely, a more promising alternative is to prune dynamically depending on the current input. A dynamic channel pruning strategy allows the network to learn to prioritize certain convolutional channels and ignore irrelevant ones. Instead of simply reducing model size at the cost of accuracy with pruning, we can accelerate convolution by selectively computing only a subset of channels predicted to be important at run-time, while considering the sparse input from the preceding convolution layer. In effect, the amount of cached activations and the number of read, write and arithmetic operations used by a well-designed dynamic model can be almost identical to an equivalently sparse statically pruned one. In addition to saving computational resources, a dynamic model preserves all neurons of the full model, which minimizes the impact on task accuracy.In this paper, we propose feature boosting and suppression (FBS) to dynamically amplify and suppress output channels computed by the convolutional layer. Intuitively, we can imagine that the flow of information of each output channel can be amplified or restricted under the control of a ""valve"". This allows salient information to flow freely while we stop all information from unimportant channels and skip their computation. Unlike pruning statically, the valves use features from the previous layer to predict the saliency of output channels. With conventional stochastic gradient descent (SGD) methods, the predictor can learn to adapt itself by observing the input and output features of the convolution operation.FBS introduces tiny auxiliary connections to existing convolutional layers. The minimal overhead added to the existing model is thus negligible when compared to the potential speed up provided by the dynamic sparsity. Existing dynamic computation strategies in CNNs BID28 BID2 produce on/off pruning decisions or execution path selections. Training them thus often resorts to reinforcement learning, which in practice is often computationally expensive. Even though FBS similarly use non-differentiable functions, contrary to these methods, the unified losses are still wellminimized with conventional SGD.We apply FBS to a custom CIFAR-10 ( BID20 classifier and popular CNN models such as VGG-16 BID33 and ResNet-18 (He et al., 2016) trained on the ImageNet dataset BID3 . Empirical results show that under the same speed-ups, FBS can produce models with validation accuracies surpassing all other channel pruning and dynamic conditional execution methods examined in the paper. BID11 , the outputs from certain channel neurons may vary drastically. The top rows in (a) and (b) are found respectively to greatly excite neurons in channels 114 and 181 of layer block 3b/conv2, whereas the bottom images elicit little activation from the same channel neurons. The number below each image indicate the maximum values observed in the channel before adding the shortcut and activation. Finally, (c) shows the distribution of maximum activations observed in the first 20 channels. In summary, we proposed feature boosting and suppression that helps CNNs to achieve significant reductions in the compute required while maintaining high accuracies. FBS fully preserves the capabilities of CNNs and predictively boosts important channels to help the accelerated models retain high accuracies. We demonstrated that FBS achieves around 2× and 5× savings in computation respectively on ResNet-18 and VGG-16 within 0.6% loss of top-5 accuracy. Under the same performance constraints, the accuracy gained by FBS surpasses all recent structured pruning and dynamic execution methods examined in this paper. In addition, it can serve as an off-the-shelf technique for accelerating many popular CNN networks and the fine-tuning process is unified in the traditional SGD which requires no algorithmic changes in training. Finally, the implementation of FBS and the optimized networks are fully open source and released to the public 1 .",We make convolutional layers run faster by dynamically boosting and suppressing channels in feature computation.,FBS ; ImageNet ; CNN ; One ; Firstly ; Secondly ; SGD.We ; al. ; first ; SGD,"conventional SGD.We ; the convolution operation ; large improvements ; models ; the deployment ; channel saliency measures ; a ""valve ; read, write and arithmetic operations ; the control ; image-based tasks",FBS ; ImageNet ; CNN ; One ; Firstly ; Secondly ; SGD.We ; al. ; first ; SGD,"In order to make deep convolutional neural networks more accurate, we introduce feature boosting and suppression (FBS), a new method to predictively amplify salient convolutionals channels and skip unimportant ones at run-time. FBS-augmented networks are trained with conventional stochastic gradient descent, making them readily available for state-of-the-art CNNs. These networks outperform channel pruning and dynamic execution schemes and demonstrate significant improvements on ImageNet classification. In contrast to channels pruning methods which permanently remove channels, FBS outperforms channels and accelerates convolution by dynamically skipping unimportant input and",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"In anomaly detection (AD), one seeks to identify whether a test sample is abnormal,  given a data set of normal samples.    A recent and promising approach to AD relies on deep generative models, such as variational autoencoders (VAEs),for unsupervised learning of the normal data distribution. In semi-supervised AD (SSAD), the data also includes a small sample of labeled anomalies. In this work,we propose two variational methods for training VAEs for SSAD. The intuitive idea in both methods is to train the encoder to ‘separate’ between latent vectors for normal and outlier data. We show that this idea can be derived from principled probabilistic formulations of the problem, and propose simple and effective algorithms.   Our methods can be applied to various data types, as we demonstrate on SSAD datasets ranging from natural images to astronomy and medicine, and can be combined with any VAE model architecture. When comparing to state-of-the-art SSAD methods that are not specific to particular data types, we obtain marked improvement in outlier detection. Anomaly detection (AD) -the task of identifying samples that are abnormal with respect to some normal data -has important applications in domains ranging from health-care, to security, and robotics (Pimentel et al., 2014) . In its common formulation, training data is provided only for normal samples, while at test time, anomalous samples need to be detected. In the probabilistic AD approach, a model of the normal data distribution is learned, and the likelihood of a test sample under this model is thresholded for classification as normal or not. Recently, deep generative models such as variational autoencoders (VAEs, Kingma & Welling 2013) and generative adversarial networks (Goodfellow et al., 2014) have shown promise for learning data distributions in AD (An & Cho, 2015; Suh et al., 2016; Schlegl et al., 2017; Wang et al., 2017) . Here, we consider the setting of semi-supervised AD (SSAD), where in addition to the normal samples, a small sample of labeled anomalies is provided (Görnitz et al., 2013) . Most importantly, this set is too small to represent the range of possible anomalies, making classification methods (either supervised or semi-supervised) unsuitable. Instead, most approaches are based on 'fixing' an unsupervised AD method to correctly classify the labeled anomalies, while still maintaining AD capabilities for unseen outliers (e.g., Görnitz et al., 2013; Muñoz-Marí et al., 2010; Ruff et al., 2019) . In this work, we present a variational approach for learning data distributions in the SSAD problem setting. We base our method on the VAE, and modify the training objective to account for the labeled outlier data. We propose two formulations for this problem. The first maximizes the log-likelihood of normal samples, while minimizing the log-likelihood of outliers, and we effectively optimize this objective by combining the standard evidence lower bound (ELBO) with the χ upper bound (CUBO, Dieng et al. 2017) . The second method is based on separating the VAE prior between normal and outlier samples. Effectively, both methods have a similar intuitive interpretation: they modify the VAE encoder to push outlier samples away from the prior distribution (see Figure 1) . Importantly, our method does not place any restriction on the VAE architecture, and can be used to modify any VAE to account for outliers. As such, it can be used for general types of data. We evaluate our methods in the comprehensive SSAD test-suite of Ruff et al. (2019) , which includes both image data and low-dimensional data sets from astronomy, medicine, and other domains, and report a marked improvement in performance compared to both shallow and deep methods. In addition, we demonstrate the flexibility of our method by modifying a conditional VAE used for generating sampling distributions for robotic motion planning (Ichter et al., 2018) to not generate way points that collide with obstacles. We proposed two VAE modifications that account for negative data examples, and used them for semi-supervised anomaly detection. We showed that these methods can be derived from natural probabilistic formulations of the problem, and that the resulting algorithms are general and effective -they outperform the state-of-the-art on diverse datasets. We further demonstrated that even a small fraction of outlier data can significantly improve anomaly detection on various datasets, and that our methods can be combined with VAE applications such as in motion planning. We see great potential in the probabilistic approach to AD using deep generative models: it has a principled probabilistic interpretation, it is agnostic to the particular type of data, and it can be implemented using expressive generative models. For specific data such as images, however, discriminative approaches that exploit domain specific methods such as geometric transformations are currently the best performers. Developing similar self-supervised methods for generative approaches is an exciting direction for future research. Another promising direction would be incorporating SSAD within energy-based models. Rayana Shebuti. Table 5 includes the complete results. The results for an ensemble of Deep SAD models are in parenthesis (CIFAR-10). The ensemble method is implemented the same way as ours: we train K = 5 separate models (i.e., each model has its own c), and the score (which is the distance from c) is the average of scores from all models in the ensemble. A.1.2 CLASSIC ANOMALY DETECTION Table 6 includes the complete results. MML-DP VAE combines both suggested objectives when training the VAE, i.e. MML and DP, and as reflected in the results, its performance is on par with them.","We proposed two VAE modifications that account for negative data examples, and used them for semi-supervised anomaly detection.",CUBO ; anomaly detection ; Kingma & Welling ; Rayana Shebuti ; Ruff et al ; Suh et al. ; Schlegl et al. ; Görnitz ; Wang ; Dieng et al.,great potential ; two VAE modifications ; the encoder ; specific data ; marked improvement ; its common formulation ; a marked improvement ; outlier samples ; other domains ; the problem,CUBO ; anomaly detection ; Kingma & Welling ; Rayana Shebuti ; Ruff et al ; Suh et al. ; Schlegl et al. ; Görnitz ; Wang ; Dieng et al.,"Anomaly detection (AD) involves identifying abnormal test samples given a data set of normal samples. Deep generative models, such as variational autoencoders (VAEs) and generative adversarial networks (Goodfellow et al., 2014) have shown promise for learning data distributions in AD. In semi-supervised AD (SSAD), the data also includes a small sample of labeled anomalies, making classification methods unsuitable. In the probabilistic AD approach, a model of the normal data distribution is learned, and the likelihood of a test sample under this model is thresholded for classification as normal or not.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Efforts to reduce the numerical precision of computations in deep learning training have yielded systems that aggressively quantize weights and activations, yet employ wide high-precision accumulators for partial sums in inner-product operations to preserve the quality of convergence. The absence of any framework to analyze the precision requirements of partial sum accumulations results in conservative design choices. This imposes an upper-bound on the reduction of complexity of multiply-accumulate units. We present a statistical approach to analyze the impact of reduced accumulation precision on deep learning training. Observing that a bad choice for accumulation precision results in loss of information that manifests itself as a reduction in variance in an ensemble of partial sums, we derive a set of equations that relate this variance to the length of accumulation and the minimum number of bits needed for accumulation. We apply our analysis to three benchmark networks: CIFAR-10 ResNet 32, ImageNet ResNet 18 and ImageNet AlexNet. In each case, with accumulation precision set in accordance with our proposed equations, the networks successfully converge to the single precision floating-point baseline. We also show that reducing accumulation precision further degrades the quality of the trained network, proving that our equations produce tight bounds. Overall this analysis enables precise tailoring of computation hardware to the application, yielding area- and power-optimal systems. Over the past decade, deep learning techniques have been remarkably successful in a wide spectrum of applications through the use of very large and deep models trained using massive datasets. This training process necessitates up to 100's of ExaOps of computation and Gigabytes of storage. It is, however, well appreciated that a range of approximate computing techniques can be brought to bear to significantly reduce this computational complexity -and amongst them, exploiting reduced numerical precision during the training process is extremely effective and has already been widely deployed BID5 .There are several reasons why reduced precision deep learning has attracted the attention of both hardware and algorithms researchers. First , it offers well defined and scalable hardware efficiency, as opposed to other complexity reduction techniques such as pruning BID7 a) , where handling sparse data is needed. Indeed , parameter complexity scales linearly while multiplication hardware complexity scales quadratically with precision bit-width BID20 . Thus, any advance towards truly binarized networks BID10 corresponds to potentially 30x -1000x complexity reduction in comparison to single precision floating-point hardware. Second , the mathematics of reduced precision has direct ties with the statistical theory of quantization BID18 . In the context of deep learning, this presents an opportunity for theoreticians to derive analytical trade-offs between model accuracy and numerical precision BID12 BID16 . The terminology FPa/b denotes an FPU whose multiplier and adder use a and b bits, respectively. Our work enables convergence in reduced precision accumulation and gains an extra 1.5× ∼ 2.2× area reduction.Most ongoing efforts on reduced precision deep learning solely focus on quantizing representations and always assume wide accumulators, i.e., ideal summations. The reason being reduced precision accumulation can result in severe training instability and accuracy degradation, as illustrated in FIG0 (a) for ResNet 18 (ImageNet) model training. This is especially unfortunate, since the hardware complexity in reduced precision floating-point numbers (needed to represent small gradients during training) BID17 BID14 ) is dominated by the accumulator bit-width. To illustrate this dominance we developed a model underpinned by the hardware synthesis of low-precision floating-point units (FPU) , that translates precision into area complexity of the FPU. Comparisons obtained from this model are shown in FIG0 (b). We observe that accumulating in high precision severely limits the hardware benefits of reduced precision computations. This presents a new angle to the problem of reduced precision deep learning training which concerns determining suitable accumulation precision and forms the basis of our paper. Our findings are that the accumulation precision requirements in deep learning training are nowhere near 32-b, and in fact could enable further complexity reduction of FPUs by a factor of 1.5 ∼ 2.2×. We have presented an analytical method to predict the precision required for partial sum accumulation in the three GEMM functions in deep learning training. Our results prove that our method is able to accurately pinpoint the minimum precision needed for the convergence of benchmark networks to the full-precision baseline. Our theoretical concepts are application agnostic, and an interesting extension would be to consider recurrent architectures such as LSTMs. In particular, training via backpropagation in time could make the GRAD accumulation very large depending on the number of past time-steps used. In such a case, our analysis is of great relevance to training precision optimization. On the practical side, this analysis is a useful tool for hardware designers implementing reduced-precision FPUs, who in the past have resorted to computationally prohibitive brute-force emulations. We believe this work addresses a critical missing link on the path to truly low-precision floating-point hardware for DNN training.",We present an analytical framework to determine accumulation bit-width requirements in all three deep learning training GEMMs and verify the validity and tightness of our method via benchmarking experiments.,three ; ImageNet AlexNet ; the past decade ; ExaOps ; Gigabytes ; First ; Second ; ImageNet ; GRAD,the minimum precision ; suitable accumulation precision ; an ensemble ; an interesting extension ; the single precision floating-point baseline ; applications ; Comparisons ; (a ; the FPU ; accumulation,three ; ImageNet AlexNet ; the past decade ; ExaOps ; Gigabytes ; First ; Second ; ImageNet ; GRAD,"Efforts to reduce the numerical precision of computations in deep learning training have yielded systems that aggressively quantize weights and activations, employing wide high-precision accumulators for partial sums in inner-product operations to preserve the quality of convergence. However, this approach imposes an upper-bound on the reduction of complexity of multiply-accumulate units due to reduced accumulation precision. We derive a set of equations that relate variance to the length of accumulation and minimum number of bits needed for accumulation. These equations enable precise tailoring of computation hardware to the application, yielding area- and power-optimal systems. In each case",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"We study the robustness to symmetric label noise of GNNs training procedures. By combining the nonlinear neural message-passing models (e.g. Graph Isomorphism Networks, GraphSAGE, etc.) with loss correction methods, we present a noise-tolerant approach for the graph classification task. Our experiments show that test accuracy can be improved under the artificial symmetric noisy setting. Large datasets are beneficial to modern machine learning models, especially neural networks. Many studies have shown that the accuracy of machine learning models grows log-linear to the amount of training data BID9 . Currently, complex machine learning models can only achieve superhuman classification results when trained with a very large dataset. However, large datasets are usually expensive to collect and create exact label. One solution to create large datasets is crowdsourcing, but this approach introduces a higher level of labeling error into the datasets as well as requires a lot of human resources BID1 . As a consequence, neural networks are prone to very high generalization error under noisy label data. Figure 1 demonstrate the accuracy results of a graph neural network trained on MUTAG dataset. Training accuracies tend to remain high while testing accuracies degrades as more label noise is added to the training data. Figure 1: GIN model trained with increasing symmetric label noise. The generalization gap increases as more noise is introduced to the training labels.Graph neural network (GNN) is a new class of neural networks which learn from graphstructured data. Typically, GNNs classify graph vertices or the whole graph itself. Given the input as the graph structure and data (e.g. feature vectors) on each vertex, GNNs training aim to learn a predictive model for classification. This new class of neural networks enables end-toend learning from a wider range of data format. In order to build large scale GNNs, it requires large and clean datasets. Since graph data is arguably harder to label than image data both at vertex-level or graph-level, graph neural networks should have a mechanism to adapt to training label error or noise.In this paper, we take the noise-correction approach to train a graph neural network with noisy labels. We study two state-of-the-art graph neural network models: Graph Isomorphism Network BID7 and GraphSAGE BID2 . Both of these models are trained under symmetric artificial label noise and tested on uncorrupted testing data. We then apply label noise estimation and loss correction techniques BID5 BID9 to propose our denoising graph neural network model (D-GNN). In this paper, we have introduced the use of loss correction for Graph Neural Networks to deal with symmetric graph label noise. We experimented on two different practical noise estimatation methods and compare them to the case when we know the exact noise matrix. Our empirical results show some improvement on noise tolerant when the correction matrix C is correctly estimated. In practice, we can consider C as a hyperparameter and tune it following some clean validation data.",We apply loss correction to graph neural networks to train a more robust to noise model.,One ; MUTAG ; GNN ; noisy labels ; two ; GraphSAGE ; Graph Neural Networks,each vertex ; order ; especially neural networks ; the whole graph ; superhuman classification results ; a hyperparameter ; noise ; labeling error ; a predictive model ; graph-level,One ; MUTAG ; GNN ; noisy labels ; two ; GraphSAGE ; Graph Neural Networks,"GNNs training procedures incorporate nonlinear neural message-passing models (e.g. Graph Isomorphism Networks, GraphSAGE, etc.) with loss correction methods. The accuracy of machine learning models grows log-linear to the amount of training data BID9. However, large datasets are expensive to collect and create exact label. The generalization gap increases as more noise is introduced to the training labels. Graph neural networks are prone to very high generalization error under noisy labels.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Supervised deep learning methods require cleanly labeled large-scale datasets, but collecting such data is difficult and sometimes impossible. There exist two popular frameworks to alleviate this problem: semi-supervised learning and robust learning to label noise. Although these frameworks relax the restriction of supervised learning, they are studied independently. Hence, the training scheme that is suitable when only small cleanly-labeled data are available remains unknown. In this study, we consider learning from bi-quality data as a generalization of these studies, in which a small portion of data is cleanly labeled, and the rest is corrupt. Under this framework, we compare recent algorithms for semi-supervised and robust learning. The results suggest that semi-supervised learning outperforms robust learning with noisy labels. We also propose a training strategy for mixing mixup techniques to learn from such bi-quality data effectively. Learning from imperfect data is essential for applying machine learning, especially data-hungry deep learning, to real-world problems. One approach to handling this problem is semi-supervised learning (SSL), where training data consist of a small amount of labeled data and a large amount of unlabeled data. Another approach is robust learning to label noise (RLL), wherein all data are labeled, but some of them are mislabeled.SSL leverages large unlabeled data to improve the performance of supervised learning on a limited number of labeled data. In the context of deep SSL, one effective method is to train neural networks to maintain consistency for a small perturbation of unlabeled inputs BID7 ; BID10 ; BID11 ). BID8 refers these methods as consistency regularization.In the RLL setting, learners need to enhance their performance using corrupted labels and avoid the performance deterioration caused by such data. This requirement is particularly important for deep neural networks because they have ample capacity to remember whole samples even if their labels are completely random BID0 BID14 ). To tackle this problem, some methods use a small amount of clean data to estimate noise transition matrix BID12 ; BID2 ) or to learn to select possibly correctly-labeled samples BID4 ; BID3 ).Although both SSL and RLL aim to alleviate the limited-data problem, they have been studied independently and evaluated using different benchmarks. However, if only a small amount of clean data is available, they can be regarded as similar problems. In such as situation, can RLL outperform SSL under the same settings? This question was our initial motivation to unify these two lines of research.In this paper, we introduce a generalization of SSL and RLL, based on the concept of trusted data BID1 ; BID2 ) in the literature of RLL. More precisely , we assumed that some labels are guaranteed to be clean, and the rest are noisy. The two learning frameworks can be unified by controlling the ratio of corrupted labels to all labels and the noisiness of label corruption.Using the shared evaluation procedure in BID8 , we compared recent SSL and RLL algorithms using image classification task and found that the existing RLL methods using a small amount of clean data cannot outperform SSL under this setting. This finding suggests that such RLL algorithms cannot use noisy labels effectively. Therefore, it is necessary to adaptively use SSL and RLL in a data-driven manner. As a baseline learning algorithm , we propose combining the mixup losses for SSL BID11 ) and RLL BID15 ); the results obtained are comparable to those of SSL-and RLL-specific methods and indicate the effective use of useful information from noisy labels. In this paper, we introduce a novel framework of weakly supervised learning by unifying SSL and RLL, which have been independently studied. To handle this problem, we propose to mix mixup for SSL and RLL. This method empirically works well and achieves competitive results with semisupervised and robust learning specific methodologies.In addition, our experiments indicate that the performance of some RLL with trusted data might be inferior to that of SSL under identical settings. This result suggests that the existing RLL methods cannot effectively exploit the information which should be extracted from noisy labels.Our proposed method does not use the estimated quality; instead some hyperparameters are introduced. The use of quality estimation may ease hyperparameter tuning, but is still an open question.",We propose to compare semi-supervised and robust learning to noisy label under a shared setting,two ; One ; RLL ; SSL ; noisy labels,quality estimation ; clean data ; the information ; a small amount ; This result ; weakly supervised learning ; their labels ; This question ; some ; these methods,two ; One ; RLL ; SSL ; noisy labels,"Supervised deep learning methods require cleanly labeled large-scale datasets, but collecting such data is difficult and sometimes impossible. There are two popular frameworks to alleviate this problem: semi-supervised learning and robust learning to label noise. These frameworks relax the restriction of supervised learning, but the training scheme that is suitable when only small cleanly-labeled data are available remains unknown. In this study, we compare recent algorithms for semi-Supervised learning, robust learning, label noise (RLL), and robust training to RLL (SSL). The results suggest that these two approaches outperform each other in the context of deep SSL",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Using higher order knowledge to reduce training data has become a popular research topic. However, the ability for available methods to draw effective decision boundaries is still limited: when training set is small, neural networks will be biased to certain labels. Based on this observation, we consider constraining output probability distribution as higher order domain knowledge. We design a novel algorithm that jointly optimizes output probability distribution on a clustered embedding space to make neural networks draw effective decision boundaries.   While directly applying probability constraint is not effective, users need to provide additional very weak supervisions: mark some batches that have output distribution greatly differ from target probability distribution. We use experiments to empirically prove that our model can converge to an accuracy higher than other state-of-art semi-supervised learning models with less high quality labeled training examples. Probability is an abstract measure on how a certain event occurs independent of features of the events. Knowing how likely a certain event occurs, people leverages such prior knowledge to their decision making. For example, doctors know certain diseases are rare, even if they are told in terms of probabilities instead of ""training examples"". Based on this knowledge, they make less predictions on these diseases than those common ones. Do neural networks behave in a similar way? Unfortunately, the answer is no. When we train a multi-layer perceptron(MLP) for MNIST classifier BID10 ) with limited labelled examples, the output distribution can be extremely biased in favor of some of the labels. In Figure 1a , we compare the predicted number of labels with ground truth. While the training accuracy is 1.0, the model clearly overfits to those training examples and leave labels between training data points undefined in high dimensional feature space. As we plot the last hidden layer of a MLP trained with 50 labelled MNIST data as shown in Figure 1b , we find neural networks fail to learn the decision boundary correctly from a limited number of examples.Thus, it is natural to consider introducing output label probability distribution as higher order knowledge when we train neural networks. Different from traditional logical constraints BID22 ) or functional constraints BID18 , we propose a novel embedding space probabilistic constraint. Because of the sparsity of high dimensional feature space with only a few labeled examples, we perform our probabilistic constraint on neural network's embedding space, which is constructed unsupervisedly by projecting data into low dimensional space through autoencoder. Based on observation by BID21 , BID23 , embedding space preserves information of separations of different label clusters. In the embedding space, we pool softmax activation (a) Strong imbalanced output distribution of labels when training set is limited (b) Chaotic embedding space in the hidden layer of the classifier trained with 50 labelled examples Figure 1 : Limited training data cannot train neural networks to learn accurate decision boundaries outputs and optimize towards target distribution. By training with very few high quality labelled examples and marking on batches that have output distribution greatly different from target probability distribution, we use experiments to empirically prove that our model can converge to a high accuracy faster than state-of-art semi-supervised learning methods.",We introduce an embedding space approach to constrain neural network output probability distribution.,,low dimensional space ; a novel embedding space probabilistic constraint ; output distribution ; space ; an abstract measure ; their decision making ; examples ; training data points ; a limited number ; experiments,,"Using higher order knowledge to reduce training data has become a popular research topic. However, the ability for available methods to draw effective decision boundaries is limited. The output probability distribution on a clustered embedding space can be extremely biased in favor of some of the labels. We use experiments to prove that our model can converge to an accuracy higher than other state-of-art semi-supervised learning models with less high quality labeled training examples. Probability is an abstract measure on how a certain event occurs independent of features of the events. It is important to consider introducing output label probability distribution as higher-order knowledge when training neural networks.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Graphs possess exotic features like variable size and absence of natural ordering of the nodes that make them difficult to analyze and compare. To circumvent this problem and learn on graphs, graph feature representation is required. Main difficulties with feature extraction lie in the trade-off between expressiveness, consistency and efficiency, i.e. the capacity to extract features that represent the structural information of the graph while being deformation-consistent and isomorphism-invariant. While state-of-the-art methods enhance expressiveness with powerful graph neural-networks, we propose to leverage natural spectral properties of graphs to study a simple graph feature: the graph Laplacian spectrum (GLS). We analyze the representational power of this object that satisfies both isomorphism-invariance, expressiveness and deformation-consistency. In particular, we propose a theoretical analysis based on graph perturbation to understand what kind of comparison between graphs we do when comparing GLS. To do so, we derive bounds for the distance between GLS that are related to the divergence to isomorphism, a standard computationally expensive graph divergence. Finally, we experiment GLS as graph representation through consistency tests and classification tasks, and show that it is a strong graph feature representation baseline. No matter where and at which scale we look, graphs are present. Social networks, public transport, information networks, molecules, any structural dependency between elements of a global system is a graph. An important task is to extract information from these graphs in order to understand whether they contain certain structural properties that can be represented and used in downstream machine learning tasks. In general, graphs are difficult to use as input of standard algorithms because of their exotic features like variable size and absence of natural orientation. Consequently, graph feature representation with equal dimensionality and dimension-wise alignment is required to learn on graphs. Any embedding method is traditionally associated to a trade-off between preservation of structural information (expressiveness) and computation time (efficiency) (Cai et al., 2018) . In the expressiveness, we particularly consider two key attributes of graph feature representation: consistency under deformation and invariance under isomorphism. The first forces the embedding to discriminate two graphs consistently with their structural dissimilarity. The second enables to have one representation for each graph, which can be a challenge since one graph has many possible orientations. In this paper, we propose to analyze the importance of satisfying the introduced criteria through a known but unused, simple, expressive and efficient candidate graph feature representation: the graph Laplacian spectrum (GLS). The Laplacian matrix of a graph is a well-known object in spectral learning (Belkin & Niyogi, 2002) for several reasons. First, the Laplacian eigenvalues give many structural information like the presence of communities and partitions (Newman, 2013) , the regularity, the closed-walks enumeration, the diameter or the connectedness of the graph (Brouwer & Haemers, 2011) . It is also interpretable in term of physics or mechanics (Bonald et al., 2018) . It is backed by efficient and robust approximate eigen decomposition algorithms enabling to scale on large graphs and huge datasets (Halko et al., 2011) . These properties give intuition that GLS can be an appropriate candidate for graph representation. In this paper we go further and analyze additional interesting properties of the Laplacian spectrum through the following contributions: (1) we build a perturbation-based framework to analyze the representation capacity of the GLS, (2) we analyze Interpretation of the GLS The smallest non-zero eigenvalue of the Laplacian is the spectral gap, corresponding the difference between the two largest eigenvalues of the Laplacian. It contains information about the connectivity of the graph. High spectral gap means high connectivity. For example, given a number of vertices in a connected graph, a minimum spectral gap indicates that the graph is a double kite (Marsden, 2013) . The largest eigenvalue gives a lower bound of the maximal node degree of the graph. The spectral gap can also be viewed as the difference in energy between the ground state and first excited state of a dynamical system (Cubitt et al., 2015) . More generally each eigenvalue of the Laplacian corresponds to the energy level of a stable configuration of the nodes in the embedding space (Bonald et al., 2018) . The lower the energy, the stabler the configuration. In (Shuman et al., 2016) , the Laplacian eigenvalues correspond to frequencies associated to a Fourier decomposition of any signal living on the vertices of the graph. Thus, the truncation of the Fourier decomposition acts as filter on the signal. Characterizing a graph by the some eigenvalues of its Laplacian is thus comparable to characterizing a melody by some fundamental frequencies. In summary, Laplacian spectrum contains many graph structural information. Methods to get such information are generally computationally expensive. In the light of these properties, we go further and analyze in the following sections the capacity of GLS to represent graph structure. In this paper, we analyzed the graph Laplacian spectrum (GLS) as whole graph representation. In particular, we showed that comparing two GLS is a good proxy for the divergence between two graphs in term of structural information. We coupled these results to the natural invariance to isomorphism, the simplicity of implementation, the computational efficiency offered by modern randomized algorithms and the rare occurrence of detrimental L-cospectral non-isomorphic graphs to propose the GLS as a strong baseline graph feature representation. A PROOF OF LEMMA 1 Proof. with L P * = diag(P * 1) − P * = D P * − P * and 1 the unit vector. Therefore, Moreover, from Weyl's eigenvalues inequalities and since eigenvalues are isomorphism invariant: Hence: Now let (λ, x) be any eigen couple of a matrix M ∈ M n×n . We can always pick i ∈ {1 . . . n} and build x such that |x i | = 1 and |x j =i | < 1. Hence: Using previous results we get: Proof. We remind that the Forbenius norm is unitarily invariant thanks to the cyclic property of the trace. For anyP ∈ O(|V 2 |) we have: We also have that Hence:",We study theoretically the consistency the Laplacian spectrum and use it as whole-graph embeddding,Bonald et al. ; Interpretation of the ; Belkin & Niyogi ; Halko ; second ; Newman ; Weyl ; al. ; Brouwer & Haemers ; Fourier,the representation capacity ; example ; a simple graph feature ; a graph ; classification tasks ; previous results ; term ; anyP ∈ ; the energy level ; the configuration,Bonald et al. ; Interpretation of the ; Belkin & Niyogi ; Halko ; second ; Newman ; Weyl ; al. ; Brouwer & Haemers ; Fourier,"Graphs possess exotic features like variable size and absence of natural ordering of nodes that make them difficult to analyze and compare. To circumvent this problem and learn on graphs, graph feature representation is required. The trade-off between expressiveness, consistency and efficiency lies in the capacity to extract features that represent structural information while being deformation-consistent and isomorphism-invariant. In order to achieve this, state-of-the-art methods enhance expressiveness with powerful graph neural-networks. The graph Laplacian spectrum (GLS) is an example of a graph feature that satisfies both expressiveness and",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"In a typical deep learning approach to a computer vision task, Convolutional Neural Networks (CNNs) are used to extract features at varying levels of abstraction from an image and compress a high dimensional input into a lower dimensional decision space through a series of transformations. In this paper, we investigate how a class of input images is eventually compressed over the course of these transformations. In particular, we use singular value decomposition to analyze the relevant variations in feature space. These variations are formalized as the effective dimension of the embedding. We consider how the effective dimension varies across layers within class. We show that across datasets and architectures, the effective dimension of a class increases before decreasing further into the network, suggesting some sort of initial whitening transformation. Further, the decrease rate of the effective dimension deeper in the network corresponds with training performance of the model. In this section, we analyze and discuss the implications of our findings. Further, we propose complementary analyses that would bolster our findings. In studied examples, neural networks initially spherize embeddings and then collapse dimensionality. The compression of the dimensionality of feature spaces via transformations on inputs is more dramatic in better-performing networks.6. Appendix is scaled by factor α, the spectral norm of α * Φ (l) is α * σ max (Φ (l) ). This is also true in a ReLU network with α > 0. Such a scaling is achieved while preserving φ (l+1) : DISPLAYFORM0 While Srebro et al. BID10 directly apply the trace-norm to bound the complexity of a completed matrix, we apply spectral normalization in Equation 1 to correct for this scale sensitivity. Hence, a small effective dimension corresponds to an eccentric feature space regardless of magnitude.",Neural networks that do a good job of classification project points into more spherical shapes before compressing them into fewer dimensions.,Convolutional Neural Networks ; σ max ; ReLU ; Srebro,initial whitening transformation ; singular value decomposition ; architectures ; Such a scaling ; spectral normalization ; that ; the decrease rate ; a ReLU network ; the trace-norm ; training performance,Convolutional Neural Networks ; σ max ; ReLU ; Srebro,"Convolutional Neural Networks (CNNs) are used to extract features at varying levels of abstraction from an image and compress a high dimensional input into a lower dimensional decision space through a series of transformations. In this paper, we explore how a class of input images is compressed over the course of these transformations. The effective dimension of an embedding is formalized as the effective dimension, and the complexity of a completed matrix is determined by spectral normalization in Equation 1. A small effective dimension corresponds to an eccentric feature space regardless of magnitude.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"We introduce bio-inspired artificial neural networks consisting of neurons that are additionally characterized by spatial positions. To simulate properties of biological systems we add the costs penalizing long connections and the proximity of neurons in a two-dimensional space. Our experiments show that in the case where the network performs two different tasks, the neurons naturally split into clusters, where each cluster is responsible for processing a different task. This behavior not only corresponds to the biological systems, but also allows for further insight into interpretability or continual learning. Neurons in the human brain naturally group into interconnected regions, forming the full neural system [1] . In this paper, we would like to construct an analogical mechanism in the case of artificial neural networks. To put this idea into practice, we supply each neuron with spatial coordinates. Motivated by biological neural systems, we impose the cost of signal transmission between connected neurons, which grows linearly with the distance between them. In consequence, we obtain artificial groups specialized in different tasks, each group containing neurons that are placed close to each other. The proposed model is examined in a double classification task, where a single network has to classify examples from two different datasets (MNIST and Fashion-MNIST). At test time, we split the network into two subnetworks based on the structure of weights, where each subnetwork represents one task. The resulting models perform their respective tasks only slightly worse than the original network, in contrast to the large performance drop observed after splitting a standard fully connected network. Our model offers a natural interpretation of neurons' responsibilities and is analogous to biological neural systems. The idea of adding spatial coordinates to each neuron and penalizing long connections was previously introduced by [2] . Although our work is based on a similar premise, the resulting models differ significantly. We use a different, simpler spatial loss function, we investigate networks with multiple hidden layers as opposed to a single hidden layer, and we focus on the cluster-forming properties of spatial networks. There have been multiple approaches to finding clusters of neurons in artificial neural networks, although most of them consider the functional aspects of the network. For instance, [3] compare variance of neuron's activations in different tasks to decide which cluster does it belong to. In another approach, [4] cluster the network by using feature vectors calculated based on correlation between the neuron and the output. In comparison, our approach uses the structure of the network -the spatial placement of the neurons and the strength of connections between the neurons. Our model is also related to continual and multi-task learning [5] and parameter reduction models for deep neural networks [6] . In particular, the effect is slightly similar to the one obtained in [7] . Authors focus on splitting network weights into a set of groups, where each is associated with a class (task). In contrast to our biologically inspired mechanism, [7] use a specialized weight regularization technique and strive towards a different goal. In [8] , a nested sparse network is constructed and different forms of knowledge are learned at each level, enabling solving multiple tasks with a single neural network. Checking the response of different groups in our proposed spatial network could be considered useful for interpreting the network predictions, which is also an open problem [9] . We have presented a connection between neuroscience and machine learning, which to our best knowledge has not yet been explored. Experiments show that our proposed spatial artificial neural network manifests behavior similar to the region-forming processes in the brain. For future work we plan to test our model on a continual learning task. We hypothesize that, since the model is able to create disjoint clusters of neurons responsible for different tasks, learning a new task could be possible without disturbing the previously created clusters. Additional constraints could be added to achieve this, such as restricting the movement of neurons with high potential, while allowing neurons with low potential to move freely. Spatial networks could also be investigated in relation to spiking neural networks. The motivation is that spiking neural networks operate in the temporal dimension, which in the brain is dictated mainly by the spatial structure of neurons. Thus, it is possible that the two types of networks have similar properties, making the spatial network a more interesting model to investigate from the neuroscientific standpoint.","Bio-inspired artificial neural networks, consisting of neurons positioned in a two-dimensional space, are capable of forming independent groups for performing different tasks.",two ; one ; neuron,instance ; Experiments ; the distance ; high potential ; each group ; our best knowledge ; The idea ; each subnetwork ; low potential ; the model,two ; one ; neuron,"In the case where the network performs two different tasks, the neurons naturally split into clusters, where each cluster is responsible for processing a different task. This behavior corresponds to the biological systems and allows for further insight into interpretability or continual learning. Neurons in the human brain naturally group into interconnected regions, forming the full neural system [1]. In this paper, we propose an analogical mechanism in the case of artificial neural networks. The idea of assigning spatial coordinates to each neuron is based on the structure of weights and the strength of connections between the neurons. The resulting models perform significantly worse than the original network, due to",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"We propose a software framework based on ideas of the Learning-Compression algorithm , that allows one to compress any neural network by different compression mechanisms (pruning, quantization, low-rank, etc.). By design, the learning of the neural net (handled by SGD) is decoupled from the compression of its parameters (handled by a signal compression function), so that the framework can be easily extended to handle different combinations of neural net and compression type. In addition, it has other advantages, such as easy integration with deep learning frameworks, efficient training time, competitive practical performance in the loss-compression tradeoff, and reasonable convergence guarantees. Our toolkit is written in Python and Pytorch and we plan to make it available by the workshop time, and eventually open it for contributions from the community.","We propose a software framework based on ideas of the Learning-Compression algorithm , that allows one to compress any neural network by different compression mechanisms (pruning, quantization, low-rank, etc.).",the Learning-Compression ; SGD ; Python ; Pytorch,pruning ; the loss-compression tradeoff ; design ; efficient training time ; different compression mechanisms ; the framework ; the compression ; the community ; a signal compression function ; the neural net,the Learning-Compression ; SGD ; Python ; Pytorch,"The Learning-Compression algorithm (SGD) is used to compress any neural network by different compression mechanisms. The learning of the neural net is decoupled from the compression of its parameters (handled by a signal compression function) and the framework can be easily extended to handle different combinations of neural net and compression type. The toolkit is written in Python and Pytorch, and it aims to make it available by the workshop time, and eventually open for contributions from the community.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"We propose a new architecture for distributed image compression from a group of distributed data sources. The work is motivated by practical needs of data-driven codec design, low power consumption, robustness, and data privacy. The proposed architecture, which we refer to as Distributed Recurrent Autoencoder for Scalable Image Compression (DRASIC), is able to train distributed encoders and one joint decoder on correlated data sources. Its compression capability is much better than the method of training codecs separately. Meanwhile, for 10 distributed sources, our distributed system remarkably performs within 2 dB peak signal-to-noise ratio (PSNR) of that of a single codec trained with all data sources. We experiment distributed sources with different correlations and show how our methodology well matches the Slepian-Wolf Theorem in Distributed Source Coding (DSC). Our method is also shown to be robust to the lack of presence of encoded data from a number of distributed sources. Moreover, it is scalable in the sense that codes can be decoded simultaneously at more than one compression quality level. To the best of our knowledge, this is the first data-driven DSC framework for general distributed code design with deep learning. It has been shown by a variety of previous works that deep neural networks (DNN) can achieve comparable results as classical image compression techniques (Toderici et al., 2015; Ballé et al., 2016; Gregor et al., 2016; Theis et al., 2017; Liu et al., 2018; Li et al., 2018; Mentzer et al., 2018) . Most of these methods are based on autoencoder networks and quantization of bottleneck representations. These models usually rely on entropy codec to further compress codes. Moreover, to achieve different compression rates it is unavoidable to train multiple models with different regularization parameters separately, which is often computationally intensive. In this work, we are motivated to develop an architecture that has the following advantages. First, unlike classical distributed source coding (DSC) which requires customized code design for different scenarios (Xiong et al., 2004) , a data-driven distributed compression framework can handle nontrivial distribution of image sources with arbitrary correlations. Second, the computation complexity of encoders (e.g. mobile devices) can be transferred to the decoder (e.g. a remote server). Such a system of low complexity encoders can be used in a variety of application domains, such as multi-view video coding (Girod et al., 2005) , sensor networks (Xiong et al., 2004) , and under-water image processing where communication bandwidth and computational power are quite restricted (Stojanovic & Preisig, 2009; Schettini & Corchs, 2010) . Third, the distributed framework can be more robust against heterogeneous noises or malfunctions of encoders, and such robustness can be crucial in, e.g., unreliable sensor networks (Girod et al., 2005; Ishwar et al., 2005; Xiao et al., 2006) . Last but not least, the architecture is naturally scalable in the sense that codes can be decoded at more than one compression quality level, and it allows efficient coding of correlated sources which are not physically co-located. This is especially attractive in video streaming applications (Guillemot et al., 2007; Gehrig, 2008) . It is tempting to think that splitting raw data into different encoders compromises the compression quality. It is thus natural to ask this question: Can distributed encoders perform as well as a single encoder trained with all data sources together? A positive answer from a theoretical perspective was given in the context of information theory, where DSC is an important problem regarding the compression of multiple correlated data sources. The Slepian-Wolf Theorem shows that lossless coding of two or more correlated data sources with separate encoders and a joint decoder can compress data as efficiently as the optimal coding using a joint encoder and decoder (Slepian & Wolf, 1973; Cover, 1975) . The extension to lossy compression with Gaussian data sources was proposed as Wyner-Ziv Theorem (Wyner & Ziv, 1976) . Although these theorems were published in 1970s, it was after about 30 years that practical applications such as Distributed Source Coding Using Syndromes (DISCUS) emerged (Pradhan & Ramchandran, 2003) . One of the main advantages of DSC is that the computation complexity of the encoder is transferred to the decoder. A system architecture with low complexity encoders can be a significant advantage in applications such as multi-view video coding and sensor networks (Girod et al., 2005; Xiong et al., 2004) . Motivated by the theoretical development of DSC, in this work we propose a DNN architecture that consists of distributed encoders and a joint decoder (illustrated in Fig. 1 and 2 ). We show that distributed encoders can perform as well as a single encoder trained with all data sources together. Our proposed DSC framework is data-driven by nature, and it can be applied to distributed data even with unknown correlation structure. The paper is outlined below. We review previous related works in Section 2. We describe our proposed architecture for general image compression and its basic modules in Subsections 3.1-3.4. Then we elaborate the Deep Distributed Source Coding framework in Subsection 3.5. Experimental results are shown in Section 4, followed by conclusions in Section 5. We introduced a data-driven Distributed Source Coding framework based on Distributed Recurrent Autoencoder for Scalable Image Compression (DRASIC). Compared to classical code design, our method has the following advantages. First, instead of explicitly estimating the correlations among data sources in advance, we use data-driven approach to learn the dependencies with the neural network parameters. Given enough training data, our method can handle an arbitrary number of sources with arbitrary correlations. Second, we showed the robustness of our framework. Unlike classical code design which may require careful data source synchronization, each distributed encoder of our model, once trained and deployed, can be used independently of others because the dependencies are already learned by the model parameters. Third, as one of the most important applications of Distributed Source Coding, low complexity encoders were shown to be feasible based on our experimental results. Data sources trained with less data and fewer number of iterations can still approach the theoretical limit obtained by pulling all the data. Last but not least, our recurrent model can reconstruct images efficiently even at low compression quality. We point out two interesting directions of future work. First, the compression quality of the proposed architecture may be improved by introducing spatially adaptive weights over different iterations, e.g. by using context models for adaptive arithmetic coding. Second, the network architecture may be further extended to handle time-dependent data sources.",We introduce a data-driven Distributed Source Coding framework based on Distributed Recurrent Autoencoder for Scalable Image Compression (DRASIC).,Gaussian ; Theis ; Xiong ; Ishwar ; Fig ; al. ; Stojanovic & Preisig ; Second ; Girod et al. ; first,this question ; two or more correlated data sources ; robustness ; it ; all data sources ; all the data ; Corchs ; spatially adaptive weights ; Syndromes ; a significant advantage,Gaussian ; Theis ; Xiong ; Ishwar ; Fig ; al. ; Stojanovic & Preisig ; Second ; Girod et al. ; first,"The Distributed Recurrent Autoencoder for Scalable Image Compression (DRASIC) is able to train distributed encoders and one joint decoder on correlated data sources. Its compression capability is significantly better than the method of training codecs separately. For 10 distributed sources, the distributed system performs within 2 dB peak signal-to-noise ratio (PSNR) of a single codec trained with all data sources, which is similar to classical distributed source coding (DSC). This is the first data-driven DSC framework for general distributed code design with deep learning. It achieves comparable results with classical image compression",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and prevalent form of graphs where each edge has a label and direction associated with it. Most of the existing approaches to handle such graphs suffer from over-parameterization and are restricted to learning representations of nodes only. In this paper, we propose CompGCN, a novel Graph Convolutional framework which jointly embeds both nodes and relations in a relational graph. CompGCN leverages a variety of entity-relation composition operations from Knowledge Graph Embedding techniques and scales with the number of relations. It also generalizes several of the existing multi-relational GCN methods. We evaluate our proposed method on multiple tasks such as node classification, link prediction, and graph classification, and achieve demonstrably superior results. We make the source code of CompGCN available to foster reproducible research. Graphs are one of the most expressive data-structures which have been used to model a variety of problems. Traditional neural network architectures like Convolutional Neural Networks (Krizhevsky et al., 2012) and Recurrent Neural Networks (Hochreiter & Schmidhuber, 1997) are constrained to handle only Euclidean data. Recently, Graph Convolutional Networks (GCNs) (Bruna et al., 2013; Defferrard et al., 2016) have been proposed to address this shortcoming, and have been successfully applied to several domains such as social networks (Hamilton et al., 2017) , knowledge graphs (Schlichtkrull et al., 2017; Shang et al., 2019) , natural language processing (Marcheggiani & Titov, 2017; Vashishth et al., 2018a; b; , drug discovery (Ramsundar et al., 2019) , crystal property prediction (Sanyal et al., 2018) , and natural sciences (Fout et al., 2017) . However, most of the existing research on GCNs (Kipf & Welling, 2016; Hamilton et al., 2017; Veličković et al., 2018) have focused on learning representations of nodes in simple undirected graphs. A more general and pervasive class of graphs are multi-relational graphs 1 . A notable example of such graphs is knowledge graphs. Most of the existing GCN based approaches for handling relational graphs (Marcheggiani & Titov, 2017; Schlichtkrull et al., 2017) suffer from overparameterization and are limited to learning only node representations. Hence, such methods are not directly applicable for tasks such as link prediction which require relation embedding vectors. Initial attempts at learning representations for relations in graphs (Monti et al., 2018; Beck et al., 2018) have shown some performance gains on tasks like node classification and neural machine translation. There has been extensive research on embedding Knowledge Graphs (KG) Wang et al., 2017) where representations of both nodes and relations are jointly learned. These methods are restricted to learning embeddings using link prediction objective. Even though GCNs can 1. We propose COMPGCN, a novel framework for incorporating multi-relational information in Graph Convolutional Networks which leverages a variety of composition operations from knowledge graph embedding techniques to jointly embed both nodes and relations in a graph. 2. We demonstrate that COMPGCN framework generalizes several existing multi-relational GCN methods (Proposition 4.1) and also scales with the increase in number of relations in the graph (Section 6.3). 3. Through extensive experiments on tasks such as node classification, link prediction, and graph classification, we demonstrate the effectiveness of our proposed method. The source code of COMPGCN and datasets used in the paper have been made available at http: //github.com/malllabiisc/CompGCN. In this paper, we proposed COMPGCN, a novel Graph Convolutional based framework for multirelational graphs which leverages a variety of composition operators from Knowledge Graph embedding techniques to jointly embed nodes and relations in a graph. Our method generalizes several existing multi-relational GCN methods. Moreover, our method alleviates the problem of over-parameterization by sharing relation embeddings across layers and using basis decomposition. , based on the average number of tails per head and heads per tail, we divide the relations into four categories: one-to-one, one-to-many, many-to-one and many-to-many. The results are summarized in Table 6 . We observe that using GCN based encoders for obtaining entity and relation embeddings helps to improve performance on all types of relations. In the case of one-to-one relations, COMPGCN gives an average improvement of around 10% on MRR compared to the best performing baseline (ConvE + W-GCN). For one-to-many, many-to-one, and many-to-many the corresponding improvements are 10.5%, 7.5%, and 4%. These results show that COMPGCN is effective at handling both simple and complex relations. Table 6 : Results on link prediction by relation category on FB15k-237 dataset. Following Wang et al. (2014a) , the relations are divided into four categories: one-to-one (1-1), one-to-many (1-N), manyto-one (N-1), and many-to-many (N-N). We find that COMPGCN helps to improve performance on all types of relations compared to existing methods. Please refer to Section A.1 for more details.",A Composition-based Graph Convolutional framework for multi-relational graphs.,Veličković et al. ; Sanyal et al. ; Convolutional Neural Networks ; four ; Recurrent Neural Networks ; Euclidean ; Ramsundar ; Marcheggiani & Titov ; Vashishth ; MRR,"Traditional neural network ; relation embeddings ; such graphs ; one-to-many, many-to-one, and many-to-many the corresponding improvements ; The results ; relation category ; the existing multi-relational GCN methods ; the problem ; number ; we",Veličković et al. ; Sanyal et al. ; Convolutional Neural Networks ; four ; Recurrent Neural Networks ; Euclidean ; Ramsundar ; Marcheggiani & Titov ; Vashishth ; MRR,"Graph Convolutional Networks (GCNs) have been successful in modeling graph-structured data. Multi-relational graphs are a more general and prevalent form of graphs where each edge has a label and direction associated with it. However, most of the existing approaches to handle such graphs suffer from over-parameterization and are restricted to learning representations of nodes only. CompGCN combines a variety of entity-relation composition operations from Knowledge Graph Embedding techniques and scales with the number of relations in a relational graph. It also generalizes several of the current multi-Relational GCN methods. The source code of Comp",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"We present a simple neural model that given a formula and a property tries to answer the question whether the formula has the given property, for example whether a propositional formula is always true. The structure of the formula is captured by a feedforward neural network recursively built for the given formula in a top-down manner. The results of this network are then processed by two recurrent neural networks. One of the interesting aspects of our model is how propositional atoms are treated. For example, the model is insensitive to their names, it only matters whether they are the same or distinct. In real-world situations a very successful approach, popularized in BID2 , to problem solving is based on a clever combination of fast instinctive (heuristic) reasoning and slow logical reasoning. The latter is exemplified by abstract logical formulae where only structural properties matter. If computers are involved, a logical formula is traditionally a syntactic object which is a subject to simple but very fast syntactic manipulations Robinson & Voronkov (2001) . Hence all but very basic decisions are postponed if possible. However, this viewpoint is rapidly changing as various AI methods are tested in the field of automated reasoning, in particular machine learning methods.A fundamental problem in using machine learning in automated reasoning is a suitable representation of logical formulae. A formula as a solely syntactic object is no longer sufficient and we have to exploit its semantic properties. Various approaches have been proposed for different logical systems. In this paper we will concentrate on the simplest yet very powerful standard logical system-classical (Boolean) propositional logic. This paper presents, as far as we know, a novel neural representation of propositional formulae that makes it possible to test whether a given formula has a given property, e.g., whether the formula is always true or not. Clearly, we try to solve a well-known CONP-complete problem. However, the fact that the problem is generally hard and requires a non-trivial search does not rule out the possibility that a decent heuristic can be learned, moreover, if only a specific subset of formulae is involved. In particular, our general goal is to obtain a useful heuristic that can help us in guiding a proof search, where we typically face numerous choice points.Unlike in natural language processing, a parse tree for a formula is available for free. Although some approaches do not exploit this feature and try to learn the structure of a formula on their own, using usually various recurrent neural networks (RNN), it is more common to take advantage of this knowledge. Moreover, it seems that the later approach has a significant edge, see BID1 . Usually propositional atoms, the basic building blocks of propositional formulae, are learned as embeddings and each logical connective is treated as a unique neural network that given the vector representation of its arguments produces a vector that represents an application of the connective on these arguments, e.g., a binary connective takes two vectors of length d, and produces a new one of length d, see BID0 . This clearly leads to tree recursive neural networks BID5 where the structure of the network follows the parse tree. Such models are built bottomup and the meaning of the formula is usually the vector produced in the root of the tree.Our model also uses the parse tree of the formula, but the knowledge is propagated in the opposite direction. We start with a vector (random or learned), representing a property we want to test, and we propagate it from the root to leaves (propositional atoms). The knowledge propagated to atoms is then processed by recurrent neural networks and a decision is produced. This makes it possible to ignore completely the names of propositional atoms and concentrate more on structural properties of formulae.The experimental results suggest that the model is more than competitive and beats other known approaches on some benchmarks. More importantly, our model seems to suffer less if bigger formulae are involved and could be more useful in real world scenarios.The structure of this paper is as follows. In Section 2 we discuss the architecture of our model in full details and also a dataset on which we will experiment is introduced there. In Section 3 we discuss an implementation of building blocks of our network, present experimental data, and shortly describe possible interpretations of our model. Some potential future modifications are briefly mentioned in Section 4. Few relevant models are mentioned in Section 5 and the paper concludes with Section 6. We have presented a novel top-down approach to represent formulae by neural networks and showed some preliminary experiments. They suggest that our approach is competitive with other known approaches and beats them on some benchmarks. More importantly, it seems that the presented model can deal better with the increasing size of formulae than other approaches. The model deals only with the structure of formulae and ignores completely for example names of individual atoms, only whether they are the same or distinct matters. (inaccessible to our model) in processing (p → q) → p we have to deal with more choices along this line of reasoning.",A top-down approach how to recursively represent propositional formulae by neural networks is presented.,two ; One ; Robinson & Voronkov ; AI ; Boolean ; formulae ; RNN,the architecture ; the possibility ; which ; some preliminary experiments ; reasoning ; our approach ; Various approaches ; usually various recurrent neural networks ; Hence all but very basic decisions ; present experimental data,two ; One ; Robinson & Voronkov ; AI ; Boolean ; formulae ; RNN,"A simple neural model, based on a formula and a property, is used to solve a problem. The structure of the formula is captured by a feedforward neural network recursively built for the given formula in a top-down manner. The results of this network are then processed by two recurrent neural networks. In real-world situations a very successful approach, popularized in BID2, is based on fast instinctive (heuristic) reasoning and slow logical reasoning. The latter is exemplified by abstract logical formulae where only structural properties matter. However, when computers are involved, a logical formula is a syntactic",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Modern deep neural networks (DNNs) require high memory consumption and large computational loads.   In order to deploy DNN algorithms efficiently on edge or mobile devices, a series of DNN compression algorithms have been explored, including the line of works on factorization methods. Factorization methods approximate the weight matrix of a DNN layer with multiplication of two or multiple low-rank matrices. However, it is hard to measure the ranks of DNN layers during the training process. Previous works mainly induce low-rank through implicit approximations or via costly singular value decomposition (SVD) process on every training step. The former approach usually induces a high accuracy loss while the latter prevents DNN factorization from efficiently reaching a high compression rate. In this work, we propose SVD training, which first applies SVD to decompose DNN's layers and then performs training on the full-rank decomposed weights. To improve the training quality and convergence, we add orthogonality regularization to the singular vectors, which ensure the valid form of SVD and avoid gradient vanishing/exploding. Low-rank is encouraged by applying sparsity-inducing regularizers on the singular values of each layer. Singular value pruning is applied at the end to reach a low-rank model. We empirically show that SVD training can significantly reduce the rank of DNN layers and achieve higher reduction on computation load under the same accuracy, comparing to not only previous factorization methods but also state-of-the-art filter pruning methods. The booming development in deep learning models and applications has enabled beyond human performance in tasks like large-scale image classification (Krizhevsky et al., 2012; He et al., 2016; Hu et al., 2018; Huang et al., 2017) , object detection (Redmon et al., 2016; Liu et al., 2016; He et al., 2017) , and semantic segmentation (Long et al., 2015; Chen et al., 2017) . Such high performance, however, comes with a high price of large memory consumption and computation load. For example, a ResNet-50 model needs approximately 4G floating-point operations (FLOPs) to classify a color image of 224 × 224 pixels. The computation load can easily expand to tens or even hundreds of GFLOPs for detection or segmentation models using state-of-the-art (SOTA) DNNs as backbones (Canziani et al., 2016 ). This is a major challenge that prevents the deployment of modern DNN models on resource-constrained platforms, such as phones, smart sensors, and drones. Model compression techniques for DNN models have been extensively studied. Some successful methods include element-wise pruning (Han et al., 2015; Liu et al., 2015; Zhang et al., 2018) , structural pruning (Wen et al., 2016; Luo et al., 2017; Li et al., 2019) , quantization (Liu et al., 2018; Wang et al., 2019) , and factorization (Jaderberg et al., 2014; Zhang et al., 2015; Yang et al., 2015; Xu et al., 2018) . Among these methods, quantization and element-wise pruning can effectively reduce model's memory consumption, but require specific hardware to realize efficient computation. Structural pruning reduces the computation load by removing redundant filters or channels. However, the complicated structures adopted in some modern DNNs (i.e., ResNet or DenseNet) enforce strict constraints on the input/output dimension of certain layers. This requires additional filter grouping during the pruning and filter rearranging after the pruning to make the pruned structure valid (Wen et al., 2017a; Ding et al., 2019) . Factorization method approximates the weight matrix of a layer with a multiplication of two or more low-rank matrices. It by nature keeps the input/output dimension of a layer unchanged, and therefore the resulted decomposed network can be supported by any common DNN computation architectures, without additional grouping and post-processing. The previous investigation show that it is feasible to approximate the weight matrices of a pretrained DNN model with the multiplication of low-rank matrices, but it may greatly degrade the performance (Jaderberg et al., 2014; Zhang et al., 2015; . Some other methods attempt to manipulate the ""directions"" of filters to implicitly reduce the rank of weight matrices (Wen et al., 2017b; Li et al., 2019) . However, the difficulties in training and the implicitness of rank representation prevent these methods from reaching a high compression rate. Nuclear norm regularizer (Xu et al., 2018) has been used to directly reduce the rank of weight matrices. Optimizing the nuclear norm requires back propagation through singular value decomposition (SVD). Applying such a numerical process on every training step is inefficient and unstable. Our work aims to explicitly achieve a low-rank DNN network during the training without applying SVD on every step. In particular, we propose SVD training by training the weight matrix of each layer in the form of its full-rank SVD. The weight matrix is decomposed into the matrices of left-singular vectors, singular values and right-singular vectors, and the training is done on the decomposed variables. Furthermore, two techniques are proposed to induce low-rank while maintaining high performance during the SVD training: (1) Singular vector orthogonality regularization which keeps the singular vector matrices close to unitary through the training. It mitigates gradient vanishing/exploding during the training, and provide a valid form of SVD to guarantee the effective rank reduction. (2) Singular value sparsification which applies sparsity-inducing regularizers on the singular values during the training to induce low-rank. The low-rank model is finally achieved through singular value pruning. We evaluate the individual contribution of each technique as well as the overall performance when putting them together via ablation studies. Results show that the proposed method constantly beats SOTA factorization and structural pruning methods on various tasks and model structures. In this work, we propose the SVD training framework, which incorporates the full-rank decomposed training and singular value pruning to reach low-rank DNNs with minor accuracy loss. We apply SVD to decompose each DNN layer before the training and directly train with the decomposed singular vectors and singular values, so we can keep an explicit measure of layers' ranks without performing the SVD on each step. Orthogonality regularizers are applied to the singular vectors during the training to keep the decomposed layers in a valid SVD form. And sparsity-inducing regularizers are applied to the singular values to explicitly induce low-rank layers. Thorough experiments are done to analyse each proposed technique. We demonstrate that the orthogonality regularization on singular vector matrices is crucial to the performance of the decomposed training process. For decomposition methods, we find that the spatial-wise method performs better than channel-wise in shallower networks while the performances are similar for deeper models. For the sparsity-inducing regularizer, we show that higher compression rate can be achieved by Hoyer regularizer comparing to that of the L 1 regularizer under low accuracy loss. We further apply the proposed method to various depth of ResNet models on both CIFAR-10 and ImageNet dataset, where we find the accuracy-#FLOPs tradeoff achieved by the proposed method constantly stays above the Pareto frontier of previous methods, including both factorization and structural pruning methods. These results prove that this work provides an effective way for learning low-rank deep neural networks. 1943-1955, 2015. A EXPERIMENT SETUPS Our experiments are done on the CIFAR-10 dataset (Krizhevsky & Hinton, 2009 ) and the ImageNet ILSVRC-2012 dataset (Russakovsky et al., 2015) . We access both datasets via the API provided in the ""TorchVision"" Python package. As recommended in the PyTorch tutorial, we normalize the data and augment the data with random crop and random horizontal flip before the training. We use batch size 100 to train CIFAR-10 model and use 256 for the ImageNet model. For all the models on CIFAR-10, both the full-rank SVD training and the low-rank finetuning are trained for 164 epochs. The learning rate is set to 0.001 initially and decayed by 0.1 at epoch 81 and 122. For models on ImageNet, the full-rank SVD training is trained for 90 epochs, with initial learning rate 0.1 and learning rate decayed by 0.1 every 30 epochs. The finetuning is done for 60 epochs, starting at learning rate 0.01 and decay by 0.1 at epoch 30. We use pretrained full-rank decomposed model (trained with the orthogonality regularizer but without sparsity-inducing regularizer) to initialize the SVD training. SGD optimizer with momentum 0.9 is used for optimizing all the models, with weight decay 5e-4 for CIFAR-10 models and 1e-4 for ImageNet models. The accuracy reported in the experiment is the best testing accuracy achieved during the finetuning process. During the SVD training, the decay parameter of the orthogonality regularizer λ o is set to 1.0 for both channel-wise and spatial-wise decomposition on CIFAR-10. On ImageNet, λ o is set to 10.0 for channel-wise decomposition and 5.0 for spatial-wise decomposition. The decay parameter λ s for the sparsity-inducing regularizer and the energy threshold used for singular value pruning are altered through different set of experiments to fully explore the accuracy-#FLOPs tradeoff. In most cases, the energy threshold is selected through a line search, where we find the highest percentage of energy that can be pruned without leading to a sudden accuracy drop. The λ s and the energy thresholds used in each set of the experiments are reported alongside the experiment results in Appendix B.",Efficiently inducing low-rank deep neural networks via SVD training with sparse singular values and orthogonal singular vectors.,Pareto ; Chen et al. ; Wen et al. ; Xu et al. ; TorchVision ; Redmon ; Wang ; Luo et al. ; Li ; al.,These results ; channels ; the-art ; rate ; most cases ; the orthogonality regularizer ; drones ; the performance ; Redmon et al ; the data,Pareto ; Chen et al. ; Wen et al. ; Xu et al. ; TorchVision ; Redmon ; Wang ; Luo et al. ; Li ; al.,"Modern deep neural networks (DNNs) require high memory consumption and large computational loads. In order to deploy DNN algorithms efficiently on edge or mobile devices, DNN compression algorithms have been explored. Factorization methods approximate the weight matrix of a DNN layer with multiplication of two or multiple low-rank matrices. However, it is difficult to measure the ranks of DNN layers during the training process. In this work, we propose SVD training, which first applies SVD to decompose DNN's layers and then performs training on full-rank decomposed weights. This approach improves the training quality and convergence, while",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Creating a knowledge base that is accurate, up-to-date and complete remains a significant challenge despite substantial efforts in automated knowledge base construction.   In this paper, we present Alexandria -- a system for unsupervised, high-precision knowledge base construction. Alexandria uses a probabilistic program to define a process of converting knowledge base facts into unstructured text.   Using probabilistic inference, we can invert this program and so retrieve facts, schemas and entities from web text. The use of a probabilistic program allows uncertainty in the text to be propagated through to the retrieved facts, which increases accuracy and helps merge facts from multiple sources. Because Alexandria does not require labelled training data, knowledge bases can be constructed with the minimum of manual input. We demonstrate this by constructing a high precision (typically 97\%+) knowledge base for people from a single seed fact. Search engines and conversational assistants require huge stores of knowledge in order to answer questions and understand basic facts about the world. As a result, there has been significant interest in creating such knowledge bases (KBs) and corresponding efforts to automate their construction and maintenance (see BID15 for a review). For example, KnowledgeVault BID4 , NELL BID0 BID11 , YAGO2 BID7 , DIG [P. BID12 , and many other systems aim either to construct a KB automatically or make an existing KB more complete. Despite these efforts, there remain significant ongoing challenges with keeping KBs up-to-date, accurate and complete. Existing automated approaches still require manual effort in the form of at least one of the following: a provided set of entities used for supervised training of components such as entity linkers/recognizers; a provided schema used to define properties/relations of entities; or a provided set of annotated texts used to train fact extractors/part of speech taggers. The holy grail of KB construction and maintenance would be a system which could learn and update its own schema, which could automatically discover new entities as they come into existence, and which could extract facts from natural text with such high precision that no human checking is needed.With this goal in mind, we present Alexandria -a system for unsupervised, high-precision knowledge base construction. At the core of Alexandria is a probabilistic program that defines a process of generating text from a knowledge base consisting of a large set of typed entities. By applying probabilistic inference to this program, we can reason in the inverse direction: going from text back to facts. This inverse reasoning allows us to retrieve facts, schemas and entities from web text. The use of a probabilistic program also provides an elegant way to handle the uncertainty inherent in natural text. An important advantage of using a generative model is that Alexandria does not require labelled data, which means it can be applied to new domains with little or no manual effort. The model is also inherently task-neutral -by varying which variables in the model are observed and which are inferred, the same model can be used for: learning a schema (relation discovery), entity discovery, entity linking, fact retrieval and other tasks, such as finding sources that support a particular fact. In this paper we demonstrate schema learning, fact retrieval, entity discovery and entity linking. We will evaluate the former two tasks, while the latter two are performed as part of these main tasks.An attractive aspect of our approach is that the entire system is defined by one coherent probabilistic model. This removes the need to create and train many separate components such as tokenizers, named entity recognizers, part-of-speech taggers, fact extractors, linkers and so on; a disadvantage of having such multiple components is that they are likely to encode different underlying assumptions, reducing the accuracy of the combined system. Furthermore, the use of a single probabilistic program allows uncertainty to be propagated consistently throughout the system -from the raw web text right through to the extracted facts (and back).Related work -There has been a significant amount of work on automated knowledge base construction BID15 . Because the Alexandria system can be used to perform many different tasks, it is related to a range of previous, task-specific systems. Here we describe the most relevant.Unsupervised learning -Open IE (Information Extraction) systems, such as Reverb BID5 and OLLIE BID8 aim to discover information across multiple domains without having labelled data for new domains. Such systems do not have an underlying schema, but instead retain information in a lexical form. This can lead to duplication of the same fact stored using different words, or can even allow conflicting facts to be stored. Representing facts in lexical form makes them hard for applications to consume, since there is no schema to query against. In contrast, Alexandria aims to infer the underlying schema of new domains and to extract facts in a consistent form separate from their lexical representation.Schema learning -the closest existing work is Biperpedia BID6 which aims to discover properties for many classes at once. Biperpedia uses search engine query logs as well as text to discover attributes, in a process that involves a number of trained classifiers and corresponding labelled training data. Alexandria's key differences are its unsupervised approach and the fact that schema learning is integrated into a single probabilistic model also used to perform other tasks.Web scale fact extraction -several existing systems can extract facts against a known schema across the entire web. These include KnowledgeVault [Dong et al., 2014] , NELL BID11 and DeepDive BID17 . Of these, KnowledgeVault is the largest scale and has performed KB completion (filling in missing values for entities where most values are known) of over 250M facts. DeepDive is perhaps the most similar system to Alexandria in that it is based around a probabilistic model -a Markov Logic Network (MLN) BID13 . DeepDive uses hand-constructed feature extractors to extract candidate facts from text for incorporation into the MLN. Because Alexandria uses a generative model of text, it can be applied directly to web text, without the need for feature extractors; in the mode described in this paper only a single seed fact is needed. In this paper, we have shown how Alexandria can perform schema learning and high-precision fact extraction unsupervised, except for a single seed example. Whilst the results in this paper are for people entities, the system has been designed to be generally applicable to many types of entity. It's worth noting that, in the process of learning about people, we have learned seed examples for other classes such as places, which we could use to do schema learning and fact extraction for these classes. By repeating this process recursively, we create the exciting possibility of using Alexandria like an Open IE system, to learn schemas, discovery entities and extract facts automatically across a large number of domains -this is our focus for future work. Our hope is that our high accuracy and strong typing will prevent 'drift' from occurring, which has reduced accuracy in previous Open IE systems.The Alexandria model does not use any joint prior across property values -such as the graph prior and tensor factorization priors used in BID4 . Incorporating such priors into the model has the potential to increase precision yet further.Alexandria's template-based language model is relatively simple compared to some NLP systems used in related work. In contrast, Alexandria's model of types and values is in general more sophisticated, particularly in its handling and propagation of uncertainty. We believe that this has allowed the system to achieve very high precision. We expected to need a more sophisticated language model to achieve high recall -in fact, the ability to process the entire web means that we can still achieve good recall -a fact expressed in a complex way on one page is often expressed simply elsewhere.The simplicity of the language model has one advantage -that it can be readily applied to text in many different languages. Indeed, we found that the system learned by itself to extract data from some non-English pages. To make full use of this would require making the built-in types multi-lingual, for example, allowing different month names in dates and different ways of writing numbers. The benefit would be to improve recall and also to learn how facts are expressed differently in different locales.We believe that Alexandria makes a step towards the holy grail of completely automatic KB construction and maintenance -we look forward to trying out the system in many new domains to see if the successful unsupervised learning of people can be replicated for other entity types.","This paper presents a system for unsupervised, high-precision knowledge base construction using a probabilistic program to define a process of converting knowledge base facts into unstructured text.",at least one ; one ; DeepDive ; KnowledgeVault ; two ; Biperpedia ; Dong et al. ; non-English ; KB ; month,natural text ; good recall ; annotated texts ; the built-in types ; lexical form ; the system ; the ability ; any joint ; a schema ; other entity types,at least one ; one ; DeepDive ; KnowledgeVault ; two ; Biperpedia ; Dong et al. ; non-English ; KB ; month,"Creating a knowledge base that is accurate, up-to-date and complete remains a significant challenge despite substantial efforts in automated knowledge base construction. Alexandria uses a probabilistic program to define a process of converting knowledge base facts into unstructured text.   The use of probabilistics inference allows uncertainty in the text to be propagated through to the retrieved facts, which increases accuracy and helps merge facts from multiple sources. The highest precision (typically 97\%+) knowledge base for people from a single seed fact can be constructed with minimum of manual input. Search engines and conversational assistants require huge stores of knowledge in order to",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Conducting reinforcement-learning experiments can be a complex and timely process. A full experimental pipeline will typically consist of a simulation of an environment, an implementation of one or many learning algorithms, a variety of additional components designed to facilitate the agent-environment interplay, and any requisite analysis, plotting, and logging thereof. In light of this complexity, this paper introduces simple rl, a new open source library for carrying out reinforcement learning experiments in Python 2 and 3 with a focus on simplicity. The goal of simple_rl is to support seamless, reproducible methods for running reinforcement learning experiments. This paper gives an overview  of the core design philosophy of the package, how it differs from existing libraries, and showcases its central features.","This paper introduces and motivates simple_rl, a new open source library for carrying out reinforcement learning experiments in Python 2 and 3 with a focus on simplicity.",one ; simple_rl,this paper ; a new open source library ; a simulation ; this complexity ; an implementation ; the core design philosophy ; it ; one or many learning algorithms ; simple rl ; Python,one ; simple_rl,"Reinforcement learning experiments typically consist of a simulation of an environment, an implementation of one or many learning algorithms, and any requisite analysis, plotting, and logging of agent-environment interplay. Simple rl, a new open source library for conducting reinforcement learning experiments in Python 2 and 3, is designed to support seamless, reproducible methods.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Hierarchical Sparse Coding (HSC) is a powerful model to efficiently represent multi-dimensional, structured data such as images. The simplest solution to solve this computationally hard problem is to decompose it into independent layerwise subproblems. However, neuroscientific evidence would suggest inter-connecting these subproblems as in the Predictive Coding (PC) theory, which adds top-down connections between consecutive layers. In this study, a new model called Sparse Deep Predictive Coding (SDPC) is introduced to assess the impact of this inter-layer feedback connection. In particular, the SDPC is compared with a Hierarchical Lasso (Hi-La) network made out of a sequence of Lasso layers. A 2-layered SDPC and a Hi-La networks are trained on 3 different databases and with different sparsity parameters on each layer. First, we show that the overall prediction error generated by SDPC is lower thanks to the feedback mechanism as it transfers prediction error between layers. Second, we demonstrate that the inference stage of the SDPC is faster to converge than for the Hi-La model. Third, we show that the SDPC also accelerates the learning process. Finally, the qualitative analysis of both models dictionaries, supported by their activation probability, show that the SDPC features are more generic and informative. Finding a ""efficient"" representation to model a given signal in a concise and efficient manner is an inverse problem that has always been central to the machine learning community. Sparse Coding (SC) has proven to be one of the most successful methods to achieve this goal. SC holds the idea that signals (e.g. images) can be encoded as a linear combination of few features (called atoms) drawn from a bigger set called the dictionary (Elad, 2010) . The pursuit of optimal coding is usually decomposed into two complementary subproblems: inference (coding) and dictionary learning. Inference consists in finding an accurate sparse representation of the input data considering the dictionaries are fixed, it could be performed using algorithms like ISTA & FISTA (Beck & Teboulle, 2009 ), Matching Pursuit (Mallat & Zhang, 1993) , Coordinate Descent (Li & Osher, 2009 ), or ADMM (Heide et al., 2015) . Once the representation is inferred, one can learn the atoms from the data using methods like gradient descent (Rubinstein et al., 2010; Kreutz-Delgado et al., 2003; Sulam et al., 2018) , or online dictionary learning (Mairal et al., 2009a) . Consequently, SC offers an unsupervised framework to learn simultaneously basis vectors (e.g. atoms) and the corresponding input representation. SC has been applied with success to image restoration (Mairal et al., 2009b) , feature extraction (Szlam et al., 2010) and classification (Yang et al., 2011; Perrinet & Bednar, 2015) . Interestingly, SC is also a field of interest for computational neuroscientists. Olshausen & Field (1997) first demonstrated that adding a sparse prior to a shallow neural network was sufficient to account for the emergence of neurons whose Receptive Fields (RFs) are spatially localized, band-pass and oriented filters, analogous to those found in the primary visual cortex (V1) of mammals (Hubel & Wiesel, 1962) . Because most of the SC algorithms are limited to single-layer network, they cannot model the hierarchical structure of the visual cortex. However, few solutions have been proposed to tackle Hierarchical Sparse Coding (HSC) as a global optimization problem (Sulam et al., 2018; Makhzani & Frey, 2013; . These methods are looking for an optimal solution of HSC without considering their plausibility in term of neuronal implementation. Consequently, the quest for reliable HSC formulation that is compatible with a neural implementation remains open. Rao & Ballard (1999) introduce the Predictive Coding (PC) to model the effect of the interaction of cortical areas in the visual cortex. PC intends to solve the inverse problem of vision by combining feedforward and feedback connections. In PC, feedback connection carries prediction of the neural activity of the lower cortical area while feedforward pass prediction error to the higher cortical area. In such a framework, neural population are updated to minimize the unexpected component of the neural signal (Friston, 2010) . PC has been applied for supervised object recognition Spratling, 2017) or unsupervised prediction of future video frames (Lotter et al., 2016) . Interestingly, PC is flexible enough to introduce a sparse prior to each layer. Therefore, one can consider PC as a bio-plausible formulation of the HSC problem. This formulation is to confront with the other bio-plausible HSC formulation that consists of a stack of independent Lasso problems (Sun et al., 2017) . To the best of our knowledge, no study has compared these two mathematically different formulations of the same problem of optimizing the Hierarchical Sparse Coding of images. What is the effect of top-down connection of PC? What are the consequences in term of computations and convergence? What are the qualitative differences concerning the learned atoms? The objective of this study is to experimentally answer these questions and to show that the PC framework could be successfully used for improving solutions to HSC problems. We start our study by defining the two different mathematical formulations to solve the HSC problem: the Hierarchical Lasso (Hi-La) that consists in stacking Lasso sub-problems, and the 2-Layers Sparse Predictive Coding (2L-SPC) that leverages PC into a deep and sparse network of bi-directionally connected layers. To experimentally compare both models, we train the 2L-SPC and Hi-La networks on 4 different databases and we vary the sparsity of each layer. First, we compare the overall prediction error of the two models and we break it down to understand its distribution among layers. Second, we analyze the number of iterations needed for the state variables of each network to reach their stability. Third, we compare the convergence of both models during the dictionary learning stage. Finally, we discuss the qualitative differences between the features learned by both networks in light of their activation probability. What are the computational advantages of inter-layer feedback connections in hierarchical sparse coding algorithms? We answered this question by comparing the Hierarchical Lasso (Hi-La) and the 2-Layers Sparse Predictive Coding (2L-SPC) models. Both are identical in every respect, except that the 2L-SPC brings inter-layer feedback connections. This extra-connection forces the internal state variables of the 2L-SPC to converge toward a trade-off between on one hand an accurate prediction passed by the lower-layer and on the other hand a facilitated predictability by the upperlayer. Experimentally, we demonstrated on 4 different databases and for a 2-layered network that the inter-layer feedback top-down connection (i) mitigates the overall prediction error by distributing it among layers, (ii) accelerates the convergence towards a stable internal state and (iii) accelerates the learning process. Besides, we qualitatively observed that top-down connections bring contextual information that helps to extract more informative and less over-fitted features. The 2L-SPC holds the novelty to consider Hierarchical Sparse Coding as a combination of local sub-problems that are tightly related. This a crucial difference with CNNs that are trained by backpropagating gradients from a global loss. To the best of our knowledge the 2L-SPC is the first one that leverage local sparse coding into a hierarchical and unsupervised algorithms (the ML-CSC from (Sulam et al., 2018 ) is equivalent to a one layer sparse coding algorithm , and the ML-ISTA from ) is trained using supervised learning). Moreover, even if our results are robust as they hold for 4 different databases and with a large spectrum of first and second layer sparsity, further work will be conducted to generalize our results to deeper networks and different sparse coding algorithms such as Coordinate Descent or ADMM. Further studies will show that our 2L-SPC framework could be used for practical applications like image inpainting, denoising, or image super-resolution.",This paper experimentally demonstrates the beneficial effect of top-down connections in Hierarchical Sparse Coding algorithm.,Hubel & Wiesel ; Olshausen & Field ; Second ; the Hierarchical Lasso ; Sparse Coding ; SC ; Perrinet & Bednar ; Szlam ; Yang et al. ; ML-CSC,the interaction ; contextual information ; (Elad ; different sparse ; ISTA ; (iii ; Wiesel ; feature ; practical applications ; a hierarchical and unsupervised algorithms,Hubel & Wiesel ; Olshausen & Field ; Second ; the Hierarchical Lasso ; Sparse Coding ; SC ; Perrinet & Bednar ; Szlam ; Yang et al. ; ML-CSC,"Hierarchical Sparse Coding (HSC) is a powerful model to efficiently represent multi-dimensional, structured data such as images. However, neuroscientific evidence suggests inter-connecting these subproblems as in the Predictive Coding theory, which adds top-down connections between consecutive layers. The SDPC is compared with a Hierarchical Lasso (Hi-La) network made out of a sequence of Lasso layers, and a 2-layered SDPC and a Hi-La networks are trained on 3 different databases and with different sparsity parameters on each layer. The overall prediction error generated by",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Revealing latent structure in data is an active field of research, having introduced exciting technologies such as variational autoencoders and adversarial networks, and is essential to push machine learning towards unsupervised knowledge discovery. However, a major challenge is the lack of suitable benchmarks for an objective and quantitative evaluation of learned representations. To address this issue we introduce Morpho-MNIST, a framework that aims to answer: ""to what extent has my model learned to represent specific factors of variation in the data?"" We extend the popular MNIST dataset by adding a morphometric analysis enabling quantitative comparison of trained models, identification of the roles of latent variables, and characterisation of sample diversity. We further propose a set of quantifiable perturbations to assess the performance of unsupervised and supervised methods on challenging tasks such as outlier detection and domain adaptation. A key factor for progress in machine learning has been the availability of well curated, easy-to-use, standardised and sufficiently large annotated datasets for benchmarking different algorithms and models. This has led to major advances in speech recognition, computer vision, and natural language processing. A commonality between these tasks is their natural formulation as supervised learning tasks, wherein performance can be measured in terms of accuracy on a test set.The general problem of representation learning (i.e. to reveal latent structure in data) is more difficult to assess due the lack of suitable benchmarks. Although the field is very active, with many recently proposed techniques such as probabilistic autoencoders and adversarial learning, it is less clear where the field stands in terms of progress or which approaches are more expressive for specific tasks. The lack of reproducible ways to quantify performance has led to subjective means of evaluation: visualisation techniques have been used to show low-dimensional projections of the latent space and visual inspection of generated or reconstructed samples are popular to provide subjective measures of descriptiveness. On the other hand, the quality of sampled images generally tells us little about how well the learned representations capture known factors of variation in the training distribution. In order to advance progress, the availability of tools for objective assessment of representation learning methods seems essential yet lacking. This paper introduces Morpho-MNIST, a collection of shape metrics and perturbations, in a step towards quantitative assessment of representation learning. We build upon one of the most popular machine learning benchmarks, MNIST, which despite its shortcomings remains widely used. While MNIST was originally constructed to facilitate research in image classification, in the form of recognising handwritten digits BID17 , it has found its use in representation learning, for example, to demonstrate that the learned latent space yields clusters consistent with digit labels. Methods aiming to disentangle the latent space claim success if individual latent variables capture specific style variations (e.g. stroke thickness, sidewards leaning digits and other visual characteristics).The main appeal of selecting MNIST as a benchmark for representation learning is that, while manifesting complex interactions between pixel intensities and underlying shapes, it has well understood and easily measurable factors of variation. More generally, MNIST remains popular in practice due to several factors: it allows reproducible comparisons with previous results reported in the literature; the dataset is sufficiently large for its complexity and consists of small, two-dimensional greyscale images defining a tractable ten-class classification problem; computation and memory requirements With Morpho-MNIST we provide a number of mechanisms to quantitatively assess representation learning with respect to measurable factors of variation in the data. We believe that this is an important asset for future research on generative models, and we would like to emphasize that the proposed morphometrics can be used post hoc to evaluate already trained models, potentially revealing novel insights and interesting observations.A similar morphometry approach could be used with other datasets such as dSprites, e.g. estimating shape location and size, number of objects/connected components. Perhaps some generic image metrics may be useful for analysis on other datasets, e.g. relating to sharpness or colour diversity, or we could even consider using the output of object detectors (analogously to the Inception-based scores; e.g. number/class of objects, bounding boxes etc.). In future work we plan to include additional perturbations, for example, mimicking imaging artefacts commonly observed in medical imaging modalities to add further complexity and realism.","This paper introduces Morpho-MNIST, a collection of shape metrics and perturbations, in a step towards quantitative evaluation of representation learning.",Morpho-MNIST ; digit labels ; two ; ten ; Morpho ; dSprites ; Inception,adversarial learning ; an important asset ; unsupervised and supervised methods ; boxes ; e.g. estimating shape location ; medical imaging modalities ; their natural formulation ; measurable factors ; practice ; progress,Morpho-MNIST ; digit labels ; two ; ten ; Morpho ; dSprites ; Inception,"Revealing latent structure in data is an active field of research, having introduced exciting technologies such as variational autoencoders and adversarial networks, and is essential to push machine learning towards unsupervised knowledge discovery. However, a major challenge is the lack of suitable benchmarks for objective and quantitative evaluation of learned representations. To address this issue, we introduce Morpho-MNIST, a collection of shape metrics and perturbations, in a step towards quantitative assessment of representation learning.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"The conversion of scanned documents to digital forms is performed using an Optical Character Recognition (OCR) software. This work focuses on improving the quality of scanned documents in order to improve the OCR output. We create an end-to-end document enhancement pipeline which takes in a set of noisy documents and produces clean ones. Deep neural network based denoising auto-encoders are trained to improve the OCR quality. We train a blind model that works on different noise levels of scanned text documents. Results are shown for blurring and watermark noise removal from noisy scanned documents. Scanned documents are stored as images and need to be processed by an Optical Character Recognition (OCR) software to extract the text contents into a digital format such as an ASCII text file. This is an active research area and there are many tools in the market that process a scanned document and extract the content in a digital format. The success with extraction of digital output is heavily dependent on the quality of the scanned document. In practice, however, there is some noise associated with the scanned document. Typical noises seen in scanned documents are blurring, watermarks, fading, and salt & pepper. With the rise of deep learning adoption in computer vision tasks, there are many neural network models available for image denoising and restoration [1] . However, most of the literature focuses on pictures (e.g., images from natural scenes) but not text documents, and the techniques used are not directly applicable due to very different nature of text document images. The designed REDNET was successfully tested on deblurring document images with various levels of intensity as well as removing both gray-level and color watermarks from text image documents. Currently, research on designing a unified network that can remove all noise types from text documents is ongoing.","We designed and tested a REDNET (ResNet Encoder-Decoder) with 8 skip connections to remove noise from documents, including blurring and watermarks, resulting in a high performance deep network for document image cleanup.",OCR ; ASCII ; salt & pepper ; REDNET,intensity ; denoising auto-encoders ; research ; that ; The designed REDNET ; the text contents ; a scanned document ; practice ; scanned text documents ; a digital format,OCR ; ASCII ; salt & pepper ; REDNET,The conversion of scanned documents to digital forms is performed using an Optical Character Recognition (OCR) software. Deep neural network based denoising auto-encoders are trained to improve OCR quality. The results for blurring and watermark noise removal from noisy scanned documents are shown in REDNET. Scanned documents are stored as images and need to be processed by an optical character scan scan software to extract the text contents into a digital format such as an ASCII text file. This is an active research area and there are many tools in the market that process a scanned document and extract the content in digital format. The success,/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Knowledge bases, massive collections of facts (RDF triples) on diverse topics, support vital modern applications. However, existing knowledge bases contain very little data compared to the wealth of information on the Web. This is because the industry standard in knowledge base creation and augmentation suffers from a serious bottleneck: they rely on domain experts to identify appropriate web sources to extract data from. Efforts to fully automate knowledge extraction have failed to improve this standard: these automated systems are able to retrieve much more data and from a broader range of sources, but they suffer from very low precision and recall. As a result, these large-scale extractions remain unexploited. In this paper, we present MIDAS, a system that harnesses the results of automated knowledge extraction pipelines to repair the bottleneck in industrial knowledge creation and augmentation processes. MIDAS automates the suggestion of good-quality web sources and describes what to extract with respect to augmenting an existing knowledge base. We make three major contributions. First, we introduce a novel concept, web source slices, to describe the contents of a web source. Second, we define a profit function to quantify the value of a web source slice with respect to augmenting an existing knowledge base. Third, we develop effective and highly-scalable algorithms to derive high-profit web source slices. We demonstrate that MIDAS produces high-profit results and outperforms the baselines significantly on both real-word and synthetic datasets. Knowledge bases support a wide range of applications and enhance search results for multiple major search engines, such as Google and Bing BID1 .The coverage and correctness of knowledge bases are crucial for the applications that use them, and for the quality of the user experience. However , there exists a gap between facts on the Web and in knowledge bases: compared to the wealth of information on the Web, most knowledge bases are largely incomplete, with many facts missing. For example , one of the largest knowledge bases, Freebase BID7 BID0 , does not provide sufficient facts for different types of cocktails such as the ingredients of Margarita. Yet, such information is explicitly profiled and described by many web sources, such as Wikipedia. Figure 1 : Two knowledge extraction procedures and Midas. The output of the automated process (b) is often discarded in industrial production due to low accuracy. Midas uses the the automatically-extracted facts to identify the right web sources for the semi-automated process under the industry standard and therefore resolves a major bottleneck.Industry standard. Industry typically follows a semi-automated knowledge extraction process to create or augment a knowledge base with facts that are new to an existing knowledge base (or new facts) from the Web. This process ( Figure 1a ) first relies on domain experts to select web sources; it then uses crowdsourcing to annotate a fraction of entities and facts and treats them as the training data; finally, it applies wrapper induction BID19 BID21 and learns Xpath patterns to extract facts from the selected web sources. Since source selection and training data preparation are carefully curated, this process achieves high precision and recall with respect to each selected web source. However, it can only produce a small volume of facts overall and cannot scale, as the source-selection step is a severe bottleneck, relying on manual curation by domain experts.Automated process. To conquer the scalability limitation in the industry standard, automated knowledge extraction BID13 BID30 attempts to extract facts with little or no human intervention. Instead of manually selecting a small set of web sources, automated extraction (Figure 1b) often takes a wide variety of web sources, e.g., ClueWeb09 BID10 , as input and uses facts in an existing knowledge base, or a small portion of labeled input web sources, as training data. This automated extraction process is able to produce a vast number of facts. However, because of the limited training data (per source), especially for uncommon facts, e.g., the ingredients of Margarita, this process suffers from low accuracy. The TAC-KBP competition showed that automated processes BID33 BID3 BID34 BID12 can hardly achieve above 0.3 recall, leaving a lot of the wealth of web information unexploited. Due to this limitation, such automatically extracted facts are often abandoned for knowledge bases in industrial production.In this paper, we propose Midas 1 , a system that harnesses the correct 2 extractions of the automated process to automatically identify suitable web sources and repair the bottleneck in the industry standard. The core insight of Midas is that the automatically extracted facts, even though they may not be of high overall accuracy and coverage, give clues about which web sources contain a large amount of valuable information, allow for easy annotation, and are worthwhile for extraction. We demonstrate this through an example.1. Our system is named after King Midas, known in Greek mythology for his ability to turn what he touched into gold. 2. We refer to correct facts as facts with confidence value ≥ 0.7 as true. Example 1. FIG1 shows a snapshot of high-confidence facts (subject , predicate, object) extracted from 5 web pages under web domain http://space.skyrocket.de. Automated extraction systems may not be able to obtain high precision and recall in extracting facts from this website due to lack of effective training data. However, the few correct extracted facts give important clues on what one could extract from this site. For each fact, the subject indicates an entity; the predicate and object values further describe properties associated with the entity. For example, fact t 1 specifies that the category property of the entity Project Mercury is space program. Entities can form groups based on their common properties. For example, entity ""Project Mercury"" and entity ""Project Gemini "" are both ""space programs that are sponsored by NASA"".The facts labeled ""Y"" in the ""new?"" column are absent from Freebase . All of these new facts are under the same sub-domain and are all "" rocket families sponsored by the NASA."" This observation provides a critical insight: one can augment Freebase by extracting facts pertaining to ""rocket families sponsored by NASA"" from http://space.skyrocket.de/doc_lau_fam.Example 1 shows that one can abstract the contents of a web source through extracted facts: A web source often includes facts of multiple groups of homogeneous entities. Each group of entities forms a particular subset of content in the web source, which we call a web source slice (or slice). The common properties shared by the group of entities not only define, but also describe the slice of facts. For example, it is easy to tell that a slice describes ""rocket families sponsored by NASA"" through its common properties, ""category = rocket family"" and ""sponsor = NASA"". Moreover, entities in a single web source slice often belong to the same type, e.g., ""rocket families sponsored by NASA"", and thus share similar predicates. The limited number of predicates in a web source slice simplifies annotation . Our objective is to discover web source slices that (1) contain a sufficient number of facts that are absent from the knowledge base we wish to augment, and (2) their extraction effort does not outweigh the benefit.However, evaluating and quantifying the suitability of a web source slice with respect to these two desired properties is not straightforward. In addition, the number of slices in a single web source often grows exponentially with the number of facts, posing a significant scalability challenge. This challenge is amplified by the massive number of sources on the Web, in various genres, languages, and domains. Even a single web domain Midas derived slides using facts extracted from a real-world , large-scale, automated knowledge extraction pipeline (name hidden for anonymity) that operates on billions of web pages. New facts refer to extracted facts that are absent from Freebase.may contain an extensive amount of knowledge. For example, as of July 2018, there are more than 45 million entries in Wikipedia [3] .Midas addresses these challenges through (1) efficient and scalable algorithms for producing web source slices, and (2) an effective profit function for measuring the utility of slices. In this paper, we first formalize the problem of identifying and describing ""good"" web sources as an optimization problem and then quantify the quality of web source slices through a profit function (Section 2). We then propose an algorithm to generate the high-profit slices in a single web source and design a scalable framework to extend this algorithm for multiple web sources (Section 3). Finally, we evaluate our proposed algorithm on both real-word and synthetic datasets and illustrate that our proposed system, Midas, is able to identify interesting web sources slices in an efficient and scalable manner (Section 4). Example 2. We applied Midas on AnonSys, a dataset extracted by a comprehensive knowledge extraction system, which includes 810M facts extracted from 218M web sources. Midas is able to identify and customize ""good"" web sources for an existing knowledge base. In FIG2 , we demonstrate the 5 highest-profit slices that Midas derived to augment Freebase. The web source slices provide new and valuable information for augmenting the existing knowledge base ; in addition, many of these web sources contain semi-structured data with respect to entities in the reported web source slice. Therefore, they are easy for annotation. In this paper, we presented Midas, an effective and highly-parallelizable system, that leverages extracted facts in web sources, for detecting high-profit web source slices to fill knowledge gaps. In particular, we defined a web source slice as a selection query that indicates what to extract and from which web source. We designed an algorithm, Midas alg , to detect high-quality slices in a web source and we proposed a highly-parallelizable framework to scale Midas to million of web sources. We analyzed the performance of our techniques in synthetic data scenarios, and we demonstrated that Midas is effective and efficient in real-world settings.However, there are still many challenges towards solving this problem due to the quality of current extraction systems. There is a substantial number of missing extractions due to the lack of training data and one cannot infer the quality of web sources with respect to such missing extractions. In our future work, we plan to extend our techniques to conquer the limitations of extractions and improve the quality of the derived web source slices.Crawling. The first step of the augmentation process is to crawl and extract the facts in a given web source. This requires training the crawler for the facts in each slice. We use a unit cost f p to model the cost of training, which includes annotating and schema matching, for each slice. The cost for the rest of the crawling process is proportional to the size of the web source BID16 . Measuring the size of web sources is hard due to their diverse design and format; instead, we estimate it based on the total number of facts extracted from the web sources, scaled proportional to an adjustable normalization factor f c : DISPLAYFORM0 De-duplication. A typical step in the augmentation process is to identify and purge redundant facts before adding them to the knowledge base. This de-duplication is often performed through linkage BID5 BID22 BID20 between the facts of the slice and those of the knowledge base. Thus, the de-duplication cost is proportional to the number of facts selected by the web source slice, subject to an adjustable normalization factor (f d ): DISPLAYFORM1 Before adding facts to a knowledge base, it is essential to verify their validity. The cost of this step is proportional to the new facts that the slice contributes, and subject to an adjustable normalization factor (f v ) that depends on the employed validation technique BID35 BID28 : DISPLAYFORM2 Finally, we compute the cost of slices in the same web domain C(S) as the sum of the respective costs of the crawling, de-duplication, and validation steps. DISPLAYFORM3 The four adjustable normalization factors included in the computation of each of the three costs relate to the particular techniques used for the corresponding steps (e.g., different de-duplication methods may result in different values for f d ). In this paper, we set these factors such that they are roughly proportional to the actual execution time of such techniques. However, one can always adjust the setting of these factors. For our experiments, we use the default values f p = 10, f c = 0.001, f d = 0.01, and f v = 0.1 (we switch to f p = 1 for the running examples in the paper). Thus, de-duplication is more costly than crawling, and validation is proportionally the most expensive operation except training.",This paper focuses on identifying high quality web sources for industrial knowledge base augmentation pipeline.,"Margarita ; NASA"" ; MIDAS ; four ; NASA ; billions ; one ; Project Gemini ; Crawling ; Project Mercury",their diverse design ; high-quality slices ; search results ; those ; web sources ; input ; both real-word ; a significant scalability challenge ; multiple web sources ; example,"Margarita ; NASA"" ; MIDAS ; four ; NASA ; billions ; one ; Project Gemini ; Crawling ; Project Mercury","Knowledge bases, massive collections of facts (RDF triples) on diverse topics, support vital modern applications. However, existing knowledge bases contain very little data compared to the wealth of information on the Web. This is because the industry standard in knowledge base creation and augmentation suffers from a serious bottleneck. These automated systems are able to retrieve much more data and from a broader range of sources, but they suffer from very low precision and recall. In this paper, we present MIDAS, a system that harnesses the results of automated knowledge extraction pipelines to repair the bottleneck in industrial knowledge creation, augmentation processes. MIDAS autom",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"We study the convergence of gradient descent (GD) and stochastic gradient descent (SGD) for training $L$-hidden-layer linear residual networks (ResNets). We prove that for training deep residual networks with certain linear transformations at input and output layers, which are fixed throughout training, both GD and SGD with zero initialization on all hidden weights can converge to the global minimum of the training loss. Moreover, when specializing to appropriate Gaussian random linear transformations, GD and SGD provably optimize wide enough deep linear ResNets. Compared with the global convergence result of GD for training standard deep linear networks \citep{du2019width}, our condition on the neural network width is sharper by a factor of $O(\kappa L)$, where $\kappa$ denotes the condition number of the covariance matrix of the training data. In addition, for the first time we establish the global convergence of SGD for training deep linear ResNets and prove a linear convergence rate when the global minimum is $0$. Despite the remarkable power of deep neural networks (DNNs) trained using stochastic gradient descent (SGD) in many machine learning applications, theoretical understanding of the properties of this algorithm, or even plain gradient descent (GD), remains limited. Many key properties of the learning process for such systems are also present in the idealized case of deep linear networks. For example, (a) the objective function is not convex; (b) errors back-propagate; and (c) there is potential for exploding and vanishing gradients. In addition to enabling study of systems with these properties in a relatively simple setting, analysis of deep linear networks also facilitates the scientific understanding of deep learning because using linear networks can control for the effect of architecture choices on the expressiveness of networks (Arora et al., 2018; Du & Hu, 2019) . For these reasons, deep linear networks have received extensive attention in recent years. One important line of theoretical investigation of deep linear networks concerns optimization landscape analysis (Kawaguchi, 2016; Hardt & Ma, 2016; Freeman & Bruna, 2016; Lu & Kawaguchi, 2017; Yun et al., 2018; Zhou & Liang, 2018) , where major findings include that any critical point of a deep linear network with square loss function is either a global minimum or a saddle point, and identifying conditions on the weight matrices that exclude saddle points. Beyond landscape analysis, another research direction aims to establish convergence guarantees for optimization algorithms (e.g. GD, SGD) for training deep linear networks. Arora et al. (2018) studied the trajectory of gradient flow and showed that depth can help accelerate the optimization of deep linear networks. Ji & Telgarsky (2019) ; Gunasekar et al. (2018) investigated the implicit bias of GD for training deep linear networks and deep linear convolutional networks respectively. More recently, Bartlett et al. (2019) ; Arora et al. (2019a) ; Shamir (2018) ; Du & Hu (2019) analyzed the optimization trajectory of GD for training deep linear networks and proved global convergence rates under certain assumptions on the training data, initialization, and neural network structure. Inspired by the great empirical success of residual networks (ResNets), Hardt & Ma (2016) considered identity parameterizations in deep linear networks, i.e., parameterizing each layer's weight matrix as I`W, which leads to the so-called deep linear ResNets. In particular, Hardt & Ma (2016) established the existence of small norm solutions for deep residual networks with sufficiently large depth L, and proved that there are no critical points other than the global minimum when the maximum spectral norm among all weight matrices is smaller than Op1{Lq. Motivated by this intriguing finding, Bartlett et al. (2019) studied the convergence rate of GD for training deep linear networks with identity initialization, which is equivalent to zero initialization in deep linear ResNets. They assumed whitened data and showed that GD can converge to the global minimum if (i) the training loss at the initialization is very close to optimal or (ii) the regression matrix Φ is symmetric and positive definite. (In fact, they proved that, when Φ is symmetric and has negative eigenvalues, GD for linear ResNets with zero-initialization does not converge.) Arora et al. (2019a) showed that GD converges under substantially weaker conditions, which can be satisfied by random initialization schemes. The convergence theory of stochastic gradient descent for training deep linear ResNets is largely missing; it remains unclear under which conditions SGD can be guaranteed to find the global minimum. In this paper, we establish the global convergence of both GD and SGD for training deep linear ResNets without any condition on the training data. More specifically, we consider the training of L-hidden-layer deep linear ResNets with fixed linear transformations at input and output layers. We prove that under certain conditions on the input and output linear transformations, GD and SGD can converge to the global minimum of the training loss function. Moreover, when specializing to appropriate Gaussian random linear transformations, we show that, as long as the neural network is wide enough, both GD and SGD with zero initialization on all hidden weights can find the global minimum. There are two main ingredients of our proof: (i) establishing restricted gradient bounds and a smoothness property; and (ii) proving that these properties hold along the optimization trajectory and further lead to global convergence. We point out the second aspect is challenging especially for SGD due to the uncertainty of its optimization trajectory caused by stochastic gradients. We summarize our main contributions as follows: • We prove the global convergence of GD and SGD for training deep linear ResNets. Specifically, we derive a generic condition on the input and output linear transformations, under which both GD and SGD with zero initialization on all hidden weights can find global minima. Based on this condition, one can design a variety of input and output transformations for training deep linear ResNets. • When applying appropriate Gaussian random linear transformations, we show that as long as the neural network width satisfies m "" Ωpkrκ 2 q, with high probability, GD can converge to the global minimum up to an -error within Opκ logp1{ qq iterations, where k, r are the output dimension and the rank of training data matrix X respectively, and κ "" }X} 2 2 {σ 2 r pXq denotes the condition number of the covariance matrix of the training data. Compared with previous convergence results for training deep linear networks from Du & Hu (2019) , our condition on the neural network width is independent of the neural network depth L, and is strictly better by a factor of OpLκq. • Using the same Gaussian random linear transformations, we also establish the convergence guarantee of SGD for training deep linear ResNets. We show that if the neural network width satisfies m "" r Ω`krκ 2 log 2 p1{ q¨n 2 {B 2˘, with constant probability, SGD can converge to the global minimum up to an -error within r O`κ 2 ´1 logp1{ q¨n{B˘iterations, where n is the training sample size and B is the minibatch size of stochastic gradient. This is the first global convergence rate of SGD for training deep linear networks. Moreover, when the global minimum of the training loss is 0, we prove that SGD can further achieve linear rate of global convergence, and the condition on the neural network width does not depend on the target error . As alluded to above, we analyze networks with d inputs, k outputs, and m ě maxtd, ku nodes in each hidden layer. Linear transformations that are fixed throughout training map the inputs to the first hidden layer, and the last hidden layer to the outputs. We prove that our bounds hold with high probability when these input and output transformations are randomly generated by Gaussian distributions. If, instead, the input transformation simply copies the inputs onto the first d components of the first hidden layer, and the output transformation takes the first k components of the last hidden layer, then our analysis does not provide a guarantee. There is a good reason for this: a slight modification of a lower bound argument from Bartlett et al. (2019) demonstrates that GD may fail to converge in this case. However, we describe a similarly simple, deterministic, choice of input and output transformations such that wide enough networks always converge. The resulting condition on the network width is weaker than that for Gaussian random transformations, and thus improves on the corresponding convergence guarantee for linear networks, which, in addition to requiring wider networks, only hold with high probability for random transformations. In this section, we will discuss several different choices of linear transformations at input and output layers and their effects to the convergence performance. For simplicity, we will only consider the condition for GD. As we stated in Subsection 3.1, GD converges if the input and output weight matrices A and B Then it is interesting to figure out what kind of choice of A and B can satisfy this condition. In Proposition 3.3, we showed that Gaussian random transformations (i.e., each entry of A and B is generated from certain Gaussian distribution) satisfy this condition with high probability, so that GD converges. Here we will discuss the following two other transformations. Identity transformations. We first consider the transformations that A "" rI dˆd , 0 dˆpm´dq s J and B "" a m{k¨rI kˆk , 0 kˆpm´kq s. which is equivalent to the setting in Bartlett et al. (2019) when m "" k "" d. Then it is clear that σ min pBq "" σ max pBq "" a m{k and σ min pAq "" σ max pAq "" 1. Now let us consider LpW p0q q. By our choices of B and A and zero initialization on weight matrices in hidden layers, in the case that d "" k, we have {2 could be as big as F˘( for example, when X and Y are orthogonal). Then plugging these results into (4.1), the condition on A and B becomes where the second inequality is due to the fact that LpW˚q ď }Y} 2 F {2. Then it is clear if }X} F ě ? 2{C, the above inequality cannot be satisfied for any choice of m, since it will be cancelled out on both sides of the inequality. Therefore, in such cases, our bound does not guarantee that GD achieves global convergence. Thus, it is consistent with the non-convergence results in (Bartlett et al., 2019) . Note that replacing the scaling factor a m{k in the definition of B with any other function of d, k and m would not help. Gaussian random initialization on hidden weights, where the input and output weights are generated by random initialization, and remain fixed throughout the training. Modified identity transformations. In fact, we show that a different type of identity transformations of A and B can satisfy the condition (4.1). Here we provide one such example. Assuming m ě d`k, we can construct two sets S 1 , S 2 Ă rms satisfying Then we construct matrices A and B as follows: where α is a parameter which will be specified later. In this way, it can be verified that BA "" 0, σ min pAq "" σ max pAq "" 1, and σ min pBq "" σ max pBq "" α. Thus it is clear that the initial training loss satisfies LpW p0q q "" }Y} 2 F {2. Then plugging these results into (4.1), the condition on A and B can be rewritten as The R.H.S. of the above inequality does not depend on α, which implies that we can choose sufficiently large α to make this inequality hold. Thus, GD can be guaranteed to achieve the global convergence. Moreover, it is worth noting that using modified identity transformation, a neural network with m "" d`k suffices to guarantee the global convergence of GD. We further remark that similar analysis can be extended to SGD. In this paper, we proved the global convergence of GD and SGD for training deep linear ResNets with square loss. More specifically, we considered fixed linear transformations at both input and output layers, and proved that under certain conditions on the transformations, GD and SGD with zero initialization on all hidden weights can converge to the global minimum. In addition, we further proved that when specializing to appropriate Gaussian random linear transformations, GD and SGD can converge as long as the neural network is wide enough. when W is staying inside a certain region. Its proof is in Section B.1. Lemma A.1. Let τ "" 1{L, then for any weight matrices satisfying max lPrLs }W l } 2 ď 0.5, it holds that, In addition, , the stochastic gradient G l in Algorithm 1 satisfies where B is the minibatch size. The gradient lower bound can be also interpreted as the Polyak-Łojasiewicz condition, which is essential to the linear convergence rate. The gradient upper bound is crucial to bound the trajectory length, since this lemma requires that max lPrLs }W l } ď 0.5. The following lemma proves the smoothness property of the training loss function LpWq when W is staying inside a certain region. Its proof is in Section B.2. Lemma A.2. Let τ "" 1{L. Then for any two collections of weight matrices, denoted by Based on these two lemmas, we are able to complete the proof of all theorems, which are provided as follows.","Under certain condition on the input and output linear transformations, both GD and SGD can achieve global convergence for training deep linear ResNets.",σ max ; Arora et al ; One ; R.H.S. ; G l ; W ; Lu & Kawaguchi ; max lPrLs ; Kawaguchi ; al.,"certain assumptions ; us ; example ; the stochastic gradient G l ; hidden weights ; the smoothness property ; a saddle point ; the setting ; a similarly simple, deterministic, choice ; linear networks",σ max ; Arora et al ; One ; R.H.S. ; G l ; W ; Lu & Kawaguchi ; max lPrLs ; Kawaguchi ; al.,"The convergence of gradient descent and stochastic gradient descent (SGD) for training deep residual networks (ResNets) with certain linear transformations at input and output layers can converge to the global minimum of the training loss. Furthermore, when specializing to appropriate Gaussian random linear transformations, GD and SGD provably optimize wide enough deep linear ResNets. The condition on the neural network width is sharper by a factor of $O(\kappa L)$, where $\kappa$ denotes the condition number of the covariance matrix of training data. The global convergence of SGD is achieved when the global maximum is $0",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"We study the problem of semantic code repair, which can be broadly defined as automatically fixing non-syntactic bugs in source code. The majority of past work in semantic code repair assumed access to unit tests against which candidate repairs could be validated. In contrast, the goal here is to develop a strong statistical model to accurately predict both bug locations and exact fixes without access to information about the intended correct behavior of the program. Achieving such a goal requires a robust contextual repair model, which we train on a large corpus of real-world source code that has been augmented with synthetically injected bugs. Our framework adopts a two-stage approach where first a large set of repair candidates are generated by rule-based processors, and then these candidates are scored by a statistical model using a novel neural network architecture which we refer to as Share, Specialize, and Compete. Specifically, the architecture (1) generates a  shared encoding of the source code using an RNN over the abstract syntax tree, (2) scores each candidate repair using specialized network modules, and (3) then normalizes these scores together so they can compete against one another in comparable probability space. We evaluate our model on a real-world test set gathered from GitHub containing four common categories of bugs. Our model is able to predict the exact correct repair 41% of the time with a single guess, compared to 13% accuracy for an attentional sequence-to-sequence model. The term automatic code repair is typically used to describe two overarching tasks: The first involves fixing syntactic errors, which are malformations that cause the code to not adhere to some language specification BID9 BID5 . The second, which is the focus of this work, involves fixing semantic bugs, which refer to any case where the actual program behavior is not the same as the behavior the programmer intended. Clearly, this covers an extremely wide range of code issues, so this work is limited to a class of semantic bugs, which we roughly define as: ""Bugs that can be identified and fixed by an experienced human programmer, without running the code or having deep contextual knowledge of the program."" This does not imply that the bugs are trivially fixable, as they often require time-consuming analysis of the code, rich background knowledge of the language and APIs, and complex logical reasoning about the original programmer's intent.Unlike previous work, we do not assume access to unit tests at training or test time. This requirement is important because it forces development of models which can infer intended semantic purpose from source code before proposing repairs, as a human programmer might. Most previous work relies on unit tests -a common theme is combining coarse-grained repair models with search algorithms to find some repair that satisfies unit tests BID10 BID18 . In contrast, our proposed task requires models to deeply understand the code in order to propose a single set of repairs. Thus, semantic code repair without unit tests presents a concrete, real-world test bed for the more general task of understanding and modifying source code.Our semantic repair model was trained on a large corpus of open-source Python projects with synthetically injected bugs. We test on both real-bug and synthetic-bug test sets. 1 To train the repair model, we first evaluated an attentional sequence-to-sequence architecture. Although this model was able to achieve non-trivial results, we believe it to be an unsuitable solution in a number of ways, such as the lack of direct competition between repair candidates at different locations. Instead, we use an alternative approach which decouples the non-statistical process of generating and applying repair proposal from the statistical process of scoring and ranking repairs.This two-stage process itself is not new, but the core novelty in this work is the specific neural framework we propose for scoring repair candidates. We refer to our architecture as a Share, Specialize, and Compete (SSC) network:• SHARE: The input code snippet is encoded with a neural network. This is a shared representation used by all repair types.• SPECIALIZE: Each repair type is associated with its own specialized neural module BID2 , which emits a score for every repair candidate of that type.• COMPETE: The raw scores from the specialized modules are normalized to compete in comparable probability space.Our model can also be thought of as an evolution of work on neural code completion and summarization BID1 BID6 . Like those systems, our SHARE network is used to learn a rich semantic understanding of the code snippet. Our SPECIALIZE modules then build on top of this representation to learn how to identify and fix specific bug types.Although we have described our framework in relation to the problem of code repair, it has a number of other possible applications in sequence transformation scenarios where the input and output sequences have high overlap. For example, it could be applied to natural language grammar correction BID17 , machine translation post editing BID11 , source code refactoring BID0 , or program optimization BID7 . Our first goal is to conceptually understand at what ""level"" the model was able to generalize to new snippets. Although the hidden activations of the neural network model are not directly interpretable, we can attempt to interpret the latent model space using nearest neighbor retrieval on the hidden vectors h i . The goal is to determine if the model is simply memorizing common n-grams, or if it is actually learning high-level repair concepts. Nearest neighbor retrieval for several test snippets are presented here:In Example 1, we see the model is able to learn a high-level pattern ""y.x = x"". In Example 2 we see the pattern ""if (x c 1 y...) elif (x c 2 y...)"". In Example 3 we see the pattern ""Strings usually use the equality (or inequality) operator."" In all cases, the surface form of the training nearest neighbor is very different from the test snippet. From this, it appears that the SSC model is able to learn a number of interesting, high-level patterns which it uses to generalize to new data.We next examined failure cases of the SSC model which a human evaluator was able to repair correctly. Here, the primary weakness of the model was that humans were able to better infer program intent by using variable names, function names, and string literals. One major fault in the current implementation is a lack of sub-word representation. For example, consider a repair of the expression ""dtypes.append(x )"" where x could be dtype or syncnode. It is easy for a human to infer that dtype is the more sensible choice even without deeper understand of the code. In future work we plan to explore character-level encoding of value strings so that lexical similarity can be modeled latently by the network.We finally examined cases where the SSC model succeeded but the human evaluator failed. Generally, we conclude that the model's primary advantage was the sheer amount of data it was able to learn from. For example, consider the expression ""if (db.version_info <= 3)"". This may not be immediately suspicious to a human, but if we analyze the reference training data we can measure that the pattern ""if (x.version_info <= y )"" is 10 times less frequent than the pattern ""if (x.version_info < y)"". Intuitively, this makes sense because if a feature is added in version y, it is not useful to check <= y. However, the neural model is able to easily learn such probabilistic distributions even without deeper understanding of why they are true. We presented a novel neural network architecture that allows specialized network modules to explicitly model different transformation types based on a shared input representation. When applied to the domain of semantic code repair, our model achieves high accuracy relative to a seq2seq baseline and an expert human evaluation. In our analysis of the results, we find that our system is able to learn fairly sophisticated repair patterns from the training data. In future work we plan to expand our model to cover a larger set of bug types, and ideally these bug types would be learned automatically from a corpus of real-world bugs. We also plan to apply the SSC model to other tasks. The application of a pooled pointer module at a single time step, to predict the variable replacement scores for each potential replacement of the token fname. The input here is the per-token representation computed by the SHARE module. Representations for variable names are passed through a pooling module which outputs per-variable pooled representations. These representations are then passed through a similarity module, as in standard pointer networks, to yield a (dynamically-sized) output dictionary containing one score for each unique variable.",A neural architecture for scoring and ranking program repair candidates to perform semantic program repair statically without access to unit tests.,two ; first ; Share ; Compete ; RNN ; GitHub ; four ; second ; SSC ; One,each potential replacement ; that type.• COMPETE ; the training data ; complex logical reasoning ; fairly sophisticated repair patterns ; the reference training data ; its own specialized neural module ; high overlap ; repairs ; direct competition,two ; first ; Share ; Compete ; RNN ; GitHub ; four ; second ; SSC ; One,"We study semantic code repair, which involves automatically fixing non-syntactic bugs in source code without access to unit tests. The goal is to develop a robust contextual repair model, which we train on a large corpus of real-world source code that has been augmented with synthetically injected bugs. Our framework adopts a two-stage approach where first a large set of repair candidates are generated by rule-based processors, and then these candidates are scored by a statistical model using a novel neural network architecture which we refer to as Share, Specialize, and Compete. The architecture generates a shared encoding of source code using an R",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Given the importance of remote sensing, surprisingly little attention has been paid to it by the representation learning community. To address it and to speed up innovation in this domain, we provide simplified access to 5 diverse remote sensing datasets in a standardized form. We specifically explore in-domain representation learning and address the question of ""what characteristics should a dataset have to be a good source for remote sensing representation learning"". The established baselines achieve state-of-the-art performance on these datasets. 
 Remote sensing via computer vision and transfer learning is an important domain to address climate change as outlined by Rolnick et al. (2019) . Among others, research in remote sensing promises to help in solving challenges in food security (precision farming), water sustainability, disaster prevention (floods/landslides/earthquake forecasting), deforestation or wild fire detection, urban planning, and monitoring of carbon stocks and fluxes or the air quality. The number of Earth observing satellites is constantly increasing, with currently over 700 satellites monitoring many aspects of the Earth's surface and atmosphere from space, generating terabytes of imagery data every day. However the ground truth data acquisition is costly, usually requiring extensive campaign preparation, people and equipment transportation, and in-field gathering of the characteristics under question. While there are remote sensing communities working on applying general deep learning methods to remote sensing problems, this domain has received relatively little attention from the representation learning community. Given its importance, it is still in an early development stage in comparison to the progress made in representation learning on natural and medical images (eg. by Raghu et al. (2019) ). Some reasons for this are the diversity of data sources (satellite types, data acquisition modes, resolutions), the need of domain knowledge and special data processing, and the wide and scattered field of applications. The scarcity of standard recognized benchmark datasets and evaluation frameworks is another. For a long time there were only small labeled remote sensing datasets available. Only recenctly new large-scale datasets (eg. by ; Sumbul et al. (2019) ) have been generated for remote sensing problems. However, a consistent evaluation framework is still missing and the performance is usually reported on non-standard splits and with varying metrics, making reproduction and quick research iteration difficult. To address this, we provide five representative and diverse remote sensing datasets in a standardized form for easy reuse. In particular, we explore the importance of in-domain representation learning for remote sensing at various data sizes and establish new state-of-the-art baseline results. The main goal of this work is to develop general remote sensing representations that can be applied by researchers to other unseen remote sensing tasks. By providing these standardized datasets, common problem definition and baselines, we hope this work will simplify and enable faster iteration of research on remote sensing and inspire general representation learning experts to test their newest methods in this critical domain. In summary, the main contributions of this work are as follows: 1. Exploring in-domain representation learning to train generic remote sensing representations. 2. Generating 5 existing remote sensing datasets in a standardized format and establishing a common evaluation protocol. Publishing the best trained representations for easy reuse in transfer learning applications 1 . 3. Establishing state-of-the-art baselines for the BigEarthNet, EuroSAT, RESISC-45, So2Sat, and UC Merced datasets. We present a common evaluation benchmark for remote sensing representation learning based on five diverse datasets. The results demonstrate the enhanced performance of in-domain representations, especially for tasks with limited number of training samples, and achieve state-of-the-art performance. The five analyzed datasets and the best trained in-domain representations are published for easy reuse by the public. We investigate dataset characteristics to be a good source for remote sensing representation learning. As the experimental results indicate, having a multi-resolution dataset helps to train more generalizable representations. Other factors seem to be label quality, number of classes, visual similarity across the classes and visual diversity within the classes. Surprisingly, we observed that representations trained on the large weakly-supervised datasets were not as successful as that of a smaller and more diverse human-curated dataset. However, some results were inconclusive and require more investigation. Understanding the main factors of a good remote sensing dataset for representation learning is a major challenge, solving which could improve performance across a wide range of remote sensing tasks and applications.",Exploration of in-domain representation learning for remote sensing datasets.,Rolnick et al ; Earth ; every day ; Raghu et al ; Sumbul ; five ; BigEarthNet ; UC Merced,natural and medical images ; training samples ; monitoring ; a major challenge ; classes ; remote sensing tasks ; the characteristics ; limited number ; generic remote sensing representations ; observing satellites,Rolnick et al ; Earth ; every day ; Raghu et al ; Sumbul ; five ; BigEarthNet ; UC Merced,"The importance of remote sensing has been neglected by the representation learning community. To address this, we provide simplified access to 5 diverse remote sensing datasets in a standardized form for easy reuse. The established baselines achieve state-of-the-art performance on these datasets. Remote sensing via computer vision and transfer learning is an important domain to address climate change as outlined by Rolnick et al. (2019). Remote sensing can help in solving challenges in food security (precision farming), water sustainability, disaster prevention (floods/landslides/earthquake forecasting), urban planning, environmental planning, and monitoring of carbon",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"One of the big challenges in machine learning applications is that training data can be different from the real-world data faced by the algorithm. In language modeling, users’ language (e.g. in private messaging) could change in a year and be completely different from what we observe in publicly available data. At the same time, public data can be used for obtaining general knowledge (i.e. general model of English). We study approaches to distributed fine-tuning of a general model on user private data with the additional requirements of maintaining the quality on the general data and minimization of communication costs. We propose a novel technique that significantly improves prediction quality on users’ language compared to a general model and outperforms gradient compression methods in terms of communication efficiency. The proposed procedure is fast and leads to an almost 70% perplexity reduction and 8.7 percentage point improvement in keystroke saving rate on informal English texts. Finally, we propose an experimental framework for evaluating differential privacy of distributed training of language models and show that our approach has good privacy guarantees. Two common problems arising after deployment of a machine learning model on user devices are discrepancy between training data and actual data stored on user devices, and the need of regular model updates. In the case of language modeling, it corresponds to the difference between language and style of the training corpus mined in the Internet and messages of the user, which account for most of the text generated on the device. Even if the training corpus includes a substantial part of informal texts (tweets, forum threads, etc.), real user data can be very different. This is a challenge for word prediction algorithms in software keyboard applications. The most general approach to improvement of customer experience in typing is integrating a separate user language model trained on device in an on-line fashion. In the simplest case it is a smoothed n-gram (e.g. Kneser-Ney n-gram model BID6 )).In BID26 continuously learned personalized language model based on LSTM was proposed but as far as each user generates only a small portion of textual data, such data by itself cannot be used for updates of the general model. Thus , for a model update, a collection of potentially sensitive data from many users is needed. As shown in , collecting data for training may be avoided. We propose a similar approach for distributed fine-tuning of language models on private data. In this sense our method can be considered as ""federated fine-tuning"" but we prefer to take more traditional term. In this setting we start with a language model trained on a large text corpus representing the general language. This model G will be updated continuously on user devices but with an additional requirement that the model must not go too far from the general language model, i.e. we don't overfit on user data.We pursue two goals: 1) to develop an algorithm of distributed fine-tuning that is fast, communication efficient and doesn't need collecting sensitive user data; and 2) to prevent the language model from forgetting ""general English"". Besides , we provide analysis of possibility of privacy violation After each round the server model G t+1 is sent to the next K elements. in our model. BID8 ) demonstrated an attack on distributed training algorithm leading to information leakage. This means that privacy analysis in necessary for such algorithms.Our main contributions are: 1) we propose an efficient procedure of distributed fine-tuning of language models immune to the problem of catastrophic forgetting BID3 ), 2) we provide experimental evaluation of on-device training time, communication costs and convergence rates of the general language model in realistic conditions, 3) we compare two most popular strategies of improving communication efficiency in the context of distributed learning, and 4) we propose an experimental framework for evaluation of differential privacy of distributed training of language models, and using this framework, we evaluate privacy guarantees of our approach.In our research we are focused on improvement of keystroke saving rate (see section 2.4) because this metric reflects customer typing experience more directly than perplexity or BLEU. We use LSTM architecture for our language model as described in BID27 and evaluate ondevice training time for this architecture. We show that the on-device training time is reasonably small, thus demonstrating the feasibility of the whole approach. We have presented our results in distributed fine-tuning of neural language models. We paid special attention to preventing a catastrophic forgetting of the general language after a model fine-tuning on the user devices. Our experiments showed that the performance of an initial model of the general English on user data can be improved significantly almost without a performance degradation on the standard English training data. We found that a combination of on-device training with random rehearsal and server-side model averaging provides the best performance for such distributed finetuning. Users' models were trained for the whole epoch that reduced communication costs while at the same time being quite fast -it took less than 3 minutes with a realistic assessment of volume of the available user data. Finally, we provided an experimental evaluation of differential privacy of our method and showed that the method has a reasonable level of differential privacy compared to other solutions. We still have to note that we provided an empirical estimation of differential privacy which holds with some high probability but not almost surely.This statistic doesn't converge to the Kolmogorov distribution as shown in W. Lilliefors (1969) . It converges to the distribution with smaller critical values at the same significance levels because we overfit on the sample data when the estimator r is plugged in. We chose a 5% significance level and critical value for it is 1.08. In 19 cases out of 20 the Lilliefors test failed to reject the null hypothesis at a 5% significance level. TAB4 provides exact values obtained during the application of the statistical test. Relying on these values along with data visualization in 3 we can state that random variable c(s) has tails that decrease like the Pareto distribution tails.The hypothesis that we accepted suggests that the cumulative distribution function of c(s) is given by the formula (8). It means that the tail distribution function for all x > x 0 is given by DISPLAYFORM0 We chose x 0 = c (k) n , so F (x 0 ) is just the ratio k/n. Thus, C can be estimated by DISPLAYFORM1 Values of C are given in the TAB4 . Finally, from formula (11) and proposition 1 it is easy to derive that (ε, δ)-differential privacy is provided by the values ε, δ that satisfy DISPLAYFORM2",We propose a method of distributed fine-tuning of language models on user devices without collection of private data,Two ; Pareto ; English ; Lilliefors ; Kneser-Ney ; W. Lilliefors ; Kolmogorov ; BLEU ; a year ; c(s,that privacy analysis ; the context ; The proposed procedure ; the device ; the case ; the cumulative distribution function ; communication ; δ)-differential privacy ; the whole approach ; a model fine-tuning,Two ; Pareto ; English ; Lilliefors ; Kneser-Ney ; W. Lilliefors ; Kolmogorov ; BLEU ; a year ; c(s,"Machine learning applications face significant challenges due to the difference between training data and real-world data. In language modeling, users’ language could change in a year and be completely different from what we observe in publicly available data. Public data can be used for obtaining general knowledge, and distributed fine-tuning of a general model on user private data with the additional requirements of maintaining the quality on the general data and minimization of communication costs. We propose a novel technique that significantly improves prediction quality on users' language and outperforms gradient compression methods in communication efficiency. The proposed procedure is fast and leads to an almost 70% perplexity",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"This paper considers multi-agent reinforcement learning (MARL) in networked system control. Specifically, each agent learns a decentralized control policy based on local observations and messages from connected neighbors. We formulate such a networked MARL (NMARL) problem as a spatiotemporal Markov decision process and introduce a spatial discount factor to stabilize the training of each local agent. Further, we propose a new differentiable communication protocol, called NeurComm, to reduce information loss and non-stationarity in NMARL. Based on experiments in realistic NMARL scenarios of adaptive traffic signal control and cooperative adaptive cruise control, an appropriate spatial discount factor effectively enhances the learning curves of non-communicative MARL algorithms, while NeurComm outperforms existing communication protocols in both learning efficiency and control performance. Reinforcement learning (RL), formulated as a Markov decision process (MDP), is a promising data-driven approach for learning adaptive control policies (Sutton & Barto, 1998) . Recent advances in deep neural networks (DNNs) further enhance its learning capacity on complex tasks. Successful algorithms include deep Q-network (DQN) (Mnih et al., 2015) , deep deterministic policy gradient (DDPG) (Lillicrap et al., 2015) , and advantage actor critic (A2C) (Mnih et al., 2016) . However, RL is not scalable in many real-world control problems. This scalability issue is addressed in multi-agent RL (MARL), where each agent learns its individual policy from only local observations. However, MARL introduces new challenges in model training and execution, due to non-stationarity and partial observability in a decentralized MDP from the viewpoint of each agent. To address these challenges, various learning methods and communication protocols are proposed to stabilize training and improve observability. This paper considers networked MARL (NMARL) in the context of networked system control (NSC), where agents are connected via a communication network for a cooperative control objective. Each agent performs decentralized control based on its local observations and messages from connected neighbors. NSC is extensively studied and widely applied. Examples include connected vehicle control (Jin & Orosz, 2014) , traffic signal control (Chu et al., 2019) , distributed sensing (Xu et al., 2018) , and networked storage operation (Qin et al., 2016) . We expect an increasing trend of NMARL based controllers in the near future, after the development of advanced communication technologies such as 5G and Internet-of-Things. Recent works studied decentralized NMARL under assumptions of global observations and local rewards (Zhang et al., 2018; Qu et al., 2019) , which are reasonable in multi-agent gaming but not suitable in NSC. First, the control infrastructures are distributed in a wide region, so collecting global observations in execution increases communication delay and failure rate, and hurts the robustness. Second, online learning is not common due to safety and efficiency concerns. Rather, each model is trained offline and tested extensively before field deployment. In online execution, the model only runs forward propagation, and its performance is constantly monitored for triggering re-training. To reflect these practical constraints in NSC, we assume 1) each agent is connected to a limited number We have formulated the spatiotemporal MDP for decentralized NSC under neighborhood communication. Further, we have introduced the spatial discount factor to enhance non-communicative MARL algorithms, and proposed a neural communication protocol NeurComm to design adaptive and efficient communicative MARL algorithms. We hope this paper provides a rethink on developing scalable and robust MARL controllers for NSC, by following practical engineering assumptions and combining appropriate learning and communication methods rather than reusing existing MARL algorithms. One future direction is improving the recurrent units to naturally control spatiotemporal information flows within the meta-DNN in a decentralized way. Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Başar. Fully decentralized multiagent reinforcement learning with networked agents. arXiv preprint arXiv:1802.08757, 2018.",This paper proposes a new formulation and a new communication protocol for networked multi-agent control problems,Lillicrap et al. ; Qu ; Sutton & Barto ; Tamer Başar ; First ; One ; DDPG ; Zhang et al. ; Tong Zhang ; al.,failure rate ; advantage actor critic ; non ; the spatiotemporal MDP ; One future direction ; decentralized NSC ; Recent advances ; these challenges ; cooperative adaptive cruise control ; online execution,Lillicrap et al. ; Qu ; Sutton & Barto ; Tamer Başar ; First ; One ; DDPG ; Zhang et al. ; Tong Zhang ; al.,"Multi-agent reinforcement learning (MARL) in networked system control (NSC) involves each agent learning a decentralized control policy based on local observations and messages from connected neighbors. We introduce a spatial discount factor to stabilize training of each local agent and introduce a differentiable communication protocol, NeurComm, to reduce information loss and non-stationarity in NMARL. In real-world scenarios, the learning curves of non-communicative MARL algorithms outperform existing communication protocols in both learning efficiency and control performance. Reinforcement learning (RL), formulated as a Markov decision process (MDP), is a promising data",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Stochastic Gradient Descent or SGD is the most popular optimization algorithm for large-scale problems. SGD estimates the gradient by uniform sampling with sample size one. There have been several other works that suggest faster epoch wise convergence by using weighted non-uniform sampling for better gradient estimates. Unfortunately, the per-iteration cost of maintaining this adaptive distribution for gradient estimation is more than calculating the full gradient. As a result, the false impression of faster convergence in iterations leads to slower convergence in time, which we call a chicken-and-egg loop. In this paper, we break this barrier by providing the first demonstration of a sampling scheme, which leads to superior gradient estimation, while keeping the sampling cost per iteration similar to that of the uniform sampling. Such an algorithm is possible due to the sampling view of Locality Sensitive Hashing (LSH), which came to light recently. As a consequence of superior and fast estimation, we reduce the running time of all existing gradient descent algorithms. We demonstrate the benefits of our proposal on both SGD and AdaGrad. In this paper, we proposed a novel LSH-based sampler with a reduction to the gradient estimation variance. We achieved it by sampling with probability proportional to the L 2 norm of the instances gradients leading to an optimal distribution that minimizes the variance of estimation. More remarkably, LSD is as computationally efficient as SGD but achieves faster convergence not only epoch wise but also time wise.Peilin Zhao and Tong Zhang. Stochastic optimization with importance sampling for regularized loss minimization. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pp. 1-9, 2015.A EPOCH PLOTS AND PROOFS Theorem 3. Let S be the bucket that sample x m is chosen from in Algorithm 2. Let p m be the sampling probability associated with sample x m . Suppose we query a sample with ✓ t . Then we have an unbiased estimator of the full gradient: DISPLAYFORM0 Proof. DISPLAYFORM1 Theorem 4. The Trace of the covariance of our estimator is: DISPLAYFORM2 Proof.",We improve the running of all existing gradient descent algorithms.,SGD ; first ; Locality Sensitive Hashing ; LSH ; AdaGrad ; Peilin Zhao ; Tong Zhang ; International Conference on Machine Learning ; Trace,Proceedings ; a sampling scheme ; gradient estimation ; the sampling probability ; our estimator ; a sample ; which ; (LSH ; a result ; the bucket,SGD ; first ; Locality Sensitive Hashing ; LSH ; AdaGrad ; Peilin Zhao ; Tong Zhang ; International Conference on Machine Learning ; Trace,"Stochastic Gradient Descent (SGD) is the most popular optimization algorithm for large-scale problems. It estimates the gradient by uniform sampling with sample size one. However, the per-iteration cost of maintaining this adaptive distribution for gradient estimation is more than the full gradient. The false impression of faster convergence in iterations leads to slower convergence in time, which is called a chicken-and-egg loop. In this paper, we introduce a sampling scheme, which leads to superior gradient estimation, while keeping the sampling cost per iteration similar to that of uniform sampling. This is due to the sampling view of Locality Sens",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Neural Network (NN) has achieved state-of-the-art performances in many tasks within image, speech, and text domains. Such great success is mainly due to special structure design to fit the particular data patterns, such as CNN capturing spatial locality and RNN modeling sequential dependency. Essentially, these specific NNs achieve good performance by leveraging the prior knowledge over corresponding domain data. Nevertheless, there are many applications with all kinds of tabular data in other domains. Since there are no shared patterns among these diverse tabular data, it is hard to design specific structures to fit them all. Without careful architecture design based on domain knowledge, it is quite challenging for NN to reach satisfactory performance in these tabular data domains. To fill the gap of NN in tabular data learning, we propose a universal neural network solution, called TabNN, to derive effective NN architectures for tabular data in all kinds of tasks automatically. Specifically, the design of TabNN follows two principles: \emph{to explicitly leverages expressive feature combinations} and \emph{to reduce model complexity}. Since GBDT has empirically proven its strength in modeling tabular data, we use GBDT to power the implementation of TabNN. Comprehensive experimental analysis on a variety of tabular datasets demonstrate that TabNN can achieve much better performance than many baseline solutions. Recent years have witnessed the extraordinary success of Neural Networks (NN), especially Deep Neural Networks, in achieving state-of-the-art performances in many domains, such as image classification BID27 , speech recognition BID25 , and text mining BID22 . Beside enlarged model capacity, such great achievement of NN is mainly due to the deliberate design of its structures derived from prior knowledge over the certain domain data. For example, Convolutional Neural Networks (CNN) BID40 have become the standard solution to address image classification since it can capture the spatial locality by using ""Local Receptive Field"" BID40 , which is a common pattern in image data. Recurrent Neural Networks (RNN) BID29 , as another example, has been widely-used on speech recognition and language modeling because its recurrent structure can effectively model the sequential dependency among speech and text data.In contrast to most of tasks in image, speech, or text domains whose input yields natural spatial or temporal dimension, many other real-world applications, e.g., click through rate prediction BID24 , time series forecasting BID49 BID11 , web search ranking BID0 BID8 , etc, bear structured input consisting of multi-dimension meaningful features. Typically, such input data can be generalized as the tabular data, as each row of the tabular corresponds to one data example and each column denotes an individual meaningful feature. Despite the success of CNN and RNN over computer vision, speech recognition, and natural language process, adopting NN over tabular data receives far less attention and yet remains quite challenging. In particular, as illustrated in previous studies BID18 , it usually leads to unsatisfactory performance on tabular data by directly using Fully Connected Neural Network (FCNN), because its fully connected model structure leads to very complex optimization hyper-planes with a high risk of falling into local optimums. Moreover, since different applications usually indicate various effective feature combinations within their respective tabular data, it is quite beneficial to recognize such feature combinations and take advantage of them to design the effective NN model on their tabular data, which however has not been well studied yet.To address these challenges, we identify two principles for the purpose of designing effective NN models on tabular data: (1) To explicitly leverage expressive feature combinations. Rather than blindly pouring all features together into FCNN and learning via back-propagation to discover the implicit feature combinations, it will be beneficial to let NN explicitly leverage the expressive feature combinations. (2) To reduce model complexity. Contrary to highly-complex FCNN with too many parameters leading to higher risk of over-fitting or falling into local optimums, it is vital to reduce the complexity of NN models by removing unnecessary parameters and encouraging parameter sharing.Inspired by these two principles, we propose a universal neural network solution, called TabNN, to derive effective NN architectures for tabular data in all kinds of tasks automatically, by leveraging the knowledge learned by GBDT model (Gradient Boosting Decision Tree) BID19 BID15 BID12 , which has empirically proven its strength in modeling tabular data BID12 . More specifically, the GBDT-powered TabNN consists of four major steps: (1) Automatic Feature Grouping (AFG) automatically discovers feature groups implying effective partial combinations based on GBDT-powered knowledge. (2) Feature Group Reduction (FGR) attempts to further cluster feature groups in order to encourage parameter sharing within the same clusters, which can accordingly reduce the complexity of the resulting NN models. (3) Recursive Encoder with Shared Embedding (RESE) aims at designing a both effective and efficient NN architecture over clustered tabular feature groups, based on the results of FGR and the feature group importance powered by GBDT. (4) Transfer Structured Knowledge from GBDT (TSKG) further leverages structured knowledge within GBDT model to provide an effective initialization for the obtained NN architecture.To illustrate the effectiveness of the proposed TabNN solution, we conduct extensive experiments on various publicly available datasets with tabular data. Comprehensive experimental analysis has shown that TabNN cannot only create effective NN architectures for various tabular data but also achieves much better performance than other solutions.In summary, the contributions of this paper are multi-fold:• We identify two principles for the purpose of designing effective NN models on tabular data.• We propose TabNN, a general solution for deriving effective NN models for tabular data by leveraging the data knowledge learned by GBDT.• Extensive experiments show that the proposed method is an off-of-shelf model, which can be ready to use in any kinds of tabular data efficiently and achieves state-of-the-art performance. To fill the gap of NN in tabular data learning, we propose a universal neural network solution, called TabNN, which can derive the effective neural architectures automatically for tabular data. The design of TabNN follows two principles, one as explicitly leveraging expressive feature combinations and the other as reducing model complexity. Since GBDT is proven to be effective in tabular data, we leverage GBDT to power the implementation of TabNN. Specifically, TabNN first leverages GBDT to automatically identify expressive feature groups and then clusters feature groups into sets to encourage parameter sharing. After that, TabNN utilizes tree importance knowledge from GBDT to construct recursive NN architectures. To enhance the training efficiency and learning performance, tree structural knowledge is also utilized to provide an effective initialization for the derived architecture. Extensive experiments on various tabular datasets show the advantages of TabNN in modeling tabular data and demonstrate the necessity of designed components in TabNN.",We propose a universal neural network solution to derive effective NN architectures for tabular data automatically.,Neural Networks ; data.• ; GBDT ; AFG ; Deep Neural Networks ; four ; Fully Connected Neural Network ; Local Receptive Field ; RNN ; FCNN,knowledge ; feature groups ; specific structures ; extensive experiments ; its strength ; enlarged model capacity ; unnecessary parameters ; their tabular data ; NN models ; satisfactory performance,Neural Networks ; data.• ; GBDT ; AFG ; Deep Neural Networks ; four ; Fully Connected Neural Network ; Local Receptive Field ; RNN ; FCNN,"Neural Network (NN) has achieved state-of-the-art performances in many tasks within image, speech, and text domains. These NNs achieve good performance by leveraging prior knowledge over corresponding domain data. However, there are many applications with tabular data in other domains, making it difficult to design specific structures to fit them all. With careful architecture design based on domain knowledge, NN is able to achieve satisfactory performance in these areas. To fill the gap of NN in tabular learning, we propose a universal neural network solution, called TabNN, to derive effective NN architectures for all kinds of tasks automatically",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Deep learning models require extensive architecture design exploration and hyperparameter optimization to perform well on a given task. The exploration of the model design space is often made by a human expert, and optimized using a combination of grid search and search heuristics over a large space of possible choices. Neural Architecture Search (NAS) is a Reinforcement Learning approach that has been proposed to automate architecture design. NAS has been successfully applied to generate Neural Networks that rival the best human-designed architectures. However, NAS requires sampling, constructing, and training hundreds to thousands of models to achieve well-performing architectures. This procedure needs to be executed from scratch for each new task. The application of NAS to a wide set of tasks currently lacks a way to transfer generalizable knowledge across tasks.
 In this paper, we present the Multitask Neural Model Search (MNMS) controller. Our goal is to learn a generalizable framework that can condition model construction on successful model searches for previously seen tasks, thus significantly speeding up the search for new tasks. We demonstrate that MNMS can conduct an automated architecture search for multiple tasks simultaneously while still learning well-performing, specialized models for each task. We then show that pre-trained MNMS controllers can transfer learning to new tasks. By leveraging knowledge from previous searches, we find that pre-trained MNMS models start from a better location in the search space and reduce search time on unseen tasks, while still discovering models that outperform published human-designed models. Designing deep learning models that work well for a task requires an extensive process of iterative architecture engineering and tuning. These design decisions are largely made by human experts guided by a combination of intuition, grid search, and search heuristics.Meta-learning aims to automate model design by using machine learning to discover good architecture and hyperparameter choices. Recent advances in meta-learning using Reinforcement Learning (RL) have made promising strides towards accelerating or even eliminating the manual parameter search. For example, Neural Architecture Search (NAS) has successfully discovered novel network architectures that rival or surpass the best human-designed architectures on challenging benchmark image recognition tasks . However, naively applying reinforcement learning to each new task for automated model construction requires sampling, constructing, and training hundreds to thousands of networks to relearn how to generate models from scratch. Human experts, on the other hand, can design and tune networks based on knowledge about underlying dependencies in the search space and experience with prior tasks. We therefore aim to automatically learn and leverage the same information.In this paper, we present Multitask Neural Model Search (MNMS), an automated model construction framework that finds the best performing models in the search space for multiple tasks simultaneously. We then show that a MNMS framework that has been pre-trained on previous tasks can construct the best performing model for entirely new tasks in significantly less time. Summary. Machine learning model design choices do not exist in a vacuum. Human experts design good models by leveraging significant prior knowledge about the intuitive relationships between these model parameters, and the performance obtained by different model designs on similar tasks. Automated model design algorithms, too, can and should learn from the models they have discovered for prior tasks. This paper demonstrates that Multitask Neural Model Search can discover good, differentiated model designs for multiple tasks simultaneously, while learning task embeddings that encode meaningful relationships between tasks. We then show that multitask training provides a good baseline for transfer learning to future tasks, allowing the MNMS framework to start from a better location in the search space and converge more quickly to high-performing designs.Limitations and future work. While the current work demonstrates that the MNMS framework can be used for multitask training and transferable architecture searches, much work remains to determine the scalability of this approach. The results of this study offer several particularly promising avenues for future research. First, studying the effects of additional simultaneous tasks on framework performance is an obvious next step in multitask training. The current framework trains the learned task embeddings by passing them directly into the controller RNN along with the sampled action embeddings. We anticipate that a more complex pre-processing structure, such as a simple encoder-decoder, could better transform these task embeddings to be used by the controller. Additionally, we currently leverage the distributed training structure described by Zoph and Le, which trains multiple sampled child architectures in parallel and asynchronously updates a shared controller parameter server . However, as we continue to scale the MNMS framework for additional simultaneous tasks, future work remains to optimize a parallel training structure and schedule specifically for efficient multitask training.Experimenting with broader richer hyperparameter search spaces also offers an exciting line of future work. For our current tasks, we defined a search space that encompassed a range of general design choices, including both real-valued parameters (such as learning rates and regularization weights) and higher-level parameters (such as the choice of word embedding table). However, we are actively adapting the controller to sample continuous real-valued parameters, rather than discrete choices from a set of predefined values, which would give the framework much greater flexibility in specifying models. Additionally, we plan to continue expanding the range of modular, higher-level parameter choices in the search space. Allowing the controller to compose these building blocks, rather than more granular design choices, can allow the framework to construct more complex architectures in much less time.Finally, much work remains to explore cases when transfer learning is and is not effective within RL-based architecture search frameworks such as MNMS. We are particularly interested in studying how transfer learning can be used to design architectures for tasks that were previously considered too resource intensive for standard NAS. For example, adapted NAS for the ImageNet classification task by directly modifying the architecture designed for a simpler image classification task. However, pretraining the architecture search framework itself on more computationally feasible tasks, rather than transferring the discovered architectures, would be a significant step towards tackling these difficult search domains.","We present Multitask Neural Model Search, a Meta-learner that can design models for multiple tasks simultaneously and transfer learning to unseen tasks.",Zoph ; hundreds to thousands ; Reinforcement Learning ; RNN ; Le ; Neural Model Search ; Neural Networks ; First ; NAS ; ImageNet,table ; a parallel training structure ; a generalizable framework ; the framework ; search time ; machine learning ; them ; transferable architecture searches ; the learned task embeddings ; new tasks,Zoph ; hundreds to thousands ; Reinforcement Learning ; RNN ; Le ; Neural Model Search ; Neural Networks ; First ; NAS ; ImageNet,"Deep learning models require extensive architecture design exploration and hyperparameter optimization to perform well on a given task. The exploration of the model design space is often made by a human expert, and optimized using a combination of grid search and search heuristics over a large space of possible choices. Neural Architecture Search (NAS) is a Reinforcement Learning approach that has been successfully applied to generate Neural Networks that rival or surpass the best human-designed architectures. However, this procedure requires sampling, constructing, and training hundreds to thousands of models to achieve well-performing architectures. The application of NAS to a wide set of tasks currently lacks a way",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Conventional deep reinforcement learning typically determines an appropriate primitive action at each timestep, which requires enormous amount of time and effort for learning an effective policy, especially in large and complex environments. To deal with the issue fundamentally, we incorporate macro actions, defined as sequences of primitive actions, into the primitive action space to form an augmented action space. The problem lies in how to find an appropriate macro action to augment the primitive action space.   The agent using a proper augmented action space is able to jump to a farther state and thus speed up the exploration process as well as facilitate the learning procedure. In previous researches, macro actions are developed by mining the most frequently used action sequences or repeating previous actions. However, the most frequently used action sequences are extracted from a past policy, which may only reinforce the original behavior of that policy. On the other hand, repeating actions may limit the diversity of behaviors of the agent. Instead, we propose to construct macro actions by a genetic algorithm, which eliminates the dependency of the macro action derivation procedure from the past policies of the agent.   Our approach appends a macro action to the primitive action space once at a time and evaluates whether the augmented action space leads to promising performance or not.    We perform extensive experiments and show that the constructed macro actions are able to speed up the learning process for a variety of deep reinforcement learning methods. Our experimental results also demonstrate that the macro actions suggested by our approach are transferable among deep reinforcement learning methods and similar environments. We further provide a comprehensive set of ablation analysis to validate our methodology. Conventional deep reinforcement learning (DRL) has been shown to demonstrate superhuman performance on a variety of environments and tasks (Mnih et al., 2013 (Mnih et al., , 2015 Salimans et al., 2017; Moriarty et al., 1999; . However, in conventional methods, agents are restricted to make decisions at each timestep, which differs much from the temporally-extended framework of decision-making in human beings. As a consequence, traditional methods (Mnih et al., 2013 Houthooft et al., 2016) require enormous amounts of sampling data in environments where goals are hard to reach or rewards are sparse. In complex environments where goals can only be achieved by executing a long sequence of primitive actions, it is difficult to perform exploration efficiently. As most real-world environments are large, complex, and usually offer sparse rewards, finding an optimal policy is still hard and challenging. It becomes crucial to explore new mechanisms to deal with these environments more efficiently and effectively. Researchers in the past few years have attempted various techniques to expand the realm of DRL to temporally-extended frameworks (Sutton et al., 1999; Vezhnevets et al., 2016; Kulkarni et al., 2016; Bacon et al., 2017; Frans et al., 2017; Daniel et al., 2016; Florensa et al., 2017; Machado et al., 2017) . In such frameworks, a high-level controller interacts with the environment by selecting temporal-extended policies usually named as ""options"". Once an option is selected, it interacts with the environment for a certain timesteps and perform primitive actions until a termination condition for that option is met. However, developing effective options either requires a significant amount of domain knowledge (Girgin et al., 2010) , or often restricted to low-dimensional and/or relatively simple environments only (Bacon et al., 2017; Heess et al., 2016; Kulkarni et al., 2016) . Instead of developing options, another branch of research directions focus on constructing macro actions (Fikes and Nilsson, 1971; Siklossy and Dowson, 1977; Minton, 1985; Pickett and Barto, 2002; Botea et al., 2005; Newton et al., 2005 Newton et al., , 2007 . A macro action (or simply ""a macro"") is an open-loop (DiStefano III et al., 1967 ) policy composed of a finite sequence of primitive actions. Once a macro is chosen, the actions will be taken by the agent without any further decision making process. Some researches in DRL attempt to construct macros from the experience of an agent (Durugkar et al., 2016; Randlov, 1999; Yoshikawa and Kurihara, 2006; Onda and Ozawa, 2009; Garcia et al., 2019) . A key benefit of these approaches is the ease to construct a desired macro without supervision (Durugkar et al., 2016) . However, these approaches may lead to biased macros. For example, the most frequently used sequence of actions may not correspond to a macro that can lead the agent to outperform its past policies. Furthermore, as agents generally perform exploration extensively in the early stages of training, the inconsistency in the early experience may perturb the construction of macros. A few researchers proposed to employ a reduced form of macro called action repeat Sharma et al., 2017) . In this formulation, primitive actions are repeated several times in a macro before the agent makes another decision. However, this formulation may limit the diversity of macros. By relaxing the agent to perform macros consisting of diversified actions, the agent is granted more chances to achieve higher performance. In addition, there are a handful of researches that requires human supervision to derive macros for improving training efficiency. The authors in McGovern et al. (1997) show that handcrafted macros can speed up training in certain tasks but hinder performance in others. The authors in Heecheol et al. (2019) generate macros from expert demonstrations via a variational auto-encoder. However, the process of obtaining such demonstrations is expensive. It would thus be favorable if there exists a method to find a macro without human intervention. Nevertheless, little attention has been paid to the construction of such macros. Our goal is to develop a methodology for constructing a macro action from possible candidates. As possible macros are allowed to have different lengths and arbitrary compositions of primitive actions, such diversified macro actions essentially form an enormous space. We define this space as the macro action space (or simply ""macro space""). Repeated action sequences are simply a small subset of the macro space. For a specific task in an environment, we hypothesize that there are good macros and bad macros in the macro space. Different macro actions have different performance impacts to an agent. Good macro actions enable the agent to jump over multiple states and reach a target state quicker and easier. On the other hand, bad macro actions may lead the agent to undesirable states. We argue that whether a macro is good or bad can only be determined by direct evaluation. In this study, we propose an evaluation method to test whether a macro is satisfactory for an agent to perform a specific task in an environment. Our method first relaxes the conventional action space (Sutton and Barto, 2018 ) with a macro to form an augmented action space. We then equip the agent with the augmented action space, and utilize the performance results as the basis for our evaluation. In order to find a good macro in the vast macro space, a systematic method is critically important and necessary. The method entails two prerequisites: a macro construction mechanism and a macro evaluation method. Although the second one is addressed above, there is still a lack of an appropriate approach to construct macros. To satisfy the above requirement, we embrace an genetic algorithm (or simply ""GA"") for macro construction. GA offers two promising properties. First, it eliminates the dependency of the macro action derivation procedure from the past policies of an agent and/or human supervision. Second, it produces diversified macros by mutation. In order to combine GA with our evaluation method, our approach comprises of three phases: (1) macro construction by GA; (2) action space augmentation; and (3) evaluation of the augmented action space. Our augmented action space contains not only the original action space defined by DRL, but also the macro(s) constructed by GA. To validate the proposed approach, we perform our experiments on Atari 2600 (Brockman et al., 2016) and ViZDoom (Kempka et al., 2016) , and compare them to two representative DRL baseline methods. We demonstrate that our proposed method is complementary to existing DRL methods, and perform favorably against the baselines. Moreover, we show that the choice of the macro have a crucial impact on the performance of an agent. Furthermore, our results reveal the existence of transferability of a few macros over similar environments or DRL methods. We additionally provide a comprehensive set of ablation analysis to justify various aspects of our approach. The contributions of this paper are summarized as follows: • We define the proposed approach as a framework. • We provide a definition of macro action space. • We introduce an augmentation method for action spaces. • We propose an evaluation method to determine whether a macro is good or not. • We establish a macro action construction method using GA for DRL. • We investigate and reveal the transferability of macro actions. The rest of this paper is organized as follows. Section 2 explains our framework. Section 3 describes our implementation details. Section 4 presents our results. Section 5 concludes. We have formally presented a methodology to construct macro actions that may potentially improve both the performance and learning efficiency of the existing DRL methods. The methodology falls within the scope of a broader framework that permits other possible combinations of the DRL method, the action space augmentation method, the evaluation method, as well as the macro action construction method. We formulated the proposed methodology as a set of algorithms, and used them as the basis for investigating the interesting properties of macro actions. Our results revealed that the macro actions constructed by our methodology are complementary to two representative DRL methods, and may demonstrate transferability among different DRL methods and similar environments. We additionally compared our methodology against three other macro construction methods to justify our design decisions. Our work paves a way for future research on macros and their applications.","We propose to construct macro actions by a genetic algorithm, which eliminates the dependency of the macro action derivation procedure from the past policies of the agent.",Sharma ; Minton ; Frans ; first ; three ; two ; Heess ; second ; Daniel et al. ; Botea,a macro construction mechanism ; the baselines ; a variety ; our evaluation method ; our framework ; these environments ; low-dimensional and/or relatively simple environments ; tasks ; future research ; the macro,Sharma ; Minton ; Frans ; first ; three ; two ; Heess ; second ; Daniel et al. ; Botea,"Conventional deep reinforcement learning typically determines an appropriate primitive action at each timestep, which requires enormous amount of time and effort for learning an effective policy, especially in large and complex environments. To deal with the issue fundamentally, we incorporate macro actions, defined as sequences of primitive actions, into the primitive action space. The problem lies in how to find an appropriate macro action to augment the primitive actions. The objective of this approach is to construct macro actions by mining the most frequently used action sequences or repeating previous actions, which may only reinforce the original behavior of the agent. However, repeating actions may limit the diversity of behaviors of the",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"To analyze deep ReLU network, we adopt a student-teacher setting in which an over-parameterized student network learns from the output of a fixed teacher network of the same depth, with Stochastic Gradient Descent (SGD). Our contributions are two-fold. First, we prove that when the gradient is zero (or bounded above by a small constant) at every data point in training, a situation called  \emph{interpolation setting}, there exists many-to-one \emph{alignment} between student and teacher nodes in the lowest layer under mild conditions. This suggests that generalization in unseen dataset is achievable, even the same condition often leads to zero training error. Second, analysis of noisy recovery and training dynamics in 2-layer network shows that strong teacher nodes (with large fan-out weights) are learned first and subtle teacher nodes are left unlearned until late stage of training. As a result, it could take a long time to converge into these small-gradient critical points. Our analysis shows that over-parameterization plays two roles: (1) it is a necessary condition for alignment to happen at the critical points, and (2) in training dynamics, it helps student nodes cover more teacher nodes with fewer iterations. Both improve generalization. Experiments justify our finding. Deep Learning has achieved great success in the recent years (Silver et al., 2016; He et al., 2016; Devlin et al., 2018) . Although networks with even one-hidden layer can fit any function (Hornik et al., 1989) , it remains an open question how such networks can generalize to new data. Different from what traditional machine learning theory predicts, empirical evidence (Zhang et al., 2017) shows more parameters in neural network lead to better generalization. How over-parameterization yields strong generalization is an important question for understanding how deep learning works. In this paper, we analyze deep ReLU networks with teacher-student setting: a fixed teacher network provides the output for a student to learn via SGD. Both teacher and student are deep ReLU networks. Similar to (Goldt et al., 2019) , the student is over-realized compared to the teacher: at each layer l, the number n l of student nodes is larger than the number m l of teacher (n l > m l ). Although over-realization is different from over-parameterization, i.e., the total number of parameters in the student model is larger than the training set size N , over-realization directly correlates with the width of networks and is a measure of over-parameterization. The student-teacher setting has a long history (Saad & Solla, 1996; 1995; Freeman & Saad, 1997; Mace & Coolen, 1998) and recently gains increasing interest (Goldt et al., 2019; Aubin et al., 2018) in analyzing 2-layered network. While worst-case performance on arbitrary data distributions may not be a good model for real structured dataset and can be hard to analyze, using a teacher network implicitly enforces an inductive bias and could potentially lead to better generalization bound. Specialization, that is, a student node becomes increasingly correlated with a teacher node during training (Saad & Solla, 1996) , is one of the important topic in this setup. If all student nodes are specialized to the teacher, then student tends to output the same as the teacher and generalization performance can be expected. Empirically, it has been observed in 2-layer networks (Saad & Solla, 1996; Goldt et al., 2019) and multi-layer networks (Tian et al., 2019; Li et al., 2016) , in both synthetic and real dataset. In contrast, theoretical analysis is limited with strong assumptions (e.g., Gaussian inputs, infinite input dimension, local convergence, 2-layer setting, small number of hidden nodes). In this paper, with arbitrary training distribution and finite input dimension, we show rigorously that when gradient at each training sample is small (i.e., the interpolation setting as suggested in (Ma In this paper, we use student-teacher setting to analyze how an (over-parameterized) deep ReLU student network trained with SGD learns from the output of a teacher. When the magnitude of gradient per sample is small (student weights are near the critical points), the teacher can be proven to be covered by (possibly multiple) students and thus the teacher network is recovered in the lowest layer. By analyzing training dynamics, we also show that strong teacher node with large v * is reconstructed first, while weak teacher node is reconstructed slowly. This reveals one important reason why the training takes long to reconstruct all teacher weights and why generalization improves with more training. As the next step, we would like to extend our analysis to finite sample case, and analyze the training dynamics in a more formal way. Verifying the insights from theoretical analysis on a large dataset (e.g., ImageNet) is also the next step. Figure 8: Mean of the max teacher correlation ρmean with student nodes over epochs in CIFAR10. More over-realization gives better student specialization across all layers and achieves strong generalization (higher evaluation accuracy on CIFAR-10 evaluation set).",This paper analyzes training dynamics and critical points of training deep ReLU network via SGD in the teacher-student setting.,Deep Learning ; Freeman & Saad ; one ; max ; Mace & Coolen ; Tian ; two ; Saad & Solla ; ImageNet ; N,one important reason ; gradient ; student and teacher nodes ; sample case ; these small-gradient critical points ; Both ; contrast ; arbitrary data distributions ; both synthetic and real dataset ; unseen dataset,Deep Learning ; Freeman & Saad ; one ; max ; Mace & Coolen ; Tian ; two ; Saad & Solla ; ImageNet ; N,"In Deep Learning, we adopt a student-teacher setting in which an over-parameterized student network learns from the output of a fixed teacher network of the same depth, with Stochastic Gradient Descent (SGD). We prove that when the gradient is zero (or bounded above by a small constant) at every data point in training, there exists many-to-one \emph{alignment} between student and teacher nodes in the lowest layer under mild conditions. This suggests that generalization in unseen dataset is achievable, even the same condition often leads to zero training error. In 2-layer network",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"MobileNets family of computer vision neural networks have fueled tremendous progress in the design and organization of resource-efficient architectures in recent years. New applications with stringent real-time requirements in highly constrained devices require further compression of MobileNets-like already computeefficient networks. Model quantization is a widely used technique to compress and accelerate neural network inference and prior works have quantized MobileNets to 4 − 6 bits albeit with a modest to significant drop in accuracy. While quantization to sub-byte values (i.e. precision ≤ 8 bits) has been valuable, even further quantization of MobileNets to binary or ternary values is necessary to realize significant energy savings and possibly runtime speedups on specialized hardware, such as ASICs and FPGAs. Under the key observation that convolutional filters at each layer of a deep neural network may respond differently to ternary quantization, we propose a novel quantization method that generates per-layer hybrid filter banks consisting of full-precision and ternary weight filters for MobileNets. The layer-wise hybrid filter banks essentially combine the strengths of full-precision and ternary weight filters to derive a compact, energy-efficient architecture for MobileNets. Using this proposed quantization method, we quantized a substantial portion of weight filters of MobileNets to ternary values resulting in 27.98% savings in energy, and a 51.07% reduction in the model size, while achieving comparable accuracy and no degradation in throughput on specialized hardware in comparison to the baseline full-precision MobileNets. Deeper and wider convolutional neural networks (CNNs) has led to outstanding predictive performance in many machine learning tasks, such as image classification ; Krizhevsky et al. (2012) ), object detection ; Ren et al. (2015) ), and semantic segmentation ; Long et al. (2015) ). However, the large model size and corresponding computational inefficiency of these networks often make it infeasible to run many real-time machine learning applications on resource-constrained mobile and embedded hardware, such as smartphones, AR/VR devices etc. To enable this computation and size compression of CNN models, one particularly effective approach has been the use of resource-efficient MobileNets architecture. MobileNets introduces depthwise-separable (DS) convolution as an efficient alternative to the standard 3-D convolution operation.While MobileNets architecture has been transformative, even further compression of MobileNets is valuable in order to make a wider range of applications available on constrained platforms (Gope et al. (2019) ). Model quantization has been a popular technique to facilitate that. Quantizing the weights of MobileNets to binary (-1,1) or ternary (-1,0,1) values in particular has the potential to achieve significant improvement in energy savings and possibly overall throughput especially on custom hardware, such as ASICs and FPGAs while reducing the resultant model size considerably. This is attributed to the replacement of multiplications by additions in binary-and ternary-weight networks. Multipliers occupy considerably more area on chip than adders ), and consume significantly more energy than addition operations (Horowitz (2014) ; Andri et al. (2018) ). A specialized hardware can therefore trade off multiplications against additions and potentially accommodate considerably more adders than multipliers to achieve a high throughput and significant savings in energy for binary-and ternary-weight networks. However, prior approaches to binary and ternary quantization (Rastegari et al. (2016) ; Alemdar et al. (2016) ; ; Tschannen et al. (2018) ) incur significant drop in prediction accuracy for MobileNets. Recent work on StrassenNets (Tschannen et al. (2018) ) presents a more mathematically profound way to approximate matrix multiplication computation (and, in turn, convolutions) using mostly ternary weights and a few full-precision weights. It essentially exploits Strassen's algorithm to approximate a matrix multiplication of a weight matrix with feature maps, where the elements of the product matrix are generated by different combination of few intermediate terms through additions. Computation of each of the intermediate terms requires a multiplication along with combination of different elements of weights and feature maps through additions. The number of intermediate terms (also called hidden layer width) in StrassenNets therefore determines the addition and multiplication budget of a convolutional layer and in turn decides the approximation error of the corresponding convolution operation. While the results in (Tschannen et al. (2018) ) using StrassenNets demonstrates no loss in predictive performance when compared to full-precision models for few networks, the effectiveness of StrassenNets is quite variable, however, depending on the neural network architecture. We observe, for example, that while strassenifying is effective in reducing the model size of DS convolutional layers, this might come with a prohibitive increase in the number of addition operations, reducing the energy efficiency of neural network inference. The exorbitant increase in additions primarily stems from the use of wide hidden layers for closely approximating each convolutional filter in a network layer. While this might be required for some of the convolutional filters in a layer, our observations indicate that all filters may not require wide strassenified hidden layers. As different filters in a network layer tend to capture different features, they may respond differently to ternary quantization, and, in turn, to strassenified convolution with a specific hidden layer units. Some filters can be harder to approximate using ternary bits than others, and have larger impact on the model accuracy loss. Furthermore, given a constrained hidden layer budget for StrassenNets, a group of filters extracting fairly similar features at a layer may respond favorably to ternary quantization, while other filters of the layer extracting significantly different features from those may not. Guided by these insights, we propose a layer-wise hybrid filter banks for the MobileNets architecture capable of giving start-of-the-art accuracy levels, while requiring a fraction of the model size and considerably fewer MAC and multiplication operations per inference. The end-to-end learning of hybrid filter banks makes this possible by keeping precision critical convolutional filters in fullprecision values and strassenifying quantization tolerant filters only to ternary values. The filters that are most sensitive to quantization errors perform traditional convolutions with input feature maps, whereas ternary quantization tolerant filters can perform strassenified convolutions using narrow hidden layers. We apply this proposed quantization scheme to the state-of-the-art MobileNets-V1 architecture. The hybrid filter banks for MobileNets achieves a 46.4% reduction in multiplications, and a 51.07% reduction in model size while incurring modest increase in additions. This translates into a 27.98% savings in energy required per inference while ensuring no degradation in throughput on a DNN hardware accelerator consisting of both MAC and adders when compared to the execution of baseline MobileNets on a MAC-only hardware accelerator. The hybrid filter banks accomplishes this with a very minimal loss in accuracy of 0.51%. To the best of our knowledge, the hybrid filter banks proposed in this work is a first step towards quantizing the already compute-efficient MobileNets architecture to ternary values with a negligible loss in accuracy on a large-scale dataset, such as ImageNet. The remainder of the paper is organized as follows. Section 2 elaborates on the incentives behind the use of per-layer hybrid filter banks for the MobileNets architecture and provides a brief overview of current quantization algorithms along with our observations of applying them to the MobileNets architecture. Failing to find a good balance between accuracy and computation costs shifts our focus towards designing layer-wise hybrid filter banks for MobileNets. Section 3 describes our hybrid filter banks. Section 4 presents results. Section 5 compares hybrid filter banks against prior works and Section 6 concludes the paper. In this work, we propose per-layer hybrid filter banks for MobileNets capable of quantizing its weights to ternary values while exhibiting start-of-the-art accuracy on a large-scale dataset and requiring a fraction of the model size and considerably lower energy per inference pass. We use 16-bit floating-point format to represent the intermediate activations and traditional weight filters of hybrid filter banks in this work. In future, we plan to explore the impact of quantizing them to 8-bit or less. In addition, it will be interesting to see how channel pruning (He et al. (2018) ; Zhuang et al. (2018) ) assists in reducing the computational complexity of strassenified MobileNets. Strasssen's algorithm can multiply 2 × 2 matrices using only 7 multiplications instead of 8 required otherwise by a naïve matrix multiplication algorithm. Figure 3 (a) specifies a set of weight matrices that can perform exact convolution of the 2 × 2 filter bank comprising f j and f k with the feature map using 7 multiplications. Note that the two filters f j and f k do not have any common values. However, owing to the presence of common value of a between f j and f k filters in Figure 3 (b), Strassen's algorithm now can compute the exact product matrix using only 6 multiplications instead of 7 required otherwise in Figure 3 (a). A set of ternary weight matrices implementing an exact convolution in this case is shown in Figure 3(b ) . B RELATION OF PER-LAYER HYBRID FILTER BANKS TO GOOGLENET ARCHITECTURE. The per-layer hybrid filter banks proposed here is inspired by the Inception module from the GoogLeNet architecture (Szegedy et al. (2015) ). In a traditional convolutional network, each layer extracts information from the previous layer in order to transform the input data into a more useful representation. However, salient features of an input volume can have extremely large variation in size. Because of this variation in the size of the required information, choosing the right kernel size","2x savings in model size, 28% energy reduction for MobileNets on ImageNet at no loss in accuracy using hybrid layers composed of conventional full-precision filters and ternary filters",Rastegari ; ImageNet ; one ; al. ; Strasssen ; Inception ; Andri et al ; GoogLeNet ; first ; Alemdar,accuracy and computation costs shifts ; a matrix multiplication ; embedded hardware ; f k ; an input volume ; the two filters ; adders ; no loss ; depthwise-separable (DS) convolution ; the impact,Rastegari ; ImageNet ; one ; al. ; Strasssen ; Inception ; Andri et al ; GoogLeNet ; first ; Alemdar,"MobileNets family of computer vision neural networks have fueled tremendous progress in the design and organization of resource-efficient architectures in recent years. However, further quantization of MobileNets to binary or ternary values is necessary to realize significant energy savings and possibly runtime speedups on specialized hardware, such as ASICs and FPGAs. The concept of per-layer hybrid filter banks (CNNs) is a novel quantization method that combines full-precision and ternaries weight filters, resulting in 27.98% savings in energy, and a 51.07% reduction in throughput. This approach enables efficient computation and",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Differently from the popular Deep Q-Network (DQN) learning, Alternating Q-learning (AltQ) does not fully fit a target Q-function at each iteration, and is generally known to be unstable and inefficient. Limited applications of AltQ mostly rely on substantially altering the algorithm architecture in order to improve its performance. Although Adam appears to be a natural solution, its performance in AltQ has rarely been studied before. In this paper, we first provide a solid exploration on how well AltQ performs with Adam. We then take a further step to improve the implementation by adopting the technique of parameter restart. More specifically, the proposed algorithms are tested on a batch of Atari 2600 games and exhibit superior performance than the DQN learning method. The convergence rate of the slightly modified version of the proposed algorithms is characterized under the linear function approximation. To the best of our knowledge, this is the first theoretical study on the Adam-type algorithms in Q-learning. Q-learning (Watkins & Dayan, 1992 ) is one of the most important model-free reinforcement learning (RL) problems, which has received considerable research attention in recent years (Bertsekas & Tsitsiklis, 1996; Even-Dar & Mansour, 2003; Hasselt, 2010; Lu et al., 2018; Achiam et al., 2019) . When the state-action space is large or continuous, parametric approximation of the Q-function is often necessary. One remarkable success of parametric Q-learning in practice is its combination with deep learning, known as the Deep Q-Network (DQN) learning (Mnih et al., 2013; 2015) . It has been applied to various applications in computer games (Bhatti et al., 2016) , traffic control (Arel et al., 2010) , recommendation systems (Zheng et al., 2018; Zhao et al., 2018) , chemistry research (Zhou et al., 2017) , etc. Its on-policy continuous variant (Silver et al., 2014) has also led to great achievements in robotics locomotion (Lillicrap et al., 2016) . The DQN algorithm is performed in a nested-loop manner, where the outer loop follows an one-step update of the Q-function (via the empirical Bellman operator for Q-learning), and the inner loop takes a supervised learning process to fit the updated (i.e., target) Q-function with a neural network. In practice, the inner loop takes a sufficiently large number of iterations under certain optimizer (e.g. stochastic gradient descent (SGD) or Adam) to fit the neural network well to the target Q-function. In contrast, a conventional Q-learning algorithm runs only one SGD step in each inner loop, in which case the overall Q-learning algorithm updates the Q-function and fits the target Q-function alternatively in each iteration. We refer to such a Q-learning algorithm with alternating updates as Alternating Q-learning (AltQ). Although significantly simpler in the update rule, AltQ is well known to be unstable and have weak performance (Mnih et al., 2016) . This is in part due to the fact that the inner loop does not fit the target Q-function sufficiently well. To fix this issue, Mnih et al. (2016) proposed a new exploration strategy and asynchronous sampling schemes over parallel computing units (rather than the simple replay sampling in DQN) in order for the AltQ algorithm to achieve comparable or better performance than DQN. As another alternative, Knight & Lerner (2018) proposed a more involved natural gradient propagation for AltQ to improve the performance. All these schemes require more sophisticated designs or hardware support, which may place AltQ less advantageous compared to the popular DQN, even with their better performances. This motivates us to ask the following first question. • Q1: Can we design a simple and easy variant of the AltQ algorithm, which uses as simple setup as DQN and does not introduce extra computational burden and heuristics, but still achieves better and more stable performance than DQN? In this paper, we provide an affirmative answer by introducing novel lightweight designs to AltQ based on Adam. Although Adam appears to be a natural tool, its performance in AltQ has rarely been studied yet. Thus, we first provide a solid exploration on how well AltQ performs with Adam (Kingma & Ba, 2014) , where the algorithm is referred to as AltQ-Adam. We then take a further step to improve the implementation of AltQ-Adam by adopting the technique of parameter restart (i.e., restart the initial setting of Adam parameters every a few iterations), and refer to the new algorithm as AltQ-AdamR. This is the first time that restart is applied for improving the performance of RL algorithms although restart has been used for conventional optimization before. In a batch of 23 Atari 2600 games, our experiments show that both AltQ-Adam and AltQ-AdamR outperform the baseline performance of DQN by 50% on average. Furthermore, AltQ-AdamR effectively reduces the performance variance and achieves a much more stable learning process. In our experiments for the linear quadratic regulator (LQR) problems, AltQ-AdamR converges even faster than the model-based value iteration (VI) solution. This is a rather surprising result given that the model-based VI has been treated as the performance upper bound for the Q-learning (including DQN) algorithms with target update . Regarding the theoretical analysis of AltQ algorithms, their convergence guarantee has been extensively studied (Melo et al., 2008; Chen et al., 2019b) . More references are given in Section 1.1. However, all the existing studies focus on the AltQ algorithms that take a simple SGD step. Such theory is not applicable to the proposed AltQ-Adam and AltQ-AdamR that implement the Adam-type update. Thus, the second intriguing question we address here is described as follows. • Q2: Can we provide the convergence guarantee for AltQ-Adam and AltQ-AdamR or their slightly modified variants (if these two algorithms do not always converge by nature)? It is well known in optimization that Adam does not always converge, and instead, a slightly modified variant AMSGrad proposed in Reddi et al. (2018) has been widely accepted as an alternative to justify the performance of Adam-type algorithms. Thus, our theoretical analysis here also focuses on such slightly modified variants AltQ-AMSGrad and AltQ-AMSGradR of the proposed algorithms. We show that under the linear function approximation (which is the structure that the current tools for analysis of Q-learning can handle), both AltQ-AMSGrad and AltQ-AMSGradR converge to the global optimal solution under standard assumptions for Qlearning. To the best of our knowledge, this is the first non-asymptotic convergence guarantee on Q-learning that incorporates Adam-type update and momentum restart. Furthermore, a slight adaptation of our proof provides the convergence rate for the AMSGrad for conventional strongly convex optimization which has not been studied before and can be of independent interest. Notations We use x := x 2 = √ x T x to denote the 2 norm of a vector x, and use x ∞ = max i |x i | to denote the infinity norm. When x, y are both vectors, x/y, xy, x 2 , √ x are all calculated in the element-wise manner, which will be used in the update of Adam and AMSGrad. We denote [n] = 1, 2, . . . , n, and x ∈ Z as the largest integer such that x ≤ x < x + 1. We propose two types of the accelerated AltQ algorithms, and demonstrate their superior performance over the state-of-the-art through a linear quadratic regulator problem and a batch of 23 Atari 2600 games. Notably, Adam is not the only scheme in the practice for general optimization. Heavy ball (Ghadimi et al., 2015) and Nesterov (Nesterov, 2013) are also popular momentum-based methods. When adopting such methods in AltQ-learning for RL problems, however, we tend to observe a less stable learning process than AltQ-Adam. This is partially caused by the fact that they optimize over a shorter historical horizon of updates than Adam. Furthermore, the restart scheme provides somewhat remarkable performance in our study. It is thus of considerable future interest to further investigate the potential of such a scheme. One possible direction is to develop an adaptive restart mechanism with changing period determined by an appropriately defined signal of restart. This will potentially relieve the effort in hyper-parameter tuning of finding a good fixed period.",New Experiments and Theory for Adam Based Q-Learning,VI ; Zhao ; Lu et al. ; two ; Qlearning ; Reddi ; Mnih ; Bellman ; Achiam et ; second,the potential ; the performance ; popular momentum-based methods ; parametric Q-learning ; (Nesterov ; vectors ; Knight ; Adam parameters ; (Arel ; the target Q-function,VI ; Zhao ; Lu et al. ; two ; Qlearning ; Reddi ; Mnih ; Bellman ; Achiam et ; second,"The popular Deep Q-Network (DQN) learning, Alternating Q-learning (AltQ) does not fully fit a target Q-function at each iteration, and is generally known to be unstable and inefficient. However, many applications rely on substantially altering the algorithm architecture in order to improve its performance. In this paper, we explore how well the Adam-type algorithms perform with the DQN algorithm, which is based on a batch of Atari 2600 games. The convergence rate of the proposed algorithms is characterized under the linear function approximation. To the best of our knowledge, this is the first theoretical study on the Adam",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Developing effective biologically plausible learning rules for deep neural networks is important for advancing connections between deep learning and neuroscience. To date, local synaptic learning rules like those employed by the brain have failed to match the performance of backpropagation in deep networks. In this work, we employ meta-learning to discover networks that learn using feedback connections and local, biologically motivated learning rules. Importantly, the feedback connections are not tied to the feedforward weights, avoiding any biologically implausible weight transport. It can be shown mathematically that this approach has sufficient expressivity to approximate any online learning algorithm. Our experiments show that the meta-trained networks effectively use feedback connections to perform online credit assignment in multi-layer architectures. Moreover, we demonstrate empirically that this model outperforms a state-of-the-art gradient-based meta-learning algorithm for continual learning on regression and classification benchmarks. This approach represents a step toward biologically plausible learning mechanisms that can not only match gradient descent-based learning, but also overcome its limitations. Deep learning has achieved impressive success in solving complex tasks, and in some cases its learned representations have been shown to match those in the brain [19, 10] . However, there is much debate over how well the learning algorithm commonly used in deep learning, backpropagation, resembles biological learning algorithms. Causes for skepticism include the facts that (1) backpropagation ignores the nonlinearities imposed by neurons in the backward pass and assumes instead that derivatives of the forward-pass nonlinearities can be applied, (2) in backpropagation, feedback path weights are exactly tied to feedforward weights, even as weights are updated with learning, and (3) backpropagation assumes alternating forward and backward passes [12] . The question of how so-called credit assignment -appropriate propagation of learning signals to non-output neurons -can be performed in biologically plausible fashion in deep neural networks remains open. We propose a new learning paradigm that aims to solve the credit assignment problem in more biologically plausible fashion. Our approach is as follows: (1) endow a deep neural network with feedback connections that propagate information about target outputs to neurons at all layers, (2) apply local plasticity rules (e.g. Hebbian or neuromodulated plasticity) to update feedforward synaptic weights following feedback projections, and (3) employ meta-learning to optimize for the initialization of feedforward weights, the setting of feedback weights, and synaptic plasticity levels. On a set of online regression and classification learning tasks, we find that meta-learned deep networks can successfully perform useful weight updates in early layers, and that feedback with local learning rules can in fact outperform gradient descent as an inner-loop learning algorithm on challenging few-shot and continual learning tasks. This work demonstrates that meta-learning procedures can optimize for neural networks that learn online using local plasticity rules and feedback connections. Several follow-up directions could be pursued. First, meta-learning of this kind is computationally expensive, as the meta-learner must backpropagate through the network's entire training procedure. In order to scale this approach, it will be important to find ways to meta-train networks that generalize to longer lifetimes than were used during meta-training, or to explore alternatives to backprop-based meta-training (e.g. evolutionary algorithms). The present work focused on the case of online learning, but the case of learning from repeated exposure to large datasets is also of interest, and scaling the method in this fashion will be crucial to exploring this regime. Future work could also increase the biological plausibility of the method. For instance, in the present implementation the feedforward and feedback + update passes occur sequentially. However, a natural extension would enable them to run in parallel. This requires ensuring (through appropriate meta-learning and/or a segregated dendrites model [6] ) that feedforward and feedback information do not interfere destructively. Third, the meta-learning procedure in this work optimizes for a precise feedforward and feedback weight initialization. Optimizing instead for a distribution of weight initializations or connectivity patterns would better reflect the stochasticity in synapse development. Another direction is to apply meta-learning to understand biological learning systems (see [9] for an example of such an effort). Well-constrained biological learning models meta-optimized in this manner might show emergence of learning circuits used in biology and even suggest new ones. [",Networks that learn with feedback connections and local plasticity rules can be optimized for using meta learning.,Hebbian ; First ; Third,update passes ; information ; complex tasks ; the stochasticity ; new ones ; synapse development ; passes ; synaptic plasticity levels ; biological learning algorithms ; that feedforward,Hebbian ; First ; Third,"The development of biologically plausible learning rules for deep neural networks is crucial for advancing connections between deep learning and neuroscience. However, local synaptic learning rules like those employed by the brain have failed to match the performance of backpropagation in deep networks. In meta-learning, we discover networks that learn using feedback connections and local, biologically motivated learning rules. The feedback connections are not tied to the feedforward weights, avoiding any biologically implausible weight transport. The meta-trained networks effectively perform online credit assignment in multi-layer architectures, outperforming a state-of-the-art gradient-based meta-Learning algorithm for continual learning",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Recurrent Neural Networks (RNNs) are powerful autoregressive sequence models for learning prevalent patterns in natural language.    Yet language generated by RNNs often shows several degenerate characteristics that are uncommon in human language; while fluent, RNN language production can be overly generic, repetitive, and even self-contradictory.   We postulate that the objective function optimized by RNN language models, which amounts to the overall perplexity of a text, is not expressive enough to capture the abstract qualities of good generation such as Grice’s Maxims. In this paper, we introduce a general learning framework that can construct a decoding objective better suited for generation. Starting with a generatively trained RNN language model, our framework learns to construct a substantially stronger generator by combining several discriminatively trained models that can collectively address the limitations of RNN generation.   Human evaluation demonstrates that text generated by the resulting generator is preferred over  that  of  baselines  by  a  large  margin  and  significantly  enhances  the  overall coherence, style, and information content of the generated text. Recurrent Neural Network (RNN) based language models such as Long Short-Term Memory Networks (LSTMs) BID6 and Gated Recurrent Units (GRUs) BID2 have achieved enormous success across a variety of language tasks due to their ability to learn fluency patterns in natural language BID8 BID10 BID17 . When used as a generator, however, the quality of language generated from RNNs deviates drastically from that of human language. While fluent, RNN-produced language displays several degenerate characteristics, favoring generic and contentless output that tends to be repetitive and self-contradictory. These issues are especially prominent when RNNs are used for open-ended, long-form text generation, as illustrated in Figure 1 .RNNs model the conditional probability P (x t |x 1 , ..., x t−1 ) of generating the next word x t given all previous words observed or generated. In theory , this conditional model should be able to learn all crucial aspects of human language production, for example, that we don't normally repeat the same content over and over. In practice , however, the learned conditional probability model often assigns higher probability to a repetitive, overly generic sentence than to higher quality sentences, as shown in Figure 1 . We postulate that this is in part because the network architectures of RNN variants do not provide a strong enough inductive bias for the model to learn the complex communication goals pursued in human writing. In addition, long-term context easily gets lost as it is explained away in the presence of more immediately relevant short-term context BID34 , and as gradients diminish over a long sequence BID21 . Consequently , RNNs acquire relatively shallow and myopic patterns, which tend to only take advantage of a small fraction of the training set vocabulary BID9 . RNNs are thus unable to generate language that matches the complexity and coherence of human generated text.Several methods in the literature attempt to mitigate these issues. Overly simple and generic generation can be improved by using a diversity-boosting objective function BID24 BID28 . Repetitive generation can be reduced by prohibiting recurrence of the same trigrams as a hard rule BID22 . Although such constraints form a partial solution, they All in all, I would highly recommend this hotel to anyone who wants to be in the heart of the action, and want to be in the heart of the action. If you want to be in the heart of the action, this is not the place for you. However, If you want to be in the middle of the action, this is the place to be.Figure 1: A Trip Advisor review generated by an RNN based LM trained on over a million reviews.are generally too coarse and both penalize good behavior (e.g. reuse of an idiom) and fail to capture more complex bad behavior (e.g. paraphrasing of the same content again and again).Hand tailoring rules is both time consuming and unstable across different generative scenarios, so we instead propose a general learning framework to construct a better decoding objective. Starting with a generatively trained RNN language model, our framework learns to construct a substantially stronger generator by combining several discriminatively trained models that can collectively address limitations of the base RNN generator. Our learning framework therefore generalizes over various existing modifications to the decoding objective. Our approach learns to overcome the particular limitations of the RNN generator directly by incorporating language generated from RNNs as negative samples to discriminatively train several companion models, each specializing in a different aspect of Grice's Maxims of communication BID4 ).Empirical results demonstrate that our learning framework is highly effective in converting a generic RNN language model into a substantially stronger generator. Human evaluation confirms that language generated by our model is preferred over that of competitive baselines by a large margin and significantly enhances the overall coherence, style, and information content of the generated text. Our work presents a unified learning framework that can learn to generate long, coherent text overcoming the limitations of RNNs as text generation models. Our framework learns a decoding objective suitable for generation through a combination of sub-models that capture linguisticallymotivated qualities of good writing. Our work makes a unique contribution that complements existing literature on long text generation that is predominantly based on seq-to-seq models with a large amount of in-domain training data; we demonstrate that the fluency of general RNN language models can be successfully guided to generate more lengthy and sensical text, which can be useful for domains where in-domain data is not sufficient to support seq-to-seq type training.We propose a general framework for learning a decoding objective in two parts: learning component models to rank candidate generations, which are motivated by different aspects of Grice's Maxims, and learning a weighing scheme that balances the influence of each of these scoring functions. This framework is amenable to any number and kind of sub-objectives, allowing for multiple qualities of good writing to be modeled across diverse domains. Human evaluation shows that the quality of the text produced by our model exceeds that of RNN baselines by a large margin and the generations score significantly higher on a Turing test evaluation.Context The location is amazingly convenient. Just 3min walk from the bus station or 1min walk from the train station and less than a 100m from the canal grande. I liked the fact that it's at the start of the city so that you begin your venice experience slowly. I recommend walking to the san marco piazza on your first time ( make sure you wear comfortable shoes ). Language Model The hotel is in a great location. The room was clean and comfortable. The staff was very friendly and helpful. The breakfast was very good. The staff was very friendly and helpful. I would recommend this hotel to anyone who is looking for a nice place to stay in venice. I would recommend this hotel to anyone who is looking for a place to stay in venice. I would recommend this hotel to anyone who is looking for a place to stay while in No Meta-Learning The staff were very friendly and helpful and the breakfast buffet Learn2Write The rooms are spacious and clean. Breakfast is very good. Human Then you exit the alley and face the magnificent san marco piazza and trust me, you will be pleasantly shocked. What an experience!!! : ) ) the abbazia rooms are decent and very clean. Breakfast is poor but adequate and wi-fi is free. The garden is very peaceful and offers some very relaxing moments. I was worried about noises from the train station next door but you can't hear a thing so no problem there. The guys at the reception are amazing. Very friendly and very helpful : ) ) what you want from a hotel in venice is a decent place to sleep, have a relaxing bath and some breakfast in the morning. From then on you will be spending all your time in town anyway so fo me the abbazia hotel was an excellent choice and i will go back for sure. Price is not cheap, but nothing is cheap in venice anyway.",We build a stronger natural language generator by discriminatively training scoring functions that rank candidate generations with respect to various qualities of good writing.,Recurrent Neural Network ; the san marco piazza ; Grice ; venice ; Long Short-Term Memory Networks ; two ; over a million ; Gated Recurrent Units ; RNN ; first,competitive baselines ; the resulting generator ; These issues ; e.g. reuse ; the same content ; me ; noises ; seq ; the morning ; more lengthy and sensical text,Recurrent Neural Network ; the san marco piazza ; Grice ; venice ; Long Short-Term Memory Networks ; two ; over a million ; Gated Recurrent Units ; RNN ; first,"Recurrent Neural Networks (RNNs) are powerful autoregressive sequence models for learning prevalent patterns in natural language. While fluent, RNN language production can be overly generic, repetitive, and self-contradictory. In this paper, we introduce a general learning framework that can construct a decoding objective better suited for RNN generation. The framework learns to construct a substantially stronger generator by combining several discriminatively trained models.   Human evaluation demonstrates that text generated by the resulting generator is preferred over  baselines  by  a large  margin  and  significantly enhances  the overall coherence, style, and",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Multimodal sentiment analysis is a core research area that studies speaker sentiment expressed from the language, visual, and acoustic modalities. The central challenge in multimodal learning involves inferring joint representations that can process and relate information from these modalities. However, existing work learns joint representations using multiple modalities as input and may be sensitive to noisy or missing modalities at test time. With the recent success of sequence to sequence models in machine translation, there is an opportunity to explore new ways of learning joint representations that may not require all input modalities at test time. In this paper, we propose a method to learn robust joint representations by translating between modalities. Our method is based on the key insight that translation from a source to a target modality provides a method of learning joint representations using only the source modality as input. We augment modality translations with a cycle consistency loss to ensure that our joint representations retain maximal information from all modalities. Once our translation model is trained with paired multimodal data, we only need data from the source modality at test-time for prediction. This ensures that our model remains robust from perturbations or missing target modalities. We train our model with a coupled translation-prediction objective and it achieves new state-of-the-art results on multimodal sentiment analysis datasets: CMU-MOSI, ICT-MMMO, and YouTube. Additional experiments show that our model learns increasingly discriminative joint representations with more input modalities while maintaining robustness to perturbations of all other modalities. Sentiment analysis, which involves identifying a speaker's opinion, is a core research problem in machine learning and natural language processing. However, language-based sentiment analysis through words, phrases, and their compositionality was found to be insufficient for inferring affective content from spoken opinions BID33 , which contain rich nonverbal behaviors in addition to verbal text. As a result, there has been a recent push towards using machine learning methods to learn joint representations from additional behavioral cues present in the visual and acoustic modalities. This research field has become known as multimodal sentiment analysis and extends the conventional textbased definition of sentiment analysis to a multimodal setup. For example, BID21 explore the additional acoustic modality while BID61 use the language, visual, and acoustic modalities present in monologue videos to predict sentiment. The abundance of multimodal data has led to the creation of multimodal datasets, such as CMU-MOSI BID66 and ICT-MMMO BID61 , as well as deep multimodal models that are highly effective at learning discriminative joint multimodal representations BID29 BID57 BID7 . Existing work learns joint representations using multiple modalities as input with neural networks BID28 , graphical models BID33 or geometric classifiers BID66 . However, this results in joint representations that are sensitive to noisy or missing modalities at test time. To address this problem, we draw inspirations from the recent success of sequence to sequence models for unsupervised representation learning BID55 . We propose the Multimodal Cyclic Translation Network model (MCTN) to learn robust joint multimodal representations by translating between modalities. FIG0 illustrates these translations between the language, visual and acoustic modalities. Our method is based on the key insight that translation from a source modality S to a target modality T results in an intermediate representation that captures joint information between modalities S and T . MCTN extends this insight using a cyclic translation loss involving both forward translations from source to target, and backward translations from the predicted target back to the source modality. Together, we call these multimodal cyclic translations to ensure that the learned joint representations capture maximal information from both modalities. We also propose a hierarchical MCTN to learn joint representations between a source modality and multiple target modalities. MCTN is trainable end-to-end with a coupled translation-prediction loss which consists of (1) the cyclic translation loss, and (2) a prediction loss to ensure that the learned joint representations are task-specific. Another advantage of MCTN is that once trained with paired multimodal data (S, T ), we only need data from the source modality S at test time to infer the joint representation and sentiment prediction. As a result, MCTN is completely robust to test-time perturbations on target modality T and missing modalities.Even though translation and generation of videos, audios, and text are difficult BID27 , our experiments show that the learned joint representations can help for discriminative tasks: MCTN achieves new state-of-the-art results on multimodal sentiment analysis using the CMU-MOSI, ICT-MMMO, and YouTube public datasets. Additional experiments show that MCTN learns increasingly discriminative joint representations with more input modalities while maintaining robustness to all target modalities. This section discusses several research questions and presents our experimental results. To conclude, this paper investigated learning joint representations via cyclic modality translations from source to target modalities. During testing, we only need the source modality for prediction which ensures that our model remains robust from noisy or missing target modalities. We demonstrate that cyclic translations and seq2seq models are especially useful for learning joint multimodal representations. In addition to achieving state-of-the-art results on three datasets, our model learns increasingly discriminative representations with more input modalities while maintaining robustness to all target modalities. Our approach presents several exciting areas for future work, such as: 1) combining our approach with the transformer architecture BID59 for modality translations, 2) exploring pretrained deep language models BID12 BID42 for translations, as well as 3) extending our translation model to work other multimodal tasks involving language and raw speech signals (prosody), videos with multiple speakers (diarization), and combinations of static and temporal data (i.e. image captioning).",We present a model that learns robust joint representations by performing hierarchical cyclic translations between multiple modalities.,CMU ; ICT ; YouTube ; the Multimodal Cyclic Translation Network ; MCTN ; three,deep language models ; all target modalities ; maximal information ; prosody ; ICT-MMMO ; natural language processing ; translation ; both modalities ; missing modalities ; the source modality,CMU ; ICT ; YouTube ; the Multimodal Cyclic Translation Network ; MCTN ; three,"Multimodal sentiment analysis is a core research area that studies speaker sentiment expressed from the language, visual, and acoustic modalities. However, existing work learns joint representations using multiple modalities as input and may be sensitive to noisy or missing modalities at test time. With the recent success of sequence to sequence models in machine translation, there is an opportunity to explore new ways of learning robust joint representations by translating between modalities, such as with a coupled translation-prediction objective. This approach is based on the key insight that translation from a source to target modality provides a method of learning joint representations with only the source mod",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"We present a new approach for efficient exploration which leverages a low-dimensional encoding of the environment learned with a combination of model-based and model-free objectives. Our approach uses intrinsic rewards that are based on a weighted distance of nearest neighbors in the low dimensional representational space to gauge novelty.
 We then leverage these intrinsic rewards for sample-efficient exploration with planning routines in representational space.
 One key element of our approach is that we perform more gradient steps in-between every environment step in order to ensure the model accuracy. We test our approach on a number of maze tasks, as well as a control problem and show that our exploration approach is more sample-efficient compared to strong baselines. In order to solve a task efficiently in Reinforcement Learning (RL), one of the main challenges is to gather informative experiences thanks to an efficient exploration of the state space. A common approach to exploration is intrinsic rewards correlated with some novelty heuristics (Schmidhuber, 2010; Houthooft et al., 2016) . With intrinsic rewards, an agent can be incentivized to efficiently explore its state space. A direct approach to calculating these novelty heuristics is to derive a reward based on the observations, such as a count-based reward (Bellemare et al., 2016; Ostrovski et al., 2017) or a prediction-error based reward (Burda et al., 2018b) . However, an issue occurs when measuring novelty directly from the raw observations, as some information in the pixel space (such as randomness) might be irrelevant. In this case, if an agent wants to efficiently explore its state space it should only focus on meaningful and novel information. In this work, we propose a method of sample-efficient exploration by leveraging novelty heuristics in a meaningful abstract state space. We leverage a low-dimensional abstract representation of states, which is learned by fitting both model-based and model-free components through a joint representation. This provides a meaningful abstract representation where states that are close temporally in dynamics are brought close together in low-dimensional representation space. We also add additional constraints to ensure that a measure of distance between states is meaningful. With this distance in representational space, we form a novelty heuristic inspired by the Novelty Search algorithm (Lehman and Stanley, 2011) to generate intrinsic rewards that we use for efficient exploration. We show that with a good low-dimensional representation of states, a policy based on planning with our novelty heuristic is able to explore with high sample-efficiency. In our experiments, we measure the effectiveness of our exploration methods by the number of samples required to explore the state space. One key element of our approach is that we perform more gradient steps in-between every environment step in order to ensure the model accuracy is high (and hence ensure an accurate novelty heuristic). Through this training scheme, our agent is also able to learn a meaningful representation of its state space in an extremely sample-efficient manner. In this paper, we show that with an interpretable abstract representation of states, our novelty metric is able to serve as an intrinsic reward that enables efficient exploration. By using this novelty metric with a combination of model-based and model-free approaches for planning, we demonstrate the efficiency of our method in multiple environments. As with most methods, our approach also has limitations. While the problem of distance metrics in high-dimensional space is partially solved in our method with the dimensionality reduction of observations by our encoder, the 2 -norm still requires a low dimension to be useful (Aggarwal et al., 2002) . This implies that our novelty metric may lose its effectiveness as we increase the dimension of our abstract representation. In addition, our exploration strategy benefits greatly from the meaningful abstractions and internal model. In some cases, the model can over-generalize with the consequence that the low-dimensional representation loses information that is crucial for the exploration of the entire state space. An interesting direction for future work would be find ways of incorporating the secondary features mentioned in Section 6.1.2. A DISCUSSION ON THE ENTROPY CONSTRAINT As for our soft constraints on representation magnitude, we use a local constraint instead of a global constraint on magnitude such that it is more suited for our novelty metric. If we are to calculate some form of intrinsic reward based on distance between neighboring states, then this distance needs to be non-zero and ideally consistent as the number of states in our history increases. In the global constraint case, if the intrinsic rewards decreases with an increase in number of states in the agent's history, then the agent will fail to be motivated to explore further. Even though the entropy maximization losses ensures the maximization of distances between random states, if we have |H s | number of states in the history of the agent, then a global constraint on representation magnitude might lead to lim Here H s is a list of all possible states in S in any order, with each possible state appearing only once. If we let k = 4, we have that: While it may seem redundant to include the 1st nearest neighbor distance in this metric (which would be itself if we've visited the state), the 1st nearest neighbor is non-zero when we calculate the novelty of a predicted state using our learned transition functionτ . From this example, we can see that there is a bias towards states with fewer direct neighbors due to the nature of our novelty metric. This poses an issue -if our goal is for sample-efficient exploration of our state space, then there is no reason to favor states with less direct neighbors.",We conduct exploration using intrinsic rewards that are based on a weighted distance of nearest neighbors in representational space.,One ; Reinforcement Learning ; Houthooft ; al. ; Ostrovski ; Burda ; Lehman ; Stanley,all possible states ; our history increases ; the state space ; the state ; an issue ; a meaningful abstract state space ; A DISCUSSION ; addition ; the secondary features ; al,One ; Reinforcement Learning ; Houthooft ; al. ; Ostrovski ; Burda ; Lehman ; Stanley,"A low-dimensional encoding of the environment learned with model-based and model-free objectives is used to create intrinsic rewards based on a weighted distance of nearest neighbors in the low dimensional representational space to gauge novelty. In order to achieve sample-efficient exploration, we use a high-dimensional representation of states, which is derived from the observations and a dynamic abstract representation. The novelty heuristic, inspired by the Novelty Search algorithm (Lehman and Stanley, 2011), generates intrinsic rewards for efficient exploration by leveraging novelty heuristics in a meaningful abstract state space. In this work, a policy based on planning with novelty he",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Meta-learning is a promising strategy for learning to efficiently learn within new tasks, using data gathered from a distribution of tasks. However, the meta-learning literature thus far has focused on the task segmented setting, where at train-time, offline data is assumed to be split according to the underlying task, and at test-time, the algorithms are optimized to learn in a single task. In this work, we enable the application of generic meta-learning algorithms to settings where this task segmentation is unavailable, such as continual online learning with a time-varying task. We present meta-learning via online changepoint analysis (MOCA), an approach which augments a meta-learning algorithm with a differentiable Bayesian changepoint detection scheme. The framework allows both training and testing directly on time series data without segmenting it into discrete tasks. We demonstrate the utility of this approach on a nonlinear meta-regression benchmark as well as two meta-image-classification benchmarks. Meta-learning methods have recently shown promise as an effective strategy for enabling efficient few-shot learning in complex domains from image classification to nonlinear regression (Finn et al., 2017; Snell et al., 2017) . These methods leverage an offline meta-training phase, in which they use data from a distribution of tasks to optimize learning performance on new tasks. These algorithms have focused on settings with task segmentation, where the learning agent knows when tasks change. At meta-train time, these algorithms assume access to a meta-dataset of datasets from individual tasks, and at meta-test time, the learner is evaluated on a single task. However, there are many applications where task segmentation is unavailable, which have thus far been under-addressed in the meta-learning literature. For example, consider a robot which must learn to adapt to a changing environment. The robot may switch from one environment to another during the course of deployment, and these task switches may not be directly observed. Furthermore, using an existing time series from interaction to craft a meta-dataset may require a difficult or expensive process of detecting switches in task. In this work, we aim to enable meta-learning in task-unsegmented settings, operating directly on time series in which the latent task undergoes discrete, unobserved switches, rather than requiring a pre-segmented meta-dataset. Equivalently, this problem can be viewed from the perspective of continual learning, in that we apply the meta-learning approach to the standard online learning problem statement wherein an agent must sequentially make predictions and learn with a potentially varying latent data generating process. To accomplish this, we integrate a Bayesian changepoint estimation scheme with existing meta-learning approaches, allowing the algorithm to reason about whether or not the task has changed in a time series. Thus, we enable a standard meta-learning algorithm, which is designed for the task segmented setting, to be both trained and tested directly on time series data without the need for task segmentation. Contributions. The primary contribution of this work is an algorithmic framework for task unsegmented meta-learning which we refer to as meta-learning via online changepoint analysis (MOCA). MOCA wraps arbitrary meta-learning algorithms in a differentiable changepoint estimation algorithm, enabling application of meta-learning algorithms directly to problems in the continuous learning setting. By backpropagating through the changepoint estimation framework, MOCA learns both a rapidly adaptive underlying predictive model (in the form of the meta-learning model), as well as an effective changepoint detection algorithm. MOCA is a generic framework which can be paired with many existing meta-learning algorithms. We demonstrate the performance of MOCA on both regression and classification settings with unobserved task switches. Future Work. While MOCA addresses a continual learning problem setting, we have not formulated MOCA as an online learning algorithm. Specifically, MOCA meta-trains on an offline timeseries, and keeps the parameters θ fixed online, whereas an online learning algorithm would not have this train/test distinction, and would consider updating θ continuously (Hazan, 2016) . However, in order to do this with MOCA, we would need to keep a running buffer of all data observed so far and to use as training data to update θ, which may be expensive in real-world domains where large volumes of data (e.g. high definition video from a large collection of cameras on an autonomous vehicle). Extending MOCA toward either strictly online training or a scheme to maintain an efficient replay buffer (Mnih et al., 2013; Vitter, 1985) , is a promising direction of future work. Indeed, it may be possible to use MOCA's changepoint analysis to inform which data to save. Beyond the continual learning extension, data efficiency may be improved by re-using information from previous tasks or modeling task evolution dynamics. Previous work (Nagabandi et al., 2019b; Jerfel et al., 2019; Knoblauch & Damoulas, 2018 ) has addressed the case in which tasks reoccur in both meta-learning and the BOCPD framework, and thus knowledge (in the form of a posterior estimate) may be re-used. In this work, we address the case in which tasks are sampled i.i.d. from a (typically continuous) distribution, and thus knowledge re-use is often impractical or adds marginal value. Broadly, moving beyond the assumption of i.i.d. tasks to task having associated dynamics (Al-Shedivat et al., 2018) represents a promising future direction. Conclusions. MOCA enables the application of existing meta-learning algorithms to problems without task segmentation, such as the problem setting of continual learning. We find that by leveraging a Bayesian perspective on meta-learning algorithms and augmenting these algorithms with a Bayesian changepoint detection scheme to automatically detect task switches within time-series, we can achieve similar predictive performance when compared to the standard task-segmented metalearning setting, without the often prohibitive requirement of supervised task segmentation. for the sinusoid with hazard 0.01. The lowest hazard was used to increase the effects of the short training horizon. A minor decrease in performance is visible for very small training horizons (around 20), but flattens off around 100 and above. It is expected that these diminishing marginal returns will occur for all systems and hazard rates. PCOC extends a line of work on meta-classification based on prototypical networks (Snell et al., 2017) . This framework maps the context data to an embedding space, after which it computes the centroid for each class. For a new data point, it models the probability of belonging to each class as the softmax of the distances between the embedded point and the class centroids, for some distance metric. For Euclidean distances (which the authors focus on), this corresponds to performing frequentist estimation of class means, under the assumption that the variance matrix for each class is the identity matrix 2 . Indeed, this corresponds to the cheapest-to-evaluate simplification of PCOC. Ren et al. (2018) propose adding a class-dependent length scale (which is a scalar), which corresponds to meta-learning a frequentist estimate of the variance for each class. Moreover, it corresponds to assuming a variance that takes the form of a scaled identity matrix. Indeed, assuming diagonality of the covariance matrix results in substantial performance improvement as the matrix inverse may be performed element-wise. This reduces the numerical complexity of this operation in the (frequently high-dimensional) embedding space from cubic to linear. However, in our implementation of MOCA, we assume diagonal covariances throughtout, resulting in comparable computational complexity to the different flavors of prototypical networks. If one were to use dense covariances, the computational performance decreases substantially (due to the necessity of matrix inversions), especially in high dimensional embedding spaces. In contrast to this previous work, PCOC has several desirable features. First, both Snell et al. (2017) and Ren et al. (2018) make the implicit assumption that the classes are balanced, whereas we perform online estimation of class probabilities via Dirichlet posterior inference. Beyond this, our approach is explicitly Bayesian, and we maintain priors over the parameters that we estimate online. This is critical for utilization in the MOCA framework. Existence of these priors allows ""zero-shot"" learning-it enables a model to classify incoming data to a certain class, even if no data belonging to that class has been observed within the current task. Finally, because the posteriors concentrate (the predictive variance decreases as more data is observed), we may better estimate when a change in the task has occurred. We also note that maximum likelihood estimation of Gaussian means is dominated by the James-Stein estimator (Stein, 1956) , which shrinks the least squares estimator toward some prior. Moreover, the James-Stein estimator paired with empirical Bayesian estimation of the prior-which is the basis for Bayesian meta-learning approaches such as ALPaCA and PCOC-has been shown to be a very effective estimator in this problem setting (Efron & Morris, 1973 ). Note that each column corresponds to one trained model, and thus the randomly varying performance across train supervision rates may be explained by simply results of minor differences in individual models.",Bayesian changepoint detection enables meta-learning directly from time series data.,Snell ; Jerfel et al. ; one ; BOCPD ; James-Stein ; Stein ; Vitter ; zero ; Knoblauch & Damoulas ; Gaussian,comparable computational complexity ; an offline timeseries ; a changing environment ; incoming data ; train supervision rates ; no data ; The primary contribution ; task evolution dynamics ; switches ; the implicit assumption,Snell ; Jerfel et al. ; one ; BOCPD ; James-Stein ; Stein ; Vitter ; zero ; Knoblauch & Damoulas ; Gaussian,"Meta-learning is a promising strategy for learning within new tasks, using data gathered from a distribution of tasks. However, the meta-learning literature has focused on task segmented settings, where offline data is assumed to be split according to the underlying task, and test-time, the algorithms are optimized to learn in a single task. In this work, we enable the application of generic Meta-learning algorithms to settings where task segmentation is unavailable, such as continual online learning with a time-varying task. This approach combines online changepoint analysis (MOCA) with a differentiable Bayesian changepoint detection scheme",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"In this paper we study image captioning as a conditional GAN training, proposing both a context-aware LSTM captioner and co-attentive discriminator, which enforces semantic alignment between images and captions. We investigate the viability of two discrete GAN training methods: Self-critical Sequence Training (SCST) and Gumbel Straight-Through (ST) and demonstrate that SCST shows more stable gradient behavior and improved results over Gumbel ST. Significant progress has been made on the task of generating image descriptions using neural image captioning. Early systems were traditionally trained using cross-entropy (CE) loss minimization BID6 BID16 . Later, reinforcement learning techniques BID13 BID9 based on policy gradient methods were introduced to directly optimize metrics such as CIDEr or SPICE BID0 . Along a similar idea, BID14 introduced Self-critical Sequence Training (SCST), a light-weight variant of REINFORCE, which produced state of the art image captioning results using CIDEr as an optimization metric. To address the problem of sentence diversity and naturalness, image captioning has been explored in the framework of GANs. However, due to the discrete nature of text generation, GAN training remains challenging and has been generally tackled either with reinforcement learning techniques BID4 BID12 BID2 or by using Gumbel softmax relaxation BID5 , as in BID15 BID7 .Despite impressive advances, image captioning is far from being a solved task. It remains a challenge to satisfactorily bridge the semantic gap between image and captions to produce diverse, creative, and ""human-like"" captions. Although applying GANs to image captioning for promoting human-like captions is a very promising direction, the discrete nature of the text generation process makes it challenging to train such systems. The recent work of BID1 showed that the task of text generation for current discrete GAN models is difficult, often producing unsatisfactory results, and requires therefore new approaches and methods.In this paper, we propose a novel GAN-based framework for image captioning that enables better language composition, more accurate compositional alignment of image and text, and light-weight efficient training of discrete sequence GAN based on SCST. In summary, we demonstrated that SCST training for discrete GAN is a promissing new approach that outperforms the Gumbel relaxation in terms of training stability and the overall performance. Moreover, we showed that our context-aware attention gives larger gains as compared to the adaptive sentinel or the traditional visual attention. Finally, our co-attention model for discriminator compares favorably against the joint embedding architecture.","Image captioning as a conditional GAN training with novel architectures, also study two discrete GAN training methods.",GAN ; two ; Sequence Training ; Gumbel Straight-Through ; SCST ; Gumbel ST ; CE ; REINFORCE ; Gumbel,"larger gains ; discrete GAN ; Self-critical Sequence Training ; our co-attention model ; cross ; a light-weight variant ; text ; unsatisfactory results ; , more accurate compositional alignment ; more stable gradient behavior",GAN ; two ; Sequence Training ; Gumbel Straight-Through ; SCST ; Gumbel ST ; CE ; REINFORCE ; Gumbel,"In this paper, we explore the feasibility of two discrete GAN training methods: Self-critical Sequence Training (SCST) and Gumbel Straight-Through (ST). Both systems were traditionally trained using cross-entropy (CE) loss minimization BID6 BID16 and reinforcement learning techniques BID13 BID9 based on policy gradient methods were introduced to directly optimize metrics such as CIDEr or SPICE BID0. In addition to self-critical sequence training, GANs have also been introduced to improve the task of image captioning. In BID15 BID7, they introduced Self",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"The knowledge that humans hold about a problem often extends far beyond a set of training data and output labels. While the success of deep learning mostly relies on supervised training, important properties cannot be inferred efficiently from end-to-end annotations alone, for example causal relations or domain-specific invariances. We present a general technique to supplement supervised training with prior knowledge expressed as relations between training instances. We illustrate the method on the task of visual question answering to exploit various auxiliary annotations, including relations of equivalence and of logical entailment between questions. Existing methods to use these annotations, including auxiliary losses and data augmentation, cannot guarantee the strict inclusion of these relations into the model since they require a careful balancing against the end-to-end objective. Our method uses these relations to shape the embedding space of the model, and treats them as strict constraints on its learned representations. %The resulting model encodes relations that better generalize across instances. In the context of VQA, this approach brings significant improvements in accuracy and robustness, in particular over the common practice of incorporating the constraints as a soft regularizer. We also show that incorporating this type of prior knowledge with our method brings consistent improvements, independently from the amount of supervised data used. It demonstrates the value of an additional training signal that is otherwise difficult to extract from end-to-end annotations alone. The capacity to generalize beyond the training data is one of the central challenges to the practical applicability of deep learning, and grows as the task considered grows more and more complex. Endto-end training provides a weak supervisory signal when the task requires a long chain of reasoning between its input and output (Glasmachers, 2017; Marcus, 2018; Zador, 2019) . A prime example is found in the task of visual question answering (VQA), where a model must predict the answer to a given text question and related image (see Fig. 1 ). Typical VQA models trained with supervision (questions/images and ground truth answers) tend to capture superficial statistical correlations, rather than the underlying reasoning steps required for strong generalization (Goyal et al., 2016; Agrawal et al., 2018; Teney & van den Hengel, 2016) . Prior knowledge that reflects a deeper understanding of the data than the ground truth answers offers an invaluable -although currently ignored -source of information to train models more effectively. We propose a method to incorporate, in deep learning models, prior knowledge that is specified as relations between training examples. Incorporating knowledge beyond end-to-end supervision is an opportunity to improve generalization by providing high-level guidance complementary to ground truth labels. The fact that two data elements are equivalent, e.g. two questions being rephrasings of each other in a question-answering task, provides more information than merely illustrating that they share the same answer. Such a constraint of equivalence exemplifies a high-level, general concept that is much more powerful than a set of examples sharing a label. Prior knowledge has previously been incorporated into network architectures in multiple ways, e.g. by sharing weights spatially in a CNN (Nowlan & Hinton, 1992) . Existing approaches are however often task-specific, and more importantly, usually operate in parameter space (Cohen & Welling, Figure 1: We demonstrate our method on the task of visual question answering (VQA), where we exploit three types of auxiliary annotations expressed as relations between training questions. This task-specific knowledge (for example the equivalence of synonymous questions) provides a training signal complementary to the end-to-end annotations of ground truth answers. 2016; Guttenberg et al., 2016; Laptev & Buhmann, 2015; Teney & Hebert, 2016) . In contrast, our approach uses knowledge expressed in embedding space, which we find far more intuitive for expressing higher-level knowledge. Indeed, in embedding space, one can specify how the network represents data. In parameter space, one can guide how the network processes these representations. While both can be useful, the former can map more directly to a task-or domain expert's knowledge of the data used. The additional knowledge that we use comes as annotations of relations between specific training instances. We do not require all of the training data to be annotated with this additional knowledge. Technically, we propose a novel training method that operates in two phases (see Fig. 2 ). First, we employ the constraints derived from the annotations as soft regularizers. They guide the optimization of the target task to loosely satisfy the constraints. Second, we retrain the earlier layers of the network by distillation (Hinton et al., 2015) using, as targets, embeddings projected on the manifold where the constraints are met perfectly. This second phase is the crux to enforce hard constraints on the learned embeddings. A major finding in our experiments is that these hard constraints are more effective the soft regularizers, in all of our test cases. We present an extensive suite of experiments on synthetic and large-scale datasets (Section 4). In the context of VQA, we apply the method on top of the popular model of to leverage three types of annotations illustrated in Fig. 1 : relations of equivalence between questions (i.e. being rephrasings of one another), of entailment (the answer to a general question being deducible from that of a more specific one), and relations of set membership, where questions are known to share some reasoning steps (e.g. questions referring to similar objects or requiring similar reasoning operations). These annotations are provided with the GQA dataset (Hudson & Manning, 2019) , but have largely been overlooked due to the difficulty of combining this type of knowledge with end-to-end training. hlWe show that imposing hard constraints on linguistic embeddings in this context is superior to the corresponding soft regularizers. We also demonstrate consistent improvements in robustness and accuracy independent from the amount of supervised data, which supports the benefits of such training signals in complement to end-to-end annotations. The contributions of this paper are summarized as follows. 1. We propose a method to exploit prior knowledge expressed as relations between training instances when training a deep learning model. The method enforces hard constraints on the internal representations learned by the model while allowing end-to-end supervised training, which alleviates issues with soft regularizers. 2. We show that the method is applicable to a range of tasks and types of prior knowledge. We describe its application to three generic types of relations (symmetric/equivalence, asymmetric, and set membership) and show that it does not require domain-specific expert knowledge. In many cases, the specification of constraints in embedding space is more intuitive than the alternative practice of designing regularizers in parameter space. 3. We demonstrate the benefits of the method on the task of VQA. We show how to exploit three types of auxiliary annotations about the training questions (equivalence, entailment, and common reasoning steps). This is the first published VQA model to make use of these annotations, which our method allows us to leverage to bring clear improvements in robustness and accuracy. Our results suggest that they provide a training signal complementary to end-to-end annotations. We presented an approach to incorporate prior knowledge in deep learning models in the form of constraints on its internal representations. We then applied the method to the task of VQA to leverage multiple types of relations between training questions. This application is of particular interest because VQA is a prime example of a task where the end-to-end supervised paradigm shows its limits, due to the long chains of reasoning that connect the inputs and outputs. The proposed approach served to shape the space of the internal representations learned by the model. Our experiments with the GQA dataset showed clear benefits in improving the accuracy and robustness of an existing VQA model. Interestingly, these benefits hold regardless of the amount of training data used for end-to-end supervision, suggesting that the type of prior knowledge used cannot otherwise be effectively captured in the model through end-to-end annotations alone. Technically, the proposed method treats the additional knowledge as hard constraints on the model's internal representations. This proved more clearly effective than existing methods to exploit this type of annotations, including soft regularizers and data augmentation. The proposed method can apply to a variety of tasks and domains that we hope to explore in future work. Concrete applications overlap with those considered for topological embeddings such as learning representations for knowledge bases, and other tree-or graph-structured data (Ganea et al., 2018; Nickel & Kiela, 2017; Vendrov et al., 2016) . Enforcing hard constraints on a learned model also allows one to provide guarantees that are otherwise impractical to meet with a purely datadriven approach. In particular, the method could be used to integrate known causal relations in a model, or known outcomes of interventions on specific training examples. This holds the promise of making further steps toward generalizable and trustworthy machine learning models.",Training method to enforce strict constraints on learned embeddings during supervised training. Applied to visual question answering.,two ; Teney & van den Hengel ; Agrawal ; Second ; Hinton ; Teney & Hebert ; Hudson & Manning ; Nowlan & Hinton ; First ; Nickel & Kiela,a task-or domain expert's knowledge ; multiple types ; the practical applicability ; knowledge bases ; the answer ; its internal representations ; Figure ; high-level guidance ; Nickel ; the accuracy,two ; Teney & van den Hengel ; Agrawal ; Second ; Hinton ; Teney & Hebert ; Hudson & Manning ; Nowlan & Hinton ; First ; Nickel & Kiela,"The knowledge that humans hold about a problem often extends beyond a set of training data and output labels. However, important properties cannot be inferred efficiently from end-to-end annotations alone, such as causal relations or domain-specific invariances. A general technique to supplement supervised training with prior knowledge expressed as relations between training instances is called visual question answering (VQA), where a model must predict the answer to a given text question and related image. This approach improves accuracy and robustness, independently from the amount of supervised data used. It demonstrates the value of an additional training signal that is otherwise difficult to extract from VQ",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"In probabilistic classification, a discriminative model based on Gaussian mixture exhibits flexible fitting capability. Nevertheless, it is difficult to determine the number of components. We propose a sparse classifier based on a discriminative Gaussian mixture model (GMM), which is named sparse discriminative Gaussian mixture (SDGM). In the SDGM, a GMM-based discriminative model is trained by sparse Bayesian learning. This learning algorithm improves the generalization capability by obtaining a sparse solution and automatically determines the number of components by removing redundant components. The SDGM can be embedded into neural networks (NNs) such as convolutional NNs and can be trained in an end-to-end manner. Experimental results indicated that the proposed method prevented overfitting by obtaining sparsity. Furthermore, we demonstrated that the proposed method outperformed a fully connected layer with the softmax function in certain cases when it was used as the last layer of a deep NN. In supervised classification, probabilistic classification is an approach that assigns a class label c to an input sample x by estimating the posterior probability P (c|x). This approach is primarily categorized into two types of models: discriminative model and generative model. The former optimizes the posterior distribution P (c|x) directly on a training set, whereas the latter finds the class conditional distribution P (x|c) and class prior P (c) and subsequently derives the posterior distribution P (c|x) using Bayes' rule. The discriminative model and generative model are mutually related (Lasserre et al., 2006; Minka, 2005) . According to Lasserre et al. (2006) , the only difference between these models is their statistical parameter constraints. Therefore, given a certain generative model, we can derive a corresponding discriminative model. For example, the discriminative model corresponding to a unimodal Gaussian distribution is logistic regression (see Appendix A for derivation). Several discriminative models corresponding to the Gaussian mixture model (GMM) have been proposed (Axelrod et al., 2006; Bahl et al., 1996; Klautau et al., 2003; Tsai & Chang, 2002; Tsuji et al., 1999; Tüske et al., 2015; Wang, 2007) . They indicate more flexible fitting capability than the generative GMM and have been applied successfully in fields such as speech recognition (Axelrod et al., 2006; Tüske et al., 2015; Wang, 2007) . The problem to address in mixture models such as the GMM is the determination of the number of components M . Classically, Akaike's information criterion and the Bayesian information criterion have been used; nevertheless, they require a considerable computational cost because a likelihood must be calculated for every candidate component number. In the generative GMM, methods that optimize M during learning exist (Crouse et al., 2011; Štepánová & Vavrečka, 2018) . However, in a discriminative GMM, a method to optimize M simultaneously during learning has not been clearly formulated. In this paper, we propose a novel GMM having two important properties: sparsity and discriminability, which is named sparse discriminative Gaussian mixture (SDGM). In the SDGM, a GMM-based discriminative model is trained by sparse Bayesian learning. This learning algorithm improves the generalization capability by obtaining a sparse solution and determines the number of components automatically by removing redundant components. Furthermore, the SDGM can be embedded into neural networks (NNs) such as convolutional NNs and trained in an end-to-end manner with an NN. To the authors best knowledge, there is no GMM that has both of sparsity and discriminability. The contributions of this study are as follows: • We propose a novel sparse classifier based on a discriminative GMM. The proposed SDGM has both sparsity and discriminability, and determines the number of components automatically. The SDGM can be considered as the theoretical extension of the discriminative GMM and the relevance vector machine (RVM) (Tipping, 2001 ). • This study attempts to connect both fields of probabilistic models and NNs. From the equivalence of a discriminative model based on Gaussian distribution to a fully connected layer, we demonstrate that the SDGM can be used as a module of a deep NN. We also show that the SDGM can show superior performance than the fully connected layer with a softmax function via an end-to-end learning with an NN on the image recognition task. In this paper, we proposed a sparse classifier based on a GMM, which is named SDGM. In the SDGM, a GMM-based discriminative model was trained by sparse Bayesian learning. This learning algorithm improved the generalization capability by obtaining a sparse solution and automatically determined the number of components by removing redundant components. The SDGM could be embedded into NNs such as convolutional NNs and could be trained in an end-to-end manner. In the experiments, we demonstrated that the SDGM could reduce the amount of weights via sparse Bayesian learning, thereby improving its generalization capability. The comparison using benchmark datasets suggested that SDGM outperforms the conventional sparse classifiers. We also demonstrated that SDGM outperformed the fully connected layer with the softmax function when it was used as the last layer of a deep NN. One of the limitations of this study is that sparse Bayesian learning was applied only when the SDGM was trained stand-alone. In future work, we will develop a sparse learning algorithm for a whole deep NN structure including the feature extraction part. This will improve the ability of the CNN for larger data classification.","A sparse classifier based on a discriminative Gaussian mixture model, which can also be embedded into a neural network.",al. ; Tüske ; c|x ; Minka ; Classically ; Štepánová & Vavrečka ; Bayes ; One ; Tsuji ; Lasserre et al,a novel sparse classifier ; The discriminative model ; NNs ; the fully connected layer ; the limitations ; a softmax function ; This approach ; the Gaussian mixture model ; sparsity ; RVM,al. ; Tüske ; c|x ; Minka ; Classically ; Štepánová & Vavrečka ; Bayes ; One ; Tsuji ; Lasserre et al,"In probabilistic classification, a discriminative Gaussian mixture model (GMM) exhibits flexible fitting capability. However, it is difficult to determine the number of components. In the SDGM, a generalization capability is achieved by obtaining a sparse solution and removing redundant components. The SDGM can be embedded into neural networks (NNs) such as convolutional NNs and can be trained in an end-to-end manner. Experimental results indicated that the proposed method prevented overfitting by obtaining sparsity. Furthermore, it outperformed a fully connected layer with softmax function in certain cases when it was used as the",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Recurrent Neural Networks (RNNs) are designed to handle sequential data but suffer from vanishing or exploding gradients.   Recent work on Unitary Recurrent Neural Networks (uRNNs) have been used to address this issue and in some cases, exceed the capabilities of Long Short-Term Memory networks (LSTMs).   We propose a simpler and novel update scheme to maintain orthogonal recurrent weight matrices without using complex valued matrices. This is done by parametrizing with a skew-symmetric matrix using the Cayley transform. Such a parametrization is unable to represent matrices with negative one eigenvalues, but this limitation is overcome by scaling the recurrent weight matrix by a diagonal matrix consisting of ones and negative ones.   The proposed training scheme involves a straightforward gradient calculation and update step. In several experiments, the proposed scaled Cayley orthogonal recurrent neural network (scoRNN) achieves superior results with fewer trainable parameters than other unitary RNNs. Deep neural networks have been used to solve numerical problems of varying complexity. RNNs have parameters that are reused at each time step of a sequential data point and have achieved state of the art performance on many sequential learning tasks. Nearly all optimization algorithms for neural networks involve some variant of gradient descent. One major obstacle to training RNNs with gradient descent is due to vanishing or exploding gradients, as described in BID1 and BID14 . This problem refers to the tendency of gradients to grow or decay exponentially in size, resulting in gradient descent steps that are too small to be effective or so large that the network oversteps the local minimum. This issue significantly diminishes RNNs' ability to learn time-based dependencies, particularly in problems with long input sequences.A variety of architectures have been introduced to overcome this difficulty. The current preferred RNN architectures are those that introduce gating mechanisms to control when information is retained or discarded, such as LSTMs BID6 and GRUs BID3 , at the cost of additional trainable parameters. More recently, the unitary evolution RNN (uRNN) BID0 ) uses a parametrization that forces the recurrent weight matrix to remain unitary throughout training, and exhibits superior performance to LSTMs on a variety of synthetic and real-world tasks. For clarity, we follow the convention of BID21 and refer to this network as the restricted-capacity uRNN.Since the introduction of uRNNs, orthogonal and unitary RNN schemes have increased in both popularity and complexity. BID21 use a multiplicative update method detailed in Tagare (2011) and BID20 to expand uRNNs' capacity to include all unitary matrices. These networks are referred to as full-capacity uRNNs. BID7 's EURNN parametrizes this same space with Givens rotations, while BID8 's GORU introduces a gating mechanism for unitary RNNs to enable short term memory. BID19 introduced modified optimization and regularization methods that restrict singular values of the recurrent matrix to an interval around 1. Each of these methods involve complex valued recurrent weights. For other work in addressing the vanishing and exploding gradient problem, see BID5 and BID11 .In this paper, we consider RNNs with a recurrent weight matrix taken from the set of all orthogonal matrices. To construct the orthognal weight matrix, we parametrize it with a skew-symmetric matrix through a scaled Cayley transform. This scaling allows us to avoid the singularity issue occuring for −1 eigenvalues that may arise in the standard Cayley transform. With the parameterization, the network optimization involves a relatively simple gradient descent update. The resulting method achieves superior performance on sequential data tasks with a smaller number of trainable parameters and hidden sizes than other unitary RNNs and LSTMs.The method we present in this paper works entirely with real matrices, and as such, our results deal only with orthogonal and skew-symmetric matrices. However , the method and all related theory remain valid for unitary and skew-Hermitian matrices in the complex case. The experimental results in this paper indicate that state of the art performance can be achieved without the increased complexity of optimization along the Stiefel manifold and using complex matrices.",A novel approach to maintain orthogonal recurrent weight matrices in a RNN.,GORU ; Givens ; Long Short-Term Memory ; decay ; Cayley ; One ; EURNN ; Tagare ; RNN ; Unitary Recurrent Neural Networks,the orthognal weight matrix ; RNNs' ability ; the proposed scaled Cayley orthogonal recurrent neural network ; trainable parameters ; the increased complexity ; hidden sizes ; some variant ; the cost ; Deep neural networks ; the standard Cayley transform,GORU ; Givens ; Long Short-Term Memory ; decay ; Cayley ; One ; EURNN ; Tagare ; RNN ; Unitary Recurrent Neural Networks,"Unitary Recurrent Neural Networks (uRNNs) are designed to handle sequential data but suffer from vanishing or exploding gradients. The proposed training scheme involves a straightforward gradient calculation and update step. In several experiments, the proposed scaled Cayley orthogonal recurrent neural network (scoRNN) achieves superior results with fewer trainable parameters than other unitary RNNs. Deep neural networks have been used to solve numerical problems of varying complexity and have achieved state of the art performance on many sequential learning tasks.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"The existence of adversarial examples, or intentional mis-predictions constructed from small changes to correctly predicted examples, is one of the most significant challenges in neural network research today. Ironically, many new defenses are based on a simple observation - the adversarial inputs themselves are not robust and small perturbations to the attacking input often recover the desired prediction. While the intuition is somewhat clear, a detailed understanding of this phenomenon is missing from the research literature. This paper presents a comprehensive experimental analysis of when and why perturbation defenses work and potential mechanisms that could explain their effectiveness (or ineffectiveness) in different settings. The attacks on Convolutional Neural Networks, such as Carlini & Wanger or PGD (Madry et al., 2017) , generate strategically placed modifications that can be easily dominated by different types of perturbations resulting in correct predictions (Dziugaite et al., 2016; Roth et al., 2019) . This suggests that the standard adversarial examples are not robust. Many defense techniques explicitly leverage this property and can be retrospectively interpreted as perturbations of the input images. However, a detailed understanding of this phenomenon is lacking from the research literature including: (1) what types of perturbations work and what is their underlying mechanism, (2) whether all attacks exhibit this property, and (3) possible counter-measures attackers can employ to defeat perturbation defenses. We can interpret a large number of recent defenses as a type of input perturbations, for example, feature squeezing (Xu et al., 2017) , frequency or JPEG compression (Dziugaite et al., 2016) , randomized smoothing (Cohen et al., 2019) , and perturbation of network structure or the inputs randomly (JafarniaJahromi et al., 2018; Zhang & Liang, 2019; Guo et al., 2017) . The defense techniques exhibit very similar gains in robustness. To show it, we start with a simple model where every example is passed through a lossy channel (stochastic or deterministic) prior to model inference. This channel induces a small perturbation to the input. We optimize the perturbation to be small enough as not to affect the prediction accuracy on clean examples but large enough to dominate any adversarial attack. We find that this trade-off is surprisingly consistent across very different families of input perturbations, where the relationship between channel distortion (the L 2 distance between channel input and output) and robustness is very similar. Why are some state-of-the-art attacks are sensitive to perturbation-based defenses? We find that many attacks execute an optimization procedure that finds an adversarial image that is very close to the original image in terms of of L 1 , L 2 , or L ∞ norm. The resultant optimum, i.e., the adversarial image, tends to exhibit a higher level of instability than natural examples, which we demonstrate from the perspective of a first-order and second-order analysis. By instability we mean that small perturbations of the example can affect the prediction confidences. The unification of perturbation-based defense also gives us some insight into how an attacker might avoid them. Our experiments suggest that all the perturbation based defenses are vulnerable to the same types of attack strategies. We argue that the optimization procedure in the attacker should find the smallest distance from the original image that closes the recovery window. In fact, we can devise a generic attacker that attacks a particularly strong lossy channel, based on the additive Laplace noise, and adaptive attacks designed on this channel are often successful against other defenses. This result implies that for many input perturbation defenses the attacker need not be fully adaptive, i.e., they do not need to know exactly what kind of transformation is used to defend the network. The non-adaptive attacks are not robust since small changes to the adversarial input often recover the original label. This is an obvious corollary to the very existence of adversarial examples that by definition are relatively close to correctly predicted examples in the input space. Random perturbations of the input can dominate the strategically placed perturbations synthesized by an attack. In fact, the results are consistent across both deterministic and stochastic channels that degrade the fidelity of the input example. From the perspective of the attacker, the recovery window can be closed to make the perturbation based recovery techniques ineffective. A PERTURBATION ANALYSIS: ADDENDUM From the Cauchy-Schwarz inequality: From the definition of maximum eigenvalue :",We identify a family of defense techniques and show that both deterministic lossy compression and randomized perturbations to the input lead to similar gains in robustness.,PGD ; al. ; JafarniaJahromi ; JPEG ; Madry ; Xu et al. ; Cohen et al. ; second ; Carlini & Wanger ; Laplace,the optimization procedure ; network structure ; The attacks ; exactly what kind ; the strategically placed perturbations ; transformation ; possible counter ; a small perturbation ; perturbations ; a particularly strong lossy channel,PGD ; al. ; JafarniaJahromi ; JPEG ; Madry ; Xu et al. ; Cohen et al. ; second ; Carlini & Wanger ; Laplace,"The existence of adversarial examples, or intentional mis-predictions constructed from small changes to correctly predicted examples, is one of the most significant challenges in neural network research today. Many new defenses are based on a simple observation - the adversarial inputs themselves are not robust and small perturbations to the attacking input often recover the desired prediction. However, a detailed understanding of perturbation defenses is lacking in the research literature. This paper presents a comprehensive experimental analysis of when and why these defenses work and potential mechanisms that could explain their effectiveness (or ineffectiveness) in different settings. The attacks on Convolutional Neural Networks,",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"In this paper, we propose a method named Dimensional reweighting Graph Convolutional Networks (DrGCNs), to tackle the problem of variance between dimensional information in the node representations of GCNs. We prove that DrGCNs can reduce the variance of the node representations by connecting our problem to the theory of the mean field. However, practically, we find that the degrees DrGCNs help vary severely on different datasets. We revisit the problem and develop a new measure K to quantify the effect. This measure guides when we should use dimensional reweighting in GCNs and how much it can help. Moreover, it offers insights to explain the improvement obtained by the proposed DrGCNs. The dimensional reweighting block is light-weighted and highly flexible to be built on most of the GCN variants. Carefully designed experiments, including several fixes on duplicates, information leaks, and wrong labels of the well-known node classification benchmark datasets, demonstrate the superior performances of DrGCNs over the existing state-of-the-art approaches. Significant improvements can also be observed on a large scale industrial dataset. Deep neural networks (DNNs) have been widely applied in various fields, including computer vision (He et al., 2016; Hu et al., 2018) , natural language processing (Devlin et al., 2019) , and speech recognition (Abdel-Hamid et al., 2014) , among many others. Graph neural networks (GNNs) is proposed for learning node presentations of networked data (Scarselli et al., 2009) , and later be extended to graph convolutional network (GCN) that achieves better performance by capturing topological information of linked graphs (Kipf & Welling, 2017) . Since then, GCNs begin to attract board interests. Starting from GraphSAGE (Hamilton et al., 2017) defining the convolutional neural network based graph learning framework as sampling and aggregation, many follow-up efforts attempt to enhance the sampling or aggregation process via various techniques, such as attention mechanism , mix-hop connection and adaptive sampling . In this paper, we study the node representations in GCNs from the perspective of covariance between dimensions. Suprisingly, applying a dimensional reweighting process to the node representations may be very useful for the improvement of GCNs. As an instance, under our proposed reweighting scheme, the input covariance between dimensions can be reduced by 68% on the Reddit dataset, which is extremely useful since we also find that the number of misclassified cases reduced by 40%, compared with the previous SOTA method. We propose Dimensional reweighting Graph Convolutional Networks (DrGCNs), in which the input of each layer of the GCN is reweighted by global node representation information. Our discovery is that the experimental performance of GCNs can be greatly improved under this simple reweighting scheme. On the other hand, with the help of mean field theory (Kadanoff, 2009; Yang et al., 2019) , this reweighting scheme is also proved to improve the stability of fully connected networks, provding insight to GCNs. To deepen the understanding to which extent the proposed reweighting scheme can help GCNs, we develop a new measure to quantify its effectiveness under different contexts (GCN variants and datasets). Experimental results verify our theoretical findings ideally that we can achieve predictable improvements on public datasets adopted in the literature over the state-of-the-art GCNs. While studying on these well-known benchmarks, we notice that two of them (Cora, Citeseer) suffer from duplicates and feature-label information leaks. We fix these problems and offer refined datasets for fair comparisons. To further validate the effectiveness, we deploy the proposed DrGCNs on A* 1 company's recommendation system and clearly demonstrate performance improvements via offline evaluations. 2 DRGCNS: DIMENSIONAL REWEIGHTING GRAPH CONVOLUTIONAL NETWORKS We We investigate the originality of the Cora and CiteSeer dataset. The two datasets are widely used for being light-weighted and easy to handle. The most popular version is provided by Planetoid (Yang et al., 2016) . The two datasets are both citation networks where each node represents a research paper, and each edge represents a citation relationship between two papers. Edges are directed but are usually handled undirectedly by GCN methods. Each paper belongs to a sub-field in computer science and is marked as its label. Papers have features of bag-of-word(BOW) vectors that each dimension represents whether the document of the paper contains a particular word in the dictionary or not. Cora has 2,708 papers with a dictionary size of 1,433, while Citeseer has 3,327 papers with a dictionary size of 3,703. A.1 CORA Cora originates in (McCallum et al., 2000) 7 with extracted information(including titles, authors, abstracts, references, download links etc.) in plain-text form. Those download links are mostly unavailable now. Before they become unavailable, (Lu & Getoor, 2003) 8 extracts a subset of 2,708 papers and assigns labels and BOW feature vectors to the papers. The dictionary is chosen from words(after stemming) 9 that occur 10 or more times in all papers and result in a dictionary size of 1,433. Planetoid (Yang et al., 2016) reordered each node to form the benchmark Cora dataset ). There exist a lot of duplicated papers (one paper appears as multiple identical papers in the dataset) in the original Cora of (McCallum et al., 2000) , and (Lu & Getoor, 2003) inherits the problem of duplicated papers. In Cora, we find 32 duplicated papers among the 2,708 papers. Another problem is the information leak. The generation process of the dictionary chooses words that occur more than 10 times, and does not exclude the label contexts of papers. Therefore, some papers may be classified easily only by looking at their labels. For instance, 61.8% of papers labeled ""reinforcement learning"" contain exactly the word ""reinforcement"" ""learning"" in their title and abstract(after stemming). Altogether 1,145(42.3%) of these papers contain their label as one or some of the dimensions of their features.","We propose a simple yet effective reweighting scheme for GCNs, theoretically supported by the mean field theory.",two ; BOW ; Devlin et al. ; one ; Planetoid ; Scarselli et al. ; Kipf & Welling ; Hamilton ; GraphSAGE ; Cora,et al ; Each paper ; insights ; download links ; various fields ; graph convolutional network ; The most popular version ; the literature ; The two datasets ; Carefully designed experiments,two ; BOW ; Devlin et al. ; one ; Planetoid ; Scarselli et al. ; Kipf & Welling ; Hamilton ; GraphSAGE ; Cora,"Dimensional reweighting Graph Convolutional Networks (DrGCNs) aims to reduce the variance of node representations by connecting our problem to the theory of the mean field. However, the degrees DrGCNs help vary severely on different datasets. A new measure K is developed to quantify the effect. It provides insights to explain the improvement obtained by the proposed DrGCN. Deep neural networks (DNNs) have been widely used in various fields, including computer vision, natural language processing, speech recognition, and speech recognition. Graph neural networks are widely used for learning node presentations of networked data (GNNs), and",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"We propose a modification to traditional Artificial Neural Networks (ANNs), which provides the ANNs with new aptitudes motivated by biological neurons.   Biological neurons work far beyond linearly summing up synaptic inputs and then transforming the integrated information.   A biological neuron change firing modes accordingly to peripheral factors (e.g., neuromodulators) as well as intrinsic ones.   Our modification connects a new type of ANN nodes, which mimic the function of biological neuromodulators and are termed modulators, to enable other traditional ANN nodes to adjust their activation sensitivities in run-time based on their input patterns.   In this manner, we enable the slope of the activation function to be context dependent.   This modification produces statistically significant improvements in comparison with traditional ANN nodes in the context of Convolutional Neural Networks and Long Short-Term Memory networks. Artificial neural networks (ANNs), such as convolutional neural networks (CNNs) BID24 and long short-term memory (LSTM) cells BID14 , have incredible capabilities and are applied in a variety of applications including computer vision, natural language analysis, and speech recognition among others. Historically, the development of ANNs (e.g., network architectures and learning algorithms) has benefited significantly from collaborations with Psych-Neuro communities BID5 BID12 BID13 BID15 BID28 Turing, 1950; BID10 BID8 BID16 BID18 BID10 . The information processing capabilities of traditional ANN nodes are rather rigid when compared to the plasticity of real neurons. A typical traditional ANN node linearly integrate its input signals and run the integration through a transformation called an activation function, which simply takes in a scalar value and outputs another. Of the most popular Activation Functions are sigmoid BID32 , tanh BID19 and ReLU BID35 .Researchers have shown that it could be beneficial to deploy layer-/node-specific activation functions in a deep ANN BID4 BID40 BID9 BID11 BID0 . However, each ANN node is traditionally stuck with a fixed activation function once trained. Therefore, the same input integration will always produce the same output. This fails to replicate the amazing capability of individual biological neurons to conduct complex nonlinear mappings from inputs to outputs BID1 BID10 BID25 . In this study , we propose one new modification to ANN architectures by adding a new type of node, termed modulators, to modulate the activation sensitivity of the ANN nodes targeted by modulators (see FIG0 for examples). In one possible setting, a modulator and its target ANN nodes share the same inputs. The modulator maps the input into a modulation signal, which is fed into each target node. Each target node multiples its input integration by the modulator signal prior to transformation by its traditional activation function. Examples of neuronal principles that may be captured by our new modification include intrinsic excitability, diverse firing modes, type 1 and type 2 forms of firing rate integration, activity dependent facilitation and depression and, most notably, neuromodulation BID27 BID38 BID45 BID37 ).Our modulator is relevant to the attention mechanism BID23 BID34 , which dynamically restricts information pathways and has been found to be very useful in practice. Attention mechanisms apply the attention weights, which are calculated in run-time, to the outputs of ANN nodes or LSTM cells. Notably, the gating mechanism in a Simple LSTM cell can also be viewed as a dynamical information modifier. A gate takes the input of the LSTM cell and outputs gating signals for filtering the outputs of its target ANN nodes in the same LSTM cell. A similar gating mechanism was proposed in the Gated Linear Unit BID6 for CNNs. Different from the attention and gating mechanisms , which are applied to the outputs of the target nodes, our modulation mechanism adjusts the sensitivities of the target ANN nodes in run-time by changing the slopes of the corresponding activation functions. Hence, the modulator can also be used as a complement to the attention and gate mechanisms.Below we will explain our modulator mechanism in detail. Experimentation shows that the modulation mechanism can help achieve better test stability and higher test performance using easy to implement and significantly simpler models. Finally, we conclude the paper with discussions on the relevance to the properties of actual neurons. We propose a modulation mechanism addition to traditional ANNs so that the shape of the activation function can be context dependent. Experimental results show that the modulated models consistently outperform their original versions. Our experiment also implied adding modulator can reduce overfitting. We demonstrated even with fewer parameters, the modulated model can still perform on par with it vanilla version of a bigger size. This modulation idea can also be expanded to other setting, such as, different modulator activation or different structure inside the modulator. It was frequently observed in preliminary testing that arbitrarily increasing model parameters actually hurt network performance, so future studies will be aimed at investigating the relationship between the number of model parameters and the performance of the network. Additionally, it will be important to determine the interaction between specific network implementations and the ideal Activation Function wrapping for slope-determining neurons. Lastly, it may be useful to investigate layer-wide single-node modulation on models with parallel LSTM's.Epigenetics refers to the activation and inactivation of genes BID46 , often as a result of environmental factors. These changes in gene-expression result in modifications to the generation and regulation of cellular proteins, such as ion channels, that regulate how the cell controls the flow of current through the cell membrane BID29 . The modulation of these proteins will strongly influence the tendency of a neuron to fire and hence affect the neurons function as a single computational node. These proteins, in turn, can influence epigenetic expression in the form of dynamic control BID20 .Regarding the effects of these signals, we can compare the output of neurons and nodes from a variety of perspectives. First and foremost, intrinsic excitability refers to the ease with which a neurons electrical potential can increase, and this feature has been found to impact plasticity itself BID7 . From this view, the output of a node in an artificial neural network would correspond to a neurons firing rate, which Intrinsic Excitability is a large contributor to, and our extra gate would be setting the node's intrinsic excitability. With the analogy of firing rate, another phenomenon can be considered. Neurons may experience various modes of information integration, typically labeled Type 1 and Type 2. Type 1 refers to continuous firing rate integration, while Type 2 refers to discontinuous information BID42 . This is computationally explained as a function of interneuron communication resulting in neuron-activity nullclines with either heavy overlap or discontinuous saddle points BID33 . In biology , a neuron may switch between Type 1 and Type 2 depending on the presence of neuromodulator BID41 . Controlling the degree to which the tanh function encodes to a binary space, our modification may be conceived as determining the form of information integration. The final possible firing rate equivalence refers to the ability of real neurons to switch between different firing modes. While the common mode of firing, Tonic firing, generally encodes information in rate frequency, neurons in a Bursting mode (though there are many types of bursts) tend to encode information in a binary mode -either firing bursts or not BID42 . Here too, our modification encompasses a biological phenomenon by enabling the switch between binary and continuous information.Another analogy to an ANN nodes output would be the neurotransmitter released. With this view, our modification is best expressed as an analogy to Activity Dependent Facilitation and Depression, phenomena which cause neurons to release either more or less neurotransmitter. Facilitation and depression occur in response to the same input: past activity BID36 . Our modification enables a network to use previous activity to determine its current sensitivity to input, allowing for both Facilitation and Depression. On the topic of neurotransmitter release , neuromodulation is the most relevant topic to the previously shown experiments. Once again, BID25 explains the situation perfectly, expressing that research BID2 BID3 has shown ""the same neuron or circuit can exhibit different input-output responses depending on a global circuit state, as reflected by the concentrations of various neuromodulators"". Relating to our modification, the slope of the activation function may be conceptualized as the mechanism of neuromodulation, with the new gate acting analogously to a source of neuromodulator for all nodes in the network.Returning to a Machine Learning approach, the ability to adjust the slope of an Activation Function has an immediate benefit in making the back-propagation gradient dynamic. For example, for Activations near 0, where the tanh Function gradient is largest, the effect of our modification on node output is minimal. However, at this point, our modification has the ability to decrease the gradient, perhaps acting as pseudo-learning-rate. On the other hand, at activations near 1 and -1, where the tanh Function gradient reaches 0, our modification causes the gradient to reappear, allowing for information to be extracted from inputs outside of the standard range. Additionally, by implementing a slope that is conditional on node input, the node has the ability to generate a wide range of functional Activation Functions, including asymmetric functions. Lastly, injecting noise has been found to help deep neural networks with noisy datasets BID47 , which is noteworthy since noise may act as a stabilizer for neuronal firing rates, BID43 . With this in mind, TAB2 .2 demonstrates increased clustering in two-dimensional node-Activation space, when the Activation Function slope is made to be dynamic. This indicates that noise may be a mediator of our modification, improving network performance through stabilization, induced by increasing the variability of the input-output relationship.In summary, we have shown evidence that nodes in LSTMs and CNNs benefit from added complexity to their input-output dynamic. Specifically, having a node that adjusts the slope of the main layer's nodes' activation functions mimics the functionality of neuromodulators and is shown to benefit the network. The exact mechanism by which this modification improves network performance remains unknown, yet it is possible to support this approach from both a neuroscientific and machine-learning perspective. We believe this demonstrates the need for further research into discovering novel non-computationally-demanding methods of applying principles of neuroscience to artificial networks. 6 APPENDIX 6.1 SUPPLEMENTARY DATA METHODOLOGY Additionally we tested our modulator gate, with τ l (·) set to sigmoid, on a much more computationally demanding three-layered LSTM network with weight drop method named awd-lstm-lm BID30 BID31 . This model was equipped to handle the Penn-Treebank dataset BID26 and was trained to minimize word perplexity. The network was trained for 500 epochs, however, the sample size was limited due to extremely long training times. On the Penn-Treebank dataset with the awd-lstm-lm implementation , sample size was restricted to 2 per condition, due to long training times and limited resources. However on the data collected, our model outperformed template perplexity, achieving an average of 58.4730 compared to the template average 58.7115. Due to the lack of a control for model parameters, interpretation of these results rests on the assumption that the author fine-tuned network parameters such that the template parameters maximized performance.",We propose a modification to traditional Artificial Neural Networks motivated by the biology of neurons to enable the shape of the activation function to be context dependent.,SUPPLEMENTARY ; the Activation Function ; one ; Intrinsic Excitability ; Penn-Treebank ; Activation Function ; First ; Facilitation ; Tonic ; Convolutional Neural Networks,stabilization ; the new gate ; individual biological neurons ; neuromodulators ; information pathways ; a complement ; the same input ; the presence ; either more or less neurotransmitter ; examples,SUPPLEMENTARY ; the Activation Function ; one ; Intrinsic Excitability ; Penn-Treebank ; Activation Function ; First ; Facilitation ; Tonic ; Convolutional Neural Networks,"We propose a modification to traditional Artificial Neural Networks (ANNs), which provides the ANNs with new aptitudes motivated by biological neurons.  Biological neurons work far beyond linearly summing up synaptic inputs and then transforming the integrated information.   In addition, biological neurons change firing modes accordingly to peripheral factors, such as neuromodulators, intrinsic ones, and intrinsic ones. This modification enables other traditional ANN nodes to adjust their activation sensitivities in run-time based on input patterns. ",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Small spacecraft now have precise attitude control systems available commercially, allowing them to slew in 3 degrees of freedom, and capture images within short notice. When combined with appropriate software, this agility can significantly increase response rate, revisit time and coverage. In prior work, we have demonstrated an algorithmic framework that combines orbital mechanics, attitude control and scheduling optimization to plan the time-varying, full-body orientation of agile, small spacecraft in a constellation. The proposed schedule optimization would run at the ground station autonomously, and the resultant schedules uplinked to the spacecraft for execution. The algorithm is generalizable over small steerable spacecraft, control capability, sensor specs, imaging requirements, and regions of interest. In this article, we modify the algorithm to run onboard small spacecraft, such that the constellation can make time-sensitive decisions to slew and capture images autonomously, without ground control. We have developed a communication module based on Delay/Disruption Tolerant Networking (DTN) for onboard data management and routing among the satellites, which will work in conjunction with the other modules to optimize the schedule of agile communication and steering. We then apply this preliminary framework on representative constellations to simulate targeted measurements of episodic precipitation events and subsequent urban floods. The command and control efficiency of our agile algorithm is compared to non-agile (11.3x improvement) and non-DTN (21% improvement) constellations. Response and revisit requirements for Earth Observation (EO) vary significantly by application, ranging from less than an hour to monitor disasters, to daily for meteorology, to weekly for land cover monitoring BID33 . Geostationary satellites provide frequent revisits, but at the cost of coarse spatial resolution, extra launch costs and no polar access. Lower Earth Orbit satellites overcome these shortcomings, but need numbers and coordination to make up for response characteristics. Adding agility to satellites and autonomy to the constellation improves the revisit/response for the same number of satellites Copyright © 2019. All rights reserved. in given orbits. However, human operators are expected to scale linearly with constellation nodes BID9 ) and operations staffing may be very costly.","We propose an algorithmic framework to schedule constellations of small spacecraft with 3-DOF re-orientation capabilities, networked with inter-sat links.",Delay/Disruption Tolerant Networking ; Earth ; EO ; less than an hour ; daily ; weekly ; Lower Earth Orbit ; Copyright,the same number ; numbers ; an algorithmic framework ; given orbits ; the ground station ; control capability ; the cost ; Response and revisit requirements ; frequent revisits ; control,Delay/Disruption Tolerant Networking ; Earth ; EO ; less than an hour ; daily ; weekly ; Lower Earth Orbit ; Copyright,"Small spacecraft now have precise attitude control systems, enabling them to slew in 3 degrees of freedom, capture images within short notice. This agility can significantly increase response rate, revisit time and coverage. In prior work, we developed an algorithmic framework that combines orbital mechanics, attitude control and scheduling optimization to plan the time-varying, full-body orientation of agile, small spacecraft in a constellation. The proposed schedule optimization is generalizable over small steerable spacecraft, control capability, sensor specs, imaging requirements, and regions of interest. In this article, we modify the algorithm to run onboard small spacecraft, such that the constellation can",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
"Neural machine translation (NMT) models learn representations containing substantial linguistic information. However, it is not clear if such information is fully distributed or if some of it can be attributed to individual neurons. We develop unsupervised methods for discovering important neurons in NMT models. Our methods rely on the intuition that different models learn similar properties, and do not require any costly external supervision. We show experimentally that translation quality depends on the discovered neurons, and find that many of them capture common linguistic phenomena. Finally, we show how to control NMT translations in predictable ways, by modifying activations of individual neurons.","Unsupervised methods for finding, analyzing, and controlling important neurons in NMT",NMT,individual neurons ; some ; it ; any costly external supervision ; translation quality ; different models ; them ; unsupervised methods ; similar properties ; predictable ways,NMT,"Neural machine translation (NMT) models learn representations containing substantial linguistic information. However, it is unclear if such information is fully distributed or if some of it can be attributed to individual neurons. We develop unsupervised methods for discovering important neurons in NMT models. These methods rely on the intuition that different models learn similar properties, and do not require external supervision. We show how to control NMT translations in predictable ways, by modifying activations of individual neurons",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_noKW,none
