Text,Abstractive,topics_kw,topics_kwplus,topics_prefix,prediction,model_dir,kw_variant
"Deep learning methods have achieved high performance in sound recognition tasks. Deciding how to feed the training data is important for further performance improvement. We propose a novel learning method for deep sound recognition: Between-Class learning (BC learning). Our strategy is to learn a discriminative feature space by recognizing the between-class sounds as between-class sounds. We generate between-class sounds by mixing two sounds belonging to different classes with a random ratio. We then input the mixed sound to the model and train the model to output the mixing ratio. The advantages of BC learning are not limited only to the increase in variation of the training data; BC learning leads to an enlargement of Fisher’s criterion in the feature space and a regularization of the positional relationship among the feature distributions of the classes. The experimental results show that BC learning improves the performance on various sound recognition networks, datasets, and data augmentation schemes, in which BC learning proves to be always beneficial. Furthermore, we construct a new deep sound recognition network (EnvNet-v2) and train it with BC learning. As a result, we achieved a performance surpasses the human level. Sound recognition has been conventionally conducted by applying classifiers such as SVM to local features such as MFCC or log-mel features BID13 BID27 BID14 . Convolutional neural networks (CNNs) BID12 , which have achieved success in image recognition tasks BID11 BID22 BID8 , have recently proven to be effective in tasks related to series data, such as speech recognition BID0 BID18 BID4 and natural language processing BID10 . Some researchers applied CNNs to sound recognition tasks and achieved high performance BID2 BID3 BID25 .The amount and quality of training data and how to feed it are important for machine learning, particularly for deep learning. Various approaches have been proposed to improve the sound recognition performance. The first approach is to efficiently use limited training data with data augmentation. Researchers proposed increasing the training data variation by altering the shape or property of sounds or adding a background noise BID25 BID20 . Researchers also proposed using additional training data created by mixing multiple training examples BID15 BID24 . The second approach is to use external data or knowledge. BID2 proposed learning rich sound representations using a large amount of unlabeled video datasets and pre-trained image recognition networks. The sound dataset expansion was also conducted BID21 BID17 BID6 .In this paper, as a novel third approach we propose a learning method for deep sound recognition: Between-Class learning (BC learning). Our strategy is to learn a discriminative feature space by recognizing the between-class sounds as between-class sounds. We generate between-class sounds by mixing two sounds belonging to different classes with a random ratio. We then input the mixed sound to the model and train the network to output the mixing ratio. Our method focuses on the characteristic of the sound, from which we can generate a new sound simply by adding the waveform data of two sounds. The advantages of BC learning are not limited only to the increase in variation of the training data; BC learning leads to an enlargement of Fisher's criterion BID5 (i.e., the ratio of the between-class distance to the within-class variance) in the feature space, and a regularization of the positional relationship among the feature distributions of the classes.The experimental results show that BC learning improves the performance on various sound recognition networks, datasets, and data augmentation schemes, in which BC learning proves to be always beneficial. Furthermore, we constructed a new deep sound recognition network (EnvNet-v2) and trained it with BC learning. As a result, we achieved a 15.1% error rate on a benchmark dataset ESC-50 (Piczak, 2015b), which surpasses the human level.We argue that our BC learning is different from the so-called data augmentation methods we introduced above. Although BC learning can be regarded as a data augmentation method from the viewpoint of using augmented data, the novelty or key point of our method is not mixing multiple sounds, but rather learning method of training the model to output the mixing ratio. This is a fundamentally different idea from previous data augmentation methods. In general, data augmentation methods aim to improve the generalization ability by generating additional training data which is likely to appear in testing phase. Thus, the problem to be solved is the same in both training and testing phase. On the other hand, BC learning uses only mixed data and labels for training, while mixed data does not appear in testing phase. BC learning is a method to improve the classification performance by solving a problem of predicting the mixing ratio between two different classes. To the best of our knowledge, this is the first time a learning method that employs a mixing ratio between different classes has been proposed. We intuitively describe why such a learning method is effective and demonstrate the effectiveness of BC learning through wide-ranging experiments. We proposed a novel learning method for deep sound recognition, called BC learning. Our method improved the performance on various networks, datasets, and data augmentation schemes. Moreover, we achieved a performance surpasses the human level by constructing a deeper network named EnvNet-v2 and training it with BC learning. BC learning is a simple and powerful method that improves various sound recognition methods and elicits the true value of large-scale networks. Furthermore, BC learning is innovative in that a discriminative feature space can be learned from betweenclass examples, without inputting pure examples. We assume that the core idea of BC learning is generic and could contribute to the improvement of the performance of tasks of other modalities.",We propose an novel learning method for deep sound recognition named BC learning.,BC ; two ; Fisher ; SVM ; MFCC ; first ; second ; third,quality ; a learning method ; class ; The first approach ; the other hand ; natural language ; the human level ; second ; an enlargement ; the feature distributions,BC ; two ; Fisher ; SVM ; MFCC ; first ; second ; third,"Deep learning methods have achieved high performance in sound recognition tasks, but determining how to feed the training data is crucial for further performance improvement. A novel learning method for deep sound recognition is Between-Class learning (BC learning), which involves recognizing between-class sounds and mixing them with a random ratio. The advantages of BC learning are not limited to the increase in variation of training data but also to the regularization of feature distributions. BC learning leads to an enlargement of Fisher’s criterion in the feature space and regularization among feature distributions of the classes. The experimental results show that BC learning improves the performance on various sound recognition",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"To analyze deep ReLU network, we adopt a student-teacher setting in which an over-parameterized student network learns from the output of a fixed teacher network of the same depth, with Stochastic Gradient Descent (SGD). Our contributions are two-fold. First, we prove that when the gradient is zero (or bounded above by a small constant) at every data point in training, a situation called  \emph{interpolation setting}, there exists many-to-one \emph{alignment} between student and teacher nodes in the lowest layer under mild conditions. This suggests that generalization in unseen dataset is achievable, even the same condition often leads to zero training error. Second, analysis of noisy recovery and training dynamics in 2-layer network shows that strong teacher nodes (with large fan-out weights) are learned first and subtle teacher nodes are left unlearned until late stage of training. As a result, it could take a long time to converge into these small-gradient critical points. Our analysis shows that over-parameterization plays two roles: (1) it is a necessary condition for alignment to happen at the critical points, and (2) in training dynamics, it helps student nodes cover more teacher nodes with fewer iterations. Both improve generalization. Experiments justify our finding. Deep Learning has achieved great success in the recent years (Silver et al., 2016; He et al., 2016; Devlin et al., 2018) . Although networks with even one-hidden layer can fit any function (Hornik et al., 1989) , it remains an open question how such networks can generalize to new data. Different from what traditional machine learning theory predicts, empirical evidence (Zhang et al., 2017) shows more parameters in neural network lead to better generalization. How over-parameterization yields strong generalization is an important question for understanding how deep learning works. In this paper, we analyze deep ReLU networks with teacher-student setting: a fixed teacher network provides the output for a student to learn via SGD. Both teacher and student are deep ReLU networks. Similar to (Goldt et al., 2019) , the student is over-realized compared to the teacher: at each layer l, the number n l of student nodes is larger than the number m l of teacher (n l > m l ). Although over-realization is different from over-parameterization, i.e., the total number of parameters in the student model is larger than the training set size N , over-realization directly correlates with the width of networks and is a measure of over-parameterization. The student-teacher setting has a long history (Saad & Solla, 1996; 1995; Freeman & Saad, 1997; Mace & Coolen, 1998) and recently gains increasing interest (Goldt et al., 2019; Aubin et al., 2018) in analyzing 2-layered network. While worst-case performance on arbitrary data distributions may not be a good model for real structured dataset and can be hard to analyze, using a teacher network implicitly enforces an inductive bias and could potentially lead to better generalization bound. Specialization, that is, a student node becomes increasingly correlated with a teacher node during training (Saad & Solla, 1996) , is one of the important topic in this setup. If all student nodes are specialized to the teacher, then student tends to output the same as the teacher and generalization performance can be expected. Empirically, it has been observed in 2-layer networks (Saad & Solla, 1996; Goldt et al., 2019) and multi-layer networks (Tian et al., 2019; Li et al., 2016) , in both synthetic and real dataset. In contrast, theoretical analysis is limited with strong assumptions (e.g., Gaussian inputs, infinite input dimension, local convergence, 2-layer setting, small number of hidden nodes). In this paper, with arbitrary training distribution and finite input dimension, we show rigorously that when gradient at each training sample is small (i.e., the interpolation setting as suggested in (Ma In this paper, we use student-teacher setting to analyze how an (over-parameterized) deep ReLU student network trained with SGD learns from the output of a teacher. When the magnitude of gradient per sample is small (student weights are near the critical points), the teacher can be proven to be covered by (possibly multiple) students and thus the teacher network is recovered in the lowest layer. By analyzing training dynamics, we also show that strong teacher node with large v * is reconstructed first, while weak teacher node is reconstructed slowly. This reveals one important reason why the training takes long to reconstruct all teacher weights and why generalization improves with more training. As the next step, we would like to extend our analysis to finite sample case, and analyze the training dynamics in a more formal way. Verifying the insights from theoretical analysis on a large dataset (e.g., ImageNet) is also the next step. Figure 8: Mean of the max teacher correlation ρmean with student nodes over epochs in CIFAR10. More over-realization gives better student specialization across all layers and achieves strong generalization (higher evaluation accuracy on CIFAR-10 evaluation set).",This paper analyzes training dynamics and critical points of training deep ReLU network via SGD in the teacher-student setting.,Devlin et al. ; two-fold ; ReLU ; Aubin ; First ; two ; ImageNet ; one ; zero ; Goldt,unseen dataset ; sample case ; an over-parameterized student network ; Tian ; two roles ; -parameterization ; Second ; Our contributions ; Solla ; (possibly multiple) students,Devlin et al. ; two-fold ; ReLU ; Aubin ; First ; two ; ImageNet ; one ; zero ; Goldt,"We use a student-teacher setting in which an over-parameterized student network learns from the output of a fixed teacher network of the same depth, with Stochastic Gradient Descent (SGD). Our contributions are two-fold. First, we prove that when the gradient is zero at every data point in training, there exists many-to-one \emph{alignment} between student and teacher nodes in the lowest layer under mild conditions. This suggests that generalization in unseen dataset is achievable, even the same condition often leads to zero training error. Second, analysis of noisy recovery and training dynamics",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Typical recent neural network designs are primarily convolutional layers, but the tricks enabling structured efficient linear layers (SELLs) have not yet been adapted to the convolutional setting. We present a method to express the weight tensor in a convolutional layer using diagonal matrices, discrete cosine transforms (DCTs) and permutations that can be optimised using standard stochastic gradient methods. A network composed of such structured efficient convolutional layers (SECL) outperforms existing low-rank networks and demonstrates competitive computational efficiency.",It's possible to substitute the weight matrix in a convolutional layer to train it as a structured efficient layer; performing as well as low-rank decomposition.,linear,the tricks ; a convolutional layer ; diagonal matrices ; Typical recent neural network designs ; We ; standard stochastic gradient methods ; linear ; structured efficient linear layers ; SELLs ; permutations,linear,"Structured efficient linear layers (SELLs) have not yet been adapted to the convolutional setting. We present a method to express the weight tensor using diagonal matrices, discrete cosine transforms (DCTs) and permutations that can be optimised using standard stochastic gradient methods. This network outperforms existing low-rank networks and demonstrates competitive computational efficiency.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"It is important to collect credible training samples $(x,y)$ for building data-intensive learning systems (e.g., a deep learning system). In the literature, there is a line of studies on eliciting distributional information from self-interested agents who hold a relevant information.   Asking people to report complex distribution $p(x)$, though theoretically viable, is challenging in practice. This is primarily due to the heavy cognitive loads required for human agents to reason and report this high dimensional information. Consider the example where we are interested in building an image classifier via first collecting a certain category of  high-dimensional image data. While classical elicitation results apply to eliciting a complex and generative (and continuous) distribution $p(x)$ for this image data, we are interested in eliciting samples $x_i \sim p(x)$ from agents. This paper introduces a deep learning aided method to incentivize credible sample contributions from selfish and rational agents. The challenge to do so is to design an incentive-compatible score function to score each reported sample to induce truthful reports, instead of an arbitrary or even adversarial one. We show that with accurate estimation of a certain $f$-divergence function we are able to achieve approximate incentive compatibility in eliciting truthful samples. We then present an efficient estimator with theoretical guarantee via studying the variational forms of $f$-divergence function. Our work complements the literature of information elicitation via introducing the problem of \emph{sample elicitation}.  We also show a connection between this sample elicitation problem and $f$-GAN, and how this connection can help reconstruct an estimator of the distribution based on collected samples. The availability of a large quantity of credible samples is crucial for building high-fidelity machine learning models. This is particularly true for deep learning systems that are data-hungry. Arguably, the most scalable way to collect a large amount of training samples is to crowdsource from a decentralized population of agents who hold relevant sample information. The most popular example is the build of ImageNet (Deng et al., 2009 ). The main challenge in eliciting private information is to properly score reported information such that the self-interested agent who holds a private information will be incentivized to report truthfully. At a first look, this problem of eliciting quality data is readily solvable with the seminal solution for eliciting distributional information, called the strictly proper scoring rule (Brier, 1950; Winkler, 1969; Savage, 1971; Matheson & Winkler, 1976; Jose et al., 2006; Gneiting & Raftery, 2007) : suppose we are interested in eliciting information about a random vector X = (X 1 , ..., X d−1 , Y ) ∈ Ω ⊆ R d , whose probability density function is denoted by p with distribution P. As the mechanism designer, if we have a sample x drawn from the true distribution P, we can apply strictly proper scoring rules to elicit p: the agent who holds p will be scored using S (p, x) . S is called strictly proper if it holds for any p and q that E x∼P [S(p, x) ] > E x∼P [S(q, x) ]. The above elicitation approach has two main caveats that limited its application: • When the outcome space |Ω| is large and is even possibly infinite, it is practically impossible for any human agents to report such a distribution with reasonable efforts. This partially inspired a line of follow-up works on eliciting property of the distributions, which we will discuss later. In this work we aim to collect credible samples from self-interested agents via studying the problem of sample elicitation. Instead of asking each agent to report the entire distribution p, we hope to elicit samples drawn from the distribution P truthfully. We consider the samples x p ∼ P and x q ∼ Q. In analogy to strictly proper scoring rules 1 , we aim to design a score function S s.t. for any q ̸ = p, where x ′ is a reference answer that can be defined using elicited reports. Often, this scoring procedure requires reports from multiple peer agents, and x ′ is chosen as a function of the reported samples from all other agents (e.g., the average across all the reported xs, or a randomly selected x). This setting will relax the requirements of high reporting complexity, and has wide applications in collecting training samples for machine learning tasks. Indeed our goal resembles similarity to property elicitation (Lambert et al., 2008; Steinwart et al., 2014; Frongillo & Kash, 2015b ), but we emphasize that our aims are different -property elicitation aims to elicit statistical properties of a distribution, while ours focus on eliciting samples drawn from the distributions. In certain scenarios, when agents do not have the complete knowledge or power to compute these properties, our setting enables elicitation of individual sample points. Our challenge lies in accurately evaluating reported samples. We first observe that the f -divergence function between two properly defined distributions of the samples can serve the purpose of incentivizing truthful report of samples. We proceed with using deep learning techniques to solve the score function design problem via a data-driven approach. We then propose a variational approach that enables us to estimate the divergence function efficiently using reported samples, via a variational form of the f -divergence function, through a deep neutral network. These estimation results help us establish an approximate incentive compatibility in eliciting truthful samples. It is worth to note that our framework also generalizes to the setting where there is no access to ground truth samples, where we can only rely on reported samples. There we show that our estimation results admit an approximate Bayesian Nash Equilibrium for agents to report truthfully. Furthermore, in our estimation framework, we use a generative adversarial approach to reconstruct the distribution from the elicited samples. We want to emphasize that the deep learning based estimators considered above are able to handle complex data. And with our deep learning solution, we are further able to provide estimates for the divergence functions used for our scoring mechanisms with provable finite sample complexity. In this paper, we focus on developing theoretical guarantees -other parametric families either can not handle complex data, e.g., it is hard to handle images using kernel methods, or do not have provable guarantees on the sample complexity. Our contributions are three-folds. (1) We tackle the problem of eliciting complex distribution via proposing a sample elicitation framework. Our deep learning aided solution concept makes it practical to solicit complex sample information from human agents. (2) Our framework covers the case when the mechanism designer has no access to ground truth information, which adds contribution to the peer prediction literature. (3) On the technical side, we develop estimators via deep learning techniques with strong theoretical guarantees. This not only helps us establish approximate incentive-compatibility, but also enables the designer to recover the targeted distribution from elicited samples. Our contribution can therefore be summarized as ""eliciting credible training samples by deep learning, for deep learning"".",This paper proposes a deep learning aided method to elicit credible samples from self-interested agents.,p(x)$ ; Gneiting & Raftery ; two ; Winkler ; q ∼ Q. ; first ; Deng et al. ; $x_i ; Lambert et al. ; Matheson & Winkler,ours ; people ; the agent ; an approximate incentive compatibility ; truthful report ; the heavy cognitive loads ; Savage ; similarity ; an efficient estimator ; samples,p(x)$ ; Gneiting & Raftery ; two ; Winkler ; q ∼ Q. ; first ; Deng et al. ; $x_i ; Lambert et al. ; Matheson & Winkler,"Credible training samples are crucial for building data-intensive learning systems. There is a line of studies on eliciting distributional information from self-interested agents who hold relevant information. However, asking people to report complex distribution $p(x)$, is challenging in practice due to the heavy cognitive loads required for human agents to reason and report this high dimensional information. This paper introduces a deep learning aided method to incentivize credible sample contributions from selfish and rational agents. The challenge is to design an incentive-compatible score function to score each reported sample to induce truthful reports instead of arbitrary or adversarial ones. This approach complements",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Due to the success of residual networks (resnets) and related architectures, shortcut connections have quickly become standard tools for building convolutional neural networks. The explanations in the literature for the apparent effectiveness of shortcuts are varied and often contradictory. We hypothesize that shortcuts work primarily because they act as linear counterparts to nonlinear layers. We test this hypothesis by using several variations on the standard residual block, with different types of linear connections, to build small (100k--1.2M parameter) image classification networks. Our experiments show that other kinds of linear connections can be even more effective than the identity shortcuts. Our results also suggest that the best type of linear connection for a given application may depend on both network width and depth. Deep convolutional neural networks have become the dominant force for many image classification tasks; see BID10 ; BID17 ; BID19 . Their ability to assimilate low-, medium-, and high-level features in an end-to-end multi-layer fashion has led to myriad groundbreaking advances in the field. In recent years, residual networks (resnets) have emerged as one of the best performing neural network archetypes in the literature; see BID6 . Through the use of identity shortcut connections, resnets have overcome the challenging technical obstacles of vanishing gradients and the apparent degradation that otherwise comes with training very deep networks. Resnets have achieved state-of-the-art performance on several image classification datasets using very deep neural networks, sometimes with over 1000 layers.Although shortcut connections appeared in the early neural network literature, e.g., BID1 ; BID14 ; BID16 , their importance became more clear in 2015 with the emergence of the HighwayNets of BID18 and resnets. The former involved gated shortcut connections that regulate the flow of information across the network, while the latter used identity shortcut connections, which are parameterless. Resnets are also presumed to be easier to train and seem to perform better in practice. In their first resnet paper, He et al. argued that identity maps let gradients flow back, enabling the training of very deep networks, and that it's easier for a layer to learn when initialized near an identity map than near a zero map (with small random weights); see also .However , in a flurry of recent activity, most notably from BID25 ; BID4 ; BID22 ; BID13 and BID23 , arguments have emerged that the effectiveness of resnets is not due to their depth, where practitioners were training networks of hundreds or thousands of layers, but rather that deep resnets are effectively creating ensembles of shallower networks, and the layers are more likely to refine and reinforce existing features than engineer new ones. These arguments assert that the achievement of resnets is less about extreme depth and more about their ability to ease backpropagation with moderate depth. Indeed , in many cases wider residual networks that were only 10-50 layers deep were shown to perform better and train in less time than very deep ones (over 100 layers). See BID25 .More recently still, others have presented many clever and creative ways to train very deep networks using variations on the shortcut theme; see for example BID8 ; BID11 ; BID26 ; ; BID0 ; BID2 ; BID27 ; BID6 ; BID12 ; BID24 ; Savarese (2016); BID19 , and BID21 . In summary, shortcut connections clearly help in practice, but there are many different, and sometimes conflicting hypotheses as to why.In this paper we investigate a new hypothesis about shortcut connections, namely, that their power lies not in the identity mapping itself, but rather just in combining linear and nonlinear functions at each layer. The tests where identity shortcuts were observed to perform better than general linear connections were all done in very deep (100 or more layers) networks. The recent evidence that wider, shallower, resnet networks can outperform deeper ones suggests that it is worth investigating whether identity connections are better than general linear connections in such networks.We first describe some of the intuition about why this might be the case. We then investigate this idea with careful experiments using relatively small networks constructed of five different types of blocks. These blocks are all variations on the idea of residual blocks (resblocks), but where the identity shortcut is replaced with a more general linear function. We call these blocks, consisting of both a linear and a nonlinear part, tandem blocks and the resulting networks tandem networks. Residual networks and several similar architectures are special cases of tandem networks.The networks we use in our experiments are relatively small (100k-1.2M parameter) image classification networks constructed from these various tandem blocks. The small networks are appropriate because the goal of the experiments is not to challenge state-of-the-art results produced by much larger models, but rather to compare the five architectures in a variety of settings in order to gain insight into their relative strengths and weaknesses. Whereas many other authors pursue extreme network depth as a goal in itself, here we limit our focus to comparing performance (in this case, classification accuracy) of different architectures.Our experiments suggest that general linear layers, which have learnable parameters, perform at least as well as the identity shortcut of resnets. This is true even when some width is sacrificed to keep the total number of parameters the same. Our results further suggest that the best specific type of linear connection to use in the blocks of a tandem network depends on several factors, including both network width and depth. We generalized residual blocks (which use identity shortcut connections) to tandem blocks (which can learn any linear connection, not just the identity). We found that general linear connections with learnable weights, have the same benefits as the identity maps in residual blocks, and they actually increase performance compared to identity maps. We also showed that linear connections do not learn identity maps, even when initialized with identity weight matrices. These results seem to confirm that the success of residual networks and related architectures is not due to special properties of identity maps, but rather is simply a result of using linear maps to complement nonlinear ones.The additional flexibility gained by replacing identity maps with convolutions led to better results in every one of our experiments. This was not due to extra parameters, as we adjusted layer widths to keep parameter counts as close to equal as possible. Instead, general linear convolutions appear to do a better job than identity maps of working together with nonlinear convolutions.Our results further suggest that tandem blocks with a single nonlinear convolution tend to outperform those with two, but blocks that use 3 × 3 convolutions for their linear connections may be better in wide networks than those with 1 × 1s.Finally, we note that there are many more possible types of tandem block than those we have considered here, and many more applications in which to test them.","We generalize residual blocks to tandem blocks, which use arbitrary linear maps instead of shortcuts, and improve performance over ResNets.",linear ; recent years ; first ; al. ; zero ; hundreds or thousands ; Savarese ; five ; two,performance ; other kinds ; their power ; The additional flexibility ; just the identity ; a given application ; Savarese ; a variety ; the HighwayNets ; the best performing neural network archetypes,linear ; recent years ; first ; al. ; zero ; hundreds or thousands ; Savarese ; five ; two,"The success of residual networks (resnets) and related architectures has quickly become standard tools for building convolutional neural networks. However, the explanations in the literature for the apparent effectiveness of shortcuts are varied and contradictory. We test this hypothesis by using several variations on the standard residual block, with different types of linear connections, to build small (100k--1.2M parameter) image classification networks. Our results suggest that linear connections are more effective than identity shortcuts. The best type of linear connection for a given application may depend on both network width and depth, as well as both network depth and depth. In recent years,",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Summarization of long sequences into a concise statement is a core problem in natural language processing, requiring non-trivial understanding of the input. Based on the promising results of graph neural networks on highly structured data, we develop a framework to extend existing sequence encoders with a graph component that can reason about long-distance relationships in weakly structured data such as text. In an extensive evaluation, we show that the resulting hybrid sequence-graph models outperform both pure sequence models as well as pure graph models on a range of summarization tasks. Summarization, the task of condensing a large and complex input into a smaller representation that retains the core semantics of the input, is a classical task for natural language processing systems. Automatic summarization requires a machine learning component to identify important entities and relationships between them, while ignoring redundancies and common concepts.Current approaches to summarization are based on the sequence-to-sequence paradigm over the words of some text, with a sequence encoder -typically a recurrent neural network, but sometimes a 1D-CNN BID34 or using self-attention BID32 -processing the input and a sequence decoder generating the output. Recent successful implementations of this paradigm have substantially improved performance by focusing on the decoder, extending it with an attention mechanism over the input sequence and copying facilities BID32 . However, while standard encoders (e.g. bidirectional LSTMs) theoretically have the ability to handle arbitrary long-distance relationships, in practice they often fail to correctly handle long texts and are easily distracted by simple noise BID19 .In this work, we focus on an improvement of sequence encoders that is compatible with a wide range of decoder choices. To mitigate the long-distance relationship problem, we draw inspiration from recent work on highly-structured objects BID23 BID20 BID14 BID12 . In this line of work, highly-structured data such as entity relationships, molecules and programs is modelled using graphs. Graph neural networks are then successfully applied to directly learn from these graph representations. Here, we propose to extend this idea to weakly-structured data such as natural language. Using existing tools, we can annotate (accepting some noise) such data with additional relationships (e.g. co-references) to obtain a graph. However , the sequential aspect of the input data is still rich in meaning, and thus we propose a hybrid model in which a standard sequence encoder generates rich input for a graph neural network. In our experiments, the resulting combination outperforms baselines that use pure sequence or pure graph-based representations.Briefly, the contributions of our work are: 1. A framework that extends standard sequence encoder models with a graph component that leverages additional structure in sequence data. 2. Application of this extension to a range of existing sequence models and an extensive evaluation on three summarization tasks from the literature. 3. We release all used code and data at https://github.com/CoderPat/structured-neural-summarization. add a parameter to this dynamic parameter list BILSTM → LSTM: adds a new parameter to the specified parameter BILSTM+GNN → LSTM:creates a new instance of the dynamic type specified BILSTM+GNN → LSTM+POINTER: add a parameter to a list of parameters Figure 1 : An example from the dataset for the METHODDOC source code summarization task along with the outputs of a baseline and our models. In the METHODNAMING dataset, this method appears as a sample requiring to predict the name Add as a subtoken sequence of length 1. We presented a framework for extending sequence encoders with a graph component that can leverage rich additional structure. In an evaluation on three different summarization tasks, we have shown that this augmentation improves the performance of a range of different sequence models across all tasks. We are excited about this initial progress and look forward to deeper integration of mixed sequence-graph modeling in a wide range of tasks across both formal and natural languages. The key insight, which we believe to be widely applicable, is that inductive biases induced by explicit relationship modeling are a simple way to boost the practical performance of existing deep learning systems. We use the datasets and splits of BID4 provided by their website. Upon scanning all methods in the dataset, the size of the corpora can be seen in Table 4 . More information can be found at BID4 .",One simple trick to improve sequence models: Compose them with a graph model,three ; BILSTM+GNN → ; BILSTM+GNN → LSTM+POINTER ; METHODNAMING ; Add,such data ; long-distance relationships ; long sequences ; existing deep learning systems ; a large and complex input ; arbitrary long-distance relationships ; standard encoders ; a recurrent neural network ; Automatic summarization ; the literature,three ; BILSTM+GNN → ; BILSTM+GNN → LSTM+POINTER ; METHODNAMING ; Add,"Natural language processing requires non-trivial understanding of input. To mitigate the long-distance relationship problem, graph neural networks on highly structured data develop a framework to extend existing sequence encoders with a graph component that can reason about weakly structured data such as text. The resulting hybrid sequence-graph models outperform both pure sequence models as well as pure graph models on a range of summarization tasks. Summarization, the task of condensing a large and complex input into a smaller representation that retains the core semantics of the input, is a classical task for natural language processing systems. Automatic summarization requires a machine learning",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against a well-defined class of adversaries. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest robustness against a first-order adversary as a natural security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Recent breakthroughs in computer vision and speech recognition are bringing trained classifiers into the center of security-critical systems. Important examples include vision for autonomous cars, face recognition, and malware detection. These developments make security aspects of machine learning increasingly important. In particular, resistance to adversarially chosen inputs is becoming a crucial design goal. While trained models tend to be very effective in classifying benign inputs, recent work BID6 BID24 BID8 BID16 BID22 shows that an adversary is often able to manipulate the input so that the model produces an incorrect output.This phenomenon has received particular attention in the context of deep neural networks, and there is now a quickly growing body of work on this topic BID7 BID13 BID20 BID25 BID23 BID27 . Computer vision presents a particularly striking challenge: very small changes to the input image can fool state-of-the-art neural networks with high probability BID24 BID8 BID16 BID22 BID15 . This holds even when the benign example was classified correctly, and the change is imperceptible to a human. Apart from the security implications, this phenomenon also demonstrates that our current models are not learning the underlying concepts in a robust manner. All these findings raise a fundamental question:How can we learn models robust to adversarial inputs?There are now many proposed defense mechanisms for the adversarial setting. Examples include defensive distillation BID18 , feature squeezing BID31 , and several detection approaches for adversarial inputs (see BID4 for references). While these works constitute important first steps in exploring the realm of possibilities, they do not offer a good understanding of the guarantees they provide. We can never be certain that a particular defense mechanism prevents the existence of some well-defined class of adversarial attacks. This makes it difficult to navigate the landscape of adversarial robustness or to fully evaluate the possible security implications. Moreover, subsequent work BID2 BID11 has shown that most of these defenses can be bypassed by stronger, adaptive adversaries.In this paper, we study the adversarial robustness of neural networks through the lens of robust optimization. We use a natural saddle point (min-max) formulation to capture the notion of security against adversarial attacks in a principled manner. This formulation allows us to be precise about the type of security guarantee we would like to achieve, i.e., the broad class of attacks we want to be resistant to (in contrast to defending only against specific known attacks). The formulation also enables us to cast both attacks and defenses into a common theoretical framework. Most prior work on adversarial examples naturally fits into this framework. In particular, adversarial training directly corresponds to optimizing this saddle point problem. Similarly, prior methods for attacking neural networks correspond to specific algorithms for solving the underlying optimization problem.Equipped with this perspective, we make the following contributions.1. We conduct a careful experimental study of the optimization landscape corresponding to this saddle point formulation. Despite the non-convexity and non-concavity of its constituent parts, we find that the underlying optimization problem is tractable after all. In particular, we provide strong evidence that first-order methods can reliably solve this problem and motivate projected gradient descent (PGD) as a universal ""first-order adversary"", i.e., the strongest attack utilizing the local first order information about the network. We supplement these insights with ideas from real analysis to further motivate adversarial training against a PGD adversary as a strong and natural defense.2. We explore the impact of network architecture on adversarial robustness and find that model capacity plays an important role. To reliably withstand strong adversarial attacks , networks require a significantly larger capacity than for correctly classifying benign examples only. This shows that a robust decision boundary of the saddle point problem can be significantly more complicated than a decision boundary that simply separates the benign data points.3. Building on the above insights, we train networks on MNIST and CIFAR10 that are robust to a wide range of adversarial attacks against adversaries bounded by 0.3 and 8 in ∞ norm respectively. Our approach is based on optimizing the aforementioned saddle point formulation and uses our optimal ""first-order adversary"". Our best MNIST model achieves an accuracy of more than 89% against the strongest adversaries in our test suite. In particular, our MNIST network is even robust against white box attacks of an iterative adversary. Our CIFAR10 model achieves an accuracy of 46% against the same adversary. Furthermore, in case of the weaker black box (transfer) attacks , our MNIST and CIFAR10 networks achieve an accuracy of more than 95% and 64%, respectively (a more detailed overview can be found in TAB0 ). To the best of our knowledge, we are the first to achieve these levels of robustness on MNIST and CIFAR10 against a broad set of attacks.Overall, these findings suggest that secure neural networks are within reach. In order to further support this claim, we have invited the community to attempt attacks against our MNIST and CIFAR10 networks in the form of an open challenge 1,2 . At the time of writing, we received about fifteen submissions to the MNIST challenge and the best submission achieved roughly 93% accuracy in a black box attack. We received no submissions for the CIFAR10 challenge that went beyond the 64% accuracy of our attack. Considering that other proposed defenses were often quickly broken BID4 , we believe that our robust models are significant progress on the defense side. Furthermore, recent work on verifiable adversarial examples showed that our proposed defense reliably increased the robustness to any ∞ -bounded attack. Our findings provide evidence that deep neural networks can be made resistant to adversarial attacks. As our theory and experiments indicate, we can design reliable adversarial training methods. One of the key insights behind this is the unexpectedly regular structure of the underlying optimization task: even though the relevant problem corresponds to the maximization of a highly non-concave function with many distinct local maxima, their values are highly concentrated. Overall, our findings give us hope that adversarially robust deep learning models may be within current reach.For the MNIST dataset, our networks are very robust, achieving high accuracy for a wide range of powerful adversaries and large perturbations. Our experiments on CIFAR10 have not reached the same level of performance yet. However, our results already show that our techniques lead to significant increase in the robustness of the network. We believe that further exploring this direction will lead to adversarially robust networks for this dataset. We thank Wojciech Matusik for kindly providing us with computing resources to perform this work.","We provide a principled, optimization-based re-look at the notion of adversarial examples, and develop methods that produce models that are adversarially robust against a wide range of adversaries.",first ; PGD ; about fifteen ; One ; maxima ; MNIST,the best submission ; reach ; a decision boundary ; These methods ; Important examples ; computing resources ; an incorrect output ; Recent work ; robustness ; This formulation,first ; PGD ; about fifteen ; One ; maxima ; MNIST,"Neural networks are vulnerable to adversarial examples, i.e., inputs that are indistinguishable from natural data and yet classified incorrectly by the network. This approach provides us with a broad and unifying view on this issue. It also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. These methods help train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest robustness against a first-order adversary as a natural security guarantee. These techniques are important steps towards fully resistant deep learning models. These developments are bringing trained classifiers into the",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"As the basic building block of Convolutional Neural Networks (CNNs), the convolutional layer is designed to extract local patterns and lacks the ability to model global context in its nature. Many efforts have been recently made to complement CNNs with the global modeling ability, especially by a family of works on global feature interaction. In these works, the global context information is incorporated into local features before they are fed into convolutional layers. However, research on neuroscience reveals that, besides influences changing the inputs to our neurons, the neurons' ability of modifying their functions dynamically according to context is essential for perceptual tasks, which has been overlooked in most of CNNs. Motivated by this, we propose one novel Context-Gated Convolution (CGC) to explicitly modify the weights of convolutional layers adaptively under the guidance of global context. As such, being aware of the global context, the modulated convolution kernel of our proposed CGC can better extract representative local patterns and compose discriminative features. Moreover, our proposed CGC is lightweight, amenable to modern CNN architectures, and consistently improves the performance of CNNs according to extensive experiments on image classification, action recognition, and machine translation. Convolutional Neural Networks (CNNs) have achieved significant successes in various tasks, e.g., image classification (He et al., 2016a; Huang et al., 2017) , object detection (Girshick et al., 2014; , image translation , action recognition (Carreira & Zisserman, 2017) , sentence/text classification Kim, 2014) , machine translation (Gehring et al., 2017) , etc. However, the sliding window mechanism of convolution makes it only capable of capturing local patterns, limiting its ability of utilizing global context. Taking the 2D convolution on the image as one example, as Figure 1a shows, the standard convolution only operates on the local image patch and thereby composes local features. According to the recent research on neuroscience (Gilbert & Li, 2013) , neurons' awareness of global context is important for us to better interpret visual scenes, stably perceive objects and effectively process complex perceptual tasks. Many methods (Vaswani et al., 2017; Wang et al., 2017a; b; Hu et al., 2018; Chen et al., 2019; Cao et al., 2019; Bello et al., 2019) have been proposed recently to introduce global context modeling modules into CNN architectures. In this paper, such a family of works is named as global feature interaction methods. As Figure 1b shows, these methods modulate intermediate feature maps by incorporating the global context with the local feature representation. For example, in Non-local modules (Wang et al., 2017b) , local features are reassembled according to global correspondence, which augments CNNs with the global context modeling ability. As was discussed by Gilbert & Li (2013) , the global context influences neurons processing information in two distinct ways: ""various forms of attention, including spatial, object oriented and feature oriented attention"" and ""rather than having a fixed functional role, neurons are adaptive processors, changing their function according to behavioral context"". The previous work (Vaswani et al., 2017; Wang et al., 2017a; b; Hu et al., 2018; Chen et al., 2019; Cao et al., 2019; Bello et al., 2019) of global feature interaction methods, shown in Figure 1b , only modifies intermediate features, namely, inputs of neurons, which corresponds to the first way. However, to the best of our knowledge, the other efficient and intuitive way, i.e., explicitly modulating the convolution kernels according to context, has not been exploited yet. Motivated by this, we will model convolutional layers as ""adaptive processors"" and explore how to leverage global context to guide the composition of local features in convolution operations. In this paper, we propose Context-Gated Convolution (CGC), as Figure 1c shows, a new perspective of complementing CNNs with the awareness of the global context. Specifically, our proposed CGC learns a series of mappings to generate gates from the global context feature maps to modulate convolution kernels accordingly. With the modulated kernels, standard convolution is performed on input feature maps, which is enabled to dynamically capture representative local patterns and compose local features of interest under the guidance of global context. Our contributions are in three-fold. • To the best of our knowledge, we make the first attempt of introducing the contextawareness to convolutional layers by modulating the weights of them according to the global context. • We propose a novel lightweight CGC to effectively generate gates for convolution kernels to modify the weights with the guidance of global context. Our CGC consists of a Context Encoding Module that encodes context information into latent representations, a Channel Interacting Module that projects them into the space of output dimension, and a Gate Decoding Module that decodes the latent representations to produce the gate. • Our Context-Gated Convolution can better capture local patterns and compose discriminative features, and consistently improve the performance of standard convolution with a negligible complexity increment in various tasks including image classification, action recognition, and machine translation. 2 CONTEXT-GATED CONVOLUTION 2.1 PRELIMINARY Without loss of generality, we consider one sample of 2D case. The input to a convolutional layer is a feature map X ∈ R c×h×w , where c is the number of channels, and h, w are respectively the height and width of the feature map. In each convolution operation, a local patch of size c × k 1 × k 2 is collected by the sliding window to multiply with the kernel W ∈ R o×c×k1×k2 of this convolutional layer, where o is the number of output channels, and k 1 , k 2 are respectively the height and width of the kernel. Therefore, only local information within each patch is extracted in one convolution operation. Although in the training process, the convolution kernels are learned from all the patches from all the images in the training set, the kernels are not adaptive to the current context during inference time. We are aware of previous works on dynamically modifying the convolution operation (Dai et al., 2017; Wu et al., 2019; Jia et al., 2016; Jo et al., 2018; Mildenhall et al., 2018) . However, two key factors distinguish our approach from those works: whether the information guiding convolution is collected globally and how it changes the parameters of convolution. Dai et al. (2017) proposed to adaptively set the offset of each element in a convolution kernel, and Wu et al. (2019) proposed to dynamically generate the weights of convolution kernels. However, in their formulations, the dynamic mechanism for modifying convolution kernels only takes local patches or segments as inputs, so it is only adaptive to local inputs, which limits their ability of leveraging rich information in global context. According to experiments in Section 3.4, our proposed CGC significantly outperforms Dynamic Convolution (Wu et al., 2019) with the help of global context awareness. Another family of works on dynamic filters (Jia et al., 2016; Jo et al., 2018; Mildenhall et al., 2018) generates weights of convolution kernels using features extracted from input images by another CNN feature extractor. The expensive feature extraction process makes it more suitable for generating a few filters, e.g., in the case of low-level image processing. It is impractical to generate weights for all the layers in a deep CNN model in this manner. However, our CGC takes input feature maps of a convolutional layer and makes it possible to dynamically modulate the weight of each convolutional layer, which systematically improves CNNs' global context modeling ability. In this paper, motivated by neuroscience research on neurons as ""adaptive processors"", we proposed Context-Gated Convolution (CGC) to incorporate global context information into CNNs. Different from previous works which usually modifies input feature maps, our CGC directly modulates convolution kernels under the guidance of global context information. We proposed three modules to efficiently generate a gate to modify the kernel. As such, our CGC is able to extract representative local patterns according to global context. The extensive experiment results show consistent performance improvements on various tasks. There are still a lot of future works that can be done. For example, ew could design task-specific gating modules to fully uncover the potential of the proposed CGC. Mohammadreza Zolfaghari, Kamaljeet Singh, and Thomas Brox. Eco: Efficient convolutional network for online video understanding. For ImageNet, we use 224 × 224 random resized cropping and random horizontal flipping for data augmentation. Then we standardize the data with mean and variance per channel. We use a standard cross-entropy loss to train all the networks with a batch size of 256 on 8 GPUs by SGD with a weight decay of 0.0001 and a momentum of 0.9 for 100 epochs. We start from a learning rate of 0.1 and decrease it by a factor of 10 every 30 epochs. For CIFAR-10, we use 32 × 32 random cropping with a padding of 4 and random horizontal flipping. We use a batch size of 128 and train on 1 GPU. We decrease the learning rate at the 81st and 122nd epochs, and ends training after 164 epochs.","A novel Context-Gated Convolution which incorporates global context information into CNNs by explicitly modulating convolution kernels, and thus captures more representative local patterns and extract discriminative features.",Channel Interacting Module ; Context-Gated Convolution ; Cao ; Jia et al. ; Wu ; Carreira & Zisserman ; fed ; Context Encoding Module ; Bello et al. ; CNN,spatial ; a convolutional layer ; its nature ; the convolution kernels ; Jia et al ; global correspondence ; Li ; Hu ; this ; modern CNN architectures,Channel Interacting Module ; Context-Gated Convolution ; Cao ; Jia et al. ; Wu ; Carreira & Zisserman ; fed ; Context Encoding Module ; Bello et al. ; CNN,"The convolutional layer is designed to extract local patterns and lacks the ability to model global context. Many efforts have been made to complement CNNs with the global modeling ability, especially by a family of works on global feature interaction. In these works, the global context information is incorporated into local features before they are fed into convolutionals layers. However, research on neuroscience reveals that, besides influences changing the inputs to our neurons, the neurons' ability of modifying their functions dynamically according to context is essential for perceptual tasks, which has been overlooked in most of CNNs. Hence, we propose one novel Context-Gated Convolution",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"The universal approximation theorem, in one of its most general versions, says that if we consider only continuous activation functions σ, then a standard feedforward neural network with one hidden layer is able to approximate any continuous multivariate function f to any given approximation threshold ε, if and only if σ is non-polynomial. In this paper, we give a direct algebraic proof of the theorem. Furthermore we shall explicitly quantify the number of hidden units required for approximation. Specifically, if X in R^n is compact, then a neural network with n input units, m output units, and a single hidden layer with {n+d choose d} hidden units (independent of m and ε), can uniformly approximate any polynomial function f:X -> R^m whose total degree is at most d for each of its m coordinate functions. In the general case that f is any continuous function, we show there exists some N in O(ε^{-n}) (independent of m), such that N hidden units would suffice to approximate f. We also show that this uniform approximation property (UAP) still holds even under seemingly strong conditions imposed on the weights. We highlight several consequences: (i) For any δ > 0, the UAP still holds if we restrict all non-bias weights w in the last layer to satisfy |w| < δ. (ii) There exists some λ>0 (depending only on f and σ), such that the UAP still holds if we restrict all non-bias weights w in the first layer to satisfy |w|>λ. (iii) If the non-bias weights in the first layer are *fixed* and randomly chosen from a suitable range, then the UAP holds with probability 1. A standard (feedforward) neural network with n input units, m output units, and with one or more hidden layers, refers to a computational model N that can compute a certain class of functions ρ : R n → R m , where ρ = ρ W is parametrized by W (called the weights of N ). Implicitly, the definition of ρ depends on a choice of some fixed function σ : R → R, called the activation function of N . Typically, σ is assumed to be continuous, and historically, the earliest commonly used activation functions were sigmoidal. A key fundamental result justifying the use of sigmoidal activation functions was due to Cybenko (1989) , Hornik et al. (1989) , and Funahashi (1989) , who independently proved the first version of what is now famously called the universal approximation theorem. This first version says that if σ is sigmoidal, then a standard neural network with one hidden layer would be able to uniformly approximate any continuous function f : X → R m whose domain X ⊆ R n is compact. Hornik (1991) extended the theorem to the case when σ is any continuous bounded non-constant activation function. Subsequently, Leshno et al. (1993) proved that for the class of continuous activation functions, a standard neural network with one hidden layer is able to uniformly approximate any continuous function f : X → R m on any compact X ⊆ R n , if and only if σ is non-polynomial. Although a single hidden layer is sufficient for the uniform approximation property (UAP) to hold, the number of hidden units required could be arbitrarily large. Given a subclass F of real-valued continuous functions on a compact set X ⊆ R n , a fixed activation function σ, and some ε > 0, let N = N (F, σ, ε) be the minimum number of hidden units required for a single-hidden-layer neural network to be able to uniformly approximate every f ∈ F within an approximation error threshold of ε. If σ is the rectified linear unit (ReLU) x → max(0, x), then N is at least Ω( 1 √ ε ) when F is the class of C 2 non-linear functions (Yarotsky, 2017) , or the class of strongly convex differentiable functions (Liang & Srikant, 2016) ; see also (Arora et al., 2018) . If σ is any smooth non-polynomial function, then N is at most O(ε −n ) for the class of C 1 functions with bounded Sobolev norm (Mhaskar, 1996) ; cf. (Pinkus, 1999, Thm. 6.8) , (Maiorov & Pinkus, 1999) . As a key highlight of this paper, we show that if σ is an arbitrary continuous non-polynomial function, then N is at most O(ε −n ) for the entire class of continuous functions. In fact, we give an explicit upper bound for N in terms of ε and the modulus of continuity of f , so better bounds could be obtained for certain subclasses F, which we discuss further in Section 4. Furthermore, even for the wider class F of all continuous functions f : X → R m , the bound is still O(ε −n ), independent of m. To prove this bound, we shall give a direct algebraic proof of the universal approximation theorem, in its general version as stated by Leshno et al. (1993) (i.e. σ is continuous and non-polynomial). An important advantage of our algebraic approach is that we are able to glean additional information on sufficient conditions that would imply the UAP. Another key highlight we have is that if F is the subclass of polynomial functions f : X → R m with total degree at most d for each coordinate function, then n+d d hidden units would suffice. In particular, notice that our bound N ≤ n+d d does not depend on the approximation error threshold ε or the output dimension m. We shall also show that the UAP holds even under strong conditions on the weights. Given any δ > 0, we can always choose the non-bias weights in the last layer to have small magnitudes no larger than δ. Furthermore, we show that there exists some λ > 0 (depending only on σ and the function f to be approximated), such that the non-bias weights in the first layer can always be chosen to have magnitudes greater than λ. Even with these seemingly strong restrictions on the weights, we show that the UAP still holds. Thus, our main results can be collectively interpreted as a quantitative refinement of the universal approximation theorem, with extensions to restricted weight values. Outline: Section 2 covers the preliminaries, including relevant details on arguments involving dense sets. Section 3 gives precise statements of our results, while Section 4 discusses the consequences of our results. Section 5 introduces our algebraic approach and includes most details of the proofs of our results; details omitted from Section 5 can be found in the appendix. Finally, Section 6 concludes our paper with further remarks. The universal approximation theorem (version of Leshno et al. (1993) ) is an immediate consequence of Theorem 3.2 and the observation that σ must be non-polynomial for the UAP to hold, which follows from the fact that the uniform closure of P ≤d (X) is P ≤d (X) itself, for every integer d ≥ 1. Alternatively, we could infer the universal approximation theorem by applying the Stone-Weirstrass theorem (Theorem 2.1) to Theorem 3.1. Given fixed n, m, d, a compact set X ⊆ R n , and σ ∈ C(R)\P ≤d−1 (R), Theorem 3.1 says that we could use a fixed number N of hidden units (independent of ε) and still be able to approximate any function f ∈ P ≤d (X, R m ) to any desired approximation error threshold ε. Our ε-free bound, although possibly surprising to some readers, is not the first instance of an ε-free bound: Neural networks with two hidden layers of sizes 2n + 1 and 4n + 3 respectively are able to uniformly approximate any f ∈ C(X), provided that we use a (somewhat pathological) activation function (Maiorov & Pinkus, 1999) ; cf. (Pinkus, 1999) . Lin et al. (2017) showed that for fixed n, d, and a fixed smooth non-linear σ, there is a fixed N (i.e. ε-free), such that a neural network with N hidden units is able to approximate any f ∈ P ≤d (X). An explicit expression for N is not given, but we were able to infer from their constructive proof that N = 4 n+d+1 d − 4 hidden units are required, over d − 1 hidden layers (for d ≥ 2). In comparison, we require less hidden units and a single hidden layer. Our proof of Theorem 3.2 is an application of Jackson's theorem (Theorem 2.2) to Theorem 3.1, which gives an explicit bound in terms of the values of the modulus of continuity ω f of the function f to be approximated. The moduli of continuity of several classes of continuous functions have explicit characterizations. For example, given constants k > 0 and 0 < α ≤ 1, recall that a continuous function f : for all x, y ∈ X, and it is called α-Hölder if there is some constant c such that |f (x)−f (y)| ≤ c x−y α for all x, y ∈ X. The modulus of continuity of a k-Lipschitz (resp. α-Hölder) continuous function f is ω f (t) = kt (resp. ω f (t) = ct α ), hence Theorem 3.2 implies the following corollary. n → R is α-Hölder continuous, then there is a constant k such that for every ε > 0, there exists some An interesting consequence of Theorem 3.3 is the following: The freezing of lower layers of a neural network, even in the extreme case that all frozen layers are randomly initialized and the last layer is the only ""non-frozen"" layer, does not necessarily reduce the representability of the resulting model. Specifically, in the single-hidden-layer case, we have shown that if the non-bias weights in the first layer are fixed and randomly chosen from some suitable fixed range, then the UAP holds with probability 1, provided that there are sufficiently many hidden units. Of course, this representability does not reveal anything about the learnability of such a model. In practice, layers are already pre-trained before being frozen. It would be interesting to understand quantitatively the difference between having pre-trained frozen layers and having randomly initialized frozen layers. Theorem 3.3 can be viewed as a result on random features, which were formally studied in relation to kernel methods (Rahimi & Recht, 2007) . In the case of ReLU activation functions, Sun et al. (2019) proved an analog of Theorem 3.3 for the approximation of functions in a reproducing kernel Hilbert space; cf. (Rahimi & Recht, 2008) . For a good discussion on the role of random features in the representability of neural networks, see (Yehudai & Shamir, 2019) . The UAP is also studied in other contexts, most notably in relation to the depth and width of neural networks. Lu et al. (2017) proved the UAP for neural networks with hidden layers of bounded width, under the assumption that ReLU is used as the activation function. Soon after, Hanin (2017) strengthened the bounded-width UAP result by considering the approximation of continuous convex functions. Recently, the role of depth in the expressive power of neural networks has gathered much interest (Delalleau & Bengio, 2011; Eldan & Shamir, 2016; Mhaskar et al., 2017; Montúfar et al., 2014; Telgarsky, 2016) . We do not address depth in this paper, but we believe it is possible that our results can be applied iteratively to deeper neural networks, perhaps in particular for the approximation of compositional functions; cf. (Mhaskar et al., 2017) . Theorem 5.6 is rather general, and could potentially be used to prove analogs of the universal approximation theorem for other classes of neural networks, such as convolutional neural networks and recurrent neural networks. In particular, finding a single suitable set of weights (as a representative of the infinitely many possible sets of weights in the given class of neural networks), with the property that its corresponding ""non-bias Vandermonde matrix"" (see Definition 5.5) is non-singular, would serve as a straightforward criterion for showing that the UAP holds for the given class of neural networks (with certain weight constraints). We formulated this criterion to be as general as we could, with the hope that it would applicable to future classes of ""neural-like"" networks. We believe our algebraic approach could be emulated to eventually yield a unified understanding of how depth, width, constraints on weights, and other architectural choices, would influence the approximation capabilities of arbitrary neural networks. Finally, we end our paper with an open-ended question. The proofs of our results in Section 5 seem to suggest that non-bias weights and bias weights play very different roles. We could impose very strong restrictions on the non-bias weights and still have the UAP. What about the bias weights? First, we recall the notion of generalized Wronskians as given in (LeVeque, 1956, Chap. 4.3) . Let ∆ 0 , . . . , ∆ N −1 be any N differential operators of the form and let x = (x 1 , . . . , x n ). Recall that λ 1 < · · · < λ N are all the n-tuples in Λ n ≤d in the colexicographic order. For each be the coefficient of the monomial q k (x) in ∆ λi p(x). Consider an arbitrary W ∈ U, and for each 1 ≤ j ≤ N , define f j ∈ P ≤d (R n ) by the map x → p(w 1,j x 1 , . . . , w n,j x n ). Note that F p,0n (W ) = (f 1 , . . . , f N ) by definition. Next, define the matrix M W (x) := [∆ i f j (x)] 1≤i,j≤N , and note that det M W (x) is the generalized Wronskian of (f 1 , . . . , f N ) associated to ∆ 1 , . . . , ∆ N . In particular, this generalized Wronskian is well-defined, since the definition of the colexicographic order implies that λ k,1 + · · · + λ k,n ≤ k for all possible k. Similar to the univariate case, (f 1 , . . . , f N ) is linearly independent if (and only if) its generalized Wronskian is not the zero function (Wolsson, 1989) . Thus, to show that W ∈ p U ind , it suffices to show that the evaluation det M W (1 n ) of this generalized Wronskian at x = 1 n gives a non-zero value, where 1 n denotes the all-ones vector in R n . Observe that the (i, j)-th entry of M W (1 n ) equals ( w It follows from the definition of the colexicographic order that λ j − λ i necessarily contains at least one strictly negative entry whenever j < i, hence we infer that M is upper triangular. The diagonal entries of M are α 0n , . . . , α 0n , and note that α λi for each 1 ≤ i ≤ N , where λ i,1 ! · · · λ i,n ! denotes the product of the factorials of the entries of the n-tuple λ i . In particular, λ i,1 ! · · · λ i,n ! = 0, and α (1) λi , which is the coefficient of the monomial q i (x) in p(x), is non-zero. Thus, det(M ) = 0. We have come to the crucial step of our proof. If we can show that det(M ) = det(Q[W ]) = 0, then det(M W (1 n )) = det(M ) det(M ) = 0, and hence we can infer that W ∈ p U ind . This means that p U ind contains the subset U ⊆ U consisting of all W such that Q[W ] is non-singular. Note that det(Q [W ] ) is a polynomial in terms of the non-bias weights in W (1) as its variables, so we could write this polynomial as r = r(W ). Consequently, if we can find a single W ∈ U such that Q[W ] is non-singular, then r(W ) is not identically zero on U, which then implies that U = {W ∈ U : r(W ) = 0} is dense in U (w.r.t. the Euclidean metric).",A quantitative refinement of the universal approximation theorem via an algebraic approach.,Cybenko ; F ; R^n ; LeVeque ; ∈ X. ; U (w.r.t. ; Hornik et al ; Liang & Srikant ; n ! ; al.,convolutional neural networks ; U (w.r.t. ; a fixed number ; j≤N ; very strong restrictions ; a direct algebraic proof ; linear ; other contexts ; the first version ; the weights,Cybenko ; F ; R^n ; LeVeque ; ∈ X. ; U (w.r.t. ; Hornik et al ; Liang & Srikant ; n ! ; al.,"The universal approximation theorem states that if we consider only continuous activation functions σ, then a standard feedforward neural network with one hidden layer is able to approximate any continuous multivariate function f to any given approximation threshold ε, if and only if σ is non-polynomial. In this paper, we define the number of hidden units required for approximation. For instance, if X in R^n is compact, a neural network, m output units, and a single hidden layer can uniformly approximate any polynomial function f:X -> R^m whose total degree is at most d for each of its m coordinate",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Deep learning models require extensive architecture design exploration and hyperparameter optimization to perform well on a given task. The exploration of the model design space is often made by a human expert, and optimized using a combination of grid search and search heuristics over a large space of possible choices. Neural Architecture Search (NAS) is a Reinforcement Learning approach that has been proposed to automate architecture design. NAS has been successfully applied to generate Neural Networks that rival the best human-designed architectures. However, NAS requires sampling, constructing, and training hundreds to thousands of models to achieve well-performing architectures. This procedure needs to be executed from scratch for each new task. The application of NAS to a wide set of tasks currently lacks a way to transfer generalizable knowledge across tasks.
 In this paper, we present the Multitask Neural Model Search (MNMS) controller. Our goal is to learn a generalizable framework that can condition model construction on successful model searches for previously seen tasks, thus significantly speeding up the search for new tasks. We demonstrate that MNMS can conduct an automated architecture search for multiple tasks simultaneously while still learning well-performing, specialized models for each task. We then show that pre-trained MNMS controllers can transfer learning to new tasks. By leveraging knowledge from previous searches, we find that pre-trained MNMS models start from a better location in the search space and reduce search time on unseen tasks, while still discovering models that outperform published human-designed models. Designing deep learning models that work well for a task requires an extensive process of iterative architecture engineering and tuning. These design decisions are largely made by human experts guided by a combination of intuition, grid search, and search heuristics.Meta-learning aims to automate model design by using machine learning to discover good architecture and hyperparameter choices. Recent advances in meta-learning using Reinforcement Learning (RL) have made promising strides towards accelerating or even eliminating the manual parameter search. For example, Neural Architecture Search (NAS) has successfully discovered novel network architectures that rival or surpass the best human-designed architectures on challenging benchmark image recognition tasks . However, naively applying reinforcement learning to each new task for automated model construction requires sampling, constructing, and training hundreds to thousands of networks to relearn how to generate models from scratch. Human experts, on the other hand, can design and tune networks based on knowledge about underlying dependencies in the search space and experience with prior tasks. We therefore aim to automatically learn and leverage the same information.In this paper, we present Multitask Neural Model Search (MNMS), an automated model construction framework that finds the best performing models in the search space for multiple tasks simultaneously. We then show that a MNMS framework that has been pre-trained on previous tasks can construct the best performing model for entirely new tasks in significantly less time. Summary. Machine learning model design choices do not exist in a vacuum. Human experts design good models by leveraging significant prior knowledge about the intuitive relationships between these model parameters, and the performance obtained by different model designs on similar tasks. Automated model design algorithms, too, can and should learn from the models they have discovered for prior tasks. This paper demonstrates that Multitask Neural Model Search can discover good, differentiated model designs for multiple tasks simultaneously, while learning task embeddings that encode meaningful relationships between tasks. We then show that multitask training provides a good baseline for transfer learning to future tasks, allowing the MNMS framework to start from a better location in the search space and converge more quickly to high-performing designs.Limitations and future work. While the current work demonstrates that the MNMS framework can be used for multitask training and transferable architecture searches, much work remains to determine the scalability of this approach. The results of this study offer several particularly promising avenues for future research. First, studying the effects of additional simultaneous tasks on framework performance is an obvious next step in multitask training. The current framework trains the learned task embeddings by passing them directly into the controller RNN along with the sampled action embeddings. We anticipate that a more complex pre-processing structure, such as a simple encoder-decoder, could better transform these task embeddings to be used by the controller. Additionally, we currently leverage the distributed training structure described by Zoph and Le, which trains multiple sampled child architectures in parallel and asynchronously updates a shared controller parameter server . However, as we continue to scale the MNMS framework for additional simultaneous tasks, future work remains to optimize a parallel training structure and schedule specifically for efficient multitask training.Experimenting with broader richer hyperparameter search spaces also offers an exciting line of future work. For our current tasks, we defined a search space that encompassed a range of general design choices, including both real-valued parameters (such as learning rates and regularization weights) and higher-level parameters (such as the choice of word embedding table). However, we are actively adapting the controller to sample continuous real-valued parameters, rather than discrete choices from a set of predefined values, which would give the framework much greater flexibility in specifying models. Additionally, we plan to continue expanding the range of modular, higher-level parameter choices in the search space. Allowing the controller to compose these building blocks, rather than more granular design choices, can allow the framework to construct more complex architectures in much less time.Finally, much work remains to explore cases when transfer learning is and is not effective within RL-based architecture search frameworks such as MNMS. We are particularly interested in studying how transfer learning can be used to design architectures for tasks that were previously considered too resource intensive for standard NAS. For example, adapted NAS for the ImageNet classification task by directly modifying the architecture designed for a simpler image classification task. However, pretraining the architecture search framework itself on more computationally feasible tasks, rather than transferring the discovered architectures, would be a significant step towards tackling these difficult search domains.","We present Multitask Neural Model Search, a Meta-learner that can design models for multiple tasks simultaneously and transfer learning to unseen tasks.",ImageNet ; hundreds to thousands ; Neural Architecture Search ; RNN ; Zoph ; Neural Networks ; Le ; Neural Model Search ; Reinforcement Learning ; NAS,the framework ; This procedure ; multitask training ; these difficult search domains ; too resource ; tasks ; First ; rates ; a task ; new tasks,ImageNet ; hundreds to thousands ; Neural Architecture Search ; RNN ; Zoph ; Neural Networks ; Le ; Neural Model Search ; Reinforcement Learning ; NAS,"Deep learning models require extensive architecture design exploration and hyperparameter optimization to perform well on a given task. These design decisions are often made by human experts, and optimized using a combination of grid search and search heuristics over a large space of possible choices. Neural Architecture Search (NAS) is a Reinforcement Learning approach that has been successfully applied to generate Neural Networks that rival or surpass human-designed architectures. However, NAS requires sampling, constructing, and training hundreds to thousands of models to achieve well-performing architectures. This procedure needs to be executed from scratch for each new task. The application of NAS to a wide set of",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"For many applications, in particular in natural science, the task is to
 determine hidden system parameters from a set of measurements. Often,
 the forward process from parameter- to measurement-space is well-defined,
 whereas the inverse problem is ambiguous: multiple parameter sets can
 result in the same measurement. To fully characterize this ambiguity, the full
 posterior parameter distribution, conditioned on an observed measurement,
 has to be determined. We argue that a particular class of neural networks
 is well suited for this task – so-called Invertible Neural Networks (INNs).
 Unlike classical neural networks, which attempt to solve the ambiguous
 inverse problem directly, INNs focus on learning the forward process, using
 additional latent output variables to capture the information otherwise
 lost. Due to invertibility, a model of the corresponding inverse process is
 learned implicitly. Given a specific measurement and the distribution of
 the latent variables, the inverse pass of the INN provides the full posterior
 over parameter space. We prove theoretically and verify experimentally, on
 artificial data and real-world problems from medicine and astrophysics, that
 INNs are a powerful analysis tool to find multi-modalities in parameter space,
 uncover parameter correlations, and identify unrecoverable parameters. When analyzing complex physical systems, a common problem is that the system parameters of interest cannot be measured directly. For many of these systems, scientists have developed sophisticated theories on how measurable quantities y arise from the hidden parameters x. We will call such mappings the forward process. However, the inverse process is required to infer the hidden states of a system from measurements. Unfortunately, the inverse is often both intractable and ill-posed, since crucial information is lost in the forward process.To fully assess the diversity of possible inverse solutions for a given measurement, an inverse solver should be able to estimate the complete posterior of the parameters, conditioned on an observation. This makes it possible to quantify uncertainty, reveal multi-modal distributions, and identify degenerate and unrecoverable parameters -all highly relevant for applications in natural science. In this paper, we ask if invertible neural networks (INNs) are a suitable model class for this task. INNs are characterized by three properties:(i ) The mapping from inputs to outputs is bijective, i.e. its inverse exists, ( ii) both forward and inverse mapping are efficiently computable, and (iii) both mappings have a tractable Jacobian, which allows explicit computation of posterior probabilities.Networks that are invertible by construction offer a unique opportunity: We can train them on the well-understood forward process x → y and get the inverse y → x for free by The standard direct approach requires a discriminative, supervised loss (SL) term between predicted and true x, causing problems when y → x is ambiguous. Our network uses a supervised loss only for the well-defined forward process x → y. Generated x are required to follow the prior p(x) by an unsupervised loss (USL), while the latent variables z are made to follow a Gaussian distribution, also by an unsupervised loss. See details in Section 3.3.running them backwards at prediction time. To counteract the inherent information loss of the forward process, we introduce additional latent output variables z, which capture the information about x that is not contained in y. Thus, our INN learns to associate hidden parameter values x with unique pairs [y, z] of measurements and latent variables. Forward training optimizes the mapping [y, z] = f (x) and implicitly determines its inverse x = f −1 (y, z) = g(y, z). Additionally, we make sure that the density p(z ) of the latent variables is shaped as a Gaussian distribution. Thus , the INN represents the desired posterior p(x | y) by a deterministic function x = g(y, z) that transforms (""pushes"") the known distribution p(z) to x-space, conditional on y.Compared to standard approaches (see FIG0 , left), INNs circumvent a fundamental difficulty of learning inverse problems: Defining a sensible supervised loss for direct posterior learning is problematic since it requires prior knowledge about that posterior's behavior, constituting a kind of hen-end-egg problem. If the loss does not match the possibly complicated (e.g. multimodal) shape of the posterior, learning will converge to incorrect or misleading solutions. Since the forward process is usually much simpler and better understood, forward training diminishes this difficulty. Specifically , we make the following contributions:• We show that the full posterior of an inverse problem can be estimated with invertible networks, both theoretically in the asymptotic limit of zero loss, and practically on synthetic and real-world data from astrophysics and medicine.• The architectural restrictions imposed by invertibility do not seem to have detrimental effects on our network's representational power.• While forward training is sufficient in the asymptotic limit, we find that a combination with unsupervised backward training improves results on finite training sets.• In our applications, our approach to learning the posterior compares favourably to approximate Bayesian computation (ABC) and conditional VAEs. This enables identifying unrecoverable parameters, parameter correlations and multimodalities. We have shown that the full posterior of an inverse problem can be estimated with invertible networks, both theoretically and practically on problems from medicine and astrophysics. We share the excitement of the application experts to develop INNs as a generic tool, helping them to better interpret their data and models, and to improve experimental setups. As a side effect, our results confirm the findings of others that the restriction to coupling layers does not noticeably reduce the expressive power of the network.In summary, we see the following fundamental advantages of our INN-based method compared to alternative approaches: Firstly, one can learn the forward process and obtain the (more complicated) inverse process 'for free', as opposed to e.g. cGANs, which focus on the inverse and learn the forward process only implicitly. Secondly, the learned posteriors are not restricted to a particular parametric form, in contrast to classical variational methods. Lastly, in comparison to ABC and related Bayesian methods, the generation of the INN posteriors is computationally very cheap. In future work, we plan to systematically analyze the properties of different invertible architectures, as well as more flexible models utilizing cycle losses, in the context of representative inverse problem. We are also interested in how our method can be scaled up to higher dimensionalities, where MMD becomes less effective. Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In CVPR, pages 2223-2232, 2017a.Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A Efros, Oliver Wang, and Eli Shechtman. Toward multimodal image-to-image translation. In Advances in Neural Information Processing Systems, pages 465-476, 2017b. DISPLAYFORM0 In Eq. 9, the Jacobians cancel out due to the inverse function theorem, i.e. the Jacobian DISPLAYFORM1 is trained as proposed, and both the supervised loss DISPLAYFORM2 and the unsupervised loss L z = D q(y, z), p(y ) p(z ) reach zero, sampling according to Eq. 1 with g = f −1 returns the true posterior p(x | y * ) for any measurement y * .Proof : We denote the chosen latent distribution as p Z (z), the distribution of observations as p Y (y), and the joint distribution of network outputs as q(y, z). As shown by BID14 , if the MMD loss converges to 0, the network outputs follow the prescribed distribution: DISPLAYFORM3 Suppose we take a posterior conditioned on a fixed y * , i.e. p(x | y * ), and transform it using the forward pass of our perfectly converged INN. From this we obtain an output distribution q * (y, z). Because L y = 0, we know that the output distribution of y (marginalized over z) must be q * (y) = δ(y − y * ). Also, because of the independence between z and y in the output, the distribution of z-outputs is still q * (z) = p Z (z) . So the joint distribution of outputs is DISPLAYFORM4 When we invert the network, and repeatedly input y * while sampling z ∼ p Z (z), this is the same as sampling [y, z] from the q * (y, z) above. Using the Lemma from above, we know that the inverted network will output samples from p(x | y * ).Corollary: If the conditions of the theorem above are fulfilled, the unsupervised reverse loss L x = D q(x), p X (x) between the marginalized outputs of the inverted network, q(x), and the prior data distribution, p X (x), will also be 0. This justifies using the loss on the prior to speed up convergence, without altering the final results.Proof: Due to the theorem, the estimated posteriors generated by the INN are correct, i.e. q(x | y * ) = p(x | y * ). If they are marginalized over observations y * from the training data, then q(x) will be equal to p X (x) by definition. As shown by BID14 , this is equivalent to L x = 0.",To analyze inverse problems with Invertible Neural Networks,Deepak Pathak ; Jacobian ; MMD ; Eli Shechtman ; Oliver Wang ; medicine.• ; Invertible Neural Networks ; Richard Zhang ; zero ; USL,"a discriminative, supervised loss (SL) term ; our network's representational power.• ; measurement-space ; the inherent information loss ; Neural Information Processing Systems ; that ; a given measurement ; Firstly ; the corresponding inverse process ; astrophysics",Deepak Pathak ; Jacobian ; MMD ; Eli Shechtman ; Oliver Wang ; medicine.• ; Invertible Neural Networks ; Richard Zhang ; zero ; USL,"For many applications, the forward process from parameter-to measurement-space is well-defined, whereas the inverse problem is ambiguous. Multiple parameter sets can result in the same measurement. To fully characterize this ambiguity, the full posterior parameter distribution, conditioned on an observed measurement, has to be determined. Invertible neural networks (INNs) are a suitable model class for this task. Unlike classical neural networks, which attempt to solve the ambiguous inverse problem directly, INNs focus on learning forward process, using additional latent output variables to capture the information otherwise lost. Due to invertibility, a model of the corresponding inverse process is",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"High-dimensional sparse reward tasks present major challenges for reinforcement learning agents.   In this work we use imitation learning to address two of these challenges:  how to learn a useful representation of the world e.g.  from pixels, and how to explore efficiently given the rarity of a reward signal? We show that adversarial imitation can work well even in this high dimensional observation space. Surprisingly the adversary itself, acting as the learned reward function, can be tiny, comprising as few as 128 parameters, and can be easily trained using the most basic GAN formulation. Our approach removes limitations present in most contemporary imitation approaches: requiring no demonstrator actions (only video), no special initial conditions or warm starts, and no explicit tracking of any single demo. The proposed agent can solve a challenging robot manipulation task of block stacking from only video demonstrations and sparse reward, in which the non-imitating agents fail to learn completely.   Furthermore, our agent learns much faster than competing approaches that depend on hand-crafted, staged dense reward functions, and also better compared to standard GAIL baselines. Finally, we develop a new adversarial goal recognizer that in some cases allows the agent to learn stacking without any task reward, purely from imitation.","Imitation from pixels, with sparse or no reward, using off-policy RL and a tiny adversarially-learned reward function.",two ; GAN ; GAIL,limitations ; reinforcement learning agents ; GAN ; that ; we ; adversarial imitation ; a reward signal ; the world ; imitation ; no explicit tracking,two ; GAN ; GAIL,"Reinforcement learning agents face two challenges: how to learn a useful representation of the world from pixels, and how to explore efficiently given the rarity of a reward signal. In this work, we show that adversarial imitation can work well even in high dimensional observation space. The adversary itself, acting as the learned reward function, can be tiny, comprising as few as 128 parameters, and can be easily trained using the most basic GAN formulation. Our approach removes limitations present in most contemporary imitation approaches, requiring no demonstrator actions (only video), no special initial conditions or warm starts, and no explicit tracking of any single demo.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Language and vision are processed as two different modal in current work for image captioning. However, recent work on Super Characters method shows the effectiveness of two-dimensional word embedding, which converts text classification problem into image classification problem. In this paper, we propose the SuperCaptioning method, which borrows the idea of two-dimensional word embedding from Super Characters method, and processes the information of language and vision together in one single CNN model. The experimental results on Flickr30k data shows the proposed method gives high quality image captions. An interactive demo is ready to show at the workshop. Image captioning outputs a sentence related to the input image. Current methods process the image and text separately BID3 BID8 BID10 BID9 BID4 BID5 BID0 BID1 . Generally, the image is processed by a CNN model to extract the image feature, and the raw text passes through embedding layer to convert into one-dimensional wordembedding vectors, e.g. a 300x1 dimensional vector. And then the extracted image feature and the word embedding vectors will be fed into another network, such as RNN, LSTM, or GRU model, to predict the next word in the image caption sequentially.Super Characters method ) is originally designed for text classification tasks. It has achieved stateof-the-art results on benchmark datasets for multiple languages, including English, Chinese, Japanese, and Korean. It is a two-step method. In the first step, the text characters are printed on a blank image, and the generated image is called Super Characters image. In the second step, the Super Characters image is fed into a CNN model for classification. The CNN model is fine-tuned from pre-trained ImageNet model.In this paper, we address the image captioning problem by employing the two-dimensional word embedding from the Super Characters method, and the resulting method is named as SuperCaptioning method. In this method, the input image and the raw text are combined together through two-dimensional embedding, and then fed into a CNN model to sequentially predict the words in the image caption. The experimental results on Flickr30k shows that the proposed method gives high quality image captions. Some examples given by SuperCaptioning method are shown in FIG2 .",Image captioning using two-dimensional word embedding.,RNN ; Super Characters ; Chinese ; fed ; second ; ImageNet ; one ; two ; SuperCaptioning ; GRU,the information ; the image feature ; Japanese ; the proposed method ; It ; the idea ; which ; one ; Language ; two different modal,RNN ; Super Characters ; Chinese ; fed ; second ; ImageNet ; one ; two ; SuperCaptioning ; GRU,"SuperCaptioning method combines two-dimensional word embedding from Super Characters method, and processes the information of language and vision together in one single CNN model. The experimental results on Flickr30k shows that the proposed method gives high quality image captions. An interactive demo is ready to show at the workshop. Current methods process the image and text separately BID3 BID8 BID10 BID9 BID4 BID5 BID0 BID1. In this paper, we address image captioning problems by employing two dimensional word embeddings from Super characters method and the resulting method, which combines the input",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Developing agents that can learn to follow natural language instructions has been an emerging research direction. While being accessible and flexible, natural language instructions can sometimes be ambiguous even to humans. To address this, we propose to utilize programs, structured in a formal language, as a precise and expressive way to specify tasks. We then devise a modular framework that learns to perform a task specified by a program – as different circumstances give rise to diverse ways to accomplish the task, our framework can perceive which circumstance it is currently under, and instruct a multitask policy accordingly to fulfill each subtask of the overall task. Experimental results on a 2D Minecraft environment not only demonstrate that the proposed framework learns to reliably accomplish program instructions and achieves zero-shot generalization to more complex instructions but also verify the efficiency of the proposed modulation mechanism for learning the multitask policy. We also conduct an analysis comparing various models which learn from programs and natural language instructions in an end-to-end fashion. Humans are capable of leveraging instructions to accomplish complex tasks. A comprehensive instruction usually comprises a set of descriptions detailing a variety of situations and the corresponding subtasks that are required to be fulfilled. To accomplish a task, we can leverage instructions to estimate the progress, recognize the current state, and perform corresponding actions. For example, to make a gourmet dish, we can follow recipes and procedurally create the desired dish by recognizing what ingredients and tools are missing, what alternatives are available, and what corresponding preparations are required. With sufficient practice, we can improve our ability to perceive (e.g. knowing when food is well-cooked) as well as master cooking skills (e.g. cutting food into same-sized pieces), and eventually accomplish difficult recipes. Can machines likewise learn to follow and exploit comprehensive instructions like humans? Utilizing expert demonstrations to instruct agents has been widely studied in (Finn et al., 2017; Yu et al., 2018b; Xu et al., 2018; Pathak et al., 2018; Stadie et al., 2017; Duan et al., 2017; Wang et al., 2017b) . However, demonstrations could be expensive to obtain and are less flexible (e.g. altering subtask orders in demonstrations is nontrivial). On the other hand, natural language instructions are flexible and expressive (Malmaud et al., 2014; Jermsurawong & Habash, 2015; Kiddon et al., 2015; Misra et al., 2018; Fried et al., 2018; Kaplan et al., 2017; Bahdanau et al., 2019 ). Yet, language has the caveat of being ambiguous even to humans, due to its lacking of structure as well as unclear coreferences and entities. Andreas et al. (2017a) ; Oh et al. (2017) investigate a hierarchical approach, where the instructions consist of a set of symbolically represented subtasks. Nonetheless, those instructions are not a function of states (i.e. describe a variety of circumstances and the corresponding desired subtasks), which substantially limits their expressiveness. We propose to utilize programs, written in a formal language, as a structured, expressive, and unambiguous representation to specify tasks. Specifically, we consider programs, which are composed of control flows (e.g. if/else and loops), environmental conditions, as well as corresponding subtasks, as shown in Figure 1 . Not only do programs have expressiveness by describing diverse situations (e.g. a river exists) and the corresponding subtasks which are required to be executed (e.g. mining wood), but they are also unambiguous due to their explicit scoping. To study the effectiveness of using programs as task specifications, we introduce a new problem, where we aim to develop a framework We are interested in learning to fulfill tasks specified by written programs. A program consists of control flows (e.g. if, while), branching conditions (e.g. is_there [River] ), and subtasks (e.g. mine(Wood)). which learns to comprehend a task specified by a program as well as perceive and interact with the environment to accomplish the task. To address this problem, we propose a modular framework, program guided agent, which exploits the structural nature of programs to decompose and execute them as well as learn to ground program tokens with the environment. Specifically, our framework consists of three modules: (1) a program interpreter that leverages a grammar provided by the programming language to parse and execute a program, (2) a perception module that learns to respond to conditional queries (e.g. is_there [River] ) produced by the interpreter, and (3) a policy that learns to fulfill a variety of subtasks (e.g. mine(Wood)) extracted from the program by the interpreter. To effectively instruct the policy with symbolically represented subtasks, we introduce a learned modulation mechanism that leverages a subtask to modulate the encoded state features instead of concatenating them. Our framework (shown in Figure 3 ) utilizes a rule-based program interpreter to deal with programs as well as learning perception module and policy when it is necessary to perceive or interact with the environment. With this modularity, our framework can generalize to more complex program-specified tasks without additional learning. To evaluate the proposed framework, we consider a Minecraft-inspired 2D gridworld environment, where an agent can navigate itself across different terrains and interact with objects, similar to Andreas et al. (2017a) ; Sohn et al. (2018) . A corresponding domain-specific language (DSL) defines the rules of constructing programs for instructing an agent to accomplish certain tasks. Our proposed framework demonstrates superior generalization ability -learning from simpler tasks while generalizing to complex tasks. We also conduct extensive analysis on various end-to-end learning models which learns from not only program instructions but also natural language descriptions. Furthermore, our proposed learned policy modulation mechanism yields a better learning efficiency compared to other commonly used methods that simply concatenate a state and goal. We propose to utilize programs, structured in a formal language, as an expressive and precise way to specify tasks instead of commonly used natural language instructions. We introduce the problem of developing a framework that can comprehend a program as well as perceive and interact with the environment to accomplish the desired task. To address this problem, we devise a modular framework, program guided agent, which executes programs with a program interpreter by altering between querying a perception module when a branching condition is encountered and instructing a policy to fulfill subtasks. We employ a policy modulation mechanism to improve the efficiency of learning the multitask policy. The experimental results on a 2D Minecraft environment demonstrate that the proposed framework learns to reliably fulfill program instructions and generalize well to more complex instructions without additional training. We also investigate the performance of various models that learn from programs and natural language descriptions in an end-to-end fashion. Michael Janner, Karthik Narasimhan, and Regina Barzilay. To fuse the information from an input domain (e.g. an image) with another condition domain (e.g. a language query, image such as segmentation map, noise, etc.), a wide range of works have demonstrated the effectiveness of predicting affine transforms based on the condition to scale and bias the input in visual question answering (Perez et al., 2018; 2017) , image synthesis (Almahairi et al., 2018; Karras et al., 2019; Park et al., 2019; Huh et al., 2019) , style transfer (Dumoulin et al., 2017) , recognition (Hu et al., 2018; Xie et al., 2018 ), reading comprehension (Dhingra et al., 2017 , few-shot learning (Oreshkin et al., 2018; Lee & Choi, 2018) , etc. Many of those works present an extensive ablation study to compare the learned modulation against traditional ways to merge the information from the input and condition domains. Recently, a few works have employed a similar learned modulation technique to reinforcement learning frameworks on learning to follow language instruction (Bahdanau et al., 2019) and metareinforcement learning (Vuorio et al., 2018; 2019) . However, there has not been a comprehensive ablation study that suggests fusing the information from the input domain (e.g. a state) and the condition domain (e.g. a goal or a task embedding) for the reinforcement learning setting. In this work, we conduct an ablation study in our 2D Minecraft environment where an agent is required to fulfill a navigational task specified by a program and show the effectiveness of learning to modulate input features with symbolically represented goal as well as present a number of modulation variations (i.e. modulating the fully-connected layers or the convolutional layers or both). We look forward to future research that verifies if this learned modulation mechanism is effective in dealing with more complex domains such as robot manipulation or locomotion.",We propose a modular framework that can accomplish tasks specified by programs and achieve zero-shot generalization to more complex tasks.,Michael Janner ; Yu et al. ; Kaplan ; Perez et al. ; Regina Barzilay ; Wang ; zero ; Sohn ; Andreas et al ; Pathak et al.,those instructions ; DSL ; we ; a river ; an extensive ablation study ; unclear coreferences ; same-sized pieces ; Misra ; zero-shot generalization ; complex tasks,Michael Janner ; Yu et al. ; Kaplan ; Perez et al. ; Regina Barzilay ; Wang ; zero ; Sohn ; Andreas et al ; Pathak et al.,"Learning to follow natural language instructions has been an emerging research direction. Programs, structured in a formal language, can sometimes be ambiguous. To address this, we propose to create a modular framework that learns to perform a task specified by a program. As different circumstances give rise to diverse ways to accomplish the task, our framework can perceive which circumstance it is currently under, and instruct a multitask policy accordingly to fulfill each subtask of the overall task. Experimental results on a 2D Minecraft environment not only demonstrate that the proposed framework learns to reliably accomplish program instructions and achieves zero-shot generalization to more complex instructions but also verify the efficiency",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Real world images often contain large amounts of private / sensitive information that should be carefully protected without reducing their utilities. In this paper, we propose a privacy-preserving deep learning framework with a learnable ob- fuscator for the image classification task. Our framework consists of three mod- els: learnable obfuscator, classifier and reconstructor. The learnable obfuscator is used to remove the sensitive information in the images and extract the feature maps from them. The reconstructor plays the role as an attacker, which tries to recover the image from the feature maps extracted by the obfuscator. In order to best protect users’ privacy in images, we design an adversarial training methodol- ogy for our framework to optimize the obfuscator. Through extensive evaluations on real world datasets, both the numerical metrics and the visualization results demonstrate that our framework is qualified to protect users’ privacy and achieve a relatively high accuracy on the image classification task. In the past few years, deep neural networks (DNNs) have achieved great breakthroughs in computer vision, speech recognition and many other areas. To support the training of DNNs, large datasets have been collected, e.g., ImageNet BID6 , MNIST (LeCun et al., 1998) and CIFAR-10/CIFAR-100 BID15 ) as image datasets, Youtube-8M (Abu-El-Haija et al., 2016) as video datasets, and AudioSet BID8 as audio datasets. These datasets are usually crowdsourced from the real world, and may carry sensitive private information, thus, leading to serious privacy problems.The new European Union's General Data Protection Regulation (GDPR) (Regulation, 2016) stipulates that personal data cannot be stored for long periods of time, and personal data requests, such as deleting personal images, should be handled within 30 days. In other words, this regulation prevents long-term storage of video/image data (e.g., from CCTV cameras), which hinders the collection of real-world datasets for training deep learning models. However, the data storage limitations do not apply if the data is anonymized.This regulation considers the trade-off between the utility and the privacy of the data. However, 30 days may not be a long enough period to collect image data and train a complex deep learning model, and deletion of data hinders re-training later when the model structure is updated or more data becomes available. GPDR allows anonymized data to be stored indefinitely, which inspires us to design a framework where an image is converted into an obfuscated intermediate representation that removes sensitive personal information while retaining suitable discriminative features for the learning task. Thus the obfuscated intermediate representation can be stored indefinitely for model training in compliance with GDPR. Contributions In this paper, we design a obfuscator-adversary framework to obtain a trainable obfuscator that fulfills the dual goals of removing sensitive information and extracting useful features for the learning task. Here, we mainly focus on image classification as the learning task, since it is a more general task in computer vision -the framework could be extended to other tasks. Our framework consists of three models, each with its own objective: the obfuscator, the classifier and the reconstructor, shown in Figure 1 . The obfuscator works as an information remover, which takes the input image and extracts feature maps that carry enough primary information for the classification task while removing sensitive private information. These feature maps are the obfuscated representation of the input image. The classifier uses the obfuscated representation to perform classification of the input image. Finally, the reconstructor plays the role as an adversary whose goal is to extract the sensitive information from the obfuscated representation. Privacy Attack:Step 1: We proposed a deep learning framework on privacy-preserving image classification tasks. Our framework has three modules, the obfuscator, classifier, and reconstructor. The obfuscator works as an feature extractor and sensitive information remover to protect users' privacy without decreasing the accuracy of the classifier. The reconstructor is an attacker, and has an opposite objective to reveal the sensitive information. Based on this antagonism, we designed an adversarial training methodology. Experiments showed our framework is qualified to protect users' privacy and achieve a relatively high accuracy on the image classification task.",We proposed a novel deep learning image classification framework that can both accurately classify images and protect users' privacy.,three ; the past few years ; Abu-El-Haija ; al. ; European Union's ; General Data Protection Regulation ; CCTV ; GPDR ; GDPR ; three modules,The new European Union's General Data Protection Regulation ; anonymized data ; Our framework ; personal images ; Figure ; the real world ; great breakthroughs ; CCTV ; The reconstructor ; the obfuscator,three ; the past few years ; Abu-El-Haija ; al. ; European Union's ; General Data Protection Regulation ; CCTV ; GPDR ; GDPR ; three modules,"Real world images often contain large amounts of private / sensitive information that should be carefully protected without reducing utilities. In this paper, we propose a privacy-preserving deep learning framework with a learnable ob- fuscator for the image classification task. Three mod- els comprise the learnable obfuscator, classifier and reconstructor, while the reconstructor plays the role of an attacker, extracting the feature maps from the images. In order to best protect users’ privacy in images, we design an adversarial training methodol- ogy for our framework to optimize the obfuscator. Through extensive evaluations on real world datasets",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"We propose to use a meta-learning objective that maximizes the speed of transfer on a modified distribution to learn how to modularize acquired knowledge. In particular, we focus on how to factor a joint distribution into appropriate conditionals, consistent with the causal directions. We explain when this can work, using the assumption that the changes in distributions are localized (e.g. to one of the marginals, for example due to an intervention on one of the variables). We prove that under this assumption of localized changes in causal mechanisms, the correct causal graph will tend to have only a few of its parameters with non-zero gradient, i.e. that need to be adapted (those of the modified variables). We argue and observe experimentally that this leads to faster adaptation, and use this property to define a meta-learning surrogate score which, in addition to a continuous parametrization of graphs, would favour correct causal graphs. Finally, motivated by the AI agent point of view (e.g. of a robot discovering its environment autonomously), we consider how the same objective can discover the causal variables themselves, as a transformation of observed low-level variables with no causal meaning. Experiments in the two-variable case validate the proposed ideas and theoretical results. The data used to train our models is often assumed to be independent and identically distributed (iid.), according to some unknown distribution. Likewise, the performance of a model is typically evaluated using test samples from the same distribution, assumed to be representative of the learned system's usage. While these assumptions are well analyzed from a statistical point of view, they are rarely satisfied in many real-world applications. For example, a medical diagnosis system trained on historical data from one hospital might perform poorly on patients from another institution, due to shifts in distribution. Ideally, we would like our models to generalize well and adapt quickly to out-of-distribution data. However, this comes at a price -in order to successfully transfer to a novel distribution, one might need additional information about data at hand. In this paper, we are not considering assumptions on the data distribution but rather on how it changes (e.g., when going from a training distribution to a transfer distribution, possibly resulting from some agent's actions). We focus on the assumption that the changes are sparse when the knowledge is represented in an appropriately modularized way, with only one or a few of the modules having changed. This is especially relevant when the distributional change is due to actions by one or more agents, because agents intervene at a particular place and time, and this is reflected in the form of the interventions discussed in the causality literature (Pearl, 2009; Peters et al., 2016) , where a single causal variable is clamped to a particular value or a random variable. In general, it is difficult for agents to influence many underlying causal variables at a time, and although this paper is not about agent learning as such, this is a property of the world that we propose to exploit here, to help discovering these variables and how they are causally related to each other. In this context, the causal graph is a powerful tool because it tells us how perturbations in the distribution of intervened variables will propagate to all other variables and affect their distributions. As expected, it is often the case that the causal structure is not known in advance. The problem of causal discovery then entails obtaining the causal graph, a feat which is in general achievable only with strong assumptions. One such assumption is that a learner that has learned to capture the correct structure of the true underlying data-generating process should still generalize to the case where the structure has been perturbed in a certain, restrictive way. This can be illustrated by considering the example of temperature and altitude (Peters et al., 2017) : a learner that has learned to capture the mechanisms of atmospheric physics by learning that it makes more sense to predict temperature from the altitude (rather than vice versa) given training data from (say) Switzerland will still remain valid when tested on out-of-distribution data from a less mountainous country like (say) the Netherlands. It has therefore been suggested that the out-of-distribution robustness of predictive models be used to guide the inference of the true causal structure (Peters et al., 2016; 2017) . How can we exploit the assumption of localized change? As we explain theoretically and verify experimentally here, if we have the right knowledge representation, then we should get fast adaptation to the transfer distribution when starting from a model that is well trained on the training distribution. This arises because of our assumption that the ground truth data generative process is obtained as the composition of independent mechanisms, and that very few ground truth mechanisms and parameters need to change when going from the training distribution to the transfer distribution. A model capturing a corresponding factorization of knowledge would thus require just a few updates, a few examples, for this adaptation to the transfer distribution. As shown below, the expected gradient on the unchanged parameters would be near 0 (if the model was already well trained on the training distribution), so the effective search space during adaptation to the transfer distribution would be greatly reduced, which tends to produce fast adaptation, as found experimentally. Thus, based on the assumption of small change in the right knowledge representation space, we can define a meta-learning objective that measures the speed of adaptation, i.e., a form of regret, in order to optimize the way in which knowledge should be represented, factorized and structured. This is the core idea presented in this paper. Returning to the example of temperature and altitude: when presented with out-of-distribution data from the Netherlands, we expect the correct model to adapt faster given a few transfer samples of actual weather data collected in the Netherlands. Analogous to the case of robustness, the adaptation speed can then be used to guide the inference of the true causal structure of the problem at hand, possibly along with other sources of signal about causal structure. Contributions. We first verify on synthetic data that the model that correctly captures the underlying causal structure adapts faster when presented with data sampled after a performing certain interventions on the true two-variable causal graph (which is unknown to the learner). This suggests that the adaptation speed can indeed function as score to assess how well the learner fits the underlying causal graph. We then use a smooth parameterization of the considered causal graph to directly optimize this score in an end-to-end manner. Finally, we show in a simple setting that the score can be exploited to disentangle the correct causal variables given an unknown mixture of the said variables. Although this paper focuses on the causal graph, the proposed objective is motivated by the more general question of discovering the underlying causal variables and their dependencies to explain the environment of the learner, and make it possible for that learner to plan appropriately under changes due to interventions, either from self or from another agent. The discovery of underlying explanatory variables has come under different names, in particular the notion of disentangling underlying variables (Bengio et al., 2013; Locatello et al., 2019) , and studied in the causal setting (Chalupka et al., 2015; 2017) and domain adaptation (Magliacane et al., 2018) . This paper is also related to meta-learning (Finn et al., 2017; Finn, 2018; Alet et al., 2018; Dasgupta et al., 2019) , to Bayesian structure learning (Koller & Friedman, 2009; Heckerman et al., 1995; Daly et al., 2011; Chickering, 2002b) , causal discovery (Pearl, 1995; Tian & Pearl, 2001; Pearl, 2009; Bareinboim & Pearl, 2016; Peters et al., 2017) and how non-stationarity makes causal discovery easier (Zhang et al., 2017) . Please see Appendix A for a longer discussion of related work. We have established, in very simple bivariate settings, that the rate at which a learner adapts to sparse changes in the distribution of observed data can be exploited to infer the causal structure, and disentangle the causal variables. This relies on the assumption that with the correct causal structure, those distributional changes are localized. We have demonstrated these ideas through theoretical results, as well as experimental validation. The source code for the experiments is available here: This work is only a first step in the direction of causal structure learning based on the speed of adaptation to modified distributions. On the experimental side, many settings other than those studied here should be considered, with different kinds of parametrizations, richer and larger causal graphs (see already Ke et al. (2019) based on a first version of this paper), or different kinds of optimization procedures. Also, more work needs to be done in exploring how the proposed ideas can be used to learn good representations in which the causal variables are disentangled. Scaling up these ideas would permit their application towards improving the way learning agents deal with non-stationarities, and thus improving sample complexity and robustness of these agents. An extreme view of disentangling is that the explanatory variables should be marginally independent, and many deep generative models (Goodfellow et al., 2016) , and Independent Component Analysis models (Hyvärinen et al., 2001; Hyvärinen et al., 2018) , are built on this assumption. However, the kinds of high-level variables that we manipulate with natural language are not marginally independent: they are related to each other through statements that are usually expressed in sentences (e.g. a classical symbolic AI fact or rule), involving only a few concepts at a time. This kind of assumption has been proposed to help discover relevant high-level representations from raw observations, such as the consciousness prior (Bengio, 2017) , with the idea that humans focus at any particular time on just a few concepts that are present to our consciousness. The work presented here could provide an interesting meta-learning approach to help learn such encoders outputting causal variables, as well as figure out how the resulting variables are related to each other. In that case, one should distinguish two important assumptions: the first one is that the causal graph is sparse, which a common assumption in structure learning (Schmidt et al., 2007) ; the second is that the changes in distributions are sparse, which is the focus of this work.",This paper proposes a meta-learning objective based on speed of adaptation to transfer distributions to discover a modular decomposition and causal variables.,Bareinboim & Pearl ; Chickering ; Peters ; Locatello et ; only one ; one ; Pearl ; Alet et al. ; Daly et al. ; Switzerland,many underlying causal variables ; parameters ; the assumption ; all other variables ; another agent ; the form ; shifts ; Magliacane ; graphs ; no causal meaning,Bareinboim & Pearl ; Chickering ; Peters ; Locatello et ; only one ; one ; Pearl ; Alet et al. ; Daly et al. ; Switzerland,"We propose to use a meta-learning objective that maximizes the speed of transfer on modified distribution to learn how to modularize acquired knowledge. In particular, we focus on how to factor a joint distribution into appropriate conditionals, consistent with the causal directions. We show that under this assumption of localized changes in causal mechanisms, the correct causal graph will tend to have only a few parameters with non-zero gradient, i.e. that need to be adapted. In addition to this, we consider how the same objective can discover the causal variables themselves, as a transformation of observed low-level variables with no causal meaning. We define",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"We propose a modification to traditional Artificial Neural Networks (ANNs), which provides the ANNs with new aptitudes motivated by biological neurons.   Biological neurons work far beyond linearly summing up synaptic inputs and then transforming the integrated information.   A biological neuron change firing modes accordingly to peripheral factors (e.g., neuromodulators) as well as intrinsic ones.   Our modification connects a new type of ANN nodes, which mimic the function of biological neuromodulators and are termed modulators, to enable other traditional ANN nodes to adjust their activation sensitivities in run-time based on their input patterns.   In this manner, we enable the slope of the activation function to be context dependent.   This modification produces statistically significant improvements in comparison with traditional ANN nodes in the context of Convolutional Neural Networks and Long Short-Term Memory networks. Artificial neural networks (ANNs), such as convolutional neural networks (CNNs) BID24 and long short-term memory (LSTM) cells BID14 , have incredible capabilities and are applied in a variety of applications including computer vision, natural language analysis, and speech recognition among others. Historically, the development of ANNs (e.g., network architectures and learning algorithms) has benefited significantly from collaborations with Psych-Neuro communities BID5 BID12 BID13 BID15 BID28 Turing, 1950; BID10 BID8 BID16 BID18 BID10 . The information processing capabilities of traditional ANN nodes are rather rigid when compared to the plasticity of real neurons. A typical traditional ANN node linearly integrate its input signals and run the integration through a transformation called an activation function, which simply takes in a scalar value and outputs another. Of the most popular Activation Functions are sigmoid BID32 , tanh BID19 and ReLU BID35 .Researchers have shown that it could be beneficial to deploy layer-/node-specific activation functions in a deep ANN BID4 BID40 BID9 BID11 BID0 . However, each ANN node is traditionally stuck with a fixed activation function once trained. Therefore, the same input integration will always produce the same output. This fails to replicate the amazing capability of individual biological neurons to conduct complex nonlinear mappings from inputs to outputs BID1 BID10 BID25 . In this study , we propose one new modification to ANN architectures by adding a new type of node, termed modulators, to modulate the activation sensitivity of the ANN nodes targeted by modulators (see FIG0 for examples). In one possible setting, a modulator and its target ANN nodes share the same inputs. The modulator maps the input into a modulation signal, which is fed into each target node. Each target node multiples its input integration by the modulator signal prior to transformation by its traditional activation function. Examples of neuronal principles that may be captured by our new modification include intrinsic excitability, diverse firing modes, type 1 and type 2 forms of firing rate integration, activity dependent facilitation and depression and, most notably, neuromodulation BID27 BID38 BID45 BID37 ).Our modulator is relevant to the attention mechanism BID23 BID34 , which dynamically restricts information pathways and has been found to be very useful in practice. Attention mechanisms apply the attention weights, which are calculated in run-time, to the outputs of ANN nodes or LSTM cells. Notably, the gating mechanism in a Simple LSTM cell can also be viewed as a dynamical information modifier. A gate takes the input of the LSTM cell and outputs gating signals for filtering the outputs of its target ANN nodes in the same LSTM cell. A similar gating mechanism was proposed in the Gated Linear Unit BID6 for CNNs. Different from the attention and gating mechanisms , which are applied to the outputs of the target nodes, our modulation mechanism adjusts the sensitivities of the target ANN nodes in run-time by changing the slopes of the corresponding activation functions. Hence, the modulator can also be used as a complement to the attention and gate mechanisms.Below we will explain our modulator mechanism in detail. Experimentation shows that the modulation mechanism can help achieve better test stability and higher test performance using easy to implement and significantly simpler models. Finally, we conclude the paper with discussions on the relevance to the properties of actual neurons. We propose a modulation mechanism addition to traditional ANNs so that the shape of the activation function can be context dependent. Experimental results show that the modulated models consistently outperform their original versions. Our experiment also implied adding modulator can reduce overfitting. We demonstrated even with fewer parameters, the modulated model can still perform on par with it vanilla version of a bigger size. This modulation idea can also be expanded to other setting, such as, different modulator activation or different structure inside the modulator. It was frequently observed in preliminary testing that arbitrarily increasing model parameters actually hurt network performance, so future studies will be aimed at investigating the relationship between the number of model parameters and the performance of the network. Additionally, it will be important to determine the interaction between specific network implementations and the ideal Activation Function wrapping for slope-determining neurons. Lastly, it may be useful to investigate layer-wide single-node modulation on models with parallel LSTM's.Epigenetics refers to the activation and inactivation of genes BID46 , often as a result of environmental factors. These changes in gene-expression result in modifications to the generation and regulation of cellular proteins, such as ion channels, that regulate how the cell controls the flow of current through the cell membrane BID29 . The modulation of these proteins will strongly influence the tendency of a neuron to fire and hence affect the neurons function as a single computational node. These proteins, in turn, can influence epigenetic expression in the form of dynamic control BID20 .Regarding the effects of these signals, we can compare the output of neurons and nodes from a variety of perspectives. First and foremost, intrinsic excitability refers to the ease with which a neurons electrical potential can increase, and this feature has been found to impact plasticity itself BID7 . From this view, the output of a node in an artificial neural network would correspond to a neurons firing rate, which Intrinsic Excitability is a large contributor to, and our extra gate would be setting the node's intrinsic excitability. With the analogy of firing rate, another phenomenon can be considered. Neurons may experience various modes of information integration, typically labeled Type 1 and Type 2. Type 1 refers to continuous firing rate integration, while Type 2 refers to discontinuous information BID42 . This is computationally explained as a function of interneuron communication resulting in neuron-activity nullclines with either heavy overlap or discontinuous saddle points BID33 . In biology , a neuron may switch between Type 1 and Type 2 depending on the presence of neuromodulator BID41 . Controlling the degree to which the tanh function encodes to a binary space, our modification may be conceived as determining the form of information integration. The final possible firing rate equivalence refers to the ability of real neurons to switch between different firing modes. While the common mode of firing, Tonic firing, generally encodes information in rate frequency, neurons in a Bursting mode (though there are many types of bursts) tend to encode information in a binary mode -either firing bursts or not BID42 . Here too, our modification encompasses a biological phenomenon by enabling the switch between binary and continuous information.Another analogy to an ANN nodes output would be the neurotransmitter released. With this view, our modification is best expressed as an analogy to Activity Dependent Facilitation and Depression, phenomena which cause neurons to release either more or less neurotransmitter. Facilitation and depression occur in response to the same input: past activity BID36 . Our modification enables a network to use previous activity to determine its current sensitivity to input, allowing for both Facilitation and Depression. On the topic of neurotransmitter release , neuromodulation is the most relevant topic to the previously shown experiments. Once again, BID25 explains the situation perfectly, expressing that research BID2 BID3 has shown ""the same neuron or circuit can exhibit different input-output responses depending on a global circuit state, as reflected by the concentrations of various neuromodulators"". Relating to our modification, the slope of the activation function may be conceptualized as the mechanism of neuromodulation, with the new gate acting analogously to a source of neuromodulator for all nodes in the network.Returning to a Machine Learning approach, the ability to adjust the slope of an Activation Function has an immediate benefit in making the back-propagation gradient dynamic. For example, for Activations near 0, where the tanh Function gradient is largest, the effect of our modification on node output is minimal. However, at this point, our modification has the ability to decrease the gradient, perhaps acting as pseudo-learning-rate. On the other hand, at activations near 1 and -1, where the tanh Function gradient reaches 0, our modification causes the gradient to reappear, allowing for information to be extracted from inputs outside of the standard range. Additionally, by implementing a slope that is conditional on node input, the node has the ability to generate a wide range of functional Activation Functions, including asymmetric functions. Lastly, injecting noise has been found to help deep neural networks with noisy datasets BID47 , which is noteworthy since noise may act as a stabilizer for neuronal firing rates, BID43 . With this in mind, TAB2 .2 demonstrates increased clustering in two-dimensional node-Activation space, when the Activation Function slope is made to be dynamic. This indicates that noise may be a mediator of our modification, improving network performance through stabilization, induced by increasing the variability of the input-output relationship.In summary, we have shown evidence that nodes in LSTMs and CNNs benefit from added complexity to their input-output dynamic. Specifically, having a node that adjusts the slope of the main layer's nodes' activation functions mimics the functionality of neuromodulators and is shown to benefit the network. The exact mechanism by which this modification improves network performance remains unknown, yet it is possible to support this approach from both a neuroscientific and machine-learning perspective. We believe this demonstrates the need for further research into discovering novel non-computationally-demanding methods of applying principles of neuroscience to artificial networks. 6 APPENDIX 6.1 SUPPLEMENTARY DATA METHODOLOGY Additionally we tested our modulator gate, with τ l (·) set to sigmoid, on a much more computationally demanding three-layered LSTM network with weight drop method named awd-lstm-lm BID30 BID31 . This model was equipped to handle the Penn-Treebank dataset BID26 and was trained to minimize word perplexity. The network was trained for 500 epochs, however, the sample size was limited due to extremely long training times. On the Penn-Treebank dataset with the awd-lstm-lm implementation , sample size was restricted to 2 per condition, due to long training times and limited resources. However on the data collected, our model outperformed template perplexity, achieving an average of 58.4730 compared to the template average 58.7115. Due to the lack of a control for model parameters, interpretation of these results rests on the assumption that the author fine-tuned network parameters such that the template parameters maximized performance.",We propose a modification to traditional Artificial Neural Networks motivated by the biology of neurons to enable the shape of the activation function to be context dependent.,ANN ; Tonic ; Activity Dependent Facilitation and Depression ; one ; Machine Learning ; Artificial Neural Networks ; ReLU ; Convolutional Neural Networks ; two ; Activation Function,our model ; neuromodulators ; the ANN nodes ; the relationship ; network architectures ; Facilitation ; a transformation ; a neurons firing rate ; asymmetric functions ; LSTM cells,ANN ; Tonic ; Activity Dependent Facilitation and Depression ; one ; Machine Learning ; Artificial Neural Networks ; ReLU ; Convolutional Neural Networks ; two ; Activation Function,"We propose a modification to traditional Artificial Neural Networks (ANNs) that provides the ANNs with new aptitudes motivated by biological neurons.  Biological neurons work far beyond linearly summing up synaptic inputs and then transforming the integrated information.   A biological neuron can alter firing modes accordingly to peripheral factors as well as intrinsic ones.  Our modification connects a new type of ANN nodes, which mimic the function of biological neuromodulators and are termed modulators, to enable other traditional ANN nodes to adjust their activation sensitivities in run-time based on their input patterns.  In this manner, the slope of the activation function",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Reinforcement learning (RL) has led to increasingly complex looking behavior in recent years. However, such complexity can be misleading and hides over-fitting. We find that visual representations may be a useful metric of complexity, and both correlates well objective optimization and causally effects reward optimization. We then propose curious representation learning (CRL) which allows us to use better visual representation learning algorithms to correspondingly increase visual representation in policy through an intrinsic objective on both simulated environments and transfer to real images. Finally, we show better visual representations induced by CRL allows us to obtain better performance on Atari without any reward than other curiosity objectives. In recent years, reinforcement learning(RL) has lead to increasingly complex behavior from simulated environments (Silver et al., 2016; OpenAI, 2018; Mnih et al., 2013; Andrychowicz et al., 2018 ). Yet despite this, there lacks a quantitative measure of intelligence in these agents. Qualitative measures can be deceptive. Consider agent Alice and Bob in Minecraft. Alice is capable of a constructing a house while Bob appears to only be able to navigate around the world. While at face value it may then appear that Alice is more complex, upon closer inspection we may find that Alice has simply memorized a set of actions to construct a house in that particular environment! How can we be certain that our agents are not simply not memorizing a set of moves? One hypothesis is that the more intelligent an agent is, the more likely the inner representations in its policy will exhibit disentangled properties of the world. Towards this end, we investigate the emergent visual representations that occur in RL policies. We investigate on various objectives and environment conditions, and find that the quality of visual representation learning correlates well with progress in reward optimization. Similarily, we find improved visual representations help agents perform better reward optimization. Thus, another natural question to ask is, how can we enable our agents to have better visual representations? While there are ways to hardcode reward functions to enable agents perform well, can we come up with a generic objective that our agents can optimize that will directly lead them to have good representations? One idea towards this is to use recent work in curiosity. In curiosity, agents are typically given rewards corresponding to surprisal of state. But another view of curiosity is that of a minimax game where a curious agent is seeking to maximize the surprisal of an uncertainty model, while the uncertainty model seeks become less surprised about new states. Thus, to enable a policy to learn good visual representations, we can treat the uncertainty model as a representation learning model. We then seek a policy that wants to lower the loss of the representation learning objective, while the model itself tries to optimize this loss. Under this objective, a policy must learn good visual representations, so that it is able to find visually surprising inputs for the vision model. We call this overall objective, Curious Representation Learning (CRL). By coupling policy learning with representation learning, we find that CRL allows us to get better policy visual representations simply by applying better visual representation learning algorithms to the model. As a result, we find that CRL obtains consistently good representations in policies across environment size and type, often beating many hard-coded domain specific objectives. As an added bonus, we find that CRL is also able to achieve better visual representation learning than other data collection methods, as it actively sees diverse inputs that surprise it. In this paper, we have shown visual representations correspond and help reward optimization. Motivated by this insight, we propose a new method, CRL, that allows us to get improved visual representations in policies through better visual representations in model. We further illustrate that these better visual representation can provide incentives to explore more in no reward scenarios. We hope that our results will inspire further exploration on both better visual representation learning models/policies and better reward optimization. We further show nearest neighbor images on VizDoom in Figure 8 . The leftmost column is the query image while the other 4 columns are the 4 nearest neighbors in embedding space. Training through CRL allows clustering of various doom objects.",We present a formulation of curiosity as a visual representation learning problem and show that it allows good visual representations in agents.,Andrychowicz ; CRL ; Minecraft ; Mnih ; Curious Representation Learning ; VizDoom ; al. ; recent years ; Atari ; Alice,its policy ; state ; a useful metric ; various objectives ; environment size ; One hypothesis ; Qualitative measures ; Andrychowicz ; policy ; better performance,Andrychowicz ; CRL ; Minecraft ; Mnih ; Curious Representation Learning ; VizDoom ; al. ; recent years ; Atari ; Alice,"Reinforcement learning (RL) has led to increasingly complex looking behavior in recent years. However, such complexity can be misleading and hides over-fitting. We propose curious representation learning (CRL), which enables us to correspondingly increase visual representation in policy through an intrinsic objective on both simulated environments and transfer to real images. Additionally, we show better visual representations induced by CRL, enabling us to obtain better performance on Atari without any reward than other curiosity objectives. In recent years, reinforcement learning(RL) led to more complex behavior from simulated environments, leading to increasingly complicated behavior from agents. Quantitative measures can be deceptive,",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Depthwise separable convolutions reduce the number of parameters and computation used in convolutional operations while increasing representational efficiency.
 They have been shown to be successful in image classification models, both in obtaining better models than previously possible for a given parameter count (the Xception architecture) and considerably reducing the number of parameters required to perform at a given level (the MobileNets family of architectures). Recently, convolutional sequence-to-sequence networks have been applied to machine translation tasks with good results. In this work, we study how depthwise separable convolutions can be applied to neural machine translation. We introduce a new architecture inspired by Xception and ByteNet, called SliceNet, which enables a significant reduction of the parameter count and amount of computation needed to obtain results like ByteNet, and, with a similar parameter count, achieves better results.
 In addition to showing that depthwise separable convolutions perform well for machine translation, we investigate the architectural changes that they enable: we observe that thanks to depthwise separability, we can increase the length of convolution windows, removing the need for filter dilation. We also introduce a new super-separable convolution operation that further reduces the number of parameters and computational cost of the models. In recent years, sequence-to-sequence recurrent neural networks (RNNs) with long short-term memory (LSTM) cells BID7 have proven successful at many natural language processing (NLP) tasks, including machine translation BID18 BID2 BID4 . In fact, the results they yielded have been so good that the gap between human translations and machine translations has narrowed significantly BID23 and LSTM-based recurrent neural networks have become standard in natural language processing.Even more recently, auto-regressive convolutional models have proven highly effective when applied to audio BID19 , image BID20 ) and text generation BID11 . Their success on sequence data in particular rivals or surpasses that of previous recurrent models BID11 BID6 . Convolutions provide the means for efficient non-local referencing across time without the need for the fully sequential processing of RNNs. However, a major critique of such models is their computational complexity and large parameter count. These are the principal concerns addressed within this work: inspired by the efficiency of depthwise separable convolutions demonstrated in the domain of vision, in particular the Xception architecture BID5 and MobileNets (Howard et al., 2017) , we generalize these techniques and apply them to the language domain, with great success. In this work, we introduced a new convolutional architecture for sequence-to-sequence tasks, called SliceNet, based on the use of depthwise separable convolutions. We showed how this architecture achieves results beating not only ByteNet but also the previous best Mixture-of-Experts models while using over two times less (non-embedding) parameters and floating point operations than ByteNet.Additionally, we have shown that filter dilation, previously thought to be a key component of successful convolutional sequence-to-sequence architectures, was not a requirement. The use of depthwise separable convolutions makes much larger convolution window sizes possible, and we found that we could achieve the best results by using larger windows instead of dilated filters. We have also introduced a new type of depthwise separable convolution, the super-separable convolution, which shows incremental performance improvements over depthwise separable convolutions.Our work is one more point on a significant trendline started with Xception and MobileNets, that indicates that in any convolutional model, whether for 1D or 2D data, it is possible to replace convolutions with depthwise separable convolutions and obtain a model that is simultaneously cheaper to run, smaller, and performs a few percentage points better. This trend is backed by both solid theoretical foundations and strong experimental results. We expect our current work to play a significant role in affirming and accelerating this trend. We only experimented on translation, but we expect that our results will apply to other sequence-to-sequence tasks and we hope to see depthwise separable convolutions replace regular convolutions in more and more use cases in the future.",Depthwise separable convolutions improve neural machine translation: the more separable the better.,Xception and ByteNet ; SliceNet ; ByteNet ; recent years ; NLP ; Howard et al. ; over two ; one ; Xception and MobileNets,a requirement ; Experts ; vision ; They ; sequence ; regular convolutions ; which ; recent years ; better models ; machine translations,Xception and ByteNet ; SliceNet ; ByteNet ; recent years ; NLP ; Howard et al. ; over two ; one ; Xception and MobileNets,"Depthwise separable convolutions reduce the parameters and computation used in convolutional operations while increasing representational efficiency. They have been successful in image classification models, both in obtaining better models than previously possible for a given parameter count and considerably reducing the number of parameters required to perform at a given level. In recent years, sequence-to-sequence recurrent neural networks (RNNs) with long short-term memory (LSTM) cells BID7 have proven successful at natural language processing tasks. However, a major critique of such models is their computational complexity and large parameter count. In this work, we examine how depth",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"A key problem in neuroscience and life sciences more generally is that the data generation process is often best thought of as a hierarchy of dynamic systems. One example of this is in-vivo calcium imaging data, where observed calcium transients are driven by a combination of electro-chemical kinetics where hypothesized trajectories around manifolds determining the frequency of these transients. A recent approach using sequential variational auto-encoders demonstrated it was possible to learn the latent dynamic structure of reaching behaviour from spiking data modelled as a Poisson process. Here we extend this approach using a ladder method to infer the spiking events driving calcium transients along with the deeper latent dynamic system. We show strong performance of this approach on a benchmark synthetic dataset against a number of alternatives. In-vivo two-photon calcium imaging provides systems neuroscientists with the ability to observe the activity of hundreds of neurons simultaneously during behavioural experiments. Such highdimensional data is ripe for techniques identifying low-dimensional latent factors driving neural dynamics. The most common methods, such as principal components analysis, ignore non-linearity and temporal dynamics in brain activity. Pandarinath et al. (2018) [1] developed a new technique using deep, recurrent, variational auto-encoders which they named Latent Factor Analysis via Dynamical Systems (LFADS). Using LFADS they found non-linear, dynamic latent variables describing highdimensional activity in the motor cortex that can decode reaching behaviour with much higher fidelity than other methods. However, LFADS was designed for application to spiking data recorded from extracellular electrodes, not for two-photon calcium imaging data. Two-photon calcium imaging poses the additional problem of identifying latent spike trains in fluorescence traces. If we continue to model the frequency of events as being generated by a Poisson process, this can be seen as hierarchy of dynamic systems (Fig 1A) , in which low dimensional dynamics generate spike train probabilities that drive fluctuations in biophysical dynamics of calcium activity (Fig 1B. Here we propose a method that extends LFADS to accommodate calcium activity using this hierarchical dynamic systems approach.",We propose an extension to LFADS capable of inferring spike trains to reconstruct calcium fluorescence traces using hierarchical VAEs.,One ; electro-chemical kinetics ; Poisson ; two ; hundreds ; Latent Factor Analysis via Dynamical Systems ; fidelity ; LFADS ; Fig,neurons ; neuroscience and life sciences ; two ; spike train probabilities ; this ; this approach ; a Poisson process ; A recent approach ; observed calcium transients ; which,One ; electro-chemical kinetics ; Poisson ; two ; hundreds ; Latent Factor Analysis via Dynamical Systems ; fidelity ; LFADS ; Fig,"The data generation process is often thought of as a hierarchy of dynamic systems. In-vivo calcium imaging data, observed calcium transients are driven by a combination of electro-chemical kinetics and hypothesized trajectories around manifolds determining the frequency of these transients. A recent approach using sequential variational auto-encoders demonstrated it was possible to learn the latent dynamic structure of reaching behaviour from spiking data modelled as a Poisson process. Here, we extend this approach using a ladder method to infer the spiking events drivingcium transients along with the deeper latent dynamic system. We show strong performance of this approach on",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Deep reinforcement learning has succeeded in sophisticated games such as Atari, Go, etc. Real-world decision making, however, often requires reasoning with partial information extracted from complex visual observations. This paper presents  Discriminative Particle Filter Reinforcement Learning (DPFRL), a new reinforcement learning framework for partial and complex observations. DPFRL encodes a differentiable particle filter with learned transition and observation models in a neural network, which allows for reasoning with partial observations over multiple time steps. While a standard particle filter relies on a generative observation model, DPFRL learns a discriminatively parameterized model that is training directly for decision making. We show that the discriminative parameterization results in significantly improved performance, especially for tasks with complex visual observations, because it circumvents the difficulty of modelling observations explicitly. In most cases, DPFRL outperforms state-of-the-art POMDP RL models in Flickering Atari Games, an existing POMDP RL benchmark, and in Natural Flickering Atari Games, a new, more challenging POMDP RL benchmark that we introduce. We further show that DPFRL performs well for visual navigation with real-world data. Deep Reinforcement Learning (DRL) has attracted significant interest with applications ranging from game playing (Mnih et al., 2013; Silver et al., 2017) to robot control and visual navigation Kahn et al., 2018; Savva et al., 2019) . However, more natural or real-world environments pose significant challenges for current DRL methods (Arulkumaran et al., 2017) , in part because they require (1) reasoning in a partially observable environment (2) reasoning with complex observations, e.g. visually rich images. For example, a robot navigating in a new environment has to (1) localize and plan a path having only partial information of the environment (2) extract the traversable space from image pixels, where the relevant geometric features are tightly coupled with irrelevant visual features, such as wall textures and lighting. Decision making under partial observability can be formulated as a partially observable Markov decision process (POMDP). Solving POMDPs requires tracking the posterior distribution of the states, called the belief. Most POMDP RL methods track the belief, represented as a vector, using a recurrent neural network (RNN) (Hausknecht & Stone, 2015; Zhu et al., 2018) . RNNs are model-free generic function approximators, and without appropriate structural priors they may need large amounts of data to learn to track a complex belief. Model-based DRL methods aim to reduce the sample complexity by learning an environment model simultaneously with the policy. In particular, to deal with partial observability, recently proposed DVRL that learns a generative observation model incorporated into the policy through a Bayes filter. Because the Bayes filter tracks the belief explicitly, DVRL performs much better than generic RNNs under partial observability. However, a Bayes filter normally assumes a generative observation model, that defines the probability p(o | h t ) of receiving an observation o = o t given the history h t of past observations and actions (Fig. 1b ). Learning this model can be very challenging since the strong generative assumption requires modeling the whole observation space, including features irrelevant for RL. When o t is an image, p(o | h t ) is a distribution over all possible images, e.g., parameterized by independent pixel-wise Gaussians with learned mean and variance. This means, e.g., to navigate in a previously unseen environment, we need to learn the < l a t e x i t s h a 1 _ b a s e 6 4 = "" r o 4 2 6 D H M J G U u G 8 x G G s K s G W z i 0 / s = "" > A A A B 7 X i c b Z B N S w M x E I Z n 6 1 e t X 1 W P X o J F q J e y K 4 I e i 1 4 8 V r D b Q r u U b J p t Y 7 P Z J Z k V S u l / 8 O J B E a / + H 2 / + G 9 N 2 D 9 r 6 Q u D h n R k y 8 4 a p F A Z d 9 9 s p r K 1 v b G 4 V t 0 s 7 u 3 v 7 B + X D I 9 8 k m W a 8 y R K Z 6 H Z I D Z d C 8 S Y K l L y d a k 7 j U P J W O L q d 1 V t P X B u R q A c c p z y I 6 U C J S D C K 1 v L 9 a t j D 8 1 6 5 4 t b c u c g q e D l U I F e j V / 7 q 9 h O W x V w h k 9 S Y j u e m G E y o R s E k n 5 a 6 m e E p Z S M 6 4 B 2 L i s b c B J P 5 t l N y Z p 0 + i R J t n 0 I y d 3 9 P T G h s z D g O b W d M c W i W a z P z v 1 o n w + g 6 m A i V Z s g V W 3 w U Z Z J g Q m a n k 7 7 Q n K E c W 6 B M C 7 s r Y U O q K U M b U M m G 4 C 2 f v A r + R c 2 z f H 9 Z q d / k c R T h B E 6 h C h 5 c Q R 3 u o A F N Y P A I z / A K b 0 7 i v D j v z s e i t e D k M 8 f w R 8 7 n D 7 8 m j p I = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = "" r o 4 2 6 D H M J G U u G 8 x G G s K s G W z i 0 / s = "" > A A A B 7 X i c b Z B N S w M x E I Z n 6 1 e t X 1 W P X o J F q J e y K 4 I e i 1 4 8 V r D b Q r u U b J p t Y 7 P Z J Z k V S u l / 8 O J B E a / + H 2 / + G 9 N 2 D 9 r 6 Q u D h n R k y 8 4 a p F A Z d 9 9 s p r K 1 v b G 4 V t 0 s 7 u 3 v 7 B + X D I 9 8 k m W a 8 y R K Z 6 H Z I D Z d C 8 S Y K l L y d a k 7 j U P J W O L q d 1 V t P X B u R q A c c p z y I 6 U C J S D C K 1 v L 9 a t j D 8 1 6 5 4 t b c u c g q e D l U I F e j V / 7 q 9 h O W x V w h k 9 S Y j u e m G E y o R s E k n 5 a 6 m e E p Z S M 6 4 B 2 L i s b c B J P 5 t l N y Z p 0 + i R J t n 0 I y d 3 9 P T G h s z D g O b W d M c W i W a z P z v 1 o n w + g 6 m A i V Z s g V W 3 w U Z Z J g Q m a n k 7 7 Q n K E c W 6 B M C 7 s r Y U O q K U M b U M m G 4 C 2 f v A r + R c 2 z f H 9 Z q d / k c R T h B E 6 h C h 5 c Q R 3 u o A F N Y P A I z / A K b 0 7 i v D j v z s e i t e D k M 8 f w R 8 7 n D 7 8 m j p I = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = "" r o 4 2 6 D H M J G U u G 8 x G G s K s G W z i 0 / s = "" > A A A B 7 X i c b Z B N S w M x E I Z n 6 1 e t X 1 W P X o J F q J e y K 4 I e i 1 4 8 V r D b Q r u U b J p t Y 7 P Z J Z k V S u l / 8 O J B E a / + H 2 / + G 9 N 2 D 9 r 6 Q u D h n R k y 8 4 a p F A Z d 9 9 s p r K 1 v b G 4 V t 0 s 7 u 3 v 7 B + X D I 9 8 k m W a 8 y R K Z 6 H Z I D Z d C 8 S Y K l L y d a k 7 j U P J W O L q d 1 V t P X B u R q A c c p z y I 6 U C J S D C K 1 v L 9 a t j D 8 1 6 5 4 t b c u c g q e D l U I F e j V / 7 q 9 h O W x V w h k 9 S Y j u e m G E y o R s E k n 5 a 6 m e E p Z S M 6 4 B 2 L i s b c B J P 5 t l N y Z p 0 + i R J t n 0 I y d 3 9 P T G h s z D g O b W d M c W i W a z P z v 1 o n w + g 6 m A i V Z s g V W 3 w U Z Z J g Q m a n k 7 7 Q n K E c W 6 B M C 7 s r Y U O q K U M b U M m G 4 C 2 f v A r + R c 2 z f H 9 Z q d / k c R T h B E 6 h C h 5 c Q R 3 u o A F N Y P A I z / A K b 0 7 i v D j v z s e i t e D k M 8 f w R 8 7 n D 7 8 m j p I = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = "" r o 4 2 6 D H M J G U u G 8 x G G s K s G W z i 0 / s = "" > A A A B 7 X i c b Z B N S w M x E I Z n 6 1 e t X 1 W P X o J F q J e y K 4 I e i 1 4 8 V r D b Q r u U b J p t Y 7 P Z J Z k V S u l / 8 O J B E a / + H 2 / + G 9 N 2 D 9 r 6 Q u D h n R k y 8 4 a p F A Z d 9 9 s p r K 1 v b G 4 V t 0 s 7 u 3 v 7 B + X D I 9 8 k m W a 8 y R K Z 6 H Z I D Z d C 8 S Y K l L y d a k 7 j U P J W O L q d 1 V t P X B u R q A c c p z y I 6 U C J S D C K 1 v L 9 a t j D 8 1 6 5 4 t b c u c g q e D l U I F e j V / 7 q 9 h O W x V w h k 9 S Y j u e m G E y o R s E k n 5 a 6 m e E p Z S M 6 4 B 2 L i s b c B J P 5 t l N y Z p 0 + i R J t n 0 I y d 3 9 P T G h s z D g O b W d M c W i W a z P z v 1 o n w + g 6 m A i V Z s g V W 3 w U Z Z J g Q m a n k 7 7 Q n K E c W 6 B M C 7 s r Y U O q K U M b U M m G 4 C 2 f v A r + R c 2 z f H 9 Z q d / k c R T h B E 6 h C h 5 c Q R 3 u o A F N Y P A I z / A K b 0 7 i v D j v z s e i t e D k M 8 f w R 8 7 n D 7 8 m j p I = < / l a t e x i t > ⇡(b t ) < l a t e x i t s h a 1 _ b a s e 6 4 = "" d M D G r E g / u U t R q i s D I A Z M / + 5 R W i g = "" > A A A B 7 3 i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F q J e S i K D H o h e P F e w H t K F s t p t 2 6 W Y T d y d C C f 0 T X j w o 4 t W / 4 8 1 / 4 7 b N Q V t f W H h 4 Z 4 a d e Y N E C o O u + + 0 U 1 t Y 3 N r e K 2 6 W d 3 b 3 9 g / L h U c v E q W a 8 y W I Z 6 0 5 A D Z d C 8 S Y K l L y T a E 6 j Q P J 2 M L 6 d 1 d t P X B s R q w e c J N y P 6 F C J U D C K 1 u r 0 E l E N + n j e L 1 f c m j s X W Q U v h w r k a v T L X 7 1 B z N K I K 2 S S G t P 1 3 A T 9 j G o U T P J p q Z c a n l A 2 p k P e t a h o x I 2 f z f e d k j P r D E g Y a / s U k r n 7 e y K j 0 X l x 3 p 2 P R W v B y W e O 4 Y + c z x 9 g o Y + F < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = "" d M D G r E g / u U t R q i s D I A Z M / + 5 R W i g = "" > A A A B 7 3 i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F q J e S i K D H o h e P F e w H t K F s t p t 2 6 W Y T d y d C C f 0 T X j w o 4 t W / 4 8 1 / 4 7 b N Q V t f W H h 4 Z 4 a d e Y N E C o O u + + 0 U 1 t Y 3 N r e K 2 6 W d 3 b 3 9 g / L h U c v E q W a 8 y W I Z 6 0 5 A D Z d C 8 S Y K l L y T a E 6 j Q P J 2 M L 6 d 1 d t P X B s R q w e c J N y P 6 F C J U D C K 1 u r 0 E l E N + n j e L 1 f c m j s X W Q U v h w r k a v T L X 7 1 B z N K I K 2 S S G t P 1 3 A T 9 j G o U T P J p q Z c a n l A 2 p k P e t a h o x I 2 f z f e d k j P r D E g Y a / s U k r n 7 e y K j 0 X l x 3 p 2 P R W v B y W e O 4 Y + c z x 9 g o Y + F < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = "" d M D G r E g / u U t R q i s D I A Z M / + 5 R W i g = "" > A A A B 7 3 i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F q J e S i K D H o h e P F e w H t K F s t p t 2 6 W Y T d y d C C f 0 T X j w o 4 t W / 4 8 1 / 4 7 b N Q V t f W H h 4 Z 4 a d e Y N E C o O u + + 0 U 1 t Y 3 N r e K 2 6 W d 3 b 3 9 g / L h U c v E q W a 8 y W I Z 6 0 5 A D Z d C 8 S Y K l L y T a E 6 j Q P J 2 M L 6 d 1 d t P X B s R q w e c J N y P 6 F C J U D C K 1 u r 0 E l E N + n j e L 1 f c m j s X W Q U v h w r k a v T L X 7 1 B z N K I K 2 S S G t P 1 3 A T 9 j G o U T P J p q Z c a n l A 2 p k P e t a h o x I 2 f z f e d k j P r D E g Y a / s U k r n 7 e y K j l L y T a E 6 j Q P J 2 M L 6 d 1 d t P X B s R q w e c J N y P 6 F C J U D C K 1 u r 0 E l E N + n j e L 1 f c m j s X W Q U v h w r k a v T L X 7 1 B z N K I K 2 S S G t P 1 3 A T 9 j G o U T P J p q Z c a n l A 2 p k P e t a h o x I 2 f z f e d k j P r D E g Y a / s U k r n 7 e y K j distribution of all possible environments with their visual appearance, lighting condition, etc. -a much harder task than learning to extract features relevant to navigation, e.g. the traversable space. We introduce the Discriminative Particle Filter Reinforcement Learning (DPFRL), a POMDP RL method that learns to explicitly track a latent belief, while circumventing the difficulty of generative observation modeling, and learns to make decisions based on features of the latent belief (Fig. 1a) . DPFRL approximates the belief by a set of weighted learnable latent particles {(h , and it tracks the particle belief by a non-parametric Bayes filter algorithm, a particle filter, encoded as a differentiable computational graph in the neural network architecture. Transition and observation models for the particle filter are neural networks learned jointly end-to-end, optimized for the overall policy. Importantly, we use a discriminatively parameterized observation model, f obs (o t , h t ), a neural network that takes in o t and h t and outputs a single value, a direct estimate of the log-likelihood as shown in Fig. 1c . The discriminative parameterization relaxes the generative assumption and avoids explicitly modeling the entire complex observation space when computing observation likelihood. The intuition is similar to that of, e.g., energy-based models (LeCun et al., 2006) and contrastive predictive coding (Oord et al., 2018) , but here the learning signal comes directly from the RL objective, backpropagating through the differentiable particle filter, thus f obs (o t , h t ) only needs to model the observation features relevant to decision making. In addition, to summarize the particle belief, we introduce novel learnable features based on Moment-Generating Functions (MGFs) (Bulmer, 1979) . MGF features are computationally efficient and permutation invariant, and they can be directly optimized to provide useful higher-order moment information for learning the policy. MGF features could be also used as learned features of any empirical distribution in application beyond RL. We evaluate DPFRL on a range of POMDP RL domains: a continuous control task from , Flickering Atari Games (Hausknecht & Stone, 2015) , Natural Flickering Atari Games, a new domain with more complex observations that we introduce, and the Habitat visual navigation domain using real-world data (Savva et al., 2019) . DPFRL outperforms state-of-the-art POMDP RL methods in most cases. Results show that the particle filter structure is effective for handling partial observations, and the discriminative parameterization allows for complex observations. We summarize our contributions as follows: 1) a differentiable particle filter based method with a discriminatively parameterized observation model for RL with partial and complex observations. 2) effective MGF features for empirical distributions, e.g., particle distributions 3) a new RL benchmark, Natural Flickering Atari Games, that introduces both partial observability and complex visual observations to the popular Atari domain. We will open source the code to enable future work. We have introduced DPFRL, a principled framework for POMDP RL in natural environments. DPFRL combines the strength of Bayesian filtering and end-to-end RL: it performs explicit belief tracking with discriminative learnable particle filters optimized directly for the RL policy. DPFRL achieved state-of-the-art results on POMDP RL benchmarks from prior work, Mountain Hike and a number of Flickering Atari Games, and it significantly outperformed alternative methods in a new, more challenging domain, Natural Flickering Atari Games, as well as for visual navigation using real-world data. We have proposed a novel MGF feature for extracting statistics from an empirical distribution. MGF feature extraction could be applied beyond RL, e.g., for general sequence prediction. DPFRL does not perform well in some particular cases, e.g., DoubleDunk. While our discriminatively parameterized observation function is less susceptible to observation noise, it does not allow for additional learning signals that improve sample efficiency, e.g., through a reconstruction loss. Future work may combine generative and discriminative modeling with the principled DPFRL framework.","We introduce DPFRL, a framework for reinforcement learning under partial and complex observations with a fully differentiable discriminative particle filter",DRL ; P ; A F N ; RNN ; P F e w H t K F s t ; Flickering Atari Games ; Savva et al. ; Silver et al. ; MGF ; Fig,the history ; y d C C ; Discriminative Particle Filter Reinforcement Learning ; independent pixel-wise Gaussians ; MGFs ; a Bayes filter ; Decision ; Hausknecht & Stone ; the-art ; robot control,DRL ; P ; A F N ; RNN ; P F e w H t K F s t ; Flickering Atari Games ; Savva et al. ; Silver et al. ; MGF ; Fig,"Deep reinforcement learning has succeeded in sophisticated games such as Atari, Go, etc. However, real-world decision making often requires reasoning with partial information extracted from complex visual observations. Discriminative Particle Filter Reinforcement Learning (DPFRL) encodes a differentiable particle filter with learned transition and observation models in a neural network, which allows reasoning over multiple time steps. While a standard particle filter relies on a generative observation model, DPFRL learns a discriminatively parameterized model that is training directly for decision making. The discriminative parameterization results in significantly improved performance, especially for tasks with complex visual",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"The problem of accelerating drug discovery relies heavily on automatic tools to optimize precursor molecules to afford them with better biochemical properties. Our work in this paper substantially extends prior state-of-the-art on graph-to-graph translation methods for molecular optimization. In particular, we realize coherent multi-resolution representations by interweaving the encoding of substructure components with the atom-level encoding of the original molecular graph. Moreover, our graph decoder is fully autoregressive, and interleaves each step of adding a new substructure with the process of resolving its attachment to the emerging molecule. We evaluate our model on multiple molecular optimization tasks and show that our model significantly outperforms previous state-of-the-art baselines. Molecular optimization seeks to modify compounds in order to improve their biochemical properties. This task can be formulated as a graph-to-graph translation problem analogous to machine translation. Given a corpus of molecular pairs {(X, Y )}, where Y is a paraphrase of X with better chemical properties, the model is trained to translate an input molecular graph into its better form. The task is difficult since the space of potential candidates is vast, and molecular properties can be complex functions of structural features. Moreover, graph generation is computationally challenging due to complex dependencies involved in the joint distribution over nodes and edges. Similar to machine translation, success in this task is predicated on the inductive biases built into the encoder-decoder architecture, in particular the process of generating molecular graphs. Prior work (Jin et al., 2019) proposed a junction tree encoder-decoder that utilized valid chemical substructures (e.g., aromatic rings) as building blocks to generate graphs. Each molecule was represented as a junction tree over chemical substructures in addition to the original atom-level graph. While successful, the approach remains limited in several ways. The tree and graph encoding were carried out separately, and decoding proceeded in strictly successive steps: first generating the junction tree for the new molecule, and then attaching its substructures together. This means the predicted attachments do not impact the subsequent substructure choices (see Figure 1a) . Moreover, the attachment prediction process is non-autoregressive, thus it can predict inconsistent substructure attachments across different nodes in the junction tree (see Figure 1b ). We propose a multi-resolution, hierarchically coupled encoder-decoder for graph generation. Our auto-regressive decoder interleaves the prediction of substructure components with their attachments to the molecule being generated. In particular, a target graph is unraveled as a sequence of triplet predictions (where to expand the graph, new substructure type, its attachment). This enables us to model strong dependencies between successive attachments and substructure choices. The encoder is designed to represent molecules at different resolutions in order to match the proposed decoding process. Specifically, the encoding of each molecule proceeds across three levels, with each layer capturing essential information for its corresponding decoding step. The graph convolution of atoms at the lowest level supports the prediction of attachments and the convolution over substructures at the highest level supports the prediction of successive substructures. Compared to prior work, our decoding process is much more efficient because it decomposes each generation step into a hierarchy of smaller steps in order to avoid combinatorial explosion. We also extend the method to handle conditional translation where desired criteria are fed as input to the translation process. This enables our method to handle different combinations of criteria at test time. Since their tree and graph decoders are isolated, the model can generate invalid junction trees which cannot be assembled into any molecule. This problem can be solved when we interleave the tree and graph decoding steps, allowing the predicted attachments to guide the substructure prediction; b) Their non-autoregressive graph decoder often predicts inconsistent local substructure attachments during training. To this end, we propose an autoregressive decoder that interleaves the prediction of substructures with their attachments. We evaluate our new model on multiple molecular optimization tasks. Our baselines include previous state-of-the-art graph generation methods (You et al., 2018a; Liu et al., 2018; Jin et al., 2019) and an atom-based translation model we implemented for a more comprehensive comparison. Our model significantly outperforms these methods in discovering molecules with desired properties, yielding 3.3% and 8.1% improvement on QED and DRD2 optimization tasks. During decoding, our model runs 6.3 times faster than previous substructure-based generation methods. We further conduct ablation studies to validate the advantage of our hierarchical decoding and multi-resolution encoding. Finally, we show that conditional translation can succeed (generalize) even when trained on molecular pairs with only 1.6% of them having desired target property combination. In this paper, we developed a hierarchical graph-to-graph translation model that generates molecular graphs using chemical substructures as building blocks. In contrast to previous work, our model is fully autoregressive and learns coherent multi-resolution representations. The experimental results show that our method outperforms previous models under various settings. A ADDITIONAL FIGURES The message passing network MPN ψ (H, {x u }, {x uv }) over graph H is defined as: Algorithm 3 LSTM MPN with T message passing iterations wu } w∈N (u)\v for all edges (u, v) ∈ H simultaneously. end for Return node representations end function Attention Layer Our attention layer is a bilinear attention function with parameter θ = {A θ }: Figure 7: Illustration of AtomG2G decoding process. Atoms marked with red circles are frontier nodes in the queue Q. In each step, the model picks the first node v t from Q and predict whether there will be new atoms attached to v t . If so, it predicts the atom type of new node u t (atom prediction). Then the model predicts the bond type between u t and other nodes in Q sequentially for |Q| steps (bond prediction, |Q| = 2). Finally, it adds the new atom to the queue Q. AtomG2G Architecture AtomG2G is an atom-based translation method that is directly comparable to HierG2G. Here molecules are represented solely as molecular graphs rather than a hierarchical graph with substructures. Table 3 : Training set size and substructure vocabulary size for each dataset.","We propose a multi-resolution, hierarchically coupled encoder-decoder for graph-to-graph translation.",Jin et al. ; first ; three ; fed ; al. ; Liu et al. ; Jin ; QED ; MPN,previous models ; its attachment ; The graph convolution ; v t ; Our attention layer ; order ; Jin ; desired properties ; Prior work ; the space,Jin et al. ; first ; three ; fed ; al. ; Liu et al. ; Jin ; QED ; MPN,"We extend prior state-of-the-art on graph-to-graph translation methods for molecular optimization. In particular, we realize coherent multi-resolution representations by interweaving substructure components with atom-level encoding of the original molecular graph. Furthermore, our graph decoder is fully autoregressive, and interleaves each step of adding a new substructure with the process of resolving its attachment to the emerging molecule. We evaluate our model on multiple molecular optimization tasks and show that our model significantly outperforms previous state of theart baselines. Molecular optimization seeks to modify compounds in order to improve their biochemical properties. This",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Self-supervision, in which a target task is improved without external supervision, has primarily been explored in settings that assume the availability of additional data. However, in many cases, particularly in healthcare, one may not have access to additional data (labeled or otherwise). In such settings, we hypothesize that self-supervision based solely on the structure of the data at-hand can help. We explore a novel self-supervision framework for time-series data, in which multiple auxiliary tasks (e.g., forecasting) are included to improve overall performance on a sequence-level target task without additional training data. We call this approach limited self-supervision, as we limit ourselves to only the data at-hand. We demonstrate the utility of limited self-supervision on three sequence-level classification tasks, two pertaining to real clinical data and one using synthetic data. Within this framework, we introduce novel forms of self-supervision and demonstrate their utility in improving performance on the target task. Our results indicate that limited self-supervision leads to a consistent improvement over a supervised baseline, across a range of domains. In particular, for the task of identifying atrial fibrillation from small amounts of electrocardiogram data, we observe a nearly 13% improvement in the area under the receiver operating characteristics curve (AUC-ROC) relative to the baseline (AUC-ROC=0.55 vs. AUC-ROC=0.62). Limited self-supervision applied to sequential data can aid in learning intermediate representations, making it particularly applicable in settings where data collection is difficult. Many problems involving sequential data, such as machine translation, sentiment analysis, and mortality prediction, are naturally framed as sequence-level tasks (Harutyunyan et al., 2017; Hassan et al., 2018; Radford et al., 2017) . Sequence-level tasks map a sequence of observations x 0:T to a single label y. Learning this mapping is often made challenging due to a high-D (dimension) low-N (number of samples) setting (Nasrabadi, 2007) . Such problems are particularly prevalent in healthcare tasks, which often involve limited quantities of labeled data captured at a high temporal resolution (e.g., electrocardiogram waveforms). In high-D low-N settings, researchers have had success with transfer learning techniques, by leveraging additional data to learn intermediate representations that are then used in the target task. When additional data are unavailable, it may be possible to improve the intermediate learned representation of the data with respect to the target task by considering additional tasks intrinsic to the data. In particular, we hypothesize that the structure of sequential data provides a rich source of innate supervision. For example, signal reconstruction or forecasting could improve the intermediate representation by capturing the underlying data-generating process. Such approaches are examples of self-supervision, where labels are derived from the input (as opposed to external sources). In this paper, we show that leveraging the sequential structure of the data at-hand can lead to improved performance on sequence-level tasks (i.e., the target task). More specifically, by considering self-supervised auxiliary tasks (e.g., signal reconstruction), in addition to the sequence-level task, one can learn useful intermediate representations of the data. Past work investigating self-supervision for sequential data has focused on full-signal reconstruction (Dai & Le, 2015) , and to a lesser extent forecasting (Ramachandran et al., 2016) . Building on past work, we examine the utility of self-supervision on sequential data when additional data are unavailable, and we propose new types of self-supervision tasks. We refer to this approach as 'limited self-supervision. ' We limit the self-supervision to the data at-hand, and focus on self-supervised auxiliary tasks relevant to sequential data ordered by time (i.e., time-series data). Our main contributions are as follows: • We demonstrate the efficacy of the proposed limited self-supervision framework for improving performance across datasets/tasks with no additional data. • We compare the utility of several different existing forms of self-supervision in our limiteddata setting, identify consistent trends across supervision types, and demonstrate the utility of combining multiple different forms of self-supervision. • We propose a new form of self-supervision, piecewise-linear autoencoding, that trades off fine-grained signal modeling and long-term dependency propagation. We demonstrate that this is the best form of limited self-supervision across all tasks. Our work suggests that there is a wide range of time-series and sequence classification tasks where limited self-supervision could improve performance. It also shows the value of including multiple, simultaneous streams of auxiliary self-supervision. Our findings present a methodological contribution, in the form of a useful new type of self-supervision, piecewise-linear autoencoding. Further, our empirical findings on when and how auxiliary tasks help can inform future work in developing self-supervision techniques. In this paper, we introduced a limited self-supervised framework, in which we sought to improve sequence-level task performance without additional data. By jointly training our target task with auxiliary self-supervised tasks, we demonstrated small but consistent improvements across three different sequence classification tasks. Our novel piecewise-linear autoencoding task emerged as the most useful auxiliary task across all datasets. In contrast, forecasting, which presents an intuitively appealing form of self-supervision, led to the smallest improvements. , continuing untilx 0:T is fully generated. To provide a shorter path for gradient flow, we decode in reverse order, generatinĝ x T :0 instead ofx 0:T (Goodfellow et al., 2016) .","We show that extra unlabeled data is not required for self-supervised auxiliary tasks to be useful for time series classification, and present new and effective auxiliary tasks.",Ramachandran ; Harutyunyan ; three ; Radford ; Nasrabadi ; AUC-ROC ; Dai & Le ; al. ; one ; two,a rich source ; healthcare ; Hassan et al. ; Our novel piecewise-linear autoencoding task ; forecasting ; data collection ; sequential data ; Our results ; limited self-supervision ; new types,Ramachandran ; Harutyunyan ; three ; Radford ; Nasrabadi ; AUC-ROC ; Dai & Le ; al. ; one ; two,"Self-supervision, in which a target task is improved without external supervision, has been explored in settings that assume additional data availability. However, in many cases, particularly in healthcare, one may not have access to additional data. In such settings, we explore a novel self-supervised framework for time-series data, where multiple auxiliary tasks are included to improve overall performance on a sequence-level target task without additional training data. This approach is similar to supervised baseline, but in healthcare settings, the data at-hand can help in learning intermediate representations. In this framework, we introduce novel forms, such as time series data",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"In recent years we have made significant progress identifying computational principles that underlie neural function. While not yet complete, we have sufficient evidence that a synthesis of these ideas could result in an understanding of how neural computation emerges from a combination of innate dynamics and plasticity, and which could potentially be used to construct new AI technologies with unique capabilities. I discuss the relevant principles, the advantages they have for computation, and how they can benefit AI. Limitations of current AI are generally recognized, but fewer people are aware that we understand enough about the brain to immediately offer novel AI formulations.","Limitations of current AI are generally recognized, but fewer people are aware that we understand enough about the brain to immediately offer novel AI formulations.",recent years ; AI,how neural computation ; we ; recent years ; the advantages ; significant progress ; sufficient evidence ; the brain ; that ; computational principles ; plasticity,recent years ; AI,"We have made significant progress identifying computational principles that underlie neural function, including innate dynamics and plasticity. A synthesis of these ideas could lead to new AI technologies with unique capabilities. In this article, I discuss the relevant principles, the advantages they have for computation, and how they can benefit AI. Limitations of current AI are generally recognized, but fewer people are aware that we understand enough about the brain to immediately offer novel AI formulations.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Dreams and our ability to recall them are among the most puzzling questions in sleep research. Specifically, putative differences in brain network dynamics between individuals with high versus low dream recall rates, are still poorly understood. In this study, we addressed this question as a classification problem where we applied deep convolutional networks (CNN) to sleep EEG recordings to predict whether subjects belonged to the high or low dream recall group (HDR and LDR resp.). Our model achieves significant accuracy levels across all the sleep stages, thereby indicating subtle signatures of dream recall in the sleep microstructure. We also visualized the feature space to inspect the subject-specificity of the learned features, thus ensuring that the network captured population level differences. Beyond being the first study to apply deep learning to sleep EEG in order to classify HDR and LDR, guided backpropagation allowed us to visualize the most discriminant features in each sleep stage. The significance of these findings and future directions are discussed.","We investigate the neural basis of dream recall using convolutional neural network and feature visualization techniques, like tSNE and guided-backpropagation.",CNN ; EEG ; HDR ; LDR ; first,HDR and LDR resp ; Dreams ; EEG ; the most discriminant features ; them ; we ; this study ; brain network dynamics ; the most puzzling questions ; the sleep microstructure,CNN ; EEG ; HDR ; LDR ; first,"Neural differences in brain network dynamics between individuals with high versus low dream recall rates are still poorly understood. In this study, we applied deep convolutional networks (CNN) to sleep EEG recordings to predict whether subjects belonged to the high or low recall group. Our model achieves significant accuracy levels across all the sleep stages, thereby indicating subtle signatures of dream recall in the sleep microstructure. We also visualized the feature space to inspect the subject-specificity of learned features, thus ensuring that the network captured population level differences. This study was the first study to apply deep learning to the sleep EEG in order to classify HDR and",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"We describe the use of an automated scheduling system for observation policy design and to schedule operations of the NASA (National Aeronautics and Space Administration) ECOSystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS). We describe the adaptation of the Compressed Large-scale Activity Scheduler and Planner (CLASP) scheduling system to the ECOSTRESS scheduling problem, highlighting multiple use cases for automated scheduling and several challenges for the scheduling technology: handling long-term campaigns with changing information, Mass Storage Unit Ring Buffer operations challenges, and orbit uncertainty. The described scheduling system has been used for operations of the ECOSTRESS instrument since its nominal operations start July 2018 and is expected to operate until mission end in Summer 2019. NASA's ECOSTRESS mission (NASA 2019) seeks to better understand how much water plants need and how they respond to stress. Two processes show how plants use water: transpiration and evaporation. Transpiration is the process of plants losing water through tiny pores in their leaves. Evaporation of water from the soil surrounding plants affects how much water the plants can use. ECOSTRESS measures the temperature of plants to understand combined evaporation and transpiration, known as evapotranspiration.ECOSTRESS launched on June 29, 2018 to the ISS (International Space Station) on a Space-X Falcon 9 rocket as part of a resupply mission. The instrument is attached to the Japanese Experiment Module Exposed Facility (JEM-EF) on the ISS and targets key biomes on the Earth's surface, as well as calibration/validation sites. Other science targets include cities and volcanoes. From the orbit of the Space Station FIG1 ), the instrument can see target regions at varying times throughout the day, rather than at a fixed time of day, allowing scientists to understand plant water use throughout the day.The instrument used for ECOSTRESS is a thermal infrared radiometer. A double-sided scan mirror, rotating at a constant 25.4 rpm, allows the telescope to view a 53°-wide nadir cross-track swath with one scan per 1.18 seconds. The nominal observation unit is a scene, made up of 44 scans, and takes roughly 52 seconds to acquire. For simplification of operations, we consider that ECOSTRESS scenes are 52 seconds long. About 1000 scenes may be acquired in a given week. FIG2 shows a set of planned observations over North America. Each square represents one 52-second long scene.CLASP BID4 was initially used prelaunch as a tool to analyze the addition of a new science campaign. CLASP was then used for operations to generate command sequences for the instrument. The command sequences are translated from the observation schedule generated by CLASP, and include other time and location dependent instrument actions besides observations, such as hardware power cycles through high radiation environments. Each mission comes with its own set of challenges, and there were three specifically that required adaptations to CLASP as follows.• ECOSTRESS has a long-term science campaign that we need to satisfy. From week to week, the orbital ephemeris can change, and thus the schedule needs to be updated each week. We need to be able to account for previously executed observations when scheduling for the future.• An issue with the instrument Mass Storage Unit (MSU) was discovered, and rather than performing an instrument firmware update, we proposed a ground-based solution that accounts for this additional complexity in the data modeling in the schedule.• The uncertainty in the orbital ephemeris (predictions of In the remainder of this paper, we describe these operational challenges and how we addressed them successfully. We also validate our methods used through computational analysis. This paper has described the use of an automated scheduling system in the analysis and operations for the ECOSTRESS mission. Changing orbital ephemeris and long-term campaign goals required adapting CLASP to consider past observations in scheduling for the future. The issue with the instrument ring buffer required scheduling with additional constraints, as well as scheduling another type of instrument activity. The uncertainty of the ISS orbital position required adapting how observations are scheduled. Through computational analysis we showed that our method for addressing the ring buffer approached the performance of schedules produced that did not have the added constraints, and that the second method of building observations up rather outperformed the method of adding a fixed amount of observational time to ensure no regions of interest were missed.",We describe the use of an automated scheduling system for observation policy design and to schedule operations of NASA's ECOSTRESS mission.,a given week ; National Aeronautics and Space Administration ; NASA ; one ; nadir ; Japanese ; three ; Planner ; ECOSTRESS ; Mass Storage Unit Ring Buffer,A double-sided scan mirror ; MSU ; Two ; the second method ; the adaptation ; the plants ; the soil ; NASA's ECOSTRESS mission ; automated scheduling ; that required adaptations,a given week ; National Aeronautics and Space Administration ; NASA ; one ; nadir ; Japanese ; three ; Planner ; ECOSTRESS ; Mass Storage Unit Ring Buffer,"The use of an automated scheduling system for observation policy design and schedule operations of the NASA (National Aeronautics and Space Administration) ECOSystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) is described as the adaptation of the Compressed Large-scale Activity Scheduler and Planner (CLASP) scheduling system to the ECOSTRESS scheduling problem, highlighting multiple use cases for automated scheduling and several challenges for scheduling technology, such as changing information, Mass Storage Unit Ring Buffer operations challenges, and orbit uncertainty. The described scheduling system has been used for operations since its nominal operations start July 2018",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"The phase problem in diffraction physics is one of the oldest inverse problems in all of science. The central difficulty that any approach to solving this inverse problem must overcome is that half of the information, namely the phase of the diffracted beam, is always missing. In the context of electron microscopy, the phase problem is generally non-linear and solutions provided by phase-retrieval techniques are known to be poor approximations to the physics of electrons interacting with matter. Here, we show that a semi-supervised learning approach can effectively solve the phase problem in electron microscopy/scattering. In particular, we introduce a new Deep Neural Network (DNN), Y-net, which simultaneously learns a reconstruction algorithm via supervised training in addition to learning a physics-based regularization via unsupervised training. We demonstrate that this constrained, semi-supervised approach is an order of magnitude more data-efficient and accurate than the same model trained in a purely supervised fashion. In addition, the architecture of the Y-net model provides for a straightforward evaluation of the consistency of the model's prediction during inference and is generally applicable to the phase problem in other settings. Advances in materials have shaped the course of human civilization from the bronze age to the silicon-powered information age. Future advances in materials science depend critically on our ability to determine, with atomic resolution (10 −10 m), a material's local electron density. This goal is within reach, for the first time, via a computational imaging technique commonly known as 4D-STEM (scanning transmission electron microscopy). Figure 1: 4D-STEM is a computational imaging technique, where a picometer-size beam is scanned across a material and a diffraction pattern is collected at each spatial location. The resultant dataset is 4-D dimensional, where each ""pixel"" is indexed by (x, y, k x , k y ), where k = α/λ, λ is the wavelength and α is the diffraction angle of the electron beam. Successful inversion of the 4D-STEM data should, in principle, provide local electron density maps of materials with higher spatial resolution and sensitivities than in another existing technique. In 4D-STEM, a picometer-sized (10 −12 m) electron beam is 2-D raster scanned across the material to collect a diffraction pattern with picometer spatial resolutions from each (x, y) position (see Figure  1 ). The resultant dataset is 4-D dimensional, where each ""pixel"" is indexed by (x, y, k x , k y ), where (k x , k y ) are the wave-vectors associated with a diffracting beam. A 4D-STEM dataset encodes information about the material's electron density from the vantage point of a single atom. Decoding electron diffraction patterns into the local electronic density is a longstanding inverse problem for two principal reasons Zuo & Spence (2013) . First, the quantum interaction of electrons with matter is strong, which produces numerous interference processes and the resultant inverse problem in non-linear. Second, diffraction patterns provide incomplete data, since they only provide intensities as opposed to the complex-valued diffracted electron wavefunction. Here, we introduce a semi-supervised and physics-constrained Deep Neural Network (DNN) to solve the phase problem in 4D-STEM, thereby reconstructing both the local electron density and the incident electron beam wavefunction. Estimating both of these quantities allows one to a posteriori quantify the reconstruction error. We also discuss how our approach is naturally extensible via differentiable programming. In summary, we found that by combining supervised and unsupervised learning to train a new DNN architecture, we can learn both a solution to an inverse problem as well as learn a physics-based regularization, leading to vastly improved reconstruction quality and data efficiency. We are currently extending the presented framework by substituting the learnable regularization with the full forward model in Equation 1 using techniques from differentiable programming.",We introduce a semi-supervised deep neural network to approximate the solution of the phase problem in electron microscopy,one ; half ; Deep Neural Network ; first ; two ; Zuo & Spence ; non-linear ; Second,this inverse problem ; poor approximations ; first ; the vantage point ; our ability ; the full forward model ; namely the phase ; Y-net ; a new DNN architecture ; principle,one ; half ; Deep Neural Network ; first ; two ; Zuo & Spence ; non-linear ; Second,"The phase problem in diffraction physics is one of the oldest inverse problems in science. It requires half of the information, namely the phase of the diffracted beam, to be missing. In contrast to electron microscopy, phase problems are non-linear and solutions provided by phase-retrieval techniques are poor approximations to the physics of electrons interacting with matter. A semi-supervised learning approach can effectively solve the phase problem. The new Deep Neural Network (DNN), Y-net, simultaneously learns a reconstruction algorithm via supervised training and a physics-based regularization via unsupervised training, is an order of",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Many processes can be concisely represented as a sequence of events leading from a starting state to an end state. Given raw ingredients, and a finished cake, an experienced chef can surmise the recipe. Building upon this intuition,  we propose a new class of visual generative models: goal-conditioned predictors (GCP). Prior work on video generation largely focuses on prediction models that only observe frames from the beginning of the video. GCP instead treats videos as start-goal transformations, making video generation easier by conditioning on the more informative context provided by the first and final frames.  Not only do existing forward prediction approaches synthesize better and longer videos when modified to become goal-conditioned,  but GCP models can also utilize structures that are not linear in time, to accomplish hierarchical prediction . To this end, we study both auto-regressive GCP models and novel tree-structured GCP models that generate frames recursively, splitting the video iteratively into finer and finer segments delineated by subgoals . In experiments across simulated and real datasets, our GCP methods generate high-quality sequences over long horizons .  Tree-structured GCPs are also substantially easier to parallelize than auto-regressive GCPs, making training  and  inference  very  efficient, and allowing the model to train on sequences that are thousands of frames in length.Finally, we demonstrate the utility of GCP approaches for imitation learning in the setting without access to expert actions . Videos are on the supplementary website: https://sites.google.com/view/video-gcp Many phenomena, both natural and artificial, are naturally characterized as transformations -the most salient information about them is contained in the start and end states, given which it is possible to fill in intermediate states from prior experience. For example, ending up in San Francisco after starting in Oakland entails getting into a car and crossing the Bay Bridge. Similarly, to an expert engineer observing a bridge, the task of reverse-engineering how it was built is well-defined and tractable. In contrast, consider the task of predicting forward in time, having observed only the steel and concrete that went into making the bridge. Such forward prediction tasks are severely underconstrained, leading to high uncertainties that compound with time, making it impossible to make meaningful predictions after only a few stages of iterative forward prediction (see Fig. 1 ). This is aggravated in highdimensional settings such as forward video prediction, which despite being the most widely studied setting for video synthesis, struggles to produce coherent video longer than a few seconds. We propose to condition video synthesis instead on the substantially more informative context of the start and the goal frame. We term such models goal-conditioned predictors (GCP). Much like the engineer observing the bridge, GCPs treat long videos as start-goal transformations and reverseengineer the full video, conditioned on the first and final frames. The simplest instantiation of GCPs modifies existing forward prediction approaches to also observe the final frame. More broadly, once we consider conditioning on the goal frame, we can devise new types of GCP models that more efficiently leverage the hierarchical structure present in real-world event sequences ( Fig. 1, right) . Just as coarse-to-fine image synthesis (Karras et al., 2017) generates a high-resolution image by iteratively adding details to a low-resolution image, we can synthesize a temporally downsampled video in the form of sequences of keyframes, and fill it in iteratively. We propose to In our experiments, all GCP variants successfully generate longer and higher-quality video than has been demonstrated with standard auto-regressive video prediction models, which only utilize the starting frames for context. Furthermore, we show that tree-structured GCPs are more parallelizable than auto-regressive models, leading to very fast training and inference. We show that we can train tree-structured GCPs on videos consisting of thousands of frames. We also study the applications of GCPs, demonstrating that they can be utilized to enable prediction-based control in simulated imitation learning scenarios. In these settings, the GCP models can be trained without access to demonstrator actions, and can synthesize visual plans directly from start and goal images, which can then be tracked using an inverse model. We presented goal-conditioned predictors (GCPs) -predictive models that generate video sequences between a given start and goal frame. GCPs must learn to understand the mechanics of the environment that they are trained in, in order to accurately predict the intermediate events that must take place in order to bring about the goal images from the start images. GCP models not only allow for substantially more accurate video prediction than conventional models that are conditioned only on the beginning context, but also allow for novel model architectures. Specifically, we explore how, in addition to more conventional auto-regressive GCPs, we can devise tree-structured GCP models that predict video sequences hierarchically, starting with the coarsest level subgoals and recursively subdividing until a full sequence is produced. Our experimental results show that GCPs can make more accurate predictions. We also demonstrate that they can be utilized in an imitation learning scenario, where they can learn behaviors from video demonstrations without example actions. Imitation from observations, without actions, is applicable in a wide range of realistic scenarios. For example, a robot could learn the mechanics of cooking from watching videos on YouTube (Damen et al., 2018) , and then use this model to learn how to cook on its own. We hope that the imitation framework presented in our work can be a step in towards effectively leveraging such data for robotic control. A DATA PROCESSING For the Human 3.6 dataset, we downsample the original videos to 64 by 64 resolution. We obtain videos of length of roughly 800 to 1600 frames, which we randomly crop in time to 500-frame sequences. We split the Human 3.6 into training, validation and test set by correspondingly 95%, 5% and 5% of the data. On the TAP dataset, we use 48949 videos for training, 200 for validation and 200 for testing.",We propose a new class of visual generative models: goal-conditioned predictors. We show experimentally that conditioning on the goal allows to reduce uncertainty and produce predictions over much longer horizons.,TAP ; al. ; Oakland ; the Bay Bridge ; Videos ; a few seconds ; GCP ; thousands ; Fig ; Karras,the original videos ; Imitation ; finer and finer segments ; high uncertainties ; that ; the Bay Bridge ; conditioning ; details ; tree-structured GCP models ; GCP approaches,TAP ; al. ; Oakland ; the Bay Bridge ; Videos ; a few seconds ; GCP ; thousands ; Fig ; Karras,"The goal-conditioned predictors (GCP) approach for imitation learning in the setting are similar to previous work on video generation, but instead of observing frames from the beginning of the video, they focus on the more informative context provided by the first and final frames. In experiments across simulated and real datasets, our GCP methods generate high-quality sequences over long horizons. Tree-structured GCP models are substantially easier to parallelize than auto-regressive GCPs, making training and inference very efficient, and allowing the model to train on sequences that are thousands of frames in length.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"We study the problem of representation learning in goal-conditioned hierarchical reinforcement learning. In such hierarchical structures, a higher-level controller solves tasks by iteratively communicating goals which a lower-level policy is trained to reach. Accordingly, the choice of representation -- the mapping of observation space to goal space -- is crucial. To study this problem, we develop a notion of sub-optimality of a representation, defined in terms of expected reward of the optimal hierarchical policy using this representation. We derive expressions which bound the sub-optimality and show how these expressions can be translated to representation learning objectives which may be optimized in practice. Results on a number of difficult continuous-control tasks show that our approach to representation learning yields qualitatively better representations as well as quantitatively better hierarchical policies, compared to existing methods. Hierarchical reinforcement learning has long held the promise of extending the successes of existing reinforcement learning (RL) methods BID9 BID22 BID16 to more complex, difficult, and temporally extended tasks BID18 BID25 BID3 . Recently, goal-conditioned hierarchical designs, in which higher-level policies communicate goals to lower-levels and lower-level policies are rewarded for reaching states (i.e. observations) which are close to these desired goals, have emerged as an effective paradigm for hierarchical RL BID17 ; BID14 ; Vezhnevets et al. (2017) , inspired by earlier work BID6 ; BID21 ). In this hierarchical design, representation learning is crucial; that is, a representation function must be chosen mapping state observations to an abstract space. Goals (desired states) are then specified by the choice of a point in this abstract space.Previous works have largely studied two ways to choose the representation: learning the representation end-to-end together with the higher-and lower-level policies (Vezhnevets et al., 2017) , or using the state space as-is for the goal space (i.e., the goal space is a subspace of the state space) BID17 BID14 . The former approach is appealing, but in practice often produces poor results (see BID17 and our own experiments), since the resulting representation is under-defined; i.e., not all possible sub-tasks are expressible as goals in the space. On the other hand, fixing the representation to be the full state means that no information is lost, but this choice is difficult to scale to higher dimensions. For example, if the state observations are entire images, the higher-level must output target images for the lower-level, which can be very difficult.We instead study how unsupervised objectives can be used to train a representation that is more concise than the full state, but also not as under-determined as in the end-to-end approach. In order to do so in a principled manner, we propose a measure of sub-optimality of a given representation. This measure aims to answer the question: How much does using the learned representation in place of the full representation cause us to lose, in terms of expected reward, against the optimal policy? This question is important, because a useful representation will compress the state, hopefully making the learning problem easier. At the same time, the compression might cause the representation to lose information, making the optimal policy impossible to express. It is therefore critical to understand how lossy a learned representation is, not in terms of reconstruction, but in terms of the ability to represent near-optimal policies on top of this representation.Our main theoretical result shows that, for a particular choice of representation learning objective, we can learn representations for which the return of the hierarchical policy approaches the return of the optimal policy within a bounded error. This suggests that, if the representation is learned with a principled objective, the 'lossy-ness' in the resulting representation should not cause a decrease in overall task performance. We then formulate a representation learning approach that optimizes this bound. We further extend our result to the case of temporal abstraction, where the higher-level controller only chooses new goals at fixed time intervals. To our knowledge, this is the first result showing that hierarchical goal-setting policies with learned representations and temporal abstraction can achieve bounded sub-optimality against the optimal policy. We further observe that the representation learning objective suggested by our theoretical result closely resembles several other recently proposed objectives based on mutual information (van den Oord et al., 2018; BID12 , suggesting an intriguing connection between mutual information and goal representations for hierarchical RL. Results on a number of difficult continuous-control navigation tasks show that our principled representation learning objective yields good qualitative and quantitative performance compared to existing methods. We have presented a principled approach to representation learning in hierarchical RL. Our approach is motivated by the desire to achieve maximum possible return, hence our notion of sub-optimality is in terms of optimal state values. Although this notion of sub-optimality is intractable to optimize directly, we are able to derive a mathematical relationship between it and a specific form of representation learning. Our resulting representation learning objective is practical and achieves impressive results on a suite of high-dimensional, continuous-control tasks.",We translate a bound on sub-optimality of representations to a practical training objective in the context of hierarchical reinforcement learning.,Vezhnevets ; two ; al. ; first ; van den ; Oord et al.,the lower-level ; goal space ; We ; a mathematical relationship ; expressions ; a point ; mapping state observations ; goal-conditioned hierarchical designs ; Results ; top,Vezhnevets ; two ; al. ; first ; van den ; Oord et al.,"In goal-conditioned hierarchical reinforcement learning, a higher-level controller solves tasks by iteratively communicating goals which a lower-level policy is trained to reach. The choice of representation is crucial, as the mapping of observation space to goal space is crucial. To define sub-optimality, we derive expressions which bound the suboptimality and show how these expressions can be translated to representation learning objectives which may be optimized in practice. The results on a number of difficult continuous-control tasks show that our approach to represent learning yields qualitatively better representations as well as quantitatively better hierarchical policies compared to existing methods. Hierarchical reinforcement",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Explaining a deep learning model can help users understand its behavior and allow researchers to discern its shortcomings. Recent work has primarily focused on explaining models for tasks like image classification or visual question answering.   In this paper, we introduce an explanation approach for image similarity models, where a model's output is a score measuring the similarity of two inputs rather than a classification.   In this task, an explanation depends on both of the input images, so standard methods do not apply. We propose an explanation method that pairs a saliency map identifying important image regions with an attribute that best explains the match.   We find that our explanations provide additional information not typically captured by saliency maps alone, and can also improve performance on the classic task of attribute recognition. Our approach's ability to generalize is demonstrated on two datasets from diverse domains, Polyvore Outfits and Animals with Attributes 2. Many problems in artificial intelligence that require reasoning about complex relationships can be solved by learning some feature embedding to measure similarity between images and/or other modalities such as text. Examples of these tasks include scoring fashion compatibility (Han et al., 2017b; Hsiao & Grauman, 2018; Vasileva et al., 2018) , image retrieval (Kiapour et al., 2015; Radenovi et al., 2018; Yelamarthi et al., 2018) , or zero-shot recognition (Bansal et al., 2018; Li et al., 2018b; Wang et al., 2018) . Reasoning about the behavior of similarity models can aid researchers in identifying potential improvements, show where two images differ for anomaly detection, promote diversity in fashion recommendation by ensuring different traits are most prominent in the top results, or simply help users understand the model's predictions which can build trust (Teach & Shortliffe, 1981) . However, prior work on producing explanations for neural networks has primarily focused on explaining classification models (e.g. (Fong & Vedaldi, 2017; Nguyen et al., 2016; Petsiuk et al., 2018; Ribeiro et al., 2016; Selvaraju et al., 2017; Zeiler & Fergus, 2014) ) and does not directly apply to similarity models. Given a single input image, such methods produce a saliency map which identifies pixels that played a significant role towards a particular class prediction (see Figure 1a for an example). On the other hand, a similarity model requires at least two images to produce a score. The interaction between both images defines which features are more important, so replacing just one of the images can result in identifying different salient traits. Another limitation of existing work is that the saliency alone may be insufficient as an explanation of (dis)similarity. For image pairs where similarity is determined by the presence or absence of an object, a saliency map may be enough to understand model behavior. However, when we consider the image pair in Figure 1b , highlighting the necklace as the region that contributes most to the similarity score is reasonable, but uninformative given that there are no other objects in the image. Instead, what is important is the fact that the necklace shares a similar color with the ring. Whether these attributes or salient parts are a better fit as an explanation is not determined by the image domain (i.e. attributes for e-commerce imagery vs. saliency for natural imagery), but instead by the images themselves. For example, an image can be matched as formal-wear because of a shirt's collar (salient part), while two images of animals can match because both have stripes (attribute). Guided by this intuition, we introduce Salient Attributes for Network Explanation (SANE). Our approach generates a saliency map to explain a model's similarity score, paired with an attribute explanation that identifies important image properties. SANE is a ""black box"" method, meaning it Existing explanation methods focus on image classification problems (left), whereas we explore explanations for image similarity models (right). We pair a saliency map, which identifies important image regions, but often provides little useful information, with an attribute (e.g., golden), which is more human-interpretable and, thus, a better explanation than saliency alone. can explain any network architecture and only needs to measure changes to a similarity score when provided with different inputs. Unlike a standard classifier, which simply predicts the most likely attributes for a given image, our explanation method predicts which attributes are important for the similarity score predicted by a model. Predictions are made for each image in a pair, and allowed to be non-symmetric, e.g., the explanation for why the ring in Figure 1b matches the necklace may be that it contains ""black"", even though the explanation for why the necklace matches the ring could be that it is ""golden."" A different similarity model may also result in different attributes being deemed important for the same pair of images. SANE combines three major components: an attribute predictor, a prior on the suitability of each attribute as an explanation, and a saliency map generator. Our underlying assumption is that at least one of the attributes present in each image should be able to explain the similarity score assigned to the pair. Given an input image, the attribute predictor outputs a confidence score and activation map for each attribute, while the saliency map generator produces regions important for the match. During training, SANE encourages overlap between the similarity saliency and attribute activation. At test time, we rank attributes as explanations for an image pair based on a weighted sum of this attribute-saliency map matching score, the explanation suitability prior of the attribute, and the likelihood that the attribute is present in the image. Although we evaluate only the top-ranked attribute in our experiments, in practice more than one attribute could be used to explain a similarity score. We find that using saliency maps as supervision for the attribute activation maps during training not only improves the attribute-saliency matching, resulting in better attribute explanations, but also boosts attribute recognition performance using standard metrics like average precision. We evaluate several candidate saliency map generation methods which are primarily adaptations of ""black box"" approaches that do not rely on a particular model architecture or require access to network parameters to produce a saliency map (Fong & Vedaldi, 2017; Petsiuk et al., 2018; Ribeiro et al., 2016; Zeiler & Fergus, 2014) . These methods generally identify important regions by measuring a change in the output class score resulting from some perturbation of the input image. Similarity models, however, typically rely on a learned embedding space to reason about relationships between images, where proximity between points or the lack thereof indicates some degree of correspondence. An explanation system for embedding models must, therefore, consider how distances between embedded points, and thus their similarity, change based on perturbing one or both of the input images. We explore two strategies for adapting these approaches to our task. First, we manipulate just a single image (the one we wish to produce an explanation for) while keeping the other image fixed. Second, we manipulate both images to allow for more complex interactions between the pair. See Section 3.2 for additional details and discussion on the ramifications of this choice. Our paper makes the following contributions: 1) we provide the the first quantitative study of explaining the behavior of image similarity models; 2) we propose a novel explanation approach that combines saliency maps and attributes; 3) we validate our method with metrics designed to link our explanations to model performance, and find that it produces more informative explanations than adaptations of prior work to this task and also improves attribute recognition performance. In this paper we introduced SANE, a method of explaining an image similarity model's behavior by identifying attributes that were important to the similarity score paired with saliency maps indicating import image regions. We confirmed humans believe our explanations are useful for explaining a model's behavior, which could help build trust, to supplement automatic metrics. In future work we believe closely integrating the saliency generator and attribute explanation model, enabling each component to take advantage of the predictions of the other, would help improve performance.",A black box approach for explaining the predictions of an image similarity model.,Wang ; Han ; two ; at least two ; al. ; Radenovi ; Second ; Vasileva ; Hsiao & Grauman ; Nguyen et al.,"both images defines ; different inputs ; image similarity models ; Figure ; ""black box"" approaches ; a particular class prediction ; potential improvements ; Teach & Shortliffe ; the classic task ; complex relationships",Wang ; Han ; two ; at least two ; al. ; Radenovi ; Second ; Vasileva ; Hsiao & Grauman ; Nguyen et al.,"Explaining a deep learning model can help users understand its behavior and allow researchers to discern its shortcomings. Recent work has focused on explaining models for tasks like image classification or visual question answering, where a model's output is a score measuring the similarity of two inputs rather than a classification. In this task, an explanation depends on both of the input images, so standard methods do not apply. We propose an explanation method that pairs a saliency map identifying important image regions with an attribute that best explains the match.   Our explanations provide additional information not typically captured by saliency maps alone, and can improve performance on the classic task of",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Supervised learning problems---particularly those involving social data---are often subjective. That is, human readers, looking at the same data, might come to legitimate but completely different conclusions based on their personal experiences. Yet in machine learning settings feedback from multiple human annotators is often reduced to a single ``ground truth'' label, thus hiding the true, potentially rich and diverse interpretations of the data found across the social spectrum. We explore the rewards and challenges of discovering and learning representative distributions of the labeling opinions of a large human population. A major, critical cost to this approach is the number of humans needed to provide enough labels not only to obtain representative samples but also to train a machine to predict representative distributions on unlabeled data. We propose aggregating label distributions over, not just individuals, but also data items, in order to maximize the costs of humans in the loop. We test different aggregation approaches on state-of-the-art deep learning models. Our results suggest that careful label aggregation methods can greatly reduce the number of samples needed to obtain representative distributions. This paper explores the problem of label aggregation in domains that are highly subjective, i.e., where different annotators may disagree for perfectly legitimate reasons. Such settings are common, if underacknowledged. Though increasingly, mass media provides stories about the unintended consequences of ignoring this diversity in machine learning.For example, Beauty.ai sponsored a worldwide beauty contest, judged by a machine learning algorithm. Though light-skinned entrants made up the majority of entrants, they nonetheless won a disproportionate number of contests. BID0 Tay, a Twitter-based learning agent, developed by Microsoft, was taught to tweet that the Holocaust was made up 2 (though the Holocaust factually existed, the same cybersocial dynamics of training bias found in subjective domains led to this outcome). ProPublica discovered that Northpointe risk assessment software-used to help judges determine sentence length for convicts-recommended longer sentences for African-American men than other groups, even when controlled for confounding factors. BID2 X:2 Fig. 1 . In this example, data items (black dots) are labeled by five human annotators each (left), where color indicates label choice, yielding an empirical label distribution y i for each data item i. By clustering similarly labeled objects, we pool together (right) the labels of all data items assigned to the same cluster k into a single, much larger sample θ k for all items in the cluster. Our research suggests that, in some cases, this larger sample (or a mixture of cluster samples) is a better representation of the true population distribution of beliefs about each data item in the cluster and can lead to better predictive supervised learning.Learning a distribution of beliefs about a data item, rather than a single ""ground truth"" label, poses unique challenges. It increases the dimensionality of the learning problem so that more data items may be needed. It also may require more labels per item to get a representative sample of the human populations' beliefs. And for most problems, labels are relatively expensive to obtain. Though crowdsourcing platforms have made this task convenient, they are frequently a resource bottleneck in supervised learning loops.Our main contribution is a method for minimizing the number of labels needed to learn to predict socially representative label distributions. It is based on the hypothesis that the sources are subjectivity are limited, and so the number of distinct distributions of beliefs over all data items is likewise limited. In other words, the label distributions are samples from a relatively small number of true, but hidden, distributions. See Figure 1 . These hidden distributions can be seen as latent classes representing population-level beliefs about the labels. According to this hypothesis, we can use unsupervised clustering algorithms to pool together the labels of data items with similar distributions into higher resolution distributions of beliefs shared commonly among all data items in the same cluster.In particular, we: (1) explore subjectivity as the problem of learning representative distributions from a target population of responses to target questions, BID1 propose clustering as a sensible means for pooling together labels from similar data items, to reduce the number of labels needed (3) test what we call our clustering hypothesis, that the label distributions of subjective data are clustered around a small number of underlying, true distributions (4) study how different label aggregation strategies and representations affect the performance of state-of-the art deep learning predictors.It would seem that bias is an inherent part of any information reduction process, such as those found in statistical learning BID29 . So it seems naive to expect that machines can learn unbiased models through unsupervised learning alone, or even for any supervised learning that assumes a singular, correct answer to most problems. We hope that this research sparks a broader debate about the best practices for machine learning with humans in the loop.The rest of this paper is organized as follows. Section 2 describes our experimental workflow, Section 3 presents our results, Section 4 discusses our study, Section 5 presents related work, and Section 6 is the conclusion. Figure 2 describes the basic experimental workflow in this study. We discuss each phase below. Note that there are two testing phases, one for determining how well each aggregation method fits the data and another for how well supervised learning algorithms trained by each aggregation strategy perform. Since these test phases share some methods, we discuss them together at the end of the section. Fig. 2 . The basic experimental workflow involves obtaining crowdsourced labels for raw data (yielding empirical label distributions for each data item), trying various strategies for aggregating and pooling those labels (including no aggregation), and finally testing how each method affects the accuracy of machine learning prediction. Note there are two testing phases: one for how well each aggregation strategy fits the data and one for machine learning performance. We also list important terms, keywords, and abbreviations associated with each phase of the workflow.",We study the problem of learning to predict the underlying diversity of beliefs present in supervised learning domains.,five ; African-American ; Beauty.ai ; Fig ; item i. ; Holocaust ; two ; Northpointe ; Microsoft ; one,"those ; A major, critical cost ; unique challenges ; each aggregation strategy perform ; our study ; representative samples ; African-American ; subjective data ; example ; careful label aggregation methods",five ; African-American ; Beauty.ai ; Fig ; item i. ; Holocaust ; two ; Northpointe ; Microsoft ; one,"Supervised learning problems are often subjective, where human readers might come to different conclusions based on their personal experiences. In machine learning settings feedback from multiple human annotators is often reduced to a single ``ground truth'' label, thus hiding the true, potentially rich and diverse interpretations of the data found across the social spectrum. This paper explores the challenges of discovering and learning representative distributions of labeling opinions of a large human population. A major, critical cost to this approach is the number of humans needed to provide enough labels not only to obtain representative samples but also to train a machine to predict representative distributions on unlabeled data. We propose aggreg",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Adversarial examples are modified samples that preserve original image structures but deviate classifiers. Researchers have put efforts into developing methods for generating adversarial examples and finding out origins. Past research put much attention on decision boundary changes caused by these methods. This paper, in contrast, discusses the origin of adversarial examples from a more underlying knowledge representation point of view. Human beings can learn and classify prototypes as well as transformations of objects. While neural networks store learned knowledge in a more hybrid way of combining all prototypes and transformations as a whole distribution. Hybrid storage may lead to lower distances between different classes so that small modifications can mislead the classifier. A one-step distribution imitation method is designed to imitate distribution of the nearest different class neighbor. Experiments show that simply by imitating distributions from a training set without any knowledge of the classifier can still lead to obvious impacts on classification results from deep networks. It also implies that adversarial examples can be in more forms than small perturbations. Potential ways of alleviating adversarial examples are discussed from the representation point of view. The first path is to change the encoding of data sent to the training step. Training data that are more prototypical can help seize more robust and accurate structural knowledge. The second path requires constructing learning frameworks with improved representations. With the more widespread use of deep neural networks, the robustness and security of these networks have aroused the attention of both academic and industrial eyes. Among these adversarial examples is one of the most interesting as well as intriguing.Since the discovery of adversarial examples in CNNs from 2013Szegedy et al. (2013 , security and robustness has become a hot topic. Researchers have put efforts into finding out sources for adversarial examples and also developing methods for automatically generating these adversarial examplesGoodfellow et al. (2014) .Most these research focus on how certain perturbations lead to changes in decision boundaries. This paper discusses the origin of adversarial examples from a more underlying knowledge representation point of view. It provides a possible reason why adversarial examples exist for current networks and uses some experiments to prove this idea. Experiments also in some way show that adversarial examples can be derived from only the training data and totally network-independent. In addition , adversarial examples may be in more forms than the usual small perturbations. At last, possible ways to alleviate this issue are discussed. In summary, this paper discusses the origin of adversarial examples from an underlying knowledge representation point of view. Neural networks store learned knowledge in a more hybrid way that combining all prototypes and transformation distributions as a whole. This hybrid storage may lead to lower distances between different classes so that small modifications may mislead the classifier.",Hybird storage and representation of learned knowledge may be a reason for adversarial examples.,one ; first ; second,all prototypes and transformation distributions ; addition ; transformations ; It ; these methods ; Adversarial examples ; contrast ; improved representations ; current networks ; lower distances,one ; first ; second,"Adversarial examples are modified samples that preserve original image structures but deviate classifiers. Past research focused on decision boundary changes caused by these methods. This paper discusses the origin of adversarial examples from a more underlying knowledge representation point of view. Human beings can learn and classify prototypes as well as transformations as a whole distribution, while neural networks store learned knowledge in a more hybrid way. Hybrid storage may lead to lower distances between different classes so that small modifications can mislead the classifier. A one-step distribution imitation method is designed to imitate distribution of the nearest different class neighbor. Experiments show that simply by imitating distributions from",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"While much recent work has targeted learning deep discrete latent variable models with variational inference, this setting remains challenging, and it is often necessary to make use of potentially high-variance gradient estimators in optimizing the ELBO. As an alternative, we propose to optimize a non-ELBO objective derived from the Bethe free energy approximation to an MRF's partition function. This objective gives rise to a saddle-point learning problem, which we train inference networks to approximately optimize. The derived objective requires no sampling, and can be efficiently computed for many MRFs of interest. We evaluate the proposed approach in learning high-order neural HMMs on text, and find that it often outperforms other approximate inference schemes in terms of true held-out log likelihood. At the same time, we find that all the approximate inference-based approaches to learning high-order neural HMMs we consider underperform learning with exact inference by a significant margin. There has been much recent interest in learning deep generative models with discrete latent variables BID29 BID28 BID12 BID25 BID15 Lee et al., 2018, inter alia) , especially in the case where these latent variables have structure -that is, where the interdependence between the discrete latents is modeled. Most recent work has focused on learning these models with variational inference BID14 , and in particular with variational autoencoders (VAEs) BID17 BID32 .Variational inference has a number of convenient properties, including that it involves the maximization of the evidence lower-bound (ELBO), a lower bound on the log marginal likelihood of the data. At the same time, when learning models with discrete latent variables variational inference may require the use of potentially high-variance gradient estimators, which are obtained during learning by sampling from the variational posterior; see Appendix A for an empirical investigation into the variance of various popular estimators when learning neural text HMMs with VAEs.In this paper we investigate learning discrete latent variable models with an alternative objective to the ELBO. In particular , we propose to approximate the intractable log marginal likelihood with an objective deriving from the Bethe free energy BID1 , a quantity which is intimately related to loopy belief propagation (LBP) BID30 BID45 BID9 BID9 , and which is the basis for ""outer approximations"" to the marginal polytope BID39 . The Bethe free energy is attractive because if all the factors in the factor graph associated with the model have low degree, it can often be evaluated efficiently, without any need for approximation by sampling (see Section 2). Of course, requiring all factors in the factor graph to be of low degree severely limits the expressiveness of directed graphical models. It does not, however , limit the expressiveness of markov random fields (MRFs) (i.e., undirected graphical models) as severely, since we can simply have an extremely loopy MRF, with arbitrary pairwise factors; see FIG1 (c) and Section 2.2.We accordingly propose to learn deep, undirected graphical models with latent variables, using a saddlepoint objective that makes use of the Bethe free energy approximation to the model's partition functions. We further amortize inference by using ""inference networks"" BID36 BID17 BID13 BID38 in optimizing the saddle-point objective. Unlike the ELBO, our objective will not form a lower bound on the log marginal likelihood, but an approximation to it. At the same time (and unlike other recent work on MRFs with a variational flavor BID21 BID23 ), this objective can be optimized efficiently, without sampling, and in our experiments in learning neural HMMs on text it outperforms other approximate inference methods in terms of held out log likelihood. We emphasize, however , that despite the improvement observed when training with the proposed objective, in our experiments all approximate inference methods were found to significantly underperform learning with exact inference; see Section 4.3. We begin with the results obtained by maximizing the true log marginal likelihood of the training data under both the directed (""Full"" in Table 1 ) and undirected models (""Pairwise MRF"" in Table 1 ), by backpropagating gradients through the relevant dynamic programs. These results establish how well our models perform under exact inference, and are shown in the last row of each subtable in Table 1 . We see that perplexities are roughly comparable between the directed and undirected models when trained with exact inference.We now consider the remaining directed HMM results of Table 1 , where the models are trained with approximate inference. In the first row of each ""Full"" subtable there, we show the result of maximizing the ELBO using a mean field-style posterior approximation and the REINFORCE BID43 gradient estimator, with an input-dependent baseline to reduce variance BID29 . The results are quite poor, with this approximate inference scheme leading to a gain of almost 200 points in perplexity over exact inference. Using the tighter IWAE BID3 objectives improves performance slightly in all cases, though the most dramatic performance improvement comes from using a first-order HMM posterior in maximizing the ELBO, which can be sampled from exactly using quantities calculated with the forward algorithm BID31 BID4 BID34 BID49 . While these results are encouraging, note that in general we may not have an exact dynamic program for sampling from a lower-order structured model, and that moreover we still appear to incur a perplexity penalty of more than 100 points over exact inference; see Appendix A for an empirical comparison of the variance of these estimators.Moving to the MRF results, the second row of each ""Pairwise MRF"" subtable in Table 1 contains the results of optimizing F as a saddle point problem. While this approach too underperforms exact inference by approximately 100 points in perplexity, somewhat remarkably it manages to consistently outperform the best approximate inference results for the directed models by a fair margin. The first row of each ""Pairwise MRF"" subtable in Table 1 attempts to determine whether the jump in perplexity when moving to the F objective is due to the approximate inference or to the approximate objective, by minimizing the F objective using the exact marginals, as calculated by a dynamic program. (Note that this is not equivalent to the negative log marginal likelihood, since the factor graphs are loopy). Interestingly, we see that this performs almost as well as the exact objective, suggesting that, at least for HMM models, the F objective is reasonable, and approximate inference remains the problem.Despite these encouraging results, we note that there are several drawbacks to the proposed approach. In particular, we find that in practice F indeed can over-or under-estimate perplexity. Moreover, while ELBO values are not perfectly correlated with their corresponding true perplexities, values of F seem even less correlated, which necessitates finding correlated proxies of perplexity that may be monitored during training. Finally, we note that explicitly calculating the projection onto the nullspace of A may be prohibitive for some models (e.g., large RBMs BID35 ), and so other approaches to tackling the constrained optimization problem are likely necessary. We have presented an objective for learning latent-variable MRFs based on the Bethe approximation to the partition function, which can often be efficiently evaluated and requires no sampling. This objective leads to slightly better held-out perplexities than other approximate inference methods when learning neural HMMs. Future work will examine scaling the proposed method to larger, non-sequential MRFs, and whether F -like objectives can be made to better correlate with the true perplexity.",Learning deep latent variable MRFs with a saddle-point objective derived from the Bethe partition function approximation.,Lee et al. ; the last row ; Bethe ; the first row ; Pairwise MRF ; MRF ; inter alia ; HMM ; first ; Appendix,a dynamic program ; inference networks ; other recent work ; the partition function ; correlated proxies ; the proposed approach ; the last row ; a saddle point problem ; the evidence ; discrete latent variables,Lee et al. ; the last row ; Bethe ; the first row ; Pairwise MRF ; MRF ; inter alia ; HMM ; first ; Appendix,"Variational inference has a number of convenient properties, including its maximization of evidence lower-bound (ELBO), a lower bound on the log marginal likelihood, and the use of potentially high-variance gradient estimators in optimizing the ELBO. In this paper, we examine learning discrete latent variable models with variational inference BID29 BID28 BID12 BID25 BID15 Lee et al., 2018, inter alia). Variational inference is a convenient method for learning deep generative models with discrete latent variables (MLMs) with lower bound, and it involves maximizing the evidence lower bound (EL",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Recent advances have made it possible to create deep complex-valued neural networks. Despite this progress, the potential power of fully complex intermediate computations and representations has not yet been explored for many challenging learning problems. Building on recent advances, we propose a novel mechanism for extracting signals in the frequency domain. As a case study, we perform audio source separation in the Fourier domain. Our extraction mechanism could be regarded as a local ensembling method that combines a complex-valued convolutional version of Feature-Wise Linear Modulation (FiLM) and a signal averaging operation. We also introduce a new explicit amplitude and phase-aware loss, which is scale and time invariant, taking into account the complex-valued components of the spectrogram. Using the Wall Street Journal Dataset, we compare our phase-aware loss to several others that operate both in the time and frequency domains and demonstrate the effectiveness of our proposed signal extraction method and proposed loss. When operating in the complex-valued frequency domain, our deep complex-valued network substantially outperforms its real-valued counterparts even with half the depth and a third of the parameters. Our proposed mechanism improves significantly deep complex-valued networks' performance and we demonstrate the usefulness of its regularizing effect. Complex-valued neural networks have been studied since long before the emergence of modern deep learning techniques (Georgiou & Koutsougeras, 1992; Zemel et al., 1995; Kim & Adalı, 2003; Hirose, 2003; Nitta, 2004) . Nevertheless, deep complex-valued models have only started to gain momentum (Reichert & Serre, 2014; Arjovsky et al., 2015; Danihelka et al., 2016; Trabelsi et al., 2017; Jose et al., 2017; Wolter & Yao, 2018b; Choi et al., 2019) , with the great majority of models in deep learning still relying on real-valued representations. The motivation for using complex-valued representations for deep learning is twofold: On the one hand, biological nervous systems actively make use of synchronization effects to gate signals between neurons -a mechanism that can be recreated in artificial systems by taking into account phase differences (Reichert & Serre, 2014) . On the other hand, complex-valued representations are better suited to certain types of data, particularly those that are naturally expressed in the frequency domain. Other benefits provided by working with complex-valued inputs in the spectral or frequency domain are computational. In particular, short-time Fourier transforms (STFTs) can be used to considerably reduce the temporal dimension of the representation for an underlying signal. This is a critical advantage, as training recurrent neural networks (RNNs) or convolutional neural networks (CNNs) on long sequences remains challenging due to unstable gradients and the computational requirements of backpropagation through time (BPTT) (Hochreiter, 1991; Bengio et al., 1994) . Applying the STFT on the raw signal, on the other hand, is computationally efficient, as in practice it is implemented with the fast Fourier transform (FFT) whose computational complexity is O(n log(n)). The aforementioned biological, representational and computational considerations provide compelling motivations for designing learning models for tasks where the complex-valued representation of the input and output data is more desirable than their real-counterpart. Recent work has provided building blocks for deep complex-valued neural networks (Trabelsi et al., 2017) . These building blocks have been shown, in many cases, to avoid numerical problems during training and, thereby, enable the use of complex-valued representations. These representations are well-suited for frequency domain signals, as they have the ability to explicitly encode frequency magnitude and phase components. This motivates us to design a new signal extraction mechanism operating in the frequency domain. In this work, our contributions are summarized as follows: 1. We present a new signal separation mechanism implementing a local ensembling procedure. More precisely, a complex-valued convolutional version of Feature-wise Linear Modulation (FiLM) (Perez et al., 2018 ) is used to create multiple separated candidates for each of the signals we aim to retrieve from a mixture of inputs. A signal averaging operation on the candidates is then performed in order to increase the robustness of the signal to noise and interference. Before the averaging procedure, a form of dropout is implemented on the signal candidates in order to reduce the amount of interference and noise correlation existing between the different candidates. 2. We propose and explore a new magnitude and phase-aware loss taking explicitly into account the magnitude and phase of signals. A key characteristic of our loss is that it is scale-and time-invariant. We test our proposed signal extraction mechanism in the audio source separation setting where we aim to retrieve distinct audio signals associated with each speaker in the input mix. Our experiments demonstrate the usefulness of our extraction method, and show its regularizing effect. In this work, we introduced a new complex-valued extraction mechanism for signal retrieval in the Fourier domain. As a case study, we considered audio source separation. We also proposed a new phase-aware loss taking, explicitly, into account the magnitude and phase of the reference and estimated signals. The amplitude and phase-aware loss improves over other frequency and time-domain losses. We believe that our proposed method could lead to new research directions where signal retrieval is needed. A APPENDIX",New Signal Extraction Method in the Fourier Domain,Georgiou & Koutsougeras ; Reichert & Serre ; half ; Feature-Wise Linear Modulation ; al. ; Arjovsky et ; Fourier ; a third ; Trabelsi ; Bengio,a new phase-aware loss taking ; phase ; the complex-valued components ; mechanism ; many challenging learning problems ; Zemel ; a case study ; (Hochreiter ; the averaging procedure ; significantly deep complex-valued networks' performance,Georgiou & Koutsougeras ; Reichert & Serre ; half ; Feature-Wise Linear Modulation ; al. ; Arjovsky et ; Fourier ; a third ; Trabelsi ; Bengio,"The potential power of fully complex intermediate computations and representations has not been explored for many challenging learning problems. To address this, we propose a novel mechanism for extracting signals in the frequency domain. This method combines a complex-valued convolutional version of Feature-Wise Linear Modulation (FiLM) and signal averaging operation. This approach improves performance and demonstrates the usefulness of its regularizing effect. Complex-valued neural networks have been studied since long before the emergence of modern deep learning techniques, with biological nervous systems actively making use of synchronization effects to gate signals between neurons. However, they have only started to gain momentum in recent",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Multi-agent reinforcement learning offers a way to study how communication could emerge in communities of agents needing to solve specific problems. In this paper, we study the emergence of communication in the negotiation environment, a semi-cooperative model of agent interaction. We introduce two communication protocols - one grounded in the semantics of the game, and one which is a priori ungrounded.   We show that self-interested agents can use the pre-grounded communication channel to negotiate fairly, but are unable to effectively use the ungrounded, cheap talk channel to do the same.   However, prosocial agents do learn to use cheap talk to find an optimal negotiating strategy, suggesting that cooperation is necessary for language to emerge. We also study communication behaviour in a setting where one agent interacts with agents in a community with different levels of prosociality and show how agent identifiability can aid negotiation. How can communication emerge? A necessary prerequisite is a task that requires coordination between multiple agents to solve, and some communication protocol for the agents to exchange messages through (see a review by BID41 on earlier work on emergent communication as well as recent deep reinforcement learning methods by BID8 and BID39 ). Given these basic requirements, an interesting question to ask is what task structures aid the emergence of communication and how different communication protocols affect task success.In the context of linguistic communication, previous work on this subject has mainly studied the emergence of communication in co-operative games like referential games, variants of the Lewis signaling game BID19 , where messages are used to disambiguate between different possible referents BID11 BID17 BID6 . Human language, though, is not merely a referential tool. Amongst other things, we communicate private information and thoughts, discuss plans, ask questions and tell jokes. Moreover, many human interactions are not fully cooperative, yet we can still successfully use language to communicate in these situations.In this paper, we study communication in the negotiation game (see Figure 1 ), an established model of non-cooperative games in classical game theory BID25 BID26 BID24 BID23 BID35 BID1 BID29 . In this game, agents are asked to establish a mutually acceptable division of a common pool of items while having their own hidden utilities for each of them. Effective communication is crucial in this game, as the agents need to exchange strategic information about their desires, infer their opponent's desires from communication, and balance between the two.Work in classical game theory on negotiation typically uses simple forms of offer / counter-offer bargaining games that do not explicitly address the question of emergent communication (Rubin- Figure 1 : High-level overview of the negotiation environment that we implement. Agent A consistently refers to the agent who goes first. stein, 1982) . Recent work on deep multi-agent reinforcement learning (MARL) has shown great success in teaching agents complex behaviour without a complex environment simulator or demonstration data BID28 BID2 BID40 BID18 . By repeatedly interacting with other agents learning at the same time, agents can gradually bootstrap complex behaviour, including motor skills BID0 and linguistic communication BID17 BID13 .We apply techniques from the MARL literature and train agents to negotiate using task success as the only supervision signal. 1 We show that, when communicating via a task-specific communication channel with inherent semantics, selfish agents can learn to negotiate fairly and divide up the item pool to the agents' mutual satisfaction. However , when communicating via cheap talk BID4 BID7 , a task-independent communication channel consisting of sequences of arbitrary symbols similar to language, selfish agents fail to exhibit negotiating behaviour at all. On the other hand, we show that cheap talk can facilitate effective negotiation in prosocial agents, which take the other agent's reward into consideration, providing experimental evidence that cooperation is necessary for language emergence BID27 .The above results are obtained from paired agents interacting exclusively with each other. In more realistic multi-agent scenarios, agents may interact with many other agents within a society. In these cases, cheap talk can have a significant effect on the evolutionary dynamics of the population BID32 ) as well as the equilibria, stability, and basins of attractions BID38 . Furthermore, it is well-known that, unless trained in a diverse environment, agents overfit to the their specific opponent or teammate . Inspired by these considerations , we perform experiments where agents interact with many agents having different prosociality levels, and find that being able to identify and model other agents' beliefs aids the negotiation success. This is consistent with experiments using models based on Theory of Mind: boundedly rational agents can collectively benefit by making inferences about the sophistication levels and beliefs of their opponents, and there is evidence that this occurs in human behavior BID44 2 GAME SETTING We showed that by communicating through a verifiable and binding communication channel, selfinterested agents can learn to negotiate fairly by reinforcement learning, using only task success as the reward signal. Moreover, cheap talk facilitated negotiation in prosocial but not in self-interested agents, corroborating theoretical results from the game theory literature BID4 ). An interesting future direction of research would be to investigate whether cheap talk can be made to emerge out of self-interested agents interacting. Recent encouraging results by BID3 show that communication can help agents cooperate. However, their signalling mechanism is heavily engineered: the speech acts are predefined and the consequences of the speech acts on the observed behaviour are deterministically hard-coded. It would be interesting to see whether a learning algorithm, such as BID9 , can discover the same result.A related paper from BID20 takes a top-down approach to learning to negotiate by leveraging dialogue data. We demonstrated a bottom up alternative towards learning communicative behaviours directly from interaction with peers. This opens up the exciting possibility of learning domain-specific reasoning capabilities from interaction, while having a general-purpose language layer at the top producing natural language.A ADDITIONAL FIGURES AND TABLES Table 7 : Joint reward success and average number of turns taken for paired agents negotiating when allowed the full 10 turns, varying the agent reward scheme and communication channel. The results are averaged across 20 seeds, with 128 games per seed. We also report the standard deviation as the ± number and the quartiles.","We teach agents to negotiate using only reinforcement learning; selfish agents can do so, but only using a trustworthy communication channel, and prosocial agents can negotiate using cheap talk.",two ; one ; Lewis ; first ; stein ; Theory of Mind,only task success ; communication behaviour ; the only supervision signal ; the exciting possibility ; a learning algorithm ; prosociality ; Multi-agent reinforcement learning ; boundedly rational agents ; variants ; earlier work,two ; one ; Lewis ; first ; stein ; Theory of Mind,"Multi-agent reinforcement learning offers a way to study how communication could emerge in communities of agents needing to solve specific problems. In this paper, we examine the emergence of communication in the negotiation environment, a semi-cooperative model of agent interaction. We introduce two communication protocols - one grounded in the semantics of the game, and one which is a priori ungrounded. We show that self-interested agents can use the pre-grounded communication channel to negotiate fairly, while prosocial agents learn to use cheap talk to find an optimal negotiating strategy, suggesting that cooperation is necessary for language to emerge. We also study communication behaviour",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"In search for more accurate predictive models, we customize capsule networks for the learning to diagnose problem. We also propose Spectral Capsule Networks, a novel variation of capsule networks, that converge faster than capsule network with EM routing. Spectral capsule networks  consist of spatial coincidence filters that detect entities based on the alignment of extracted features on a one-dimensional linear subspace. Experiments on a public benchmark learning to diagnose dataset not only shows the success of capsule networks on this task, but also confirm the faster convergence of the spectral capsule networks. The potential for improvement of the quality of care via artificial intelligence has led to significant advances in predictive modeling for healthcare BID11 BID1 BID16 BID0 BID12 BID19 BID15 . For accurate prediction, the models in healthcare need to not only identify risk factors, but also distill the complex and hierarchal temporal interactions among symptoms, conditions, and medications.It has been argued that traditional deep neural networks might not be efficient in capturing the hierarchical structure of the entities in the images BID13 BID2 BID4 BID23 . They argue that networks that preserve variations in the input perform superior to those that drop variations (equivariant vs. invariant architectures), as the upper layer can have access to the spatial relationship of the entities detected by the lower layers. In particular, in capsule networks BID7 BID17 BID8 ) the capsules are designed to have both activation and pose components, where the latter is responsible for preserving the variations in the detected entity.In this work, we first develop a version of capsule networks with EM routing (EM-Capsules) and show that it can accurately predict diagnoses. We observe that EM-Capsules converge slowly in our dataset and are sensitive to the selection of hyperparameters such as learning rate. To address these issues, we propose Spectral Capsule Networks (S-Capsules) that are also spatial coincidence filters, similar to EM-Capsules. In contrast to EM-Capsules, S-Capsules measure the coincidence as the degree of alignment of the votes from below capsules in a one-dimensional linear subspace, rather than centralized clusters. In S-Capsules, the variation (pose) component is the normal vector of a linear subspace that preserves most of the variance in the votes coming from below capsules and the activation component is computed based on the ratio of preserved variance.Our experiments on a benchmark learning to diagnose task BID5 ) defined on the publicly available MIMIC-III dataset BID9 highlight the success of capsule networks. Moreover, we confirm that the proposed S-Capsules converge faster than EM-Capsules. Finally, we show that the elements of the S-Capsules' variation (pose) vector are significantly correlated with the commonly used hand-engineered features. In this work, we customized capsule networks with EM routing BID8 ) for learning to diagnose task. We also proposed spectral capsule networks to improve stability and convergence speed of the capsule networks. Similar to EM-Capsules, S-Capsules are also spatial coincidence filters and look for agreement of the below capsules. However, spectral capsules measure the agreement by the amount of alignment in a linear subspace, rather than a centralized cluster. Setting aside the attention mechanism in EM-Capsules, the connection between S-Capsules and EM-Capsules is analogous to the connection between Gaussian Mixture Models and Principal Component Analysis. This analogy suggests why S-Capsules are more robust during the training. Our preliminary results confirm the superior convergence speed of the proposed S-Capsule network and preservation of variations in the data in its pose vectors.",A new capsule network that converges faster on our healthcare benchmark experiments.,Gaussian ; MIMIC-III ; Spectral Capsule Networks ; the S-Capsules' ; Mixture Models ; first ; Principal Component Analysis ; S-Capsules ; one ; S-Capsule,a benchmark learning ; the variance ; these issues ; capsule networks ; spatial coincidence filters ; Our preliminary results ; the success ; first ; problem ; variations,Gaussian ; MIMIC-III ; Spectral Capsule Networks ; the S-Capsules' ; Mixture Models ; first ; Principal Component Analysis ; S-Capsules ; one ; S-Capsule,"In search of more accurate predictive models, we design capsule networks for learning to diagnose problem. We also propose Spectral Capsule Networks, a novel variation of capsule networks that converge faster than capsule networks with EM routing. Spectral capsule networks consist of spatial coincidence filters that detect entities based on the alignment of extracted features on a one-dimensional linear subspace. Experiments on a public benchmark learning dataset not only shows the success of capsules on this task, but also confirm the faster convergence of the spectral capsule networks. The potential for improvement of the quality of care via artificial intelligence has led to significant advances in predictive modeling for healthcare BID",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Formal verification of machine learning models has attracted attention recently, and significant progress has been made on proving simple properties like robustness to small perturbations of the input features. In this context, it has also been observed that folding the verification procedure into training makes it easier to train verifiably robust models. In this paper, we extend the applicability of verified training by extending it to (1) recurrent neural network architectures and (2) complex specifications that go beyond simple adversarial robustness, particularly specifications that capture temporal properties like requiring that a robot periodically visits a charging station or that a language model always produces sentences of bounded length. Experiments show that while models trained using standard training often violate desired specifications, our verified training method produces models that both perform well (in terms of test error or reward) and can be shown to be provably consistent with specifications. While deep neural networks (DNNs) have shown immense progress on diverse tasks (Sutskever et al., 2014; Mnih et al., 2015; Silver et al., 2016) , they are often deployed without formal guarantees of their correctness and functionality. Their performance is typically evaluated using test data, or sometimes with adversarial evaluation (Carlini & Wagner, 2017; Ebrahimi et al., 2018; Wang et al., 2019) . However, such evaluation does not provide formal guarantees regarding the absence of rare but possibly catastrophic failures (Administration; Board; Ross & Swetlitz, 2018) . Researchers have therefore started investigating formal verification techniques for DNNs. Most of the focus in this direction has been restricted to feedforward networks and robustness to adversarial perturbations (Tjeng et al., 2017; Raghunathan et al., 2018b; Ko et al., 2019) . However, many practically relevant systems involve DNNs that lead to sequential outputs (e.g., an RNN that generates captions for images, or the states of an RL agent). These sequential outputs can be interpreted as real-valued, discrete-time signals. For such signals, it is of interest to provide guarantees with respect to temporal specifications (e.g., absence of repetitions in a generated sequence, or that a generated sequence halts appropriately). Temporal logic provides a compact and intuitive formalism for capturing such properties that deal with temporal abstractions. Here, we focus on Signal Temporal Logic (STL) (Donzé & Maler, 2010) as the specification language and exploit its quantitative semantics to integrate a verification procedure into training to provide guarantees with regard to temporal specifications. Our approach builds on recent work , which is based on propagating differentiable numerical bounds through DNNs, to include specifications that go beyond adversarial robustness. Additionally, we propose extensions to ; that allow us to train auto-regressive GRUs/RNNs to certifiably satisfy temporal specifications. We focus on the problem of verified training for consistency rather than post-facto verification. To summarize, our contributions are as: • We present extensions to ; that allow us to extend verified training to novel architectures and specifications, including complex temporal specifications. To handle the auto-regressive decoder often used in RNN-based systems, we leverage differentiable approximations of the non-differentiable operations. • We empirically demonstrate the applicability of our approach to ensure verifiable consistency with temporal specifications while maintaining the ability of neural networks to achieve high accuracy on the underlying tasks across domains. For supervised learning, verified training on the train-data enables us to provide similar verification guarantees for unseen test-data. • We show that verified training results in robust DNNs whose specification conformance is significantly easier to guarantee than those trained adversarially or with data augmentation. Temporal properties are commonly desired from DNNs in settings where the outputs have a sequential nature. We extend verified training to tasks that require temporal properties to be satisfied, and to architectures such as auto-regressive RNNs whose outputs have a sequential nature. Our experiments suggest that verified training leads to DNNs that are more verifiable, and often with fewer failures. Future work includes extending verification/verified training to unbounded temporal properties. Another important direction is to develop better bound propagation techniques that can be leveraged for verified training. In the RL setting, an important direction is data-driven verification in the absence of a known model of the environment.",Neural Network Verification for Temporal Properties and Sequence Generation Models,Board ; Mnih ; Raghunathan ; Wang ; Signal Temporal Logic ; Donzé & Maler ; Carlini & Wagner ; al. ; Silver et al. ; Ross & Swetlitz,"real-valued, discrete-time signals ; robustness ; Board ; an important direction ; the applicability ; test data ; their correctness ; both ; particularly specifications ; us",Board ; Mnih ; Raghunathan ; Wang ; Signal Temporal Logic ; Donzé & Maler ; Carlini & Wagner ; al. ; Silver et al. ; Ross & Swetlitz,"Formal verification of machine learning models has attracted attention recently, and significant progress has been made on proving simple properties like robustness to small perturbations of input features. However, folding the verification procedure into training makes it easier to train verifiably robust models. In this paper, we extend the applicability of verified training to recurrent neural network architectures and complex specifications that go beyond simple adversarial robustness, particularly specifications that capture temporal properties like requiring that a robot periodically visits a charging station or that a language model always produces sentences of bounded length. Experiments show that while models trained using standard training often violate desired specifications, our",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Sequence-to-sequence models with soft attention have been successfully applied to a wide variety of problems, but their decoding process incurs a quadratic time and space cost and is inapplicable to real-time sequence transduction. To address these issues, we propose Monotonic Chunkwise Attention (MoChA), which adaptively splits the input sequence into small chunks over which soft attention is computed. We show that models utilizing MoChA can be trained efficiently with standard backpropagation while allowing online and linear-time decoding at test time. When applied to online speech recognition, we obtain state-of-the-art results and match the performance of a model using an offline soft attention mechanism. In document summarization experiments where we do not expect monotonic alignments, we show significantly improved performance compared to a baseline monotonic attention-based model. Sequence-to-sequence models BID27 BID5 ) with a soft attention mechanism have been successfully applied to a plethora of sequence transduction problems BID20 BID30 BID7 BID29 BID26 . In their most familiar form, these models process an input sequence with an encoder recurrent neural network (RNN) to produce a sequence of hidden states, referred to as a memory. A decoder RNN then autoregressively produces the output sequence. At each output timestep, the decoder is directly conditioned by an attention mechanism, which allows the decoder to refer back to entries in the encoder's hidden state sequence. This use of the encoder's hidden states as a memory gives the model the ability to bridge long input-output time lags BID24 , which provides a distinct advantage over sequence-to-sequence models lacking an attention mechanism . Furthermore, visualizing where in the input the model was attending to at each output timestep produces an input-output alignment which provides valuable insight into the model's behavior.As originally defined, soft attention inspects every entry of the memory at each output timestep, effectively allowing the model to condition on any arbitrary input sequence entry. This flexibility comes at a distinct cost, namely that decoding with a soft attention mechanism has a quadratic time and space cost O(T U ), where T and U are the input and output sequence lengths respectively. This precludes its use on very long sequences, e.g. summarizing extremely long documents. In addition, because soft attention considers the possibility of attending to every entry in the memory at every output timestep, it must wait until the input sequence has been processed before producing output. This makes it inapplicable to real-time sequence transduction problems. BID25 recently pointed out that these issues can be mitigated when the input-output alignment is monotonic, i.e. the correspondence between elements in the input and output sequence does not involve reordering. This property is present in various real-world problems, such as speech recognition and synthesis, where the input and output share a natural temporal order (see, for example, fig. 2 ). In other settings, the alignment only involves local reorderings, e.g. machine translation for certain language pairs BID3 .Based on this observation, BID25 introduced an attention mechanism that explicitly enforces a hard monotonic input-output alignment, which allows for online and linear-time decoding. Figure 1: Schematics of the attention mechanisms discussed in this paper. Each node represents the possibility of the model attending to a given memory entry (horizontal axis) at a given output timestep (vertical axis). (a) In soft attention, the model assigns a probability (represented by the shade of gray of each node) to each memory entry at each output timestep. The context vector is computed as the weighted average of the memory, weighted by these probabilities. (b) At test time, monotonic attention inspects memory entries from left-to-right, choosing whether to move on to the next memory entry (shown as nodes with ×) or stop and attend (shown as black nodes). The context vector is hard-assigned to the memory entry that was attended to. At the next output timestep, it starts again from where it left off. (c) MoChA utilizes a hard monotonic attention mechanism to choose the endpoint (shown as nodes with bold borders) of the chunk over which it attends. The chunk boundaries (here, with a window size of 3) are shown as dotted lines. The model then performs soft attention (with attention weighting shown as the shade of gray) over the chunk, and computes the context vector as the chunk's weighted average.However, the hard monotonicity constraint also limits the expressivity of the model compared to soft attention (which can induce an arbitrary soft alignment). Indeed, experimentally it was shown that the performance of sequence-to-sequence models utilizing this monotonic attention mechanism lagged behind that of standard soft attention.In this paper, we aim to close this gap by introducing a novel attention mechanism which retains the online and linear-time benefits of hard monotonic attention while allowing for soft alignments. Our approach, which we dub ""Monotonic Chunkwise Attention"" (MoChA), allows the model to perform soft attention over small chunks of the memory preceding where a hard monotonic attention mechanism has chosen to attend. It also has a training procedure which allows it to be straightforwardly applied to existing sequence-to-sequence models and trained with standard backpropagation. We show experimentally that MoChA effectively closes the gap between monotonic and soft attention on online speech recognition and provides a 20% relative improvement over monotonic attention on document summarization (a task which does not exhibit monotonic alignments). These benefits incur only a modest increase in the number of parameters and computational cost. We also provide a discussion of related work and ideas for future research using our proposed mechanism. We have proposed MoChA, an attention mechanism which performs soft attention over adaptivelylocated chunks of the input sequence. MoChA allows for online and linear-time decoding, while also facilitating local input-output reorderings. Experimentally, we showed that MoChA obtains state-of-the-art performance on an online speech recognition task, and that it substantially outperformed a hard monotonic attention-based model on document summarization. In future work, we are interested in applying MoChA to additional problems with (approximately) monotonic alignments, such as speech synthesis BID29 and morphological inflection BID1 . We would also like to investigate ways to allow the chunk size w to also vary adaptively. To facilitate building on our work, we provide an example implementation of MoChA online.",An online and linear-time attention mechanism that performs soft attention over adaptively-located chunks of the input sequence.,Monotonic Chunkwise Attention ; RNN ; fig ; MoChA,This use ; online ; a soft attention mechanism ; test time ; O(T U ; certain language ; a wide variety ; an input sequence ; (c) MoChA ; a hard monotonic attention-based model,Monotonic Chunkwise Attention ; RNN ; fig ; MoChA,"Sequence-to-sequence models with soft attention have been successfully applied to a wide variety of problems, but their decoding process incurs quadratic time and space cost and is inapplicable to real-time sequence transduction. To address these issues, we propose Monotonic Chunkwise Attention (MoChA), which adaptively splits the input sequence into small chunks over which soft attention is computed, allowing online and linear-time decoding at test time. When applied to online speech recognition, we obtain state-of-the-art results and match the performance of a model using an offline soft attention mechanism.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Sample inefficiency is a long-lasting problem in reinforcement learning (RL).   The state-of-the-art uses action value function to derive policy while it usually involves an extensive search over the state-action space and unstable optimization. Towards the sample-efficient RL, we propose ranking policy gradient (RPG), a policy gradient method that learns the optimal rank of a set of discrete actions.   To accelerate the learning of policy gradient methods, we establish the equivalence between maximizing the lower bound of return and imitating a near-optimal policy without accessing any oracles. These results lead to a general off-policy learning framework, which preserves the optimality, reduces variance, and improves the sample-efficiency. We conduct extensive experiments showing that when consolidating with the off-policy learning framework, RPG substantially reduces the sample complexity, comparing to the state-of-the-art. One of the major challenges in reinforcement learning (RL) is the high sample complexity (Kakade et al., 2003) , which is the number of samples must be collected to conduct successful learning. There are different reasons leading to poor sample efficiency of RL (Yu, 2018) . Because policy gradient algorithms directly optimizing return estimated from rollouts (e.g., REINFORCE (Williams, 1992) ) could suffer from high variance (Sutton & Barto, 2018) , value function baselines were introduced by actor-critic methods to reduce the variance and improve the sample-efficiency. However, since a value function is associated with a certain policy, the samples collected by former policies cannot be readily used without complicated manipulations (Degris et al., 2012) and extensive parameter tuning (Nachum et al., 2017) . Such an on-policy requirement increases the difficulty of sampleefficient learning. On the other hand, off-policy methods, such as one-step Q-learning (Watkins & Dayan, 1992) and variants of deep Q networks (DQN) (Mnih et al., 2015; Hessel et al., 2017; Dabney et al., 2018; Van Hasselt et al., 2016; Schaul et al., 2015) , enjoys the advantage of learning from any trajectory sampled from the same environment (i.e., off-policy learning), are currently among the most sampleefficient algorithms. These algorithms, however, often require extensive searching (Bertsekas & Tsitsiklis, 1996, Chap. 5) over the large state-action space to estimate the optimal action value function. Another deficiency is that, the combination of off-policy learning, bootstrapping, and function approximation, making up what Sutton & Barto (2018) called the ""deadly triad"", can easily lead to unstable or even divergent learning (Sutton & Barto, 2018, Chap. 11) . These inherent issues limit their sample-efficiency. Towards addressing the aforementioned challenge, we approach the sample-efficient reinforcement learning from a ranking perspective. Instead of estimating optimal action value function, we concentrate on learning optimal rank of actions. The rank of actions depends on the relative action values. As long as the relative action values preserve the same rank of actions as the optimal action values (Q-values), we choose the same optimal action. To learn optimal relative action values, we propose the ranking policy gradient (RPG) that optimizes the actions' rank with respect to the long-term reward by learning the pairwise relationship among actions. Ranking Policy Gradient (RPG) that directly optimizes relative action values to maximize the return is a policy gradient method. The track of off-policy actor-critic methods (Degris et al., 2012; Gu et al., 2016; Wang et al., 2016) have made substantial progress on improving the sample-efficiency of policy gradient. However, the fundamental difficulty of learning stability associated with the bias-variance trade-off remains (Nachum et al., 2017) . In this work, we first exploit the equivalence between RL optimizing the lower bound of return and supervised learning that imitates a specific optimal policy. Build upon this theoretical foundation, we propose a general off-policy learning framework that equips the generalized policy iteration (Sutton & Barto, 2018, Chap. 4) with an external step of supervised learning. The proposed off-policy learning not only enjoys the property of optimality preserving (unbiasedness), but also largely reduces the variance of policy gradient because of its independence of the horizon and reward scale. Besides, we empirically show that there is a trade-off between optimality and sample-efficiency. Last but not least, we demonstrate that the proposed approach, consolidating the RPG with off-policy learning, significantly outperforms the state-of-the-art (Hessel et al., 2017; Bellemare et al., 2017; Dabney et al., 2018; Mnih et al., 2015) . In this work, we introduced ranking policy gradient (RPG) methods that, for the first time, resolve RL problem from a ranking perspective. Furthermore, towards the sample-efficient RL, we propose an off-policy learning framework that allows RL agents to be trained in a supervised learning paradigm. The off-policy learning framework uses generalized policy iteration for exploration and exploit the stableness of supervised learning for policy learning, which accomplishes the unbiasedness, variance reduction, off-policy learning, and sample efficiency at the same time. Last but not least, empirical results show that RPG achieves superior performance as compared to the state-of-the-art. Corollary 3. The pairwise ranking policy as shown in Eq (2) constructs a probability distribution over the set of actions when the action space m is equal to 2, given any relative action values λi, i = 1, 2. For the cases with m > 2, this conclusion does not hold in general. It is easy to verify that π(ai|s) > 0, ∑ 2 i=1 π(ai|s) = 1 holds and the same conclusion cannot be applied to m > 2 by constructing counterexamples. However, we can introduce a dummy action a ′ to form a probability distribution for RPG. During policy learning, the algorithm will increase the probability of best actions and the probability of dummy action will decrease. Ideally, if RPG converges to an optimal deterministic policy, the probability of taking best action is equal to one and π(a ′ |s) = 0. Similarly, we can introduce a dummy trajectory τ ′ with trajectory reward r(τ The trajectory probability forms a probability distribution since The proof of a valid trajectory probability is similar to the following proof on π(a|s) is a valid probability distribution with a dummy action. The practical influence of this is negligible since our goal is to increase the probability of (near)-optimal trajectories. To present in a clear way, we avoid mentioning dummy trajectory τ ′ in Proof 9.2 while it can be seamlessly included. This condition can be easily satisfied since in RPG we only focus on the relative relationship of λ-values and we can constrain its range so that λm satisfies the condition 1. Furthermore, since we can see that m 1 m−1 > 1 is decreasing w.r.t to action dimension m. The larger the action dimension, the less constraint we have on the λ-values. ′ and set π(a = a ′ |s) = 1 − ∑ i π(a = ai|s), which will construct a valid probability distribution (π(a|s)) over the action space A ∪ a ′ . Proof. Since we have π(a = ai|s) > 0 ∀i = 1, ..., m and ∑ i π(a = ai|s) + π(a = a ′ |s) = 1. To prove this is a valid probability distribution, we only need to show that π(a = a ′ |s) ≥ 0, ∀m ≥ 2, i.e. 9.4 LISTWISE POLICY GRADIENT In order to learn the stochastic policy that optimizes the ranking of actions with respect to the return, we now introduce the Listwise Policy Gradient (LPG) method. In RL, we want to optimize the probability of each action (ai) to be ranked higher among all actions, which is the sum of the probabilities of all permutations such that the action ai in the top position of the list. This probability is computationally prohibitive since we need to consider the probability of m! permutations. Luckily, based on Cao et al. (2007) [Theorem 6], we can model the such probability of action ai to be ranked highest given a set of relative action values by a simple softmax formulation, as described in Theorem 3. Theorem 3 (Theorem 6 Cao et al. (2007) where ϕ( * ) is any increasing, strictly positive function. A common choice of ϕ is the exponential function. Closely built upon the foundations from learning to rank Cao et al. (2007) where the listwise ranking policy π θ parameterized by θ is given by Eq (17) for tasks with deterministic optimal policies: a = arg max or Eq (18) is the probability that action i being ranked highest, given the current state and all the relative action values λ1 . . . λm. The proof of Theorem 4 exactly follows the direct policy differentiation Peters & Schaal (2008); Williams (1992) by replacing the policy to the form of the softmax function. The action probability π(ai|s), ∀i = 1, ..., m forms a probability distribution over the set of discrete actions [Cao et al. (2007) Lemma 7] . Theorem 4 states that the vanilla policy gradient Williams (1992) parameterized by a softmax layer is optimizing the probability of each action to be ranked highest, with respect to the long-term reward. Condition 2 If we want to preserve the optimality by TRS, the optimal trajectories of MDP needs to cover all initial states or equivalently, all initial states will lead to at least one optimal trajectory. Similarly, the near-optimality is preserved for all MDPs that its near-optimal trajectories cover all initial states. Theoretically, it is possible to transfer more general MDPs to satisfy Condition 2 and preserve the optimality with potential-based reward shaping Ng et al. (1999) . More concretely, consider the deterministic binary tree MDP (M1) with the set of initial states S1 = {s1, s . This reward shaping requires more prior knowledge, which may not be feasible in practice. A more realistic method is to design a dynamic trajectory reward shaping approach. In the beginning, we set c(s) = mins∈S 1 r(τ |s(τ, 1) = s), ∀s ∈ S1. Take M1 as an example, c(s) = 3, ∀s ∈ S1. During the exploration stage, we track the current best trajectory of each initial state and update c(s) with its trajectory reward. Nevertheless, if the Condition 2 is not satisfied, we need more sophisticated prior knowledge other than a predefined trajectory reward threshold c to construct the replay buffer (training dataset of UNOP). The practical implementation of trajectory reward shaping and rigorously theoretical study for general MDPs are beyond the scope of this work. Under review as a conference paper at ICLR 2020","We propose ranking policy gradient that learns the optimal rank of actions to maximize return. We propose a general off-policy learning framework with the properties of optimality preserving, variance reduction, and sample-efficiency.",first ; RL ; Van Hasselt ; the Listwise Policy Gradient ; Eq ; al. ; RPG ; Wang ; Schaul et al. ; Williams,the first time ; it ; Hessel ; review ; the optimality ; unstable or even divergent learning ; the optimal action value function ; sampleefficient learning ; samples ; its range,first ; RL ; Van Hasselt ; the Listwise Policy Gradient ; Eq ; al. ; RPG ; Wang ; Schaul et al. ; Williams,"In reinforcement learning (RL), sample inefficiency is a long-lasting problem. The state-of-the-art uses action value function to derive policy while the state-based approach involves extensive search over state-action space and unstable optimization. To accelerate the learning of policy gradient methods, we establish the equivalence between maximizing the lower bound of return and imitating a near-optimal policy without using any oracles. These results lead to a general off-policy learning framework, which preserves the optimality, reduces variance, and improves the sample-efficiency. Additionally, when consolidating with the off-, and off-, policy",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"We propose a novel quantitative measure to predict the performance of a deep neural network classifier, where the measure is derived exclusively from the graph structure of the network. We expect that this measure is a fundamental first step in developing a method to evaluate new network architectures and reduce the reliance on the computationally expensive trial and error or ""brute force"" optimisation processes involved in model selection. The measure is derived in the context of multi-layer perceptrons (MLPs), but the definitions are shown to be useful also in the context of deep convolutional neural networks (CNN), where it is able to estimate and compare the relative performance of different types of neural networks, such as VGG, ResNet, and DenseNet. Our measure is also used to study the effects of some important ""hidden"" hyper-parameters of the DenseNet architecture, such as number of layers, growth rate and the dimension of 1x1 convolutions in DenseNet-BC. Ultimately, our measure facilitates the optimisation of the DenseNet design, which shows improved results compared to the baseline.
 Deep neural networks (DNN) have achieved outstanding results in several classification tasks (Huang et al., 2017; He et al., 2016) . There is some theoretical understanding of the workings of individual elements, such as convolutional filters, activation funtions, and normalisation (Goodfellow et al., 2016; LeCun et al., 2015; Schmidhuber, 2015) . However, current ideas behind the DNN graph design are still based on ad-hoc principles (Mishkin et al., 2017) . These principles are largely qualitative and tend to improve classification accuracy -examples of these principles include: an increase of the network depth (Szegedy et al., 2015) , and an increase of the representation dimensionality (by, for example, expanding the number of channels in deeper parts of the DNN) (Huang et al., 2017; He et al., 2016) . We notice that an effective DNN graph design is largely independent of the data set, as long as the type of data (e.g., images) and task (e.g., classification) are similar. Hence, we argue that good design principles can be encoded in a quantitative measure of the graph and should be justified by a quantitative assessment of the DNN architecture performance. An alternative way of designing a DNN graph structure is based on (meta-)optimisation methods (Jenatton et al., 2017; Kandasamy et al., 2019; Mendoza et al., 2016; Snoek et al., 2012) . Although useful in practice, such optimisation methods add little to our understanding of the design principles of new DNN graphs and are computationally challenging to execute. DNNs form a hierarchical structure of filters that can be seen as a directed graph. The first layers of this graph contain neurons that are active for low level patterns, such as edges and patches (in the case of images) (Zeiler & Fergus, 2014) , while deeper layer neurons are active for more complex visual patterns, such as faces or cars, formed by a hierarchical combination of a large number of simpler filters (Zeiler & Fergus, 2014) . In such representation, each neuron behaves like a binary classifier of the visual pattern learned by the neuron. Also, the strength of the activation is related to how well the pattern is matched. We argue that this linear separability promoted by the neurons is the key ingredient behind an effective quantitative measure of model performance. In this paper, we introduce a new measure that can be a proxy for DNN model performance. This proposed measure is first formulated in the context of Multi Layer Perceptrons (MLPs) to quantitatively predict the classification accuracy of the model. Then, we extend the applicability of the measure to predict the classification accuracy of the following CNNs: VGG (Simonyan & Zisserman, 2014) , ResNet (He et al., 2016) and DenseNet (Huang et al., 2017) . The experiments demonstrate how this quantity can be used to predict the ""correct"" depth of a simple feed forward DNN with constraints on the parameter budget. The experiments also show how the proposed quantity can be used to improve the design of DenseNet (Huang et al., 2017) and in the study of the effects of some important ""hidden"" hyper-parameters such as the dimension of 1 × 1 convolutions in the bottlenecks of DenseNet-BC, the number of layers, and the growth rate. Our measure (Z) is calculated based only on the graph structure of the model and assumes that the initialisation and training strategies are close to optimal. We argue that the choice of these strategies is important because it guarantees the realisation of the model potential for a particular classification problem. However, we see these strategies as somewhat orthogonal to network design. We show in Fig. 3 that the value of Z is a good predictor of the optimal depth Λ for a simple feed forward CNN. We also show that the value of Z is a good predictor of accuracy for small values of r (channels of bottleneck layer), and for larger values of r, while Z ""saturates"", the accuracy tends to remain stable. We conjecture that this happens because regularisation techniques are likely to reduce the effective number of channels when this allows an increase in the number of paths. We find indicative support of this mechanism from the study of the dimensionality of the PCA decomposition of MLP classifiers ( Fig. 2(right) ). Figure 6: This example shows the rules to calculate the contribution to the total number of paths given by each layer in a NN. Each N i can be ≤ than the corresponding channels in the NN. We will leave the proof for this conjecture for a future work, note that this behaviour does not undermine the method -the implementation of models with optimal Z appears to offer best performance with the minimum number of parameters. We now provide guidelines on how to formulate Z -see Fig. 6 . When there is a skip connection around a layer, the number of channels in such layer can contribute fully to the paths (N i ). When two layers are in sequence, the contribution to the paths is the difference of the channels after the reduction due to regularisation (N i − N i−1 ). When the filter size changes (e.g., 1 × 1 followed by 3 × 3 convolution), the channels of the first layer need to be divided by the ratio of the dimension of the filters (",A quantitative measure to predict the performances of deep neural network models.,Kandasamy ; VGG ; first ; Z ; ResNet ; al. ; N ; DenseNet-BC ; DenseNet ; Zeiler & Fergus,this ; network design ; the applicability ; the graph structure ; the relative performance ; the ratio ; the baseline ; Huang et al. ; the reliance ; new DNN graphs,Kandasamy ; VGG ; first ; Z ; ResNet ; al. ; N ; DenseNet-BC ; DenseNet ; Zeiler & Fergus,"We propose a quantitative measure to predict the performance of a deep neural network classifier based on the graph structure of the network. This measure is a fundamental first step in developing a method to evaluate new network architectures and reduce the need for expensive trial and error or ""brute force"" optimisation processes involved in model selection. The measure is derived in the context of multi-layer perceptrons (MLPs), but the definitions are useful also in the deep convolutional neural networks (CNN), where it is able to estimate and compare the relative performance of different types of neural networks, such as VGG, ResNet, and D",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Unsupervised domain adaptation is a promising avenue to enhance the performance of deep neural networks on a target domain, using labels only from a source domain. However, the two predominant methods, domain discrepancy reduction learning and semi-supervised learning, are not readily applicable when source and target domains do not share a common label space. This paper addresses the above scenario by learning a representation space that retains discriminative power on both the (labeled) source and (unlabeled) target domains while keeping representations for the two domains well-separated. Inspired by a theoretical analysis, we first reformulate the disjoint classification task, where the source and target domains correspond to non-overlapping class labels, to a verification one. To handle both within and cross domain verifications, we propose a Feature Transfer Network (FTN) to separate the target feature space from the original source space while aligned with a transformed source space. Moreover, we present a non-parametric multi-class entropy minimization loss to further boost the discriminative power of FTNs on the target domain. In experiments, we first illustrate how FTN works in a controlled setting of adapting from MNIST-M to MNIST with disjoint digit classes between the two domains and then demonstrate the effectiveness of FTNs through state-of-the-art performances on a cross-ethnicity face recognition problem.
 Despite strong performances on facial analysis using deep neural networks BID17 BID15 Schroff et al., 2015; Parkhi et al., 2015) , learning a model that generalizes across variations in attributes like ethnicity, gender or age remains a challenge. For example, it is reported by BID5 that commercial engines tend to make mistakes at detecting gender for images of darker-skinned females. Such biases have enormous social consequences, such as conscious or unconscious discrimination in law enforcement, surveillance or security (WIRED, 2018a; b; NYTimes, 2018; GIZMODO, 2018) . A typical solution is to collect and annotate more data along the underrepresented dimension, but such efforts are laborious and time consuming. This paper proposes a novel deep unsupervised domain adaptation approach to overcome such biases in face verification and identification.Deep domain adaptation (Long et al., 2013; BID22 BID12 Sohn et al., 2017; Haeusser et al., 2017; Luo et al., 2017) allows porting a deep neural network to a target domain without extensive labeling efforts. Currently, there are two predominant approaches to deep domain adaptation. The first approach, domain divergence reduction learning, is motivated by the works of BID0 BID1 . It aims to reduce the source-target domain divergence using domain adversarial training BID12 Sohn et al., 2017; BID19 or maximum mean discrepancy minimization BID22 Long et al., 2015; , while leveraging supervised loss from labeled source examples to maintain feature space discriminative power. Since the theoretical basis of this approach BID0 assumes a common task between domains, it is usually applied to a classification problem where the source and target domains share the same label space and task definition. The second approach considers domain adaptation as a semi-supervised learning problem and applies techniques such as entropy minimization (Grandvalet & Bengio, 2005) or self-ensembling (Laine & Aila, 2017; BID18 BID11 on target examples to encourage decisive and consistent predictions.However, neither of those are applicable if the label spaces of source and target domains do not align. As a motivating example, consider a cross-ethnicity generalization of face recognition problem, where the source ethnicity (e.g., Caucasian) contains labeled examples and the target ethnicity (e.g., African-American) contains only unlabeled examples. When it is cast as a classification problem, the tasks of the two domains are different due to disjoint label spaces. Moreover, examples from different ethnicity domains almost certainly belong to different identity classes. To satisfy such additional label constraints, representations of examples from different domains should ideally be distant from each other in the embedding space, which conflicts with the requirements of domain divergence reduction learning as well as entropy minimization on target examples with source domain class labels.In this work, we aim at learning a shared representation space between a source and target domain with disjoint label spaces that not only remains discriminative over both domains but also keep representations of examples from different domains well-separated, when provided with additional label constraints. Firstly, to overcome the limitation of domain adversarial neural network (DANN) BID12 , we propose to convert disjoint classification tasks (i.e., the source and target domains correspond to non-overlapping class labels) into a unified binary verification task. We term adaptation across such source and target domains as cross-domain distance metric adaptation (CD2MA). We demonstrate a generalization of the theory of domain adaptation BID0 to our setup, which bounds the empirical risk for within-domain verification of two examples drawn from the unlabeled target domain. While the theory does not guarantee verification between examples from different domains, we propose approaches that also address such cross-domain verification tasks.To this end, we introduce a Feature Transfer Network (FTN) that separates the target features from the source features while simultaneously aligning them with an auxiliary domain of transformed source features. Specifically, we learn a shared feature extractor that maps examples from different domains to representations far apart. Simultaneously, we learn a feature transfer module that transforms the source representation space to another space used to align with the target representation space through a domain adversarial loss. By forging this alignment, the discriminative power from the augmented source representation space would ideally be transferred to the target representation space. The verification setup also allows us to introduce a novel entropy minimization loss in the form of N -pair metric loss (Sohn, 2016) , termed multi-class entropy minimization (MCEM), to further leverage unlabeled target examples whose label structure is not known. MCEM samples pairs of examples from a discovered label structure within the target domain using an offline hierarchical clustering algorithm such as HDBSCAN BID6 , computes the N -pair metric loss among these examples (Sohn, 2016) , and backpropagates the resulting error derivatives.In experiments, we first perform on a controlled setting by adapting between disjoint sets of digit classes. Specifically, we adapt from 0-4 of MNIST-M BID12 dataset to 5-9 of MNIST dataset and demonstrate the effectiveness of FTN in learning to align and separate domains. Then, we assess the impact of our proposed unsupervised CD2MA method on a challenging cross-ethnicity face recognition task, whose source domain contains face images of Caucasian identities and the target domain of non-Caucasian identities, such as African-American or East-Asian. This is an important problem since existing face recognition datasets show significant label biases towards Caucasian ethnicity, leading to sub-optimal recognition performance for other ethnicities. The proposed method demonstrates significant improvement in face verification and identification compared to a source-only baseline model and a standard DANN. Our proposed method also closely matches the performance upper bounds obtained by training with fully labeled source and target domains. We address the challenge of unsupervised domain adaptation when the source and the target domains have disjoint label spaces by formulating the classification problem into a verification task. We propose a Feature Transfer Network, allowing simultaneous optimization of domain adversarial loss and domain separation loss, as well as a variant of N -pair metric loss for entropy minimization on the target domain where the ground-truth label structure is unknown, to further improve the adaptation quality. Our proposed framework excels at both within-domain and cross-domain verification tasks.As an application, we demonstrate cross-ethnicity face verification that overcomes label biases in training data, achieving high accuracy even for unlabeled ethnicity domains, which we believe is a result with vital social significance.Vinod Nair and Geoffrey E Hinton. Following (Haeusser et al., 2017) , we preprocess the data by subtracting a channel-wise pixel mean and dividing by channel-wise standard deviation of pixel values. For MNIST examples, we also apply color-intensity inversion. All images are resized into 32×32 with 3 channels.Our feature generator module is composed of 6 convolution layers and 3 max-pooling layers followed by 2 fully-connected layers. We use ReLU (Nair & Hinton, 2010) after convolution layers. The output dimension of the feature generator module is 128 and is normalized to have L2-norm of 2. The full description of the generator module is in TAB5 .The feature transfer module maps 128 dimensional vector into the same dimensional vector using two fully-connected layers (128 − 256 − 256 − 128) and residual connection as in Figure 1 (a) . Discriminator architectures are similar to that in Figure 1 (b) but with fully-connected layers whose output dimensions are 128 instead of 256.We use Adam stochastic optimizer with learning rate of 0.0003, λ 1 = 0.3 and λ 2 = 0.03 to train FTN.",A new theory of unsupervised domain adaptation for distance metric learning and its application to face recognition across diverse ethnicity variations.,second ; FTN ; two ; non-Caucasian ; MNIST-M ; NYTimes ; Nair & Hinton ; Parkhi ; Schroff et al. ; Caucasian,domain adaptation ; two examples ; the above scenario ; source ; a standard DANN ; a common task ; more data ; Grandvalet & Bengio ; FTNs ; facial analysis,second ; FTN ; two ; non-Caucasian ; MNIST-M ; NYTimes ; Nair & Hinton ; Parkhi ; Schroff et al. ; Caucasian,"Unsupervised domain adaptation is a promising avenue to enhance the performance of deep neural networks on target domains, using labels only from a source domain. However, the two predominant methods, domain discrepancy reduction learning and semi-supervised learning, are not readily applicable when source and target domains do not share a common label space. This paper addresses the above scenario by learning a representation space that retains discriminative power on both the (labeled) source and nonlabeled target domains while keeping representations for the two domains well-separated. The disjoint classification task and the verification task are modified to transform the source, target,",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Training neural networks on large datasets can be accelerated by distributing the workload over a network of machines. As datasets grow ever larger, networks of hundreds or thousands of machines become economically viable. The time cost of communicating gradients limits the effectiveness of using such large machine counts, as may the increased chance of network faults. We explore a particularly simple algorithm for robust, communication-efficient learning---signSGD. Workers transmit only the sign of their gradient vector to a server, and the overall update is decided by a majority vote. This algorithm uses 32x less communication per iteration than full-precision, distributed SGD. Under natural conditions verified by experiment, we prove that signSGD converges in the large and mini-batch settings, establishing convergence for a parameter regime of Adam as a byproduct. Aggregating sign gradients by majority vote means that no individual worker has too much power. We prove that unlike SGD, majority vote is robust when up to 50% of workers behave adversarially. The class of adversaries we consider includes as special cases those that invert or randomise their gradient estimate. On the practical side, we built our distributed training system in Pytorch. Benchmarking against the state of the art collective communications library (NCCL), our framework---with the parameter server housed entirely on one machine---led to a 25% reduction in time for training resnet50 on Imagenet when using 15 AWS p3.2xlarge machines. The most powerful supercomputer in the world is currently a cluster of over 27,000 GPUs at Oak Ridge National Labs (TOP500, 2018). Distributed algorithms designed for such large-scale systems typically involve both computation and communication: worker nodes compute intermediate results locally, before sharing them with their peers. When devising new machine learning algorithms for distribution over networks of thousands of workers, we posit the following desiderata: D1 fast algorithmic convergence; D2 good generalisation performance; We have analysed the theoretical and empirical properties of a very simple algorithm for distributed, stochastic optimisation. We have shown that SIGNSGD with majority vote aggregation is robust and communication efficient, whilst its per-iteration convergence rate is competitive with SGD for training large-scale convolutional neural nets on image datasets. We believe that it is important to understand this simple algorithm before going on to devise more complex learning algorithms.An important takeaway from our theory is that mini-batch SIGNSGD should converge if the gradient noise is Gaussian. This means that the performance of SIGNSGD may be improved by increasing the per-worker mini-batch size, since this should make the noise 'more Gaussian' according to the Central Limit Theorem.We will now give some possible directions for future work. Our implementation of majority vote may be further optimised by breaking up the parameter server and distributing it across machines. This would prevent a single machine from becoming a communication bottleneck as in our experiments. Though our framework speeds up Imagenet training, we still have a test set gap. Future work could attempt to devise new regularisation schemes for signed updates to close this gap. Promising future work could also explore the link between SIGNSGD and model compression.","Workers send gradient signs to the server, and the update is decided by majority vote. We show that this algorithm is convergent, communication efficient and fault tolerant, both in theory and in practice.",thousands ; AWS ; hundreds or thousands ; Oak Ridge National Labs ; SIGNSGD ; Pytorch ; Gaussian ; one ; Adam ; the Central Limit Theorem,it ; a network ; the art collective communications library ; some possible directions ; the gradient noise ; datasets ; Pytorch ; a very simple algorithm ; their gradient vector ; such large machine counts,thousands ; AWS ; hundreds or thousands ; Oak Ridge National Labs ; SIGNSGD ; Pytorch ; Gaussian ; one ; Adam ; the Central Limit Theorem,"Training neural networks on large datasets can be accelerated by distributing the workload over a network of machines. As datasets grow ever larger, networks of hundreds or thousands of machines become economically viable. The time cost of communicating gradients limits the effectiveness of using such large machine counts, as well as the increased chance of network faults. A simple algorithm for robust, communication-efficient learning---signSGD, where workers transmit only the sign of their gradient vector to a server, and the overall update is determined by a majority vote. This algorithm uses 32x less communication per iteration than full-precision, distributed SGD, and is more efficient",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Large matrix inversions have often been cited as a major impediment to scaling Gaussian process (GP) models. With the use of GPs as building blocks for ever more sophisticated Bayesian deep learning models, removing these impediments is a necessary step for achieving large scale results. We present a variational approximation for a wide range of GP models that does not require a matrix inverse to be performed at each optimisation step. Our bound instead directly parameterises a free matrix, which is an additional variational parameter. At the local maxima of the bound, this matrix is equal to the matrix inverse. We prove that our bound gives the same guarantees as earlier variational approximations. We demonstrate some beneficial properties of the bound experimentally, although significant wall clock time speed improvements will require future improvements in optimisation and implementation. One major obstacle to the wider adoption of Gaussian Process (GP) (Rasmussen and Williams, 2006) based models is their computational cost, which is mainly caused by matrix inverses and determinants. Advances in variational approximate inference methods have reduced the size of the matrices on which expensive operations need to be performed, leading to O N M 2 time costs instead of O N 3 (Titsias, 2009), with approximations arbitrarily good with M N (Burt et al., 2019) . Minibatches of size B N can be used for training at a cost of O BM 2 + M 3 per iteration (Hensman et al., 2013) . The usefulness of training with small minibatches is hampered by the iteration cost being dominated by O M 3 , which again comes from an inverse and determinant. The computation is usually done using the Cholesky decomposition, which requires serial operations and high-precision arithmetic. So in addition to being an asymptotically expensive operation, it is also poorly suited to modern deep learning hardware. Removing these per-iteration matrix operations therefore seems necessary to speed up training. In this work, we provide a variational lower bound that can be computed without expensive matrix operations like inversion. Our bound can be used as a drop-in replacement to the existing variational method of Hensman et al. (2013 Hensman et al. ( , 2015 , and can therefore directly be applied in a wide variety of models, such as deep GPs (Damianou and Lawrence, 2013) . We focus on the theoretical properties of this new bound, and show some initial experimental results for optimising this bound. We hope to realise the full promise in scalability that this new bound has in future work. We presented new variational bounds for GP models that function as drop-in replacements to those developed by Hensman et al. (2013 Hensman et al. ( , 2015 , but without needing to compute expensive matrix operations each iteration. We prove their properties and show that they behave as expected in simple experiments using a single layer and deep GP. We believe this method to be promising, as it removes the most frequently cited impediment against the scaling of GP models. However, more improvements are needed to obtain the full practical benefits.","We present a variational lower bound for GP models that can be optimised without computing expensive matrix operations like inverses, while providing the same guarantees as existing variational approximations.",GP ; Rasmussen ; Hensman et al ; M N ; Gaussian Process ; Gaussian ; Bayesian ; Williams ; Cholesky ; maxima,a wide variety ; a major impediment ; (Burt et al ; those ; the full promise ; the use ; maxima ; expensive matrix operations ; which ; We,GP ; Rasmussen ; Hensman et al ; M N ; Gaussian Process ; Gaussian ; Bayesian ; Williams ; Cholesky ; maxima,"Large matrix inversions are a major impediment to scaling Gaussian process (GP) models. With the use of GPs as building blocks for ever more sophisticated Bayesian deep learning models, removing these impediments is a crucial step for achieving large scale results. Our bound directly parameterises a free matrix, which is an additional variational parameter. At the local maxima of the bound, this matrix is equal to the matrix inverse. We prove that our bound gives the same guarantees as earlier variational approximations. We demonstrate some beneficial properties of this bound experimentally, although significant wall clock time speed improvements will require future improvements",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"We address the issue of limit cycling behavior in training Generative Adversarial Networks and propose the use of Optimistic Mirror Decent (OMD) for training Wasserstein GANs. Recent theoretical results have shown that optimistic mirror decent (OMD) can enjoy faster regret rates in the context of zero-sum games. WGANs is exactly a context of solving a zero-sum game with simultaneous no-regret dynamics.   Moreover, we show that optimistic mirror decent addresses the limit cycling problem in training WGANs. We formally show that in the case of bi-linear zero-sum games the last iterate of OMD dynamics converges to an equilibrium, in contrast to GD dynamics which are bound to cycle. We also portray the huge qualitative difference between GD and OMD dynamics with toy examples, even when GD is modified with many adaptations proposed in the recent literature, such as gradient penalty or momentum. We apply OMD WGAN training to a bioinformatics problem of generating DNA sequences. We observe that models trained with OMD achieve consistently smaller KL divergence with respect to the true underlying distribution, than models trained with GD variants. Finally, we introduce a new algorithm, Optimistic Adam, which is an optimistic variant of Adam. We apply it to WGAN training on CIFAR10 and observe improved performance in terms of inception score as compared to Adam. Generative Adversarial Networks (GANs) BID4 have proven a very successful approach for fitting generative models in complex structured spaces, such as distributions over images. GANs frame the question of fitting a generative model from a data set of samples from some distribution as a zero-sum game between a Generator (G) and a discriminator (D). The Generator is represented as a deep neural network which takes as input random noise and outputs a sample in the same space of the sampled data set, trying to approximate a sample from the underlying distribution of data. The discriminator, also modeled as a deep neural network is attempting to discriminate between a true sample and a sample generated by the generator. The hope is that at the equilibrium of this zero-sum game the generator will learn to generate samples in a manner that is indistinguishable from the true samples and hence has essentially learned the underlying data distribution.Despite their success at generating visually appealing samples when applied to image generation tasks, GANs are very finicky to train. One particular problem, raised for instance in a recent survey as a major issue BID5 is the instability of the training process. Typically training of GANs is achieved by solving the zero-sum game via running simultaneously a variant of a Stochastic Gradient Descent algorithm for both players (potentially training the discriminator more frequently than the generator).The latter amounts essentially to solving the zero-sum game via running no-regret dynamics for each player. However , it is known from results in game theory, that no-regret dynamics in zerosum games can very often lead to limit oscillatory behavior, rather than converge to an equilibrium. Even in convex-concave zero-sum games it is only the average of the weights of the two players that constitutes an equilibrium and not the last-iterate. In fact recent theoretical results of Mertikopoulos et al. (2017) show the strong result that no variant of GD that falls in the large class of Follow-theRegularized-Leader (FTRL) algorithms can converge to an equilibrium in terms of the last-iterate and are bound to converge to limit cycles around the equilibrium.Averaging the weights of neural nets is a prohibitive approach in particular because the zero-sum game that is defined by training one deep net against another is not a convex-concave zero-sum game. Thus it seems essential to identify training algorithms that make the last iterate of the training be very close to the equilibrium, rather than only the average.Contributions. In this paper we propose training GANs, and in particular Wasserstein GANs BID1 , via a variant of gradient descent known as Optimistic Mirror Descent. Optimistic Mirror Descent (OMD) takes advantage of the fact that the opponent in a zero-sum game is also training via a similar algorithm and uses the predictability of the strategy of the opponent to achieve faster regret rates. It has been shown in the recent literature that Optimistic Mirror Descent and its generalization of Optimistic Follow-the-Regularized-Leader (OFTRL), achieve faster convergence rates than gradient descent in convex-concave zero-sum games (Rakhlin & Sridharan, 2013a; b) and even in general normal form games (Syrgkanis et al., 2015) . Hence, even from the perspective of faster training, OMD should be preferred over GD due to its better worst-case guarantees and since it is a very small change over GD.Moreover, we prove the surprising theoretical result that for a large class of zero-sum games (namely bi-linear games), OMD actually converges to an equilibrium in terms of the last iterate. Hence, we give strong theoretical evidence that OMD can help in achieving the long sought-after stability and last-iterate convergence required for GAN training. The latter theoretical result is of independent interest, since solving zero-sum games via no-regret dynamics has found applications in many areas of machine learning, such as boosting BID2 . Avoiding limit cycles in such approaches could help improve the performance of the resulting solutions.We complement our theoretical result with toy simulations that portray exactly the large qualitative difference between OMD as opposed to GD (and its many variants, including gradient penalty, momentum, adaptive step size etc.). We show that even in a simple distribution learning setting where the generator simply needs to learn the mean of a multi-variate distribution, GD leads to limit cycles, while OMD converges pointwise.Moreover, we give a more complex application to the problem of learning to generate distributions of DNA sequences of the same cellular function. DNA sequences that carry out the same function in the genome, such as binding to a specific transcription factor, follow the same nucleotide distribution. Characterizing the DNA distribution of different cellular functions is essential for understanding the functional landscape of the human genome and predicting the clinical consequence of DNA mutations (Zeng et al., 2015; 2016; Zeng & Gifford, 2017) . We perform a simulation study where we generate samples of DNA sequences from a known distribution. Subsequently we train a GAN to attempt to learn this underlying distribution. We show that OMD achieves consistently better performance than GD variants in terms of the Kullback-Leibler (KL) divergence between the distribution learned by the Generator and the true distribution.Finally, we apply optimism to training GANs for images and introduce the Optimistic Adam algorithm. We show that it achieves better performance than Adam, in terms of inception score, when trained on CIFAR10.",We propose the use of optimistic mirror decent to address cycling problems in the training of GANs. We also introduce the Optimistic Adam algorithm,GD ; two ; Wasserstein ; Optimistic Mirror Decent ; Optimistic Adam ; Mertikopoulos et al ; Generative Adversarial Networks ; zero ; Optimistic Mirror Descent ; Zeng et al.,last-iterate convergence ; faster regret rates ; no-regret dynamics ; the same function ; exactly the large qualitative difference ; a zero-sum game ; KL ; Syrgkanis et al ; a new algorithm ; the huge qualitative difference,GD ; two ; Wasserstein ; Optimistic Mirror Decent ; Optimistic Adam ; Mertikopoulos et al ; Generative Adversarial Networks ; zero ; Optimistic Mirror Descent ; Zeng et al.,"Generative Adversarial Networks (GANs) BID4 introduce Optimistic Mirror Decent (OMD) for training Wasserstein GANs. Recent theoretical results have shown that optimistic mirror decent can enjoy faster regret rates in zero-sum games. WGANs is exactly a context of solving a zero -sum game with simultaneous no-regret dynamics.   Additionally, we show that optimistic mirrors decent addresses the limit cycling problem in training GGANs. We formally show that in the case of bi-linear zero-ssum games the last iterate of OMD dynamics converges to an equilibrium, in",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"The Lottery Ticket Hypothesis from Frankle & Carbin (2019) conjectures that, for typically-sized neural networks, it is possible to find small sub-networks which train faster and yield superior performance than their original counterparts. The proposed algorithm to search for such sub-networks (winning tickets), Iterative Magnitude Pruning (IMP), consistently finds sub-networks with 90-95% less parameters which indeed train faster and better than the overparameterized models they were extracted from, creating potential applications to problems such as transfer learning.

 In this paper, we propose a new algorithm to search for winning tickets, Continuous Sparsification, which continuously removes parameters from a network during training, and learns the sub-network's structure with gradient-based methods instead of relying on pruning strategies. We show empirically that our method is capable of finding tickets that outperforms the ones learned by Iterative Magnitude Pruning, and at the same time providing up to 5 times faster search, when measured in number of training epochs. Although deep neural networks have become ubiquitous in fields such as computer vision and natural language processing, extreme overparameterization is typically required to achieve state-ofthe-art results (Xie et al., 2017; Devlin et al., 2018) , causing higher training costs and hindering applications where memory or inference time are constrained. Recent theoretical work suggest that overparameterization plays a key role in both the capacity and generalization of a network (Neyshabur et al., 2018) , and in training dynamics (Allen-Zhu et al., 2019) . However, it remains unclear whether overparameterization is truly necessary to train networks to state-of-the-art performance. At the same time, empirical approaches have been successful in finding less overparameterized neural networks, either by reducing the network after training (Han et al., 2015; or through more efficient architectures that can be trained from scratch (Iandola et al., 2016) . Recently, the combination of these two approaches lead to new methods which discover efficient architectures through optimization instead of design Savarese & Maire, 2019) . Nonetheless, parameter efficiency is typically maximized by pruning an already trained network. The fact that pruned networks are hard to train from scratch (Han et al., 2015; suggests that, while overparameterization is not necessary for a model's capacity, it might be required for successful network training. Recently, this idea has been put into question by , where heavily pruned networks are trained faster than their original counterparts, often yielding superior performance. A key finding is that the same parameter initialization should be used when re-training the pruned network. A winning ticket, defined by a sub-network and a setting of randomly-initialized parameters, is quickly trainable and has already found applications in, for example, transfer learning (Morcos et al., 2019; Mehta, 2019; Soelen & Sheppard, 2019) , making the search for winning tickets a problem of independent interest. Currently, the standard algorithm to find winning tickets is Iterative Magnitude Pruning (IMP) , which consists of a repeating a 2-stage procedure that alternates between parameter optimization and pruning. As a result, IMP relies on a sensible choice for pruning strategy, and is time-consuming: finding a winning ticket with 1% of the original parameters in a 6-layer CNN requires over 20 rounds of training followed by pruning, totalling over 1000 epochs . Choosing a parameter's magnitude as pruning criterion has also shown to be sub-optimal in some settings (Zhou et al., 2019) , leading to the question of whether better winning tickets can be found by different pruning methods. Moreover, at each iteration, IMP resets the parameters of the network back to initialization, hence considerable time is spent on re-training similar networks with different sparsities. With the goal of speeding up the search for winning tickets in deep neural networks, we design a novel method, Continuous Sparsification, which continuously removes weights from a network during training, instead of following a strategy to prune parameters at discrete time intervals. Unlike IMP, our method approaches the search for sparse networks as a 0 -regularized optimization problem (Louizos et al., 2017) , resulting in a method that can be fully described in the optimization framework. To approximate 0 -regularization, we propose a smooth re-parameterization, allowing for the subnetwork's structure to be directly learned with gradient-based methods. Unlike previous works, our re-parameterization is deterministic, proving more convenient for the tasks of pruning and ticket search, while also yielding faster training times. Experimentally, our method offers superior performance when pruning VGG to extreme regimes, and is capable of finding winning tickets in Residual Networks trained on CIFAR-10 at a fraction of time taken by Iterative Magnitude Pruning. In particular, Continuous Sparsification successfully finds tickets in under 5 iterations, compared to 20 iterations required by Iterative Magnitude Pruning in the same setting. To further speed up the search for sub-networks, our method abdicates parameter rewinding, a key ingredient of Iterative Magnitude Pruning. By showing superior results without rewinding, our experiments offer insights on how ticket search should be performed. With , we now realize that sparse sub-networks can indeed be successfully trained from scratch, putting in question the belief that overparameterization is required for proper optimization of neural networks. Such sub-networks, called winning tickets, can be potentially used to significantly decrease the required resources for training deep networks, as they are shown to transfer between different, but similar, tasks (Mehta, 2019; Soelen & Sheppard, 2019) . Currently, the search for winning tickets is a poorly explored problem, where Iterative Magnitude Pruning stands as the only algorithm suited for this task, and it is unclear whether its key ingredients -post-training magnitude pruning and parameter rewinding -are the correct choices for the task. Here, we approach the problem of finding sparse sub-networks as an 0 -regularized optimization problem, which we approximate through a smooth, parameterized relaxation of the step function. Our proposed algorithm for finding winning tickets, Continuous Sparsification, removes parameters automatically and continuously during training, and can be fully described by the optimization framework. We show empirically that, indeed, post-training pruning might not be a sensible choice for finding winning tickets, raising questions on how the search for tickets differs from standard network compression. With this work, we hope to further motivate the problem of quickly finding tickets in overparameterized networks, as recent work suggests that the task might be highly relevant to transfer learning and mobile applications.",We propose a new algorithm that quickly finds winning tickets in neural networks.,Continuous Sparsification ; Han ; two ; Frankle & Carbin ; Devlin ; CNN ; The Lottery Ticket Hypothesis ; Iterative Magnitude Pruning ; Xie ; Mehta,Our proposed algorithm ; deep networks ; gradient-based methods ; an already trained network ; The proposed algorithm ; al. ; they ; winning tickets ; a key ingredient ; higher training costs,Continuous Sparsification ; Han ; two ; Frankle & Carbin ; Devlin ; CNN ; The Lottery Ticket Hypothesis ; Iterative Magnitude Pruning ; Xie ; Mehta,"The Lottery Ticket Hypothesis from Frankle & Carbin (2019) posits that, for typically-sized neural networks, it is possible to find small sub-networks which train faster and yield superior performance than their original counterparts. The proposed algorithm, Iterative Magnitude Pruning (IMP), consistently finds sub-nets with 90-95% less parameters which indeed outperform overparameterized models they were extracted from, creating potential applications to transfer learning. In this paper, we propose a new algorithm to search for winning tickets, Continuous Sparsification, which continuously removes parameters from a network during training,",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Capturing spatiotemporal dynamics is an essential topic in video recognition. In this paper, we present learnable higher-order operation as a generic family of building blocks for capturing higher-order correlations from high dimensional input video space. We prove that several successful architectures for visual classification tasks are in the family of higher-order neural networks, theoretical and experimental analysis demonstrates their underlying mechanism is higher-order.   On the task of video recognition, even using RGB only without fine-tuning with other video datasets, our higher-order models can achieve results on par with or better than the existing state-of-the-art methods on both Something-Something (V1 and V2) and Charades datasets. Actions in videos arise from motions of objects with respect to other objects and/or the background. To understand an action, an effective architecture should recognize not only the appearance of the target object associated with the action, but also how it relates to other objects in the scene, in both space and time. Figure 1 shows four different categories of actions. Each column shown an action where, in temporal order, the figures above occur before the figures below. Recognizing the hand and the object is not enough. To distinguish left to right motion from right to left motion, the model must know how the hand moves against the background. It is more complicated to classify pull and push since it is an XOR operation on the relative positions of the hand and the object resulting from the hand's movements. Figure 1a , since the hand moves from left to right and the hand is on the right side of the iron, it is pull from left to right. Figure 1d has the same hand movement, but it is a different category since the hand is on the left of the pen. Figure 1b is a reverse action of Figure 1a , but it is not pull from right to left. The key point here is the need for recognizing patterns in spatiotemporal context. Even the same hand-iron-background combination has different meanings in different spatiotemporal contexts. The number of combinations increases sharply as scenes become more complicated and the number of objects involved increases. It would be difficult for conventional convolutions which recognize fixed patterns that are determined by the fixed filter parameters to capture the variety of variations that distinguish the action classes. To recognize every object-in-context pattern, the model needs to have more detailed filters, potentially leading to a blow up of the number parameters. On the other hand, although the object-in-context patterns can vary, they are related through a higherorder structure: pushing an iron, pushing a pen, pulling an iron, and so on affects the spatio-temporal relations of the involved structures to one another in similar ways. We hypothesize that the structure of object-in-context patterns can be learned, i.e., the model can learn to conclude object-in-context pattern given the context, and propose a corresponding feature extractor. Explicitly, let X and Y respectively represent the input and output of a convolution. Let y p and {x p } represent a specific position of Y and the set of positions of X from which y p is computed, respectively. Denote conventional convolution operation as Y = f (X; Θ) where Θ is the shared parameters at different positions. The parameters act as determined feature extractors as As we analyze, the visual pattern of the target object can vary in different contexts, and determined feature extractors (filters) that ignore this dependence are not optimal. We replace the fixed filters with context-dependent filters y p = f ({x p }; w p ) where the filters w p are in turn obtained as w p = g({x p }; Θ). The mapping g is the structure of object-in-context patterns and Θ are the learned parameters as we hypothesize. The entire relation between Y and X can be compactly represented through the higher-order function Y = f (X; g(X; Θ)). The proposed model is able to capture spatiotemporal contexts effectively. We test our method on four benchmark datasets for action recognition: Kinetics-400 (Carreira & Zisserman, 2017) , Something-Something V1 (Mahdisoltani et al., 2018) , Something-Something V2, and Charades datasets (Sigurdsson et al., 2016) . Specifically, we make comprehensive ablation studies on Something-Something V1 datasets and further evaluate on the other three datasets to demonstrate the generality of our proposed method. The experiments establish significant advantages of the proposed models over existing algorithms, achieving results on par with or better than the current state-of-the-art methods. In this paper, we have introduced higher-order networks to the task of action recognition. Higherorder networks are constructed by a general building block, termed as H-block, which aims to model position-varying contextual information. As demonstrated on the Something-Something (V1 and V2), Kinetics-400 and Charades datasets, the proposed higher-order networks are able to achieve state-of-the-art results, even using only RGB mobility inputs without fine-tuning with other image or video datasets. The good performance may be ascribed to the fact that higher-order networks are a natural for context modeling. The actual model itself is not restricted to visual tasks, but may be applied in any task where a context governs the interpretation of an input feature, such as cross-modal or multi-modal operations. In future work, we plan to investigate the benefits of our higher-order model and its extensions, in a variety of other visual, text and cross-modal tasks. M. Zolfaghari, K. Singh, and T. Brox. Eco: Efficient convolutional network for online video understanding. In European Conference on Computer Vision (ECCV), 2018. A APPENDIX Table 6 shows the factorization of different context fields. For example, we stack three convolutions with kernel size 3 × 3 × 3 to get a 7 × 7 × 7 context field. 3 × 3 × 3 1 × 3 × 3 3 × 1 × 1 1 × 1 × 1 3 × 5 × 5 1 × 3 × 3 3 × 3 × 3 1 × 1 × 1 5 × 5 × 5 1 × 3 × 3 3 × 3 × 3 3 × 1 × 1 5 × 7 × 7 1 × 3 × 3 3 × 3 × 3 3 × 3 × 3 7 × 7 × 7 3 × 3 × 3 3 × 3 × 3 3 × 3 × 3 Table 7 shows our backbone ResNet-50 I3D model. We use T×H×W to represent the dimensions of kernels and output feature maps. T = {8, 32}, and the corresponding input size is 8×224×224 and 32×224×224.",Proposed higher order operation for context learning,Mahdisoltani ; Charades ; three ; g(X ; European Conference on Computer Vision ; T×H×W ; XOR ; RGB ; four ; al.,video datasets ; their underlying mechanism ; {x p ; online video understanding ; future work ; our higher-order models ; Mahdisoltani ; existing algorithms ; the hand ; the background,Mahdisoltani ; Charades ; three ; g(X ; European Conference on Computer Vision ; T×H×W ; XOR ; RGB ; four ; al.,"Learningable higher-order operation is a generic family of building blocks for capturing spatiotemporal dynamics from high dimensional input video space. In this paper, we show that several successful architectures for visual classification tasks are in the family of lower-order neural networks, demonstrating that their underlying mechanism is higher order. In video recognition, even using RGB only without fine-tuning with other video datasets, we can achieve results on par with or better than the existing state-of-the-art methods on both Something-Something (V1 and V2) and Charades datasets. Actions in videos arise from motions of objects with respect",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"In this paper, we design a generic framework for learning a robust text classification model that achieves accuracy comparable to standard full models under test-time
 budget constraints. We take a different approach from existing methods and learn to dynamically delete a large fraction of unimportant words by a low-complexity selector such that the high-complexity classifier only needs to process a small fraction of important words. In addition, we propose a new data aggregation method to train the classifier, allowing it to make accurate predictions even on fragmented sequence of words. Our end-to-end method achieves state-of-the-art performance while its computational complexity scales linearly with the small fraction of important words in the whole corpus. Besides, a single deep neural network classifier trained by our framework can be dynamically tuned to different budget levels at inference time. Recent advances in deep neural networks (DNN) has improved the performance of natural language processing tasks such as document classification, question answering, and sentiment analysis BID29 BID20 BID22 BID31 . These approaches process the entire text and construct representations of words and phrases in order to perform target tasks. While these models do realize high accuracy, their computational-time scales linearly with the size of the documents, which can be slow for documents containing many sentences. In this context, various approaches based on modifying the existing RNN or LSTM architecture have been proposed BID21 ; BID31 to speed-up processing. However, processing is still fundamentally sequential, which in turn requires loading entire documents to process, limiting compute gains. We proposed a budgeted learning framework for learning a robust classifier under test-time budget constraints. We demonstrated that training classifiers with data aggregation work well with low-complexity selectors based on word-embedding or bag-of-word model and achieve good performance with fragmented input. The future work includes applying the proposed framework to other text reading tasks and improving the data aggregation strategy by applying learning to search approaches BID4 .","Modular framework for document classification and data aggregation technique for making the framework robust to various distortion, and noise and focus only on the important words.",RNN,different budget levels ; phrases ; existing methods ; deep neural networks ; speed-up processing ; a single deep neural network classifier ; words ; a generic framework ; a small fraction ; addition,RNN,"In this paper, we design a generic framework for learning a robust text classification model that achieves accuracy comparable to standard full models under test-time budget constraints. We dynamically delete a large fraction of unimportant words by low-complexity selectors, making them only need to process a small fraction of important words. In addition, we propose a new data aggregation method to train the classifier, allowing it to make accurate predictions even on fragmented sequence of words. Our end-to-end method achieves state-of-the-art performance while its computational complexity scales linearly with the size of the documents and can be dynamically tuned to",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"In this paper, we propose a residual non-local attention network for high-quality image restoration. Without considering the uneven distribution of information in the corrupted images, previous methods are restricted by local convolutional operation and equal treatment of spatial- and channel-wise features. To address this issue, we design local and non-local attention blocks to extract features that capture the long-range dependencies between pixels and pay more attention to the challenging parts. Specifically, we design trunk branch and (non-)local mask branch in each (non-)local attention block. The trunk branch is used to extract hierarchical features. Local and non-local mask branches aim to adaptively rescale these hierarchical features with mixed attentions. The local mask branch concentrates on more local structures with convolutional operations, while non-local attention considers more about long-range dependencies in the whole feature map. Furthermore, we propose residual local and non-local attention learning to train the very deep network, which further enhance the representation ability of the network. Our proposed method can be generalized for various image restoration applications, such as image denoising, demosaicing, compression artifacts reduction, and super-resolution. Experiments demonstrate that our method obtains comparable or better results compared with recently leading methods quantitatively and visually. Image restoration aims to recover high-quality (HQ) images from their corrupted low-quality (LQ) observations and plays a fundamental role in various high-level vision tasks. It is a typical ill-posed problem due to the irreversible nature of the image degradation process. Some most widely studied image restoration tasks include image denoising, demosaicing, and compression artifacts reduction. By distinctively modelling the restoration process from LQ observations to HQ objectives, i.e., without assumption for a specific restoration task when modelling, these tasks can be uniformly addressed in the same framework. Recently, deep convolutional neural network (CNN) has shown extraordinary capability of modelling various vision problems, ranging from low-level (e.g., image denoising (Zhang et al., 2017a) , compression artifacts reduction BID10 , and image super-resolution BID43 BID37 BID40 ) to high-level (e.g., image recognition ) vision applications.However, there are mainly three issues in the existing CNN based methods above. First, the receptive field size of these networks is relatively small. Most of them extract features in a local way with convolutional operation, which fails to capture the long-range dependencies between pixels in the whole image. A larger receptive field size allows to make better use of training inputs and more context information. This would be very helpful to capture the latent degradation model of LQ images, especially when the images suffer from heavy corruptions. Second, distinctive ability of these networks is also limited. Let's take image denoising as an example. For a noisy image, the noise may appear in both the plain and textural regions. Noise removal would be easier in the plain area than that in the textural one. It is desired to make the denoising model focus on textual area more. However, most previous denoising methods neglect to consider different contents in the noisy input and treat them equally. This would result in over-smoothed outputs and some textural details would also fail to be recovered. Third, all channel-wise features are treated equally in those networks. This naive treatment lacks flexibility in dealing with different types of information (e.g., low-and high-frequency information). For a set of features, some contain more information related to HQ image and the others may contain more information related to corruptions. The interdependencies among channels should be considered for more accurate image restoration.To address the above issues, we propose the very deep residual non-local attention networks (RNAN) for high-quality image restoration. We design residual local and non-local attention blocks as the basic building modules for the very deep network. Each attention block consists of trunk and mask branches. We introduce residual block for trunk branch and extract hierarchical features. For mask branch, we conduct feature downscaling and upscaling with largestride convolution and deconvolution to enlarge receptive field size. Furthermore, we incorporate non-local block in the mask branch to obtain residual non-local mixed attention. We apply RNAN for various restoration tasks, including image denoising, demosaicing, and compression artifacts reduction. Extensive experiments show that our proposed RNAN achieves state-of-the-art results compared with other recent leading methods in all tasks. To the best of our knowledge, this is the first time to consider residual non-local attention for image restoration problems.The main contributions of this work are three-fold:• We propose the very deep residual non-local networks for high-quality image restoration.The powerful networks are based on our proposed residual local and non-local attention blocks, which consist of trunk and mask branches. The network obtains non-local mixed attention with non-local block in the mask branch. Such attention mechanis helps to learn local and non-local information from the hierarchical features.• We propose residual non-local attention learning to train very deep networks by preserving more low-level features, being more suitable for image restoration. Using non-local lowlevel and high-level attention from the very deep network, we can pursue better network representational ability and finally obtain high-quality image restoration results.• We demonstrate with extensive experiments that our RNAN is powerful for various image restoration tasks. RNAN achieves superior results over leading methods for image denoising, demosaicing, compression artifacts reduction, and super-resolution. In addition, RNAN achieves superior performance with moderate model size and performs very fast.",New state-of-the-art framework for image restoration,CNN ; Zhang et al. ; three ; First ; Second ; Third ; RNAN ; results.•,image denoising ; our RNAN ; The trunk branch ; local and non-local attention blocks ; the image degradation process ; better use ; RNAN ; The local mask branch ; those networks ; the very deep residual non-local networks,CNN ; Zhang et al. ; three ; First ; Second ; Third ; RNAN ; results.•,"In this paper, we propose a residual non-local attention network for high-quality image restoration. Despite the uneven distribution of information in corrupted images, previous methods were restricted by local convolutional operation and equal treatment of spatial- and channel-wise features. To address this issue, we design trunk branch and non-)local mask branches in each (non-)local attention block. The trunk branch is used to extract hierarchical features, while the non-, local mask branches are used to adaptively rescale these hierarchical features with mixed attentions. The local mask branch concentrates on more local structures with convolutionals operations, while non",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Foveation is an important part of human vision, and a number of deep networks have also used foveation. However, there have been few systematic comparisons between foveating and non-foveating deep networks, and between different variable-resolution downsampling methods. Here we define several such methods, and compare their performance on ImageNet recognition with a Densenet-121 network. The best variable-resolution method slightly outperforms uniform downsampling. Thus in our experiments, foveation does not substantially help or hinder object recognition in deep networks.",We compare object recognition performance on images that are downsampled uniformly and with three different foveation schemes.,ImageNet,deep networks ; several such methods ; ImageNet ; we ; Foveation ; foveating deep networks ; human vision ; an important part ; a number ; our experiments,ImageNet,"Foveation is an important part of human vision, and a number of deep networks have also used foveation. However, there have been systematic comparisons between foveating and non-foveating deep networks, and between different variable-resolution downsampling methods. Here we define several such methods, and compare their performance on ImageNet recognition with a Densenet-121 network. Foveation does not significantly help or hinder object recognition in deep networks.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"What would be learned by variational autoencoder(VAE) and what influence the disentanglement of VAE? This paper tries to preliminarily address VAE's intrinsic dimension, real factor, disentanglement and indicator issues theoretically in the idealistic situation and implementation issue practically through noise modeling perspective in the realistic case.   On intrinsic dimension issue, due to information conservation, the idealistic VAE learns and only learns intrinsic factor dimension. Besides, suggested by mutual information separation property, the constraint induced by Gaussian prior to the VAE objective encourages the information sparsity in dimension. On disentanglement issue,   subsequently, inspired by information conservation theorem the clarification on disentanglement in this paper is made. On real factor issue, due to factor equivalence, the idealistic VAE possibly learns any factor set in the equivalence class.   On indicator issue, the behavior of current disentanglement metric is discussed, and several performance indicators regarding the disentanglement and generating influence are subsequently raised to evaluate the performance of VAE model and to supervise the used factors. On implementation issue, the experiments under noise modeling and constraints empirically testify the theoretical analysis and also show their own characteristic in pursuing disentanglement. Variational AutoEncoder(VAE)s BID9 , Rezende et al. (2014) ) have shown their powerful human-like abilities: modelling causal relationship, unsupervisedly extracting disentangled factors/representation BID1 ) and generating signals with abundant diversities in a ""latent-factor-controllable"" way. Those capabilities enable the knowledge transferring through shared causes/factors among different tasks/experiences, emphasized as the important human advantages against the current machine by BID12 and compling with the ideal mental imagery mechanism in memory and thinking. Benefitted from those capabilities, VAEs have been widely applied to various applications, including disentangled representations learning of images and time series BID6 , BID11 , Mathieu et al. (2016) , BID3 ), few-shot and transfer learning (Rezende et al. (2016) , BID8 , BID7 ), causal relationships modeling (Louizos et al. (2017) ), pixel trajectory predicting (Walker et al. (2016) ), joint multi-modal inference learning (Suzuki et al. (2016) ), increasing diversity in imitation learning (Wang et al. (2017) ), generation with memory BID16 ) and etc.However, the lack of public theoretical study regarding the generating and inference procedure induced by VAEs is tripping the research process: Idealistic VAE learns and only learns the intrinsic factor dimension. The illustration of the information conservation theorem 1. Suppose that the oracle data, denoted by random variable x, is generated by y (with P independent unit Gaussian random variables) with a homeomorphism mapping x = φ(y ). Idealistic VAE will be forced to learn the factor z (with H independent unit Gaussian random variables) that generates the x with a homeomorphism mapping x = ψ(z). It yields z = ψ −1 • φ(y) and y = φ −1 • ψ(z). Then according to the information conservation theorem, it must hold that H = P . Gaussian-VAE (Kingma & Welling (2013) , Rezende et al. (2014) ) is an scalable unsupervised representation learning model BID6 ), and since Gaussian distribution can be continuously and reversibly mapping to many other distributions, the theoretical analysis on it is also instructive for other continuous latent factors VAE. DISPLAYFORM0",This paper tries to preliminarily address the disentanglement theoretically in the idealistic situation and practically through noise modelling perspective in the realistic case.,VAE ; Gaussian ; Rezende et al ; modelling causal ; Mathieu ; Louizos ; Wang et al. ; φ(y ; Gaussian-VAE,the research process ; indicator ; Those capabilities ; H independent unit ; random variable ; the idealistic situation ; modelling causal ; Wang et al ; disentanglement ; intrinsic factor dimension,VAE ; Gaussian ; Rezende et al ; modelling causal ; Mathieu ; Louizos ; Wang et al. ; φ(y ; Gaussian-VAE,"Variational AutoEncoder(VAE)s BID9 and Rezende et al. (2014) have shown their powerful human-like abilities in modelling causal relationship, unsupervisedly extracting disentangled factors/representation BID1) and generating signals with abundant diversities in a ""latent-factor-controllable"" way. These capabilities enable knowledge transfer through shared causes/factors among different tasks/experiences, enabling human advantages against the current machine by BID12 and complicating with the ideal mental imagery mechanism in memory and thinking. On real factor issue, due to factor equivalence,",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Lifelong learning is the problem of learning multiple consecutive tasks in a sequential manner where knowledge gained from previous tasks is retained and used for future learning. It is essential towards the development of intelligent machines that can adapt to their surroundings. In this work we focus on a lifelong learning approach to generative modeling where we continuously incorporate newly observed streaming distributions into our learnt model. We do so through a student-teacher architecture which allows us to learn and preserve all the distributions seen so far without the need to retain the past data nor the past models. Through the introduction of a novel cross-model regularizer, the student model leverages the information learnt by the teacher, which acts as a summary of everything seen till now. The regularizer has the additional benefit of reducing the effect of catastrophic interference that appears when we learn over streaming data. We demonstrate its efficacy on streaming distributions as well as its ability to learn a common latent representation across a complex transfer learning scenario.
 Deep unsupervised generative learning allows us to take advantage of the massive amount of unlabeled data available in order to build models that efficiently compress and learn an approximation of the true data distribution. It has numerous applications such as image denoising, inpainting, super-resolution, structured prediction, clustering, pre-training and many more. However, something that is lacking in the modern ML toolbox is an efficient way to learn these deep generative models in a sequential, lifelong setting.In a lot of real world scenarios we observe distributions sequentially. Examples of this include streaming data from sensors such as cameras and microphones or other similar time series data. A system can also be resource limited wherein all of the past data or learnt models cannot be stored. We are interested in the lifelong learning setting for generative models where data arrives sequentially in a stream and where the storage of all data is infeasible. Within the stream, instances are generated according to some non-observed distribution which changes at given time-points. We assume we know the time points at which the transitions occur and whether the latent distribution is a completely new one or one that has been observed before. We do not however know the underlying identity of the individual distributions. Our goal is to learn a generative model that can summarize all the distributions seen so far in the stream. We give an example of such a setting in figure 1(a ) using MNIST BID19 , where we have three unique distributions and one that is repeated.Since we only observe one distribution at a time we need to develop a strategy of retaining the previously learnt knowledge (i.e. the previously learnt distributions) and integrate it into future learning. To accumulate additional distributions in the current generative model we utilize a student-teacher architecture similar to that in distillation methods BID9 ; BID4 . The teacher contains a summary of all past distributions and is used to augment the data used to train the student model. The student model thus receives data samples from the currently observable distribution as well as synthetic data samples from previous distributions. This allows the student model to learn a distribution that summarizes the current as well as all previously observed distributions. Once a new distribution shift occurs the existing teacher model is discarded, the student becomes the teacher and a new student is instantiated.We further leverage the generative model of the teacher by introducing a regularizer in the learning objective function of the student that brings the posterior distribution of the latter close to that of the former. This allows us to build upon and extend the teacher's generative model in the student each time the latter is re-instantiated (rather than re-learning it from scratch). By coupling this regularizer with a weight transfer from the teacher to the student we also allow for faster convergence of the student model. We empirically show that the regularizer allows us to learn a much larger set of distributions without catastrophic interference BID23 .We build our lifelong generative models over Variational Autoencoders (VAEs) BID17 . VAEs learn the posterior distribution of a latent variable model using an encoder network; they generate data by sampling from a prior and decoding the sample through a conditional distribution learnt by a decoder network.Using a vanilla VAE as a teacher to generate synthetic data for the student is problematic due to a couple of limitations of the VAE generative process. 1) Sampling the prior can select a point in the latent space that is in between two separate distributions, causing generation of unrealistic synthetic data and eventually leading to loss of previously learnt distributions. 2) Additionally, data points mapped to the posterior that are further away from the prior mean will be sampled less frequently resulting in an unbalanced sampling of the constituent distributions. Both limitations can be understood by visually inspecting the learnt posterior distribution of a standard VAE evaluated on test images from MNIST as shown in figure 1(b). To address the VAE's sampling limitations we decompose the latent variable vector into a continuous and a discrete component. The discrete component is used to summarize the discriminative information of the individual generative distributions while the continuous caters for the remaining sample variability. By independently sampling the discrete and continuous components we preserve the distributional boundaries and circumvent the two problems above.This sampling strategy, combined with the proposed regularizer allows us to learn and remember all the individual distributions observed in the past. In addition we are also able to generate samples from any of the past distributions at will; we call this property consistent sampling. In this work we propose a novel method for learning generative models over streaming data following the lifelong learning principles. The principal assumption for the data is that they are generated by multiple distributions and presented to the learner in a sequential manner (a set of observations from a single distribution followed by a distributional transition). A key limitation for the learning is that the method can only access data generated by the current distribution and has no access to any of the data generated by any of the previous distributions.The proposed method is based on a dual student-teacher architecture where the teacher's role is to preserve the past knowledge and aid the student in future learning. We argue for and augment the standard VAE's ELBO objective by terms helping the teacher-student knowledge transfer. We demonstrate on a series of experiments the benefits this augmented objective brings in the lifelong learning settings by supporting the retention of previously learned knowledge (models) and limiting the usual effects of catastrophic interference.In our future work we will explore the possibilities to extend our architecture to GAN-like BID8 learning with the prospect to further improve the generative abilities of our method. GANs, however, do not use a metric for measuring the quality of the learned distributions such as the marginal likelihood or the ELBO in their objective and therefore the transfer of our architecture to these is not straightforward. Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017.",Lifelong distributional learning through a student-teacher architecture coupled with a cross model posterior regularizer.,Roland Vollgraf ; VAE ; ML ; GAN ; Han ; Variational Autoencoders ; Kashif Rasul ; between two ; three ; one,The principal assumption ; all the individual distributions ; previously learnt distributions ; which ; streaming data ; Fashion-mnist ; image denoising ; two ; the past models ; all past distributions,Roland Vollgraf ; VAE ; ML ; GAN ; Han ; Variational Autoencoders ; Kashif Rasul ; between two ; three ; one,"Lifelong learning is the process of learning multiple consecutive tasks in a sequential manner where knowledge gained from previous tasks is retained and used for future learning. It is essential for intelligent machines that can adapt to their surroundings. In this work we focus on a lifelong learning approach to generative modeling where we continuously incorporate newly observed streaming distributions into our learnt model. This approach enables us to learn and preserve all the distributions seen so far without the need to retain the past data nor the past models. Through the introduction of a novel cross-model regularizer, the student model leverages the information learnt by the teacher, which acts as a summary",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Wasserstein GAN(WGAN) is a model that minimizes the Wasserstein distance between a data distribution and sample distribution. Recent studies have proposed stabilizing the training process for the WGAN and implementing the Lipschitz constraint. In this study, we prove the local stability of optimizing the simple gradient penalty $\mu$-WGAN(SGP $\mu$-WGAN) under suitable assumptions regarding the equilibrium and penalty measure $\mu$. The measure valued differentiation concept is employed to deal with the derivative of the penalty terms, which is helpful for handling abstract singular measures with lower dimensional support. Based on this analysis, we claim that penalizing the data manifold or sample manifold is the key to regularizing the original WGAN with a gradient penalty. Experimental results obtained with unintuitive penalty measures that satisfy our assumptions are also provided to support our theoretical results. Deep generative models reached a turning point after generative adversarial networks (GANs) were proposed by BID2 . GANs are capable of modeling data with complex structures. For example, DCGAN can sample realistic images using a convolutional neural network (CNN) structure BID12 . GANs have been implemented in many applications in the field of computer vision with good results, such as super-resolution, image translation, and text-to-image generation BID7 BID6 Zhang et al., 2017; BID13 .However , despite these successes, GANs are affected by training instability and mode collapse problems. GANs often fail to converge, which can result in unrealistic fake samples. Furthermore , even if GANs successfully synthesize realistic data, the fake samples exhibit little variability.A common solution to this instability problem is injecting an instance noise and finding different divergences. The injection of instance noise into real and fake samples during the training procedure was proposed by Sønderby et al. (2017) , where its positive impact on the low dimensional support for the data distribution was shown to be a regularizing factor based on the Wasserstein distance, as demonstrated analytically by . In f -GAN, f -divergence between the target and generator distributions was suggested which generalizes the divergence between two distributions BID11 . In addition, a gradient penalty term which is related with Sobolev IPM(Integral Probability Metric) between data distribution and sample distribution was suggested by BID9 .The Wasserstein GAN (WGAN) is known to resolve the problems of generic GANs by selecting the Wasserstein distance as the divergence . However, WGAN often fails with simple examples because the Lipschitz constraint on discriminator is rarely achieved during the optimization process and weight clipping. Thus, mimicking the Lipschitz constraint on the discriminator by using a gradient penalty was proposed by BID3 .Noise injection and regularizing with a gradient penalty appear to be equivalent. The addition of instance noise in f -GAN can be approximated to adding a zero centered gradient penalty BID14 . Thus, regularizing GAN with a simple gradient penalty term was suggested by BID8 who provided a proof of its stability.Based on a theoretical analysis of the dynamic system, BID10 proved the local exponential stability of the gradient-based optimization dynamics in GANs by treating the simultaneous gradient descent algorithm with a dynamic system approach. These previous studies were useful because they showed that the local behavior of GANs can be explained using dynamic system tools and the related Jacobian's eigenvalues.In this study, we aim to prove the convergence property of the simple gradient penalty µ-Wasserstein GAN(SGP µ-WGAN) dynamic system under general gradient penalty measures µ. To the best of our knowledge , our study is the first theoretical approach to GAN stability analysis which deals with abstract singular penalty measure. In addition, measure valued differentiation BID4 ) is applied to take the derivative on the integral with a parametric measure, which is helpful for handling an abstract measure and its integral in our proof.The main contributions of this study are as follows.• We prove the regularized effect and local stability of the dynamic system for a general penalty measure under suitable assumptions. The assumptions are written as both a tractable strong version and intractable weak version. To prove the main theorem, we also introduce the measure valued differentiation concept to handle the parametric measure.• Based on the proof of the stability , we explain the reason for the success of previous penalty measures. We claim that the support of a penalty measure will be strongly related to the stability, where the weight on the limiting penalty measure might affect the speed of convergence.• We experimentally examined the general convergence results by applying two test penalty measures to several examples. The proposed test measures are unintuitive but they still satisfy the assumptions and similar convergence results were obtained in the experiment. In this study, we proved the local stability of simple gradient penalty µ-WGAN optimization for a general class of finite measure µ. This proof provides insight into the success of regularization with previously proposed penalty measures. We explored previously proposed analyses based on various gradient penalty methods. Furthermore, our theoretical approach was supported by experiments using unintuitive penalty measures. In future research, our works can be extended to alternative gradient descent algorithm and its related optimal hyperparameters. Stability at non-realizable equilibrium points is one of the important topics on stability of GANs. Optimal penalty measure for achieving the best convergence speed can be also investigated using a spectral theory, which provides the mathematical analysis on stability of GAN with a precise information on the convergence theory.",This paper deals with stability of simple gradient penalty $\mu$-WGAN optimization by introducing a concept of measure valued differentiation.,DCGAN ; GAN ; Wasserstein ; two ; WGAN ; $\mu$-WGAN(SGP $ ; zero ; Lipschitz ; \mu$-WGAN ; al.,Deep generative models ; alternative gradient descent algorithm ; suitable assumptions ; sample distribution ; image ; two distributions ; CNN ; which ; dynamic system tools ; This proof,DCGAN ; GAN ; Wasserstein ; two ; WGAN ; $\mu$-WGAN(SGP $ ; zero ; Lipschitz ; \mu$-WGAN ; al.,"Wasserstein GAN(WGAN) is a model that minimizes the Wasserstein distance between a data distribution and sample distribution. Recent studies have proposed stabilizing the training process for the WGAN and implementing the Lipschitz constraint. This study demonstrates the local stability of optimizing the simple gradient penalty $\mu$-WGAN(SGP $\mu$. The measure valued differentiation concept is employed to deal with the derivative of penalty terms, which is helpful for handling abstract singular measures with lower dimensional support. Based on this analysis, we assert that penalizing the data manifold or sample manifold is the key to regularizing the",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Few-Shot Learning (learning with limited labeled data) aims to overcome the limitations of traditional machine learning approaches which require thousands of labeled examples to train an effective model. Considered as a hallmark of human intelligence, the community has recently witnessed several contributions on this topic, in particular through meta-learning, where a model learns how to learn an effective model for few-shot learning. The main idea is to acquire prior knowledge from a set of training tasks, which is then used to perform (few-shot) test tasks. Most existing work assumes that both training and test tasks are drawn from the same distribution, and a large amount of labeled data is available in the training tasks. This is a very strong assumption which restricts the usage of meta-learning strategies in the real world where ample training tasks following the same distribution as test tasks may not be available. In this paper, we propose a novel meta-learning paradigm wherein a few-shot learning model is learnt, which simultaneously overcomes domain shift between the train and test tasks via adversarial domain adaptation. We demonstrate the efficacy the proposed method through extensive experiments. Few-Shot Learning aims to learn a prediction model from very limited amount of labelled data BID13 . Specifically, given a K−shot, N −class data for a classification task, the aim is to learn a multi-class classification model for N − classes, with K−labeled training examples for each class. Here K is usually a small number (e.g. 1, or 5). Considered as one of the hallmarks of human intelligence BID12 , this topic has received considerable interest in recent years BID13 BID11 BID33 BID2 . Modern techniques solve this problem through meta-learning, using an episodic learning paradigm. The main idea is to use a labeled training dataset to effectively acquire prior knowledge, such that this knowledge can be transferred to novel tasks where few-shot learning is to be performed. Different from traditional transfer learning BID21 BID35 , here few-shot tasks are simulated using the labeled training data through episodes, in order to acquire prior knowledge that is specifically tailored for performing few-shot tasks. For example, given a set of labeled training data with a finite label space Y train , the epsiodic paradigm is used to acquire prior knowledge which is stored in a model. Each episode is generated i.i.d from an unknown task distribution τ train . This model is then used to do a novel few shot classification task which is drawn from an unknown task distribution τ test . The test task comprises small amount of labeled data with a finite label space Y test , and the sets Y train and Y test are (possibly) mutually exclusive. Using this labeled data, and acquired prior knowledge, the goal is to predict the labels of all unlabeled instances in the test task.A very restrictive assumption of existing meta-learning approaches for few-shot learning is that train and test tasks are drawn from the same distribution, i.e., τ train = τ test . In this scenario, the metalearner's objective is to minimize its expected loss over the tasks drawn from the task distribution τ train . This assumption prohibits the use of meta-learning strategies for real-world applications, where training tasks with ample labeled data, and drawn from the same distribution as the test tasks are very unlikely to be available. Consider the case of a researcher or practitioner who wishes to train a prediction model for their own dataset where labeled data is very limited. It is unreasonable to assume that they would have a large corpus of labeled data for a set of related tasks in the same domain. Without this, they are not able to train effective few-shot models for their task. A more desirable option is to use the training tasks where ample training data is available, and adapt the model to be effective on test tasks in a different domain. A possible way to tackle this problem could be through the use of domain adaptation techniques BID4 BID7 that address the domain shift between the training and test data. However, all of these approaches address the single-task scenario, i.e., Y train = Y test , where the training data and test data are sampled from the same task but there is a domain shift at a data-level. This is in contrast to the meta-learning setting where the training data contains multiple tasks and the goal is to learn new tasks from test data, i.e., domain shift exists at a task-level and Y train ∩ Y test = ∅. As a result, these domain adaptation approaches cannot be directly applied. We show an overview of different problem settings in TAB0 . DISPLAYFORM0 In order to solve the few-shot learning problem under a domain shift we propose a novel meta-learning paradigm: Meta-Learning with Domain Adaptation (MLDA). Existing meta-learning approaches for few-shot learning use only the given training data to learn a model, and as a result they do not account for any domain shift between the training tasks and the few-shot test tasks. In contrast, we assume that the model has access to the unlabeled instances in the domain of the few-shot test tasks prior to the training procedure, and utilize these instances for incorporating the domain-shift information. We train the model under the episodic-learning paradigm, but in each episode we aim to train a model which achieves two goals: first the model should be good at few-shot learning, and second the model should be unaffected by a possible domain shift. The first goal is achieved by updating the model based on the few-shot learning loss suffered by the model for a given episode. The second goal is achieved by an adversarial domain adaptation approach, where a mapping is used which styles the training task to resemble the test task. In this way, the trained model can perform few-shot predictions on the test tasks, and achieve what we term task-level domain adaptation.The episodic update is done via Prototypical Networks BID28 (as a specific instantiation, though other approaches can be applied), where on a simulated few-shot task (a small support set behaves as training, and a query set behaves as test data), an embedding is produced for both support and query instances. The mean of support embedding of each class is the prototype, and query instances are labeled based on their distance to these prototypes. Based on the loss on these query instances, the embedding function is updated. For achieving invariance to domain shift, we follow the principle of adversarial domain adaptation, but we differ from the traditional approaches in that we are performing task-level domain adaptation, whereas they performed data-level domain adaptation. The early approaches to adversarial domain adaptation aimed at obtaining a feature embedding that was invariant to both the training domain and the test domain, as well as learning a prediction model in the training domain BID4 . However, these approaches possibly learnt a highly unconstrained feature embedding (particularly when the embedding was very high dimensional), and were outperformed by GAN-based approaches (often used for image translation) BID29 BID38 BID7 . As a result we use a mapping function to style the training tasks to resemble test tasks, and optimize it using a GAN loss. The overall framework delivers a model that uses training tasks from one distribution to meta-learn a few-shot model for a task from another distribution. We perform extensive experiments to show the efficacy of the proposed method.2 RELATED WORK 2.1 META-LEARNING FOR FEW-SHOT LEARNING Few-Shot Learning refers to learning a prediction model from small amount of labeled data BID1 BID12 . Early approaches used a Bayesian model BID1 , or hand-designed priors BID13 . More recently, meta-learning approaches have become extremely successful for addressing few-shot learning BID33 BID2 . Instead of training a model directly on the few-shot data, meta-learning approaches use a corpus of labeled data, and simulate few-shot tasks on them to learn how to do few-shot learning. Some approaches follow the non-parametric principle, and develop a differentiable K−nearest neighbour solution BID33 BID27 BID28 . The main concept is to learn an embedding space that is tailored for performing effective K-nearest neighbour. BID20 extend these approaches with metric scaling to condition the embedding based on the given task. Another category of meta-learning aims to learn how to quickly adapt a model in few gradient steps for a few-shot learning task BID2 BID23 . These optimization based approaches aim to learn an initialization from a set of training tasks, which can be quickly adapted (e.g. one-step gradient update) when presented with a novel few-shot task. Some other approaches consider using a ""memory""-based approach BID26 BID19 . There have also been approaches that try to enhance meta-learning performance through use of additional information. For example, BID24 use unlabeled data to develop semi supervised few-shot learning. BID37 use external data to generate concepts, and performs meta-learning in the concept space. However, all of these approaches assume that the training tasks and testing tasks are drawn from the same distribution (τ train = τ test ). If there is a task-level domain shift, the above approaches will fail to perform few-shot learning on novel test tasks. Our approach of meta-learning with domain adaptation overcomes this domain shift, to perform few-shot learning on tasks in a different domain. In this paper we investigated a novel problem setting: Meta-Learning for few-shot learning under task-level domain shift. Existing meta learning paradigm for few-shot learning was designed under the assumption that both training tasks and test tasks were drawn from the same distribution. This may not be the case for real world applications, where researchers may not find ample labeled data to simulate training tasks to be drawn from the same distribution as their test tasks. To alleviate this, we propose a meta learning with domain adaptation paradigm, which performs meta-learning by incorporating few-shot learning and task-level domain adaptation unified into a single meta-learner. We instantiate our few-shot model with Prototypical Networks and adopt an adversarial approach for task level domain adaptation. We conduct several experiments to validate the proposed ideas.6 APPENDIX: DATASET CONSTRUCTION DISPLAYFORM0 Here we show the details of the original Omniglot dataset and the statistical details, and some examples of how the characters look in a different domain. The meta-train, meta-test, and domain adaptation split of classes we used are based on the same split used in prior work. There is no overlap of classes or instances among the three sets, i.e., they are all mutually exclusive both at instance and class-level. 7 APPENDIX: MODEL CONFIGURATION AND TRAINING For MLDA, we followed training procedures adopted similar to BID38 and BID28 . Specifically, for CycleGAN, we changed the parameters related to image dimensions (scaling and cropping pre-processing) to keep the generated image size fixed to the original size i.e. 28 × 28 for Omniglot/Omniglot-M and 84 × 84 for OfficeHome Clipart/Product. The generative networks are the same as the original work BID38 , each including two stride-2 convolutions with residual blocks, and two fractionally-strided convolutions with stride 1 2 . For all experiments, we used 6 blocks to generate images. We initialized the learning rate to 0.0002 and kept this learning rate for training till 100 epochs. The model after each epoch was used to translate source task to target task. Weights were initialized with Gaussian distribution with mean 0 and standard deviation 0.02. We use the Adam solver with a batch size of 1. We also modified the loss function for diffent settings of MLDA. Specifically, for MLDA, we removed the losses related to target → source (B → A) mapping and set λ idt = 0. For MLDA+idt, we set λ idt = 0.1. For MLDA+idt+revMap, we kept the loss function the same as the original CycleGAN.For Prototypical Networks, we followed the best hyperparameter settings in BID28 . We used the same embedding architecture in the original work, including four convoluational blocks, each of which comprises a 64-filter 3 × 3 convolution, batch normalization layer, ReLU activation, and 2 × 2 max-pooling layer. This results in 64-dimensional output space for Omniglot/Omniglotm and 1600-dimensional output space for HomeOffice Clipart/Product. For Omniglot/Omniglotm experiments, the learning rate was set to 0.001 and reduced by half every 2K iterations, starting from iteration 2K. The network is trained for a total of 20K iterations. For OfficeHome Product/Clipart experiments, we initialized the learning rate to 0.001 and decayed the learning rate by half every 25K iterations, starting from iteration 25K. The model is trained up to 100K iterations. We also use Adam solver to optimize the networks. Following BID28 , we chose squared Euclidean distance to perform kNN classification as this metric showed superior performance in prior work.For all the baselines, we reused the official code and ran them with default hyperparameters. We only modified parameters to make the models compatible with the image resolution and number of classes in Omniglot/Omniglot-M and Product/Clipart datasets. In all experiments, we set N c classes and N S support points per class identical at training and test-time. We also fixed 15 query points per class per episode in all experiments. We computed the classification accuracy by averaging over 600 randomly generated episodes from the Meta-test set.",Meta Learning for Few Shot learning assumes that training tasks and test tasks are drawn from the same distribution. What do you do if they are not? Meta Learning with task-level Domain Adaptation!,one ; recent years ; Few-Shot Learning ; ReLU ; Meta-Learning ; Prototypical Networks ; Euclidean ; first ; two ; MLDA,the Meta-test set ; ReLU activation ; Each episode ; a given episode ; adversarial domain adaptation ; DATASET ; N − classes ; meta-learning approaches ; this learning rate ; who,one ; recent years ; Few-Shot Learning ; ReLU ; Meta-Learning ; Prototypical Networks ; Euclidean ; first ; two ; MLDA,"Few-Shot Learning (learning with limited labeled data) aims to overcome the limitations of traditional machine learning approaches, which require thousands of labeled examples to train an effective model. It has received significant interest in recent years due to its use in meta-learning, where a model learns how to acquire prior knowledge from a set of training tasks, which is then used to perform (few-shot) test tasks. However, most existing work assumes that both training and test tasks are drawn from the same distribution, and a large amount of labeled data is available in the training tasks. This is a strong assumption which restricts the usage of meta-",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Interactions such as double negation in sentences and scene interactions in images are common forms of complex dependencies captured by state-of-the-art machine learning models. We propose Mahé, a novel approach to provide Model-Agnostic Hierarchical Explanations of how powerful machine learning models, such as deep neural networks, capture these interactions as either dependent on or free of the context of data instances. Specifically, Mahé provides context-dependent explanations by a novel local interpretation algorithm that effectively captures any-order interactions, and obtains context-free explanations through generalizing context-dependent interactions to explain global behaviors. Experimental results show that Mahé obtains improved local interaction interpretations over state-of-the-art methods and successfully provides explanations of interactions that are context-free. State-of-the-art machine learning models, such as deep neural networks, are exceptional at modeling complex dependencies in structured data, such as text BID44 BID40 , images BID6 BID12 , and DNA sequences BID0 BID48 . However, there has been no clear explanation on what type of dependencies are captured in the black-box models that perform so well BID30 .In this paper, we make one of the first attempts at solving this important problem through interpreting two forms of structures, i.e., context-dependent representations and context-free representations. A context-dependent representation is the one in which a model's prediction depends specifically on a data instance level (such as a sentence or an image). In order to illustrate the concept, we consider an example in image analysis. A yellow round-shape object can be identified as the sun or the moon given its context, either bright blue sky or dark night. A context-free representation is one where the representation behaves similarly independent of instances (i.e., global behaviors). In a hypothetical task of classifying sentiment in sentences, each sentence carries very different meaning, but when ""not"" and ""bad"" depend on each other, their sentiment contribution is almost always positive -i.e., the structure is context-free.To investigate context-dependent and context-free structure, we lend to existing definitions in interpretable machine learning BID29 BID13 . A context-dependent interpretation is a local interpretation of the dependencies at or within the vicinity of a single data instance. Conversely , a context-free interpretation is a global interpretation of how those dependencies behave in a model irrespective of data instances. In this work , we study a key form of dependency: an interaction relationship between the prediction and input features. Interactions can describe arbitrarily complex relationships between these variables and are commonly captured by state-of-the-art models like deep neural networks BID42 . Interactions which are context-dependent or context-free are therefore local or global interactions, respectively.We propose Mahé, a framework for explaining the context-dependent and context-free structures of any complex prediction model, with a focus on explaining neural networks. The context-dependent explanations are built based on recent work on local intepretations (such as BID29 ). Specifically, Mahé takes as input a model to explain and a data instance, and returns a hierarchical explanation, a format proposed by to show local group-variable relationships used in predictions (Figure 1 ). To provide context-free Input into complex ML model:Step FORMULA0 Step FORMULA3 Step 3 Fit ( ) In this work, we proposed Mahé, a model-agnostic framework of providing context-dependent and context-free explanations of local interactions. Mahé has demonstrated the capability of outperforming existing approaches to local interaction interpretation and has shown that local interactions can be context-free. In future work, we wish to make the process of finding context-free interactions more efficient, and study to what extent model behavior can be changed by editing its interactions or univariate effects. Finally, we would like to study the interpretations provided by Mahé more closely to find new insights into structured data. Table 6 : Examples of context-dependent hierarchical explanations on Sentiment-LSTM. The interaction attribution of g K (·) is shown at each K − 1 level, K ≥ 1 ( §4.1) in color. Green means positively contributing to sentiment, and red the opposite. Visualized attributions of linear LIME and Mahé are normalized to the max attribution magnitudes (max magn.) shown. Top-5 attributions by magnitude are shown for LIME.",A new framework for context-dependent and context-free explanations of predictions,Mahé ; Model-Agnostic Hierarchical Explanations ; first ; two ; ML ; § ; max ; max magn ; LIME,dark night ; the prediction ; how powerful machine learning models ; sentiment ; existing approaches ; the moon ; A context-dependent representation ; max ; context-dependent interactions ; learning,Mahé ; Model-Agnostic Hierarchical Explanations ; first ; two ; ML ; § ; max ; max magn ; LIME,"State-of-the-art machine learning models capture complex dependencies in structured data, such as double negation in sentences and scene interactions in images. Mahé provides context-dependent explanations by a novel local interpretation algorithm that effectively captures any-order interactions, and generalizes context-free explanations through generalizing context-independent interactions to explain global behaviors. Experimental results show that Mahé obtains improved local interaction interpretations over state-of theart methods and successfully provides explanations of interactions that are context-less. State of theart machines are exceptional at modeling complex dependencies. However, there has been no clear explanation on what type of dependencies",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"We introduce a new normalization technique that exhibits the fast convergence properties of batch normalization using a transformation of layer weights instead of layer outputs. The proposed technique keeps the contribution of positive and negative weights to the layer output in equilibrium. We validate our method on a set of standard benchmarks including CIFAR-10/100, SVHN and ILSVRC 2012 ImageNet. The introduction of normalizing layers to neural networks has in no small part contributed to the deep learning revolution in machine learning. The most successful of these techniques in the image classification domain is the batch normalization (BatchNorm) layer BID4 , which works by normalizing the univariate first and second order statistics between layers.Batchnorm has seen near universal adoption in image classification tasks due to its surprisingly multifaceted benefits. Compared to an unnormalized network, its has been widely observed that using batch norm empirically results in:• Stability over a wide range of step sizes • Faster convergence (particularly with larger step sizes)• Improved generalizationThe multiple effects of BatchNorm make it both hard to replace and hard to analyze. In this paper we introduce Equilibrium Normalization (EquiNorm), a normalization that works in weight space and still uses a form of batch statistics unlike previous weight space approaches. EquiNorm results in very rapid convergence, even more so than BatchNorm, however as we will show in our experiments, this also results in a tendency to overfit. When combined with additional regularisation, EquiNorm can significantly outperform BatchNorm, which benefits less from this additional regularisation.",An alternative normalization technique to batch normalization,SVHN ; first ; second ; • Faster ; Equilibrium Normalization ; EquiNorm ; BatchNorm,an unnormalized network ; We ; the deep learning revolution ; sizes)• ; Stability ; positive and negative weights ; first ; image classification tasks ; The proposed technique ; batch normalization,SVHN ; first ; second ; • Faster ; Equilibrium Normalization ; EquiNorm ; BatchNorm,"Batchnorm is a new normalization technique that exhibits the fast convergence properties of batch normalization using a transformation of layer weights instead of layer outputs. The proposed technique keeps the contribution of positive and negative weights to the layer output in equilibrium. It has been widely observed that using batch norm empirically results in:• Stability over a wide range of step sizes • Faster convergence (particularly with larger step sizes)• Improved generalizationBatchNorm has seen near universal adoption in image classification tasks due to its surprisingly multifaceted benefits. Compared to an unnormalized network, it is more prone to overfitting, resulting in faster convergence",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Making deep convolutional neural networks more accurate typically comes at the cost of increased computational and memory resources. In this paper, we reduce this cost by exploiting the fact that the importance of features computed by convolutional layers is highly input-dependent, and propose feature boosting and suppression (FBS), a new method to predictively amplify salient convolutional channels and skip unimportant ones at run-time. FBS introduces small auxiliary connections to existing convolutional layers. In contrast to channel pruning methods which permanently remove channels, it preserves the full network structures and accelerates convolution by dynamically skipping unimportant input and output channels. FBS-augmented networks are trained with conventional stochastic gradient descent, making it readily available for many state-of-the-art CNNs. We compare FBS to a range of existing channel pruning and dynamic execution schemes and demonstrate large improvements on ImageNet classification. Experiments show that FBS can respectively provide 5× and 2× savings in compute on VGG-16 and ResNet-18, both with less than 0.6% top-5 accuracy loss. State-of-the-art vision and image-based tasks such as image classification BID19 BID33 BID11 , object detection BID31 and segmentation BID26 are all built upon deep convolutional neural networks (CNNs). While CNN architectures have evolved to become more efficient, the general trend has been to use larger models with greater memory utilization, bandwidth and compute requirements to achieve higher accuracy. The formidable amount of computational resources used by CNNs present a great challenge in the deployment of CNNs in both cost-sensitive cloud services and low-powered edge computing applications.One common approach to reduce the memory, bandwidth and compute costs is to prune over-parameterized CNNs. If performed in a coarse-grain manner this approach is known as channel pruning BID37 BID25 BID35 . Channel pruning evaluates channel saliency measures and removes all input and output connections from unimportant channels-generating a smaller dense model. A saliency-based pruning method, however, has threefold disadvantages. Firstly, by removing channels, the capabilities of CNNs are permanently lost, and the resulting CNN may never regain its accuracy for difficult inputs for which the removed channels were responsible. Secondly, despite the fact that channel pruning may drastically shrink model size, without careful design, computational resources cannot be effectively reduced in a CNN without a detrimental impact on its accuracy. Finally, the saliency of a neuron is not static, which can be illustrated by the feature visualization in FIG0 . Here, a CNN is shown a set of input images, certain channel neurons in a convolutional output may get highly excited, whereas another set of images elicit little response from the same channels. This is in line with our understanding of CNNs that neurons in a convolutional layer specialize in recognizing distinct features, and the relative importance of a neuron depends heavily on the inputs.The above shortcomings prompt the question: why should we prune by static importance, if the importance is highly input-dependent? Surely, a more promising alternative is to prune dynamically depending on the current input. A dynamic channel pruning strategy allows the network to learn to prioritize certain convolutional channels and ignore irrelevant ones. Instead of simply reducing model size at the cost of accuracy with pruning, we can accelerate convolution by selectively computing only a subset of channels predicted to be important at run-time, while considering the sparse input from the preceding convolution layer. In effect, the amount of cached activations and the number of read, write and arithmetic operations used by a well-designed dynamic model can be almost identical to an equivalently sparse statically pruned one. In addition to saving computational resources, a dynamic model preserves all neurons of the full model, which minimizes the impact on task accuracy.In this paper, we propose feature boosting and suppression (FBS) to dynamically amplify and suppress output channels computed by the convolutional layer. Intuitively, we can imagine that the flow of information of each output channel can be amplified or restricted under the control of a ""valve"". This allows salient information to flow freely while we stop all information from unimportant channels and skip their computation. Unlike pruning statically, the valves use features from the previous layer to predict the saliency of output channels. With conventional stochastic gradient descent (SGD) methods, the predictor can learn to adapt itself by observing the input and output features of the convolution operation.FBS introduces tiny auxiliary connections to existing convolutional layers. The minimal overhead added to the existing model is thus negligible when compared to the potential speed up provided by the dynamic sparsity. Existing dynamic computation strategies in CNNs BID28 BID2 produce on/off pruning decisions or execution path selections. Training them thus often resorts to reinforcement learning, which in practice is often computationally expensive. Even though FBS similarly use non-differentiable functions, contrary to these methods, the unified losses are still wellminimized with conventional SGD.We apply FBS to a custom CIFAR-10 ( BID20 classifier and popular CNN models such as VGG-16 BID33 and ResNet-18 (He et al., 2016) trained on the ImageNet dataset BID3 . Empirical results show that under the same speed-ups, FBS can produce models with validation accuracies surpassing all other channel pruning and dynamic conditional execution methods examined in the paper. BID11 , the outputs from certain channel neurons may vary drastically. The top rows in (a) and (b) are found respectively to greatly excite neurons in channels 114 and 181 of layer block 3b/conv2, whereas the bottom images elicit little activation from the same channel neurons. The number below each image indicate the maximum values observed in the channel before adding the shortcut and activation. Finally, (c) shows the distribution of maximum activations observed in the first 20 channels. In summary, we proposed feature boosting and suppression that helps CNNs to achieve significant reductions in the compute required while maintaining high accuracies. FBS fully preserves the capabilities of CNNs and predictively boosts important channels to help the accelerated models retain high accuracies. We demonstrated that FBS achieves around 2× and 5× savings in computation respectively on ResNet-18 and VGG-16 within 0.6% loss of top-5 accuracy. Under the same performance constraints, the accuracy gained by FBS surpasses all recent structured pruning and dynamic execution methods examined in this paper. In addition, it can serve as an off-the-shelf technique for accelerating many popular CNN networks and the fine-tuning process is unified in the traditional SGD which requires no algorithmic changes in training. Finally, the implementation of FBS and the optimized networks are fully open source and released to the public 1 .",We make convolutional layers run faster by dynamically boosting and suppressing channels in feature computation.,FBS ; ImageNet ; CNN ; One ; Firstly ; Secondly ; SGD.We ; al. ; first ; SGD,models ; which ; pruning ; computation ; activation ; unimportant input ; SGD.We ; reinforcement learning ; One common approach ; image-based tasks,FBS ; ImageNet ; CNN ; One ; Firstly ; Secondly ; SGD.We ; al. ; first ; SGD,"The cost of making deep convolutional neural networks more accurate comes at the cost of increased computational and memory resources. In this paper, we address this issue by introducing feature boosting and suppression (FBS), a new method to predictively amplify salient convolutionals channels and skip unimportant ones at run-time. FBS preserves the full network structures and accelerates convolution by dynamically skipping unimportant input and output channels. The trained stochastic gradient descent, making it readily available for many state-of-the-art CNNs. We compare FBS to a range of existing channel pruning and dynamic execution schemes and",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Image translation between two domains is a class of problems aiming to learn mapping from an input image in the source domain to an output image in the target domain. It has been applied to numerous applications, such as data augmentation, domain adaptation, and unsupervised training. When paired training data is not accessible, image translation becomes an ill-posed problem. We constrain the problem with the assumption that the translated image needs to be perceptually similar to the original image and also appears to be drawn from the new domain, and propose a simple yet effective image translation model consisting of a single generator trained with a self-regularization term and an adversarial term. We further notice that existing image translation techniques are agnostic to the subjects of interest and often introduce unwanted changes or artifacts to the input. Thus we propose to add an attention module to predict an attention map to guide the image translation process. The module learns to attend to key parts of the image while keeping everything else unaltered, essentially avoiding undesired artifacts or changes. The predicted attention map also opens door to applications such as unsupervised segmentation and saliency detection. Extensive experiments and evaluations show that our model while being simpler, achieves significantly better performance than existing image translation methods. We propose to use a simple model with attention for image translation and domain adaption and achieve superior performance in a variety of tasks demonstrated by both qualitative and quantitative measures. We show that the attention module is particularly helpful to focus the translation on region of interest, remove unwanted changes or artifacts, and may also be used for unsupervised segmentation or saliency detection.",We propose a simple generative model for unsupervised image translation and saliency detection.,two,the source domain ; numerous applications ; Image translation ; applications ; the subjects ; unsupervised segmentation ; an output image ; When paired training data ; the attention module ; everything,two,"Image translation between two domains is a class of problems aiming to learn mapping from an input image in the source domain to an output image in target domain. It has been applied to numerous applications, such as data augmentation, domain adaptation, and unsupervised training. However, when paired training data is unavailable, image translation becomes an ill-posed problem. To address this issue, we introduce an attention module to guide the image translation process. The attention map helps to focus the translation on region of interest and remove unwanted changes or artifacts. The predicted attention map also opens door to applications such as unssupervised segmentation and saliency",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Text generation is ubiquitous in many NLP tasks, from summarization, to dialogue and machine translation. The dominant parametric approach is based on locally normalized models which predict one word at a time. While these work remarkably well, they are plagued by exposure bias due to the greedy nature of the generation process. In this work, we investigate un-normalized energy-based models (EBMs) which operate not at the token but at the sequence level. In order to make training tractable, we first work in the residual of a pretrained locally normalized language model and second we train using noise contrastive estimation. Furthermore, since the EBM works at the sequence level, we can leverage pretrained bi-directional contextual representations, such as BERT and RoBERTa. Our experiments on two large language modeling datasets show that residual EBMs yield lower perplexity compared to locally normalized baselines. Moreover, generation via importance sampling is very efficient and of higher quality than the baseline models according to human evaluation. The dominant approach to parametric text generation is based on large neural auto-regressive models (Radford et al., 2019) . These models can be trained efficiently via maximum likelihood and they can efficiently generate samples of remarkable quality. Key to their success is local normalization, i.e. they are defined in terms of a product of conditional distributions, one for each token in the sequence. Such distributions are relatively cheap to compute with modern hardware given the limited vocabulary size of common sub-word units like BPE (Sennrich et al., 2015) . Unfortunately, local normalization also brings some drawbacks. First, the designer of the model needs to specify the order in which tokens are generated. Second, at training time the model is conditioned on ground truth context while at test time it is conditioned on its own generations, a discrepancy referred to as exposure bias (Ranzato et al., 2016) . Finally, while heuristics like beam search somewhat help rescore at the sequence level, generation generally lacks long-range coherency because it is produced by the greedy selection of one token at the time without lookahead. Energy-based models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007 ) are a more general framework which potentially address all these issues, as they do not require any local normalization. They only require the definition of an energy function defined over the whole input sequence. Training aims at shaping the energy function such that regions of high density of training data points have lower energy than elsewhere. In principle, EBMs are ideal for modeling text as they can score the whole input at once, they are not prone to label bias (Bottou, 1991) and they may enable generation of large chunks of text, which should help improve coherency. However, so far EBMs had limited application in text generation, because sampling from the model is intractable, and so is maximum likelihood training. The problem is that shaping the energy function is accomplished by updating the model parameters such that the energy is decreased at the training data points (a.k.a. positive examples) and increased at other data points (a.k.a. negative examples). In maximum likelihood training negatives are generated from the model, but in text application we cannot use gradient-based MCMC methods (Teh et al., 2003; Du & Mordatch, 2019) and Gibbs sampling (Welling et al., 2005) is too slow to be practical. Generating negatives by local perturbations of the ground truth would be efficient but hardly useful for generation purposes, when at test time the model needs to generate from scratch. Recently, Bakhtin et al. (2019) carefully studied the problem of training a discriminator to distinguish human written text from language model generations. They experimented with different language model and discriminator architectures, training/test time corpora and concluded that the discriminator can generalize rather well to weaker language models when the training/test corpora match. Bakhtin et al. (2019) found that the learned discriminator is not robust to random perturbations, and argued that the discriminator operates in the ""residual"" space of the language model. Concurrently, Grover et al. (2019) proposed a general approach to ""de-bias"" a generator, by simply training a discriminator and using its output for importance sampling. In this work, we build upon these two works. First, we formalize the residual interpretation by Bakhtin et al. (2019) and use a generative model of the form: where P LM (x) is a locally normalized language model which is fixed during training, and E θ is the energy function parameterized by θ. The resulting model P θ (x) is globally normalized due to the energy term. Note that the same residual formulation was also used in Rosenfeld et al. (2001) ; Wang & Ou (2018b) ; Parshakova et al. (2019) . This formulation has multi-fold benefits. First, by incorporating a locally normalized language model, we can leverage recent advancements in locally normalized language modeling. Second, the language model provides a natural proposal distribution for training (Bakhtin et al., 2019) as we shall see in §3. Third, training can be made efficient by using the conditional noise contrastive estimation objective (Gutmann & Hyvärinen, 2010) . Lastly, this formulation also enables efficient evaluation and generation via importance sampling (Horvitz & Thompson, 1952; Grover et al., 2019) . In some sense, this last point is perhaps the central contribution of the paper, as it allows estimating perplexity of the residual EBM, and thus allows these EBMs to be compared in a standard way to other models. Indeed, in §4 we show that our joint model decreases perplexity on two large datasets, when compared to various auto-regressive language model baselines. Finally, the EBM generations are significantly preferred by humans according to our qualitative evaluation. To the best of our knowledge, this is the first time that an EBM has demonstrated improved generation ability against very strong auto-regressive baselines, both in terms of estimated perplexity and through human evaluation. We investigated an EBM trained on the residual of a pretrained autoregressive language model (Wang & Ou, 2018b; Parshakova et al., 2019) . The resulting joint model scores sequences holistically, thanks to the energy function. Training is very efficient and consists of a binary classification task between positives from the training set and pregenerated negatives from the fixed language model. Generation is also very efficient as it amounts to resampling from the large set of negatives produced by the base language model. Our estimates show that the resulting model has lower perplexity than the base language model. Finally, this approach may be interpreted as a natural way to finetune a large bidirectional transformer like BERT for text generation applications. In the future, we plan to investigate other ways to generate negatives that may strike a better tradeoff between the amount of compute each negative requires and their closeness to the joint model distribution. It would also be interesting to explore other loss functions and the generation of longer pieces of text by using this model auto-regressively at the chunk level, as opposed to the token level. A APPENDIX",We show that Energy-Based models when trained on the residual of an auto-regressive language model can be used effectively and efficiently to generate text.,corpora ; second ; Bottou ; Bakhtin et al ; Grover et al ; al. ; NLP ; Bakhtin et al. ; a.k.a ; BERT,this last point ; recent advancements ; random perturbations ; Horvitz & Thompson ; this work ; the generation ; two large language ; two ; summarization ; the definition,corpora ; second ; Bottou ; Bakhtin et al ; Grover et al ; al. ; NLP ; Bakhtin et al. ; a.k.a ; BERT,"Text generation is ubiquitous in NLP tasks, from summarization, to dialogue and machine translation. The dominant parametric approach is based on locally normalized models which predict one word at a time. While these work remarkably well, they are plagued by exposure bias due to greedy nature of the generation process. In this work, we investigate un-normalized energy-based models (EBMs) which operate not at the token but at the sequence level. In order to make training tractable, we first work in the residual of a pretrained locally normalized language model and second we train using noise contrastive estimation. Furthermore, since the E",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"We present a technique to improve the generalization of deep representations learned on small labeled datasets by introducing self-supervised tasks as auxiliary loss functions. Although recent research has shown benefits of self-supervised learning (SSL) on large unlabeled datasets, its utility on small datasets is unknown. We find that SSL reduces the relative error rate of few-shot meta-learners by 4%-27%, even when the datasets are small and only utilizing images within the datasets. The improvements are greater when the training set is smaller or the task is more challenging. Though the benefits of SSL may increase with larger training sets, we observe that SSL can have a negative impact on performance when there is a domain shift between distribution of images used for meta-learning and SSL. Based on this analysis we present a technique that automatically select images for SSL from a large, generic pool of unlabeled images for a given dataset using a domain classifier that provides further improvements. We present results using several meta-learners and self-supervised tasks across datasets with varying degrees of domain shifts and label sizes to characterize the effectiveness of SSL for few-shot learning. Current machine learning algorithms require enormous amounts of training data to learn new tasks. This is a problem for many practical problems across domains such as biology and medicine where labeled data is hard to come by. In contrast, we humans can quickly learn new concepts from limited training data. We are able to do this by relying on our past ""visual experience"". Recent work attempts to emulate this by training a feature representation to classify a training dataset of ""base"" classes with the hope that the resulting representation generalizes not just to unseen examples of the same classes but also to novel classes, which may have very few training examples (called fewshot learning). However, training for base class classification can force the network to only encode features that are useful for distinguishing between base classes. In the process, it might discard semantic information that is irrelevant for base classes but critical for novel classes. This might be especially true when the base dataset is small or when the class distinctions are challenging. One way to recover this useful semantic information is to leverage representation learning techniques that do not use class labels, namely, unsupervised or self-supervised learning. The key idea is to learn about statistical regularities (e.g., spatial relationship between patches, orientation of an images) that might be a cue to semantics. Despite recent advances, these techniques have only been applied to a few domains (e.g., entry-level classes on internet imagery), and under the assumption that large amounts of unlabeled images are available. Their applicability to the general few-shot scenario described above is unclear. In particular, can these techniques help prevent overfitting to base classes and improve performance on novel classes in the few-shot setting? If so, does the benefit generalize across domains and to more challenging tasks? Moreover, can we use self-supervised training to boost performance in domains where even unlabeled images are hard to get? This paper seeks to answer these questions. We show that with no additional training data, adding a self-supervised task as an auxiliary task (Figure 1 ) improves the performance of existing few-shot techniques on benchmarks across a multitude of domains ( Figure 2 ). Intriguingly, we find that the benefits of self-supervision increase with the difficulty of the task, for example when training from a smaller base dataset, or with degraded inputs such as low resolution or greyscale images (Figure 3 ). One might surmise that as with traditional SSL, additional unlabeled images might improve performance further. But what unlabeled images should we use for novel problem domains where unlabeled data is not freely available? To answer this, we conduct a series of experiments with additional unlabeled data from different domains. We find that adding more unlabeled images improves performance only when the images used for self-supervision are within the same domain as the base classes ( Figure 4a ); otherwise they can even negatively impact the performance of the few-shot learner (Figure 4b, 4c, 4d) . Based on this analysis we present a simple approach that uses a domain classifier to pick similar-domain unlabeled images for self-supervision from a large, generic pool of images. The resulting method improves over the performance of a model trained with self-supervised learning from images within the domain (Figure 4c ). Taken together, this results in a powerful, general and practical approach for improving few-shot learning from small datasets in novel domains. Finally, these benefits are also observed on standard classification tasks (Appendix A.3). We have shown that self-supervision improves transferability of representations on few-shot learning tasks across a range of different domains. Surprisingly, we found that self-supervision is more beneficial for more challenging problems, especially when the number of images used for selfsupervision is small, orders of magnitude smaller than previously reported results. This has a practical benefit that the images within small datasets can be used for self-supervision without relying on a large-scale external dataset. We have also shown that additional unlabeled images can improve performance only if they are from the same or similar domains. Finally, for domains where unlabeled data is limited, we present a novel, simple approach to automatically identify such similar-domain images from a larger pool. Table 1 : Example images and dataset statistics. For few-shot learning experiments the classes are split into base, val, and novel set. Image representations learned on base set are evaluated on the novel set while val set is used for cross-validation. These datasets vary in the number of classes but are orders of magnitude smaller than ImageNet dataset. A.2 RESULTS ON FEW-SHOT LEARNING Table 2 shows the performance of ProtoNet with different self-supervision on seven datasets. We also test the accuracy of the model on novel classes when trained only with self-supervision on the base set of images. Compared to the randomly initialized model (""None"" rows), training the network to predict rotations gives around 2% to 21% improvements on all datasets, while solving jigsaw puzzles only improves on aircrafts and flowers. However, these numbers are significantly worse than learning with supervised labels on the base set, in line with the current literature. Table 3 shows the performance of ProtoNet with jigsaw puzzle loss on harder benchmarks. The results on degraded version of the datasets are shown in the top part, and the bottom part shows the results of using only 20% of the images in the base categories. The gains using SSL are higher in this setting. Table 4 : Performance on few-shot transfer task using different meta-learners. Using jigsaw puzzle loss gives improvements across different meta-learners on most of the datasets. ProtoNet with jigsaw loss performs the best on all five datasets.",Self-supervision improves few-shot recognition on small and challenging datasets without relying on extra data; Extra data helps only when it is from the same or similar domain.,SSL ; One ; ImageNet ; ProtoNet ; seven ; five,"a novel, simple approach ; a domain shift ; no additional training data ; the accuracy ; the novel ; a large, generic pool ; We ; few-shot learning tasks ; limited training data ; few-shot learning",SSL ; One ; ImageNet ; ProtoNet ; seven ; five,"We present a technique to improve the generalization of deep representations on small labeled datasets by introducing self-supervised tasks as auxiliary loss functions. However, its utility on large unlabeled datasets is unknown. We find that SSL reduces the relative error rate of few-shot meta-learners by 4%-27%, even when datasets are small and only utilizing images within the datasets. The improvements are greater when the training set is smaller or the task is more challenging. Although the benefits of SSL may increase with larger training sets, it can have a negative impact on performance when there is a domain shift between distribution of images used for meta",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Partial differential equations (PDEs)  play a prominent role in many disciplines such as applied mathematics, physics, chemistry, material science, computer science, etc. PDEs are commonly derived based on physical laws or empirical observations. However, the governing equations for many complex systems in modern applications are still not fully known. With the rapid development of sensors, computational power, and data storage in the past decade, huge quantities of data can be easily collected and efficiently stored. Such vast quantity of data offers new opportunities for data-driven discovery of hidden physical laws. Inspired by the latest development of neural network designs in deep learning, we propose a new feed-forward deep network, called PDE-Net, to fulfill two objectives at the same time: to accurately predict dynamics of complex systems and to uncover the underlying hidden PDE models. The basic idea of the proposed PDE-Net is to learn differential operators by learning convolution kernels (filters), and apply neural networks or other machine learning methods to approximate the unknown nonlinear responses. Comparing with existing approaches, which either assume the form of the nonlinear response is known or fix certain finite difference approximations of differential operators, our approach has the most flexibility by learning both differential operators and the nonlinear responses. A special feature of the proposed PDE-Net is that all filters are properly constrained, which enables us to easily identify the governing PDE models while still maintaining the expressive and predictive power of the network. These constrains are carefully designed by fully exploiting the relation between the orders of differential operators and the orders of sum rules of filters (an important concept originated from wavelet theory). We also discuss relations of the PDE-Net with some existing networks in computer vision such as Network-In-Network (NIN) and Residual Neural Network (ResNet). Numerical experiments show that the PDE-Net has the potential to uncover the hidden PDE of the observed dynamics, and predict the dynamical behavior for a relatively long time, even in a noisy environment. Differential equations, especially partial differential equations(PDEs), play a prominent role in many disciplines to describe the governing physical laws underlying a given system of interest. Traditionally, PDEs are derived based on simple physical principles such as conservation laws, minimum energy principles, or based on empirical observations. Important examples include the NavierStokes equations in fluid dynamics, the Maxwell's equations for electromagnetic propagation, and the Schrödinger's equations in quantum mechanics. However, many complex systems in modern applications (such as many problems in climate science, neuroscience, finance, etc.) still have eluded mechanisms, and the governing equations of these systems are only partially known. With the rapid development of sensors, computational power, and data storage in the last decade, huge quantities of data can be easily collected and efficiently stored . Such vast quantity of data offers new opportunities for data-driven discovery of potentially new physical laws. Then, one may ask the following interesting and intriguing question: can we learn a PDE model (if there exists one) from a given data set and perform accurate and efficient predictions using the learned model?One of earlier attempts on data-driven discovery of hidden physical laws is by BID0 and BID19 . Their main idea is to compare numerical differentiations of the experimental data with analytic derivatives of candidate functions, and apply the symbolic regression and the evolutionary algorithm to determining the nonlinear dynamical system. Recently , BID1 , BID18 , BID17 and BID21 propose an alternative approach using sparse regression. They construct a dictionary of simple functions and partial derivatives that are likely to appear in the unknown governing equations. Then, they take advantage of sparsity promoting techniques to select candidates that most accurately represent the data. When the form of the nonlinear response of a PDE is known, except for some scalar parameters, presented a framework to learn these unknown parameters by introducing regularity between two consecutive time step using Gaussian process. More recently, introduced a new class of universal function approximators called the physics informed neural networks which is capable of discovering nonlinear PDEs parameterized by scalars.These recent work greatly advanced the progress of the problem. However, symbolic regression is expensive and does not scale very well to large systems. The sparse regression method requires to fix certain numerical approximations of the spatial differentiations in the dictionary beforehand, which limits the expressive and predictive power of the dictionary. Although the framework presented by ; is able to learn hidden physical laws using less data than the approach of sparse regression, the explicit form of the PDEs are assumed to be known except for a few scalar learnable parameters. Therefore, extracting governing equations from data in a less restrictive setting remains a great challenge.The main objective of this paper is to accurately predict the dynamics of complex systems and to uncover the underlying hidden PDE models (should they exist) at the same time, with minimal prior knowledge on the systems. Our inspiration comes from the latest development of deep learning techniques in computer vision. An interesting fact is that some popular networks in computer vision, such as ResNet BID9 b) , have close relationship with PDEs BID4 BID6 BID8 BID20 BID13 . Furthermore, the deeper is the network , the more expressive power the network possesses, which may enable us to learn more complex dynamics arose from fields other than computer vision. However, existing deep networks designed in deep learning mostly emphasis on expressive power and prediction accuracy. These networks are not transparent enough to be able to reveal the underlying PDE models, although they may perfectly fit the observed data and perform accurate predictions. Therefore, we need to carefully design the network by combining knowledge from deep learning and applied mathematics so that we can learn the governing PDEs of the dynamics and make accurate predictions at the same time. Note that our work is closely related to BID4 where the authors designed their network based on discretization of quasilinear parabolic equations. However, it is not clear if the dynamics of image denoising has to be governed by PDEs, nor did the authors attempt to recover the PDE (should there exists one).In this paper, we design a deep feed-forward network , named PDE-Net, based on the following generic nonlinear evolution PDE DISPLAYFORM0 The objective of the PDE-Net is to learn the form of the nonlinear response F and to perform accurate predictions. Unlike the existing work, the proposed network only requires minor knowledge on the form of the nonlinear response function F , and requires no knowledge on the involved differential operators (except for their maximum possible order) and their associated discrete approximations. The nonlinear response function F can be learned using neural networks or other machine learning methods, while discrete approximations of the differential operators are learned using convolution kernels (i.e. filters) jointly with the learning of the response function F . If we have a prior knowledge on the form of the response function F , we can easily adjust the network architecture by taking advantage of the additional information. This may simplify the training and improve the results. We will also discuss relations of the PDE-Net to some existing networks in computer vision such as Network-In-Network (NIN) and ResNet. Details are given in Section 2.In Section 3 and Section 4, we conduct numerical experiments on a linear PDE (convection-diffusion equation) and a nonlinear PDE (convection-diffusion equation with a nonlinear source). We generate data set for each PDE using high precision numerical methods and add Gaussian noise to mimic real situations. Our numerical results show that the PDE-Net can uncover the hidden equations of the observed dynamics, and can predict the dynamical behavior for a relatively long time, even in a noisy environment.A particular novelty of our approach is that we impose appropriate constraints on the learnable filters in order to easily identify the governing PDE models while still maintaining the expressive and pre-dictive power of the network. This makes our approach different from existing deep convolutional networks which mostly emphasis on the prediction accuracy of the networks, as well as all the existing approaches of learning PDEs from data which assume either the form of the response function is known or have fixed approximations of the differential operators. In other words, our proposed approach not only has vast flexibility in fitting observed dynamics and is able to accurately predict its future behavior, but is also able to reveal the hidden equations driving the observed dynamics. The constraints on the filters are motivated by the earlier work of BID2 ; where general relations between wavelet frame transforms and differential operators were established. In particular, it was observed in that we can relate filters and finite difference approximation of differential operators by examining the orders of sum rules of the filters (an important concept in wavelet theory and closely related to vanishing moments of wavelet functions). These constraints on the filters may also be useful in network designs for machine learning tasks in computer vision.2 PDE-NET: A FLEXIBLE DEEP ARCHTECTURE TO LEARN PDES FROM DATA Given a series of measurements of some physical quantities {u(t, ·) : DISPLAYFORM1 : Ω → R, we want to discover the governing PDEs of the data. We assume that the observed data are associated with a PDE that takes the following general form: DISPLAYFORM2 (1) Our objective is to design a feed-forward network, named the PDE-Net, that approximates the PDE (1) in the way that: 1) we can predict the dynamical behavior of the equation for as long time as possible ; 2) we are able to reveal the form of the response function F and the differential operators involved. There are two main components of the PDE-Net that are combined together in the same network : one is automatic determination on the differential operators involved in the PDE and their discrete approximations; the other is to approximate the nonlinear response function F . In this section, we start with discussions on the relation between convolutions and differentiations in discrete setting. This section presents numerical results of training the PDE-Net using the data set described in the previous subsection. We will specifically observe how the learned PDE-Net performs in terms of prediction of dynamical behavior and identification of the underlying PDE model. Furthermore, we will investigate the effects of some of the hyper-parameters (e.g. size of the filters, number of δt-blocks) on the learned PDE-Net. This section presents numerical results of the trained PDE-Net using the data set described in Section 4.1. We will observe how the trained PDE-Net performs in terms of prediction of dynamical behavior and identification of the underlying PDE model. In this paper, we designed a deep feed-forward network, called the PDE-Net, to discover the hidden PDE model from the observed dynamics and to predict the dynamical behavior. The PDE-Net consists of two major components which are jointly trained: to approximate differential operations by convolutions with properly constrained filters, and to approximate the nonlinear response by deep neural networks or other machine learning methods. The PDE-Net is suitable for learning PDEs as general as in (1). However, if we have a prior knowledge on the form of the response function F , we can easily adjust the network architecture by taking advantage of the additional information. This may simplify the training and improve the results. As an example, we considered a linear variable-coefficient convection-diffusion equation. The results show that the PDE-Net can uncover the hidden equation of the observed dynamics, and predict the dynamical behavior for a relatively long time, even in a noisy environment. Furthermore, having deep structure (i.e. multiple δt-blocks) and larger learnable filters can improve the PDE-Net in terms of stability and can prolong reliable predictions. As part of the future work, we will try the proposed framework on real data sets. One of the important directions is to uncover hidden variables which cannot be measured by sensors directly, such as in data assimilation. Another interesting direction which is worth exploring is to learn stable and consistent numerical schemes for a given PDE model based on the architecture of the PDE-Net.","This paper proposes a new feed-forward network, call PDE-Net, to learn PDEs from data.",NavierStokes ; PDE-Net ; quantum mechanics ; Residual Neural Network ; F ; ResNet ; PDE ; the past decade ; two ; Maxwell,the more expressive power ; neural network designs ; Gaussian ; hidden variables ; certain finite difference approximations ; an alternative approach ; the experimental data ; interest ; the network ; Gaussian noise,NavierStokes ; PDE-Net ; quantum mechanics ; Residual Neural Network ; F ; ResNet ; PDE ; the past decade ; two ; Maxwell,"Differential equations (PDEs) play a prominent role in many disciplines, such as applied mathematics, physics, chemistry, material science, computer science, etc. They are commonly derived from physical laws or empirical observations. However, the governing equations for complex systems are still not fully known. With the rapid development of sensors, computational power, and data storage in the past decade, huge quantities of data can be easily collected and efficiently stored. Such vast quantity of data offers new opportunities for data-driven discovery of hidden physical laws. Inspired by the latest development of neural network designs in deep learning, we propose a new feed-forward deep",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Data augmentation (DA) has been widely utilized to improve generalization in training deep neural networks. Recently, human-designed data augmentation has been gradually replaced by automatically learned augmentation policy. Through finding the best policy in well-designed search space of data augmentation, AutoAugment (Cubuk et al., 2019) can significantly improve validation accuracy on image classification tasks. However, this approach is not computationally practical for large-scale problems. In this paper, we develop an adversarial method to arrive at a computationally-affordable solution called Adversarial AutoAugment, which can simultaneously optimize target related object and augmentation policy search loss. The augmentation policy network attempts to increase the training loss of a target network through generating adversarial augmentation policies, while the target network can learn more robust features from harder examples to improve the generalization. In contrast to prior work, we reuse the computation in target network training for policy evaluation, and dispense with the retraining of the target network. Compared to AutoAugment, this leads to about 12x reduction in computing cost and 11x shortening in time overhead on ImageNet. We show experimental results of our approach on CIFAR-10/CIFAR-100, ImageNet, and demonstrate significant performance improvements over state-of-the-art. On CIFAR-10, we achieve a top-1 test error of 1.36%, which is the currently best performing single model. On ImageNet, we achieve a leading performance of top-1 accuracy 79.40% on ResNet-50 and 80.00% on ResNet-50-D without extra data. Massive amount of data have promoted the great success of deep learning in academia and industry. The performance of deep neural networks (DNNs) would be improved substantially when more supervised data is available or better data augmentation method is adapted. Data augmentation such as rotation, flipping, cropping, etc., is a powerful technique to increase the amount and diversity of data. Experiments show that the generalization of a neural network can be efficiently improved through manually designing data augmentation policies. However, this needs lots of knowledge of human expert, and sometimes shows the weak transferability across different tasks and datasets in practical applications. Inspired by neural architecture search (NAS) (Zoph & Le, 2016; Zoph et al., 2017; Zhong et al., 2018a; b; Guo et al., 2018) , a reinforcement learning (RL) (Williams, 1992) method called AutoAugment is proposed by Cubuk et al. (2019) , which can automatically learn the augmentation policy from data and provide an exciting performance improvement on image classification tasks. However, the computing cost is huge for training and evaluating thousands of sampled policies in the search process. Although proxy tasks, i.e., smaller models and reduced datasets, are taken to accelerate the searching process, tens of thousands of GPU-hours of consumption are still required. In addition, these data augmentation policies optimized on proxy tasks are not guaranteed to be optimal on the target task, and the fixed augmentation policy is also sub-optimal for the whole training process. In this paper, we introduce the idea of adversarial learning into automatic data augmentation. The policy network tries to combat the overfitting of the target network through generating adversarial policies with the training process. To oppose this, robust features are learned in the target network, which leads to a significant performance improvement. Meanwhile, the augmentation policy search is performed along with the training of a target network, and the computation in network training is reused for policy evaluation, which can extremely reduce the search cost and make our method more computing-efficient.",We introduce the idea of adversarial learning into automatic data augmentation to improve the generalization  of a targe network.,al. ; Zoph ; GPU-hours ; Guo et al. ; Zoph & Le ; AutoAugment ; Adversarial AutoAugment ; Zhong ; Williams ; ImageNet,training ; a reinforcement learning ; lots ; DA ; image classification tasks ; the augmentation policy search ; target related object ; ImageNet ; deep neural networks ; a leading performance,al. ; Zoph ; GPU-hours ; Guo et al. ; Zoph & Le ; AutoAugment ; Adversarial AutoAugment ; Zhong ; Williams ; ImageNet,"Data augmentation (DA) has been widely used to improve generalization in training deep neural networks. However, it has been gradually replaced by automatically learned augmentation policy. By finding the best policy in well-designed search space of data augmentation, AutoAugment can significantly improve validation accuracy on image classification tasks. This approach is not computationally practical for large-scale problems. In this paper, we develop an adversarial method to arrive at a computationally-affordable solution called Adversarial Auto Augment, which can simultaneously optimize target related object and augmentation policies, and simultaneously reduce time overhead on ImageNet. The",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies “image” domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples. We made an accompanying demo video here: https://www.cs.toronto.edu/~huang/TimbreTron/index.html which we strongly encourage you to watch before reading the paper. Timbre is a perceptual characteristic that distinguishes one musical instrument from another playing the same note with the same intensity and duration. Modeling timbre is very hard, and it has been referred to as ""the psychoacoustician's multidimensional waste-basket category for everything that cannot be labeled pitch or loudness"" 2 . The timbre of a single note at a single pitch has a nonlinear dependence on the volume, time and even the particular way the instrument is played by the performer. While there is a substantial body of research in timbre modelling and synthesis BID6 ; BID32 ; BID35 BID36 ), state-of-the-art musical sound libraries used by orchestral composers for analog instruments (e.g. the Vienna Symphonic Library (GmbH, 2018) ) are still obtained by extremely careful audio sampling of real instrument recordings. Being able to model and manipulate timbre electronically carries importance for musicians who wish to experiment with different sounds, or compose for multiple instruments. (Appendix A discusses the components of music in more detail.)In this paper, we consider the problem of high quality timbre transfer between audio clips obtained with different instruments. More specifically, the goal is to transform the timbre of a musical recording to match a set of reference recordings while preserving other musical content, such as pitch and loudness. We take inspiration from recent successes in style transfer for images using neural networks BID13 BID22 BID7 ). An appealing strategy would be to directly apply image-based style transfer techniques to time-frequency representations of images, such as short-time Fourier transform (STFT) spectrograms. However, needing to convert the generated spectrogram into a waveform presents a fundamental obstacle, since accurate reconstruction requires phase information, which is difficult to predict BID10 , and existing techniques for inferring phase (e.g., BID16 ) can produce characteristic artifacts which are undesirable for high quality audio generation BID34 .Recent years have seen rapid progress on audio generation methods that directly generate high-quality waveforms, such as WaveNet BID41 , SampleRNN BID28 , and Tacotron2 BID34 ). WaveNet 's ability to condition on abstract audio representations is particularly relevant, since it enables one to perform manipulations in high-level auditory representations from which reconstruction would have previously been impractical. Tacotron2 performs high-level processing on time-frequency representations of speech, and then uses WaveNet to output high-quality audio conditioned on the generated mel spectrogram.We adapt this general strategy to the music domain. We propose TimbreTron, a pipeline that performs CQT-based timbre transfer with high-quality waveform output. It is trained only on unrelated samples of two instruments. For our time-frequency representation, we choose the constant Q transform (CQT), a perceptually motivated representation of music BID4 . We show that this representation is particularly well-suited to musical timbre transfer and other manipulations due to its pitch equivariance and the way it simultaneously achieves high frequency resolution at low frequencies and high temporal resolution at high frequencies, a property that STFT lacks.TimbreTron performs timbre transfer by three steps, shown in Figure 1 . First, it computes the CQT spectrogram and treats its log-magnitude values as an image (discarding phase information). Second, it performs timbre transfer in the log-CQT domain using a CycleGAN (Zhu et al., 2017) . Finally, it converts the generated log-CQT to a waveform using a conditional WaveNet synthesizer (which implicitly must infer the missing phase information). Empirically, our TimbreTron can successfully perform musical timbre transfer on some instrument pairs. The generated audio samples have realistic timbre that matches the target timbre while otherwise expressing the same musical content (e.g., rhythm, loudness, pitch). We empirically verified that the use of a CQT representation is a crucial component in TimbreTron as it consistently yields qualitatively better timbre transfer than its STFT counterpart. We presented the TimbreTron, a pipeline for perfoming high-quality timbre transfer on musical waveforms using CQT-domain style transfer. We perform the timbre transfer in the time-frequency domain, and then reconstruct the inputs using a WaveNet (circumventing the difficulty of phase recovery from an amplitude CQT). The CQT is particularly well suited to convolutional architectures due to its approximate pitch equivariance. The entire pipeline can be trained on unrelated real-world music segments, and intriguingly, the MIDI-trained CycleGAN demonstrated generalization capability to real-world musical signals. Based on an AMT study, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and poly-","We present the TimbreTron, a pipeline for perfoming high-quality timbre transfer on musical waveforms using CQT-domain style transfer.",the Vienna Symphonic Library ; TimbreTron ; mel ; Timbre ; Second ; Zhu et al. ; the Constant Q Transform ; one ; WaveNet ; GmbH,generalization capability ; musical waveforms ; the musical content ; this paper ; a time-frequency representation ; the Vienna Symphonic Library ; independent manipulation ; WaveNet 's ability ; realistic timbre ; a single note,the Vienna Symphonic Library ; TimbreTron ; mel ; Timbre ; Second ; Zhu et al. ; the Constant Q Transform ; one ; WaveNet ; GmbH,"In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content. This approach depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. TimbreTron is a method for this approach, which applies ""image"" domain style transfer to a time-frequency representation of the audio signal and then produces a high quality waveform using a conditional WaveNet synthesizer. The Constant Q Transform representation is particularly well-suited to convolutional architectures due",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"This paper aims to address the limitations of mutual information estimators based on variational optimization. By redefining the cost using generalized functions from nonextensive statistical mechanics we raise the upper bound of previous estimators and enable the control of the bias variance trade off. Variational based estimators outperform previous methods especially in high dependence high dimensional scenarios found in machine learning setups. Despite their performance, these estimators either exhibit a high variance or are upper bounded by log(batch size). Our approach inspired by nonextensive statistical mechanics uses different generalizations for the logarithm and the exponential in the partition function. This enables the estimator to capture changes in mutual information over a wider range of dimensions and correlations of the input variables whereas previous estimators saturate them. Understanding the relationship between two variables is a fundamental problem in machine learning, finance, signal processing and other fields. To quantify such a relationship, we use mutual information, which measures the mutual dependence between two variables. The mutual information I(X, Y ) represents the ratio of two probabilities that can account for the nonlinear dependence and is defined as follows: It is a major challenge to estimate the mutual information in practical scenarios that involve limited samples without knowing the distributions or higher-order statistics McAllester & Statos (2018) ; Paninski (2003) . For instance, existing methods such as the k-NN based Kraskov et al. (2004) and its variations Gao et al. (2015) ; Wang et al. (2009); Lord et al. (2018) , or KDE based Khan et al. (2007) ; Suzuki et al. (2008) calculate the mutual information by estimating the probability density from the available samples. Although these approaches perform well in the low dimensional and low dependence case, they do not scale well when either the dimension of the variables or the dependence between variables increases. Such scenarios are often encountered in machine learning setups. Estimators based on variational bounds, Belghazi et al. (2018) ; Poole et al. (2018) ; Nguyen et al. (2010) ; ; Zhang (2007); Foster & Grassberger (2011) , perform much better in this scenarios. These estimators are inspired by the Donsker & Varadhan (1983) representation which states that there exists a function from the sample space to real number that satisfies the following equality: I(X, Y ) = sup f :Ω→R E p(x,y) [f (x, y)] − log E p(y) e f (x,y) Estimators based on variational bounds replace the function f in the above equation with a neural network trained to maximize a lower bound of the mutual information. The training process terminates when the lower bounds exhibit convergence, and these bounds are then interpreted as the estimated mutual information values. This NN-based approach requires good representations of lower bounds and guaranteed convergence for a wide range of dependence between the input variables which leads to numerous challenges. Current state-of-the-art estimators, when applied to high dimensional high dependence scenarios, either exhibit a high variance or are bounded by log(K), where K is the batch size. In this work: 1. We propose new variational lower bounds on the mutual information inspired by methods from nonextensive statistical mechanics. 2. We review generalized versions of the logarithm and exponential function, define a generalized version of the partition function, and use them to control the trade off between variance and bias of the estimator. 3. We outperform previous estimators in capturing the trend when varying the correlation and the dimension of the input variables by using different generalizations for the logarithm and partition function. The main goal of the current work was to improve the performance of mutual information estimators in the high dependence high dimensional scenarios which are often encountered in machine learning setups. We reviewed previous variational lower bounds and extended them using generalized logarithm and exponential functions from nonextensive statistical mechanics. One of the most significant findings to emerge from this study was that we can control the trade off between the bias and the variance of the estimator by independently tuning the generalizations q of the logarithm and partition function. As a result, we are able to better capture the trend when varying the correlation and dependence of the input variables. This method greatly improves upon the I NCE estimator which has a low variance but a high bias and I α estimator which requires two critics that can sometimes be challenging to train. The major limitation of the proposed estimator is that its results are not equal in value with the classical mutual information due to use of generalization. Despite that, I NES still captures the dependence between the input variable and can be applicable to machine learning problems where a mutual information estimator is needed such as feature selection, variational autoencoders, and generative adversarial networks. Addition and subtraction operations in q-algebra are defined as follow:",Mutual information estimator based nonextensive statistical mechanics,McAllester & Statos ; Belghazi ; Poole ; the Donsker & Varadhan ; p(x ; two ; Gao ; Wang ; Foster & Grassberger ; KDE,changes ; feature selection ; the control ; This paper ; which ; they ; al. ; machine learning setups ; good representations ; its results,McAllester & Statos ; Belghazi ; Poole ; the Donsker & Varadhan ; p(x ; two ; Gao ; Wang ; Foster & Grassberger ; KDE,"Variational based estimators outperform previous methods in high dependence high dimensional scenarios. These estimators either exhibit a high variance or are upper bounded by log(batch size). Our approach uses different generalizations for logarithm and exponential in the partition function. By redefining the cost using nonextensive statistical mechanics, we raise the upper bound of previous estimators and enable the control of bias variance trade off. However, these estimators do not scale well in low dimensional and low dependence scenarios, where the dimension of the variables or dependence between variables increases significantly. This is a major challenge in machine learning, finance, signal processing",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Knowledge Distillation (KD) is a widely used technique in recent deep learning research to obtain small and simple models whose performance is on a par with their large and complex counterparts. Standard Knowledge Distillation tends to be time-consuming because of the training time spent to obtain a teacher model that would then provide guidance for the student model. It might be possible to cut short the time by training a teacher model on the fly, but it is not trivial to have such a high-capacity teacher that gives quality guidance to student models this way. To improve this, we present a novel framework of Knowledge Distillation exploiting dark knowledge from the whole training set. In this framework, we propose a simple and effective implementation named Distillation by Utilizing Peer Samples (DUPS) in one generation. We verify our algorithm on numerous experiments. Compared with standard training on modern architectures, DUPS achieves an average improvement of 1%-2% on various tasks with nearly zero extra cost. Considering some typical Knowledge Distillation methods which are much more time-consuming, we also get comparable or even better performance using DUPS. Recent years have witnessed continuous development of deep neural network models. A general trend is that improvements in model performance are usually coupled with more complex architecture designs and higher cost of computation. In order to obtain more compact models with higher quality, the idea of Knowledge Distillation (KD) first emerged in the form of knowledge transfer between models (Buciluǎ et al., 2006) . KD takes advantage of the ""dark knowledge"" by transferring it from teacher models to student models so as to facilitate the latter's training process (Hinton et al., 2015) . Student models, with the availability of softened output vectors from teacher models in KD, have access to richer information in comparison to directly learning from hard labels provided by training set. KD significantly improves smaller models' performance, and thus it further allows model compression. Although great progress has been made in this area, much more training cost is incurred due to involved time-consuming mid-output (e.g. feature maps) alignment when training student models, on top of extra training of a huge teacher model. It is ad meaningful objective of finding more efficient KD methods. Recent works by (Furlanello et al., 2018) and (Lan et al., 2018b) show that a stronger teacher model is not the necessary condition for improving the student model. Their research shows that it is possible that the student model's performance can be significantly improved by an identically structured teacher model. Although the techniques remain inefficient due to the cost of multi-generation (at least one extra) training of teacher models, these works give important hints that cheaper teachers with considerable effectiveness may exist. Recently (Yang et al., 2018) extend these works, trying to obtain continuously improved teachers by introducing the cyclic learning rate technique in one-generation training. They propose Snapshot Distillation (SD), which uses models obtained from earlier checkpoints as teachers and skips the process of separately training a teacher model. Inspired by recent interesting ideas of dataset distillation for objectives on other research areas, we propose a novel approach for KD in this paper. Instead of relying on the assitance of a separate teacher model or checkpoint, we exploit hidden knowledge in the dataset to generate a surrogate teacher. Specifically, we first define a more general framework of knowledge distillation utilizing the whole dataset to generate extra supervision signals, rather than using a single sample alone. Then we propose a very simple yet effective implementation of one-generation KD, called Distillation by Utilizing Peer Samples (DUPS). In DUPS, each sample borrows continuously boosted secondary information from a random subset of peer samples belonging to the same category on the fly. We perform extensive experiments on CIFAR100 dataset, with various modern architectures such as PreActResNet, WideResNet, and ResNeXt, demonstrating that our proposed DUPS gains significant improvement compared to standard SGD training with nearly zero extra computation cost. DUPS also outperforms recent one-generation KD method SnapShot Distillation (Yang et al., 2018) on most architectures. Moreover, we validate our algorithm on more practical tasks, include ImageNet classification, transfer learning, and language model. Experiments show that DUPS is generally effective across different tasks. In summary, our main contributions include: 1) To the best of our knowledge, we are the first to propose an extension framework of Knowledge Distillation utilizing the whole training set other than a single sample. 2) Under this framework we implement a general on-the-fly algorithm DUPS which achieves significant improvement at almost no extra cost. The rest of the paper is organized as follows. Section 2 presents prior works related to this paper. Section 3 introduces our methodology of the general knowledge distillation framework. Section 4 demonstrates our experimental results and provides some discussions. Section 5 concludes the paper. Here we give a short discussion about how and why DUPS brings benefits. We first demonstrate some empirical characteristics of DUPS observed in our experiments. We plot the learning curve of the whole training procedure of PreActResNet18 as Fig. 1 . For better demonstration purpose, we divide the training process into only 4 stages for DUPS training, with each stage consisting of 40 epochs. We observe that SGD and DUPS display almost the same standard of performance in the first stage as expected. While at the 41th epoch, both training and test accuracy of DUPS get a sharp rise due to involving teacher signal generated in the 40th epoch. Then both training and test accuracy drop slightly for a few epochs, and then return to the trend of slowly rising for the remaining epochs until next stage. A similar phenomenon also appears at the beginning of next stage, although the magnitude of accuracy improvement becomes much more smaller. As the model reaches the beginning of final stage, we no longer see increases in accuracy since training is nearly saturated. We notice that since the first sharp rising, DUPS continuously outperforms SGD by a stable gap for the following training epochs until convergence. We also investigate the influence of different choices of hyper-parameters specific to DUPS. The most important two are the number of stages and number of random peer samples. We run a grid search method to validate different combinations of these two variables. We use update intervals, or equivalently number of epochs per stage, instead of number of stages for clarity in this experiments. In Fig. 2 we see that performance of DUPS does not seem to be very sensitive to most combinations of the hyperparameters. When the number of peer samples increases to 5 or more, model accuracy tends to be over 77.3%. Even when the number of peer samples is low, a good choice of the value of update interval can boost the model performance significantly. For example, when number of peer samples is 1 and update interval is set to be between 20 to 40, DUPS still delivers satisfying results which is comparable to its best performance. Low accuracy of the model only happens consistently when the value of update interval is large. If update interval is set to 80, the model, teacher-student knowledge transfer only takes place once during the whole training process. Consequently, the opportunity to distill the knowledge obtained from the dataset is too rare for the model to benefit from DUPS. In this paper, we have introduced a general framework for one-generation KD: We incorporate the information contained within the dataset into teacher-student optimization. We have also proposed an effective implementation of this general framework named DUPS. With extensive experiments, this simple yet effective algorithm is verified to be effective in improving model performance in tasks like image classification, transfer learning and language modeling with almost no additional cost in training resources. The demonstrated success of DUPS imply that utilizing dataset information during training potentially allow us to gain even more benefits.",We present a novel framework of Knowledge Distillation utilizing peer samples as the teacher,nearly zero ; one ; Snapshot Distillation ; Yang et al. ; Hinton ; al. ; Standard Knowledge Distillation ; first ; the beginning of next stage ; at least one,dataset distillation ; the general knowledge distillation framework ; the whole training set ; extra supervision signals ; this experiments ; SD ; a huge teacher model ; SGD ; more complex architecture designs ; Buciluǎ et al,nearly zero ; one ; Snapshot Distillation ; Yang et al. ; Hinton ; al. ; Standard Knowledge Distillation ; first ; the beginning of next stage ; at least one,"Learning Distillation (KD) is a widely used technique in deep learning research to obtain small and simple models whose performance is on a par with their large and complex counterparts. However, it is not trivial to have such a high-capacity teacher that provides quality guidance to student models. To improve this, we present a novel framework of Knowledge Distillation exploiting dark knowledge from the whole training set. In this framework, we propose a simple and effective implementation named Distillation by Utilizing Peer Samples (DUPS) in one generation. Compared with standard training on modern architectures, DUPS achieves an average improvement of 1%-",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"This paper considers multi-agent reinforcement learning (MARL) in networked system control. Specifically, each agent learns a decentralized control policy based on local observations and messages from connected neighbors. We formulate such a networked MARL (NMARL) problem as a spatiotemporal Markov decision process and introduce a spatial discount factor to stabilize the training of each local agent. Further, we propose a new differentiable communication protocol, called NeurComm, to reduce information loss and non-stationarity in NMARL. Based on experiments in realistic NMARL scenarios of adaptive traffic signal control and cooperative adaptive cruise control, an appropriate spatial discount factor effectively enhances the learning curves of non-communicative MARL algorithms, while NeurComm outperforms existing communication protocols in both learning efficiency and control performance. Reinforcement learning (RL), formulated as a Markov decision process (MDP), is a promising data-driven approach for learning adaptive control policies (Sutton & Barto, 1998) . Recent advances in deep neural networks (DNNs) further enhance its learning capacity on complex tasks. Successful algorithms include deep Q-network (DQN) (Mnih et al., 2015) , deep deterministic policy gradient (DDPG) (Lillicrap et al., 2015) , and advantage actor critic (A2C) (Mnih et al., 2016) . However, RL is not scalable in many real-world control problems. This scalability issue is addressed in multi-agent RL (MARL), where each agent learns its individual policy from only local observations. However, MARL introduces new challenges in model training and execution, due to non-stationarity and partial observability in a decentralized MDP from the viewpoint of each agent. To address these challenges, various learning methods and communication protocols are proposed to stabilize training and improve observability. This paper considers networked MARL (NMARL) in the context of networked system control (NSC), where agents are connected via a communication network for a cooperative control objective. Each agent performs decentralized control based on its local observations and messages from connected neighbors. NSC is extensively studied and widely applied. Examples include connected vehicle control (Jin & Orosz, 2014) , traffic signal control (Chu et al., 2019) , distributed sensing (Xu et al., 2018) , and networked storage operation (Qin et al., 2016) . We expect an increasing trend of NMARL based controllers in the near future, after the development of advanced communication technologies such as 5G and Internet-of-Things. Recent works studied decentralized NMARL under assumptions of global observations and local rewards (Zhang et al., 2018; Qu et al., 2019) , which are reasonable in multi-agent gaming but not suitable in NSC. First, the control infrastructures are distributed in a wide region, so collecting global observations in execution increases communication delay and failure rate, and hurts the robustness. Second, online learning is not common due to safety and efficiency concerns. Rather, each model is trained offline and tested extensively before field deployment. In online execution, the model only runs forward propagation, and its performance is constantly monitored for triggering re-training. To reflect these practical constraints in NSC, we assume 1) each agent is connected to a limited number We have formulated the spatiotemporal MDP for decentralized NSC under neighborhood communication. Further, we have introduced the spatial discount factor to enhance non-communicative MARL algorithms, and proposed a neural communication protocol NeurComm to design adaptive and efficient communicative MARL algorithms. We hope this paper provides a rethink on developing scalable and robust MARL controllers for NSC, by following practical engineering assumptions and combining appropriate learning and communication methods rather than reusing existing MARL algorithms. One future direction is improving the recurrent units to naturally control spatiotemporal information flows within the meta-DNN in a decentralized way. Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Başar. Fully decentralized multiagent reinforcement learning with networked agents. arXiv preprint arXiv:1802.08757, 2018.",This paper proposes a new formulation and a new communication protocol for networked multi-agent control problems,Jin & Orosz ; NMARL ; Markov ; Second ; MARL ; DDPG ; Tong Zhang ; Sutton & Barto ; MDP ; Qin,its learning capacity ; the context ; One ; deep deterministic policy gradient ; local rewards ; Recent advances ; cooperative adaptive cruise control ; Mnih ; a decentralized control policy ; such a networked MARL (NMARL) problem,Jin & Orosz ; NMARL ; Markov ; Second ; MARL ; DDPG ; Tong Zhang ; Sutton & Barto ; MDP ; Qin,"Multi-agent reinforcement learning (MARL) in networked system control (NSC) involves each agent learning a decentralized control policy based on local observations and messages from connected neighbors. This model training and execution is challenging due to non-stationarity and partial observability in a decentralized MDP from the viewpoint of each agent. Various learning methods and communication protocols are proposed to stabilize training and improve observability. The concept of networked MARL (NMARL) is a promising data-driven approach for learning adaptive control policies (Sutton & Barto, 1998) and recent advances in deep neural networks (DNNs)",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Adversarial examples have been shown to be an effective way of assessing the robustness of neural sequence-to-sequence (seq2seq) models, by applying perturbations to the input of a model leading to large degradation in performance. However, these perturbations are only indicative of a weakness in the model if they do not change the semantics of the input in a way that would change the expected output. Using the example of machine translation (MT), we propose a new evaluation framework for adversarial attacks on seq2seq models taking meaning preservation into account and demonstrate that existing methods may not preserve meaning in general. Based on these findings, we propose new constraints for attacks on word-based MT systems and show, via human and automatic evaluation, that they produce more semantically similar adversarial inputs. Furthermore, we show that performing adversarial training with meaning-preserving attacks is beneficial to the model in terms of adversarial robustness without hurting test performance. Attacking a machine learning model with adversarial perturbations is the process of making changes to its input to maximize an adversarial goal, such as mis-classification BID34 or mistranslation BID37 . These attacks provide insight into the vulnerabilities of machine learning models and their brittleness to samples outside the training distribution. This is critical for systems sensitive to safety or security, e.g. self-driving cars BID1 .Adversarial attacks were first defined and investigated for computer vision systems BID34 ; BID9 ; BID22 inter alia), where they benefit from the fact that images are expressed in continuous space, making minuscule perturbations largely imperceptible to the human eye. In discrete spaces such as natural language sentences, the situation is more problematic; even a flip of a single word or character is generally perceptible by a human reader. Thus, most of the mathematical framework in previous work is not directly applicable to discrete text data. Moreover, there is no canonical distance metric for textual data like the 2 norm in real-valued vector spaces such as images, and evaluating the level of semantic similarity between two sentences is a field of research of its own BID2 . This elicits a natural question: what does the term ""adversarial perturbation"" mean in the context of natural language processing (NLP)?We propose a simple but natural criterion for adversarial examples in NLP, particularly seq2seq models: adversarial examples should be meaning-preserving on the source side, but meaning-destroying on the target side. The focus on explicitly evaluating meaning preservation is in contrast to previous work on adversarial examples for seq2seq models BID0 BID37 BID4 BID7 . Nonetheless, this feature is extremely important; given two sentences with equivalent meaning, we would expect a good model to produce two outputs with equivalent meaning.In other words, any meaning-preserving perturbation that results in the model output changing drastically highlights a fault of the model.A first technical contribution of the paper is to lay out a method for formalizing this concept of meaning-preserving perturbations ( §2). This makes it possible to evaluate the effectiveness of adversarial attacks or defenses either using gold-standard human evaluation, or approximations that can be calculated without human intervention. We further propose a simple method of imbuing gradient-based word substitution attacks ( §3.1) with simple constraints aimed at increasing the chance that the meaning is preserved ( §3.2).Our experiments are designed to answer several questions about meaning preservation in seq2seq models. First, we evaluate our proposed ""source-meaning-preserving, target-meaning-destroying"" criterion for adversarial examples using both manual and automatic evaluation ( §4.2) and find that a less widely used evaluation metric (chrF) provides significantly better correlation with human judgments than the more widely used BLEU and METEOR metrics. We proceed to perform an evaluation of adversarial example generation techniques, finding that constrained substitution attacks do preserve meaning to a higher degree than unconstrained attacks while still degrading the performance of the systems across different languages and model architectures ( §4.3). Finally we apply existing methods for adversarial training to the adversarial examples with these constraints and show that making adversarial inputs more semantically similar to the source is beneficial for robustness to adversarial attacks and does not decrease test performance ( §5). This paper highlights the performance of meaning-preserving adversarial perturbations for NLP models (with a focus on seq2seq). We proposed a general evaluation framework for adversarial perturbations and compared various automatic metrics as alternatives to human judgment to instantiate this framework. We then confirmed that, in the context of MT, ""naive"" attacks do not preserve meaning in general, and proposed alternatives to remedy this issue. Finally, we have shown the utility of adversarial training in this paradigm. We hope that this helps future work in this area of research to evaluate meaning conservation more consistently.",How you should evaluate adversarial attacks on seq2seq,MT systems ; first ; two ; NLP)?We ; NLP ; § ; BLEU ; MT,research ; they ; BLEU ; the utility ; meaning preservation ; safety ; the training distribution ; adversarial perturbations ; new constraints ; defenses,MT systems ; first ; two ; NLP)?We ; NLP ; § ; BLEU ; MT,"Adversarial examples have been shown to be effective in assessing the robustness of neural sequence-to-sequence (seq2seq) models, by applying perturbations to the input of a model leading to large degradation in performance. However, these perturbs are only indicative of a weakness in the model if they do not change the semantics of the input in a way that would change the expected output. Based on these findings, we propose new constraints for attacks on word-based MT systems and show that they produce more semantically similar adversarial inputs. Furthermore, we show that performing adversarial training with meaning-preserving",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Deep neural networks can learn meaningful representations of data. However, these representations are hard to interpret. For example, visualizing a latent layer is generally only possible for at most three dimensions. Neural networks are able to learn and benefit from much higher dimensional representations but these are not visually interpretable because nodes have arbitrary ordering within a layer. Here, we utilize the ability of the human observer to identify patterns in structured representations to visualize higher dimensions. To do so, we propose a class of regularizations we call \textit{Graph Spectral Regularizations} that impose graph-structure on latent layers. This is achieved by treating activations as signals on a predefined graph and constraining those activations using graph filters, such as low pass and wavelet-like filters. This framework allows for any kind of graph as well as filter to achieve a wide range of structured regularizations depending on the inference needs of the data. First, we show a synthetic example that the graph-structured layer can reveal topological features of the data. Next, we show that a smoothing regularization can impose semantically consistent ordering of nodes when applied to capsule nets. Further, we show that the graph-structured layer, using wavelet-like spatially localized filters, can form localized receptive fields for improved image and biomedical data interpretation. In other words, the mapping between latent layer, neurons and the output space becomes clear due to the localization of the activations. Finally, we show that when structured as a grid, the representations create coherent images that allow for image-processing techniques such as convolutions. Neural networks have revolutionized many areas of machine learning including image and natural language processing. However, one of the major challenges for neural networks is that they are still black boxes to the user. It is not quite clear how network internals map from inputs to outputs or how to interpret the features learned by the nodes. This is mainly because the features are not constrained to have specific structure or characteristics. Existing regularizations constrain the learned code to have certain properties. However, they are not designed to specifically aid in interpretation of the latent encoding. For example, L 1 regularization induces sparsity in the activations but does not impose specific structure between dimensions.Here, we introduce a new class of regularizations called Graph Spectral Regularizations that result in activations that are filtered on a predefined graph. We define specific members of this class for applications. First, we introduce a (graph) Laplacian smoothing regularization which enforces smoothly varying activations while at the same time reconstructing the data. This regularization is useful for learning features with specific topologies. For instance, we show that on a clusterstructured topology where features correspond to hierarchical cluster structure in the data it reflects the abstract grouping of features. We also show it is useful for inducing feature consistency between nodes of capsule networks BID11 . The graph regularization semantically aligns the features such that they appear in the same order in each capsule. When trained on MNIST digits, we find that each of our 10 capsules consisting of 16 nodes encodes the same transformation (rotation, scale, skew, etc) of a particular digit in the same node.While the Laplacian smoothing regularizations is useful in the context where the features of the data have a recognizable topology, often we don't know the explicit structure of the data. Instead, we would like to extract the topology of the data itself. Thus, we design a filter that encourages the graph structure layer to learn data-shape features. We achieve this by using a spatially localized, Gaussian filter to localize the activations for any particular data point. We ensure that only one of a dictionary of localized filters is chosen as the activation via a spectral bottleneck layer preceding the graph-structured layer. We show that spatially-localized filter regularizations are useful for detecting circular and linear topologies of data that are not immediately reflected by the observed features. We also explore a biological system -a single-cell protein expression dataset depicting T cell development in the thymus -that has continuous progression structure. The graph structured layer (with a ring graph) reveals the data to have a Y-shaped topology reflecting the bifurcation into CD4 (regulatory) and CD8 (cytotoxic) T cells, confirming known T cell biology.Finally, we show that the graph-structured layer, when imposing a 2D grid, creates a ""pseudo"" image that can be analyzed by convolution layers. We show that such re-encoded images of MNIST digits have localized receptive fields that can be used for classification and visual interpretability. Interestingly, we find that the convolution obviates the need for a spectral bottleneck as the convolution and max pooling themselves may provide that function.Our contributions are as follows:• A framework for imposing graph structure on latent layers using graph spectral regularizations.• A Laplacian graph smoothing regularization and its applications in learning feature smoothness and consistency.• Spatially localized graph regularizations using a spectral bottleneck based on a dictionary of Gaussian Kernels and its application in recognizing data topology.• Applications of graph spectral regularizations, natural and biological datasets to demonstrate feature interpretability and data topology.The rest of this paper is organized as follows. We first define graph structured layers and two techniques utilizing this layer in Section 2. Then we present experiments demonstrating improved interpretability in Section 3. Finally, we wrap up with conclusions in Section 4.",Imposing graph structure on neural network layers for improved visual interpretability.,Spectral Regularizations ; Gaussian ; two ; linear ; Laplacian ; at most three ; First ; only one ; max ; one,"data topology ; Our contributions ; any kind ; the features ; patterns ; Gaussian ; \textit{Graph Spectral Regularizations ; feature consistency ; a ""pseudo"" image ; improved image",Spectral Regularizations ; Gaussian ; two ; linear ; Laplacian ; at most three ; First ; only one ; max ; one,"Deep neural networks can learn meaningful representations of data but these representations are difficult to interpret. For example, visualizing a latent layer is only possible for at most three dimensions. Neural networks are able to learn and benefit from higher dimensional representations but they are not visually interpretable because nodes have arbitrary ordering within a layer. To address this, we propose a class of regularizations we call \textit{Graph Spectral Regularizations} that impose graph-structure on latent layers. This is achieved by treating activations as signals on a predefined graph and constraining those activations using graph filters, such as low pass and wavelet",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Many tasks in natural language processing involve comparing two sentences to compute some notion of relevance, entailment, or similarity. Typically this comparison is done either at the word level or at the sentence level, with no attempt to leverage the inherent structure of the sentence. When sentence structure is used for comparison, it is obtained during a non-differentiable pre-processing step, leading to propagation of errors. We introduce a model of structured alignments between sentences, showing how to compare two sentences by matching their latent structures. Using a structured attention mechanism, our model matches possible spans in the first sentence to possible spans in the second sentence, simultaneously discovering the tree structure of each sentence and performing a comparison, in a model that is fully differentiable and is trained only on the comparison objective. We evaluate this model on two sentence comparison tasks: the Stanford natural language inference dataset and the TREC-QA dataset. We find that comparing spans results in superior performance to comparing words individually, and that the learned trees are consistent with actual linguistic structures. There are many tasks in natural language processing that require comparing two sentences: natural language inference BID1 BID28 and paraphrase detection BID44 are classification tasks over sentence pairs, and question answering often requires an alignment between a question and a passage of text that may contain the answer BID39 BID31 BID17 .Neural models for these tasks almost always perform comparisons between the two sentences either at the word level BID29 ), or at the sentence level BID1 . Word-level comparisons ignore the inherent structure of the sentences being compared, at best relying on a recurrent neural network such as an LSTM BID15 to incorporate some amount of context from neighboring words into each word's representation. Sentence-level comparisons can incorporate the structure of each sentence individually BID2 BID36 , but cannot easily compare substructures between the sentences, as these are all squashed into a single vector. Some models do incorporate sentence structure by comparing subtrees between the two sentences BID46 BID4 , but require pipelined approaches where a parser is run in a non-differentiable preprocessing step, losing the benefits of end-to-end training.In this paper we propose a method, which we call structured alignment networks, to perform comparisons between substructures in two sentences, without relying on an external, non-differentiable parser. We use a structured attention mechanism BID18 BID25 to compute a structured alignment between the two sentences, jointly learning a latent tree structure for each sentence and aligning spans between the two sentences.Our method constructs a CKY chart for each sentence using the inside-outside algorithm BID27 , which is fully differentiable BID22 BID13 . This chart has a node for each possible span in the sentence, and a score for the likelihood of that span being a constituent in a parse of the sentence, marginalized over all possible parses. We take these two charts and find alignments between them, representing each span in each sentence with a structured attention over spans in the other sentence. These span representations , weighted by the span's likelihood, are then used to compare the two sentences. In this way we can perform comparisons between sentences that leverage the internal structure of each sentence in an end-to-end, fully differentiable model, trained only on one final objective. We evaluate this model on several sentence comparison datasets. In experiments on SNLI BID1 and TREC-QA (Voorhees & Tice, 2000) , we find that comparing sentences at the span level consistently outperforms comparing at the word level. Additionally, and in contrast to prior work , we find that learning sentence structure on the comparison objective results in well-formed trees that closely mimic syntax. Our results provide strong motivation for incorporating latent structure into models that implicitly or expliclty compare two sentences. We have considered the problem of comparing two sentences in natural language processing models. We have shown how to move beyond word-and sentence-level comparison to comparing spans between the two sentences, without the need for an external parser. Through experiments on several sentence comparison datasets, we have seen that span comparisons consistently outperform wordlevel comparisons, with no additional supervision. We additionally found our model was able to discover latent tree structures that closely mimic syntax, without any syntactic supervision.Our results have several implications for future work. First, the success of span comparisons over word-level comparisons suggests that it may be profitable to include such comparisons in more complex models, either for comparing two sentences directly, or as intermediate parts of models for more complex tasks, such as reading comprehension. Second, though we have not yet done a formal comparison with prior work on grammar induction, our model's ability to infer trees that look like syntax from a semantic objective is intriguing, and suggestive of future opportunities in grammar induction research. Also, the speed of the model remains a problem, with the insideoutside algorithm involved, the speed of the full model will be be 15-20 times slower than the decomposable attention model, mainly due the the fact this dynamic programming method can not be effectively accelerated on a GPU.",Matching sentences by learning the latent constituency tree structures with a variant of the inside-outside algorithm embedded as a neural network layer.,two ; first ; second ; Stanford ; CKY ; one ; Voorhees & Tice ; GPU,Our method ; sentence structure ; Voorhees & Tice ; the full model ; their latent structures ; Word-level comparisons ; these tasks ; words ; the second sentence ; Voorhees,two ; first ; second ; Stanford ; CKY ; one ; Voorhees & Tice ; GPU,"Natural language processing involves comparing two sentences to compute some notion of relevance, entailment, or similarity. Typically this comparison is done either at the word level or at the sentence level, with no attempt to leverage the inherent structure of the sentence. When sentence structure is used for comparison, it is obtained during a non-differentiable pre-processing step, leading to propagation of errors. A structured attention mechanism is used to match possible spans to possible spans, simultaneously discovering the tree structure of each sentence and performing a comparison in a model that is fully differentiable and trained only on the comparison objective. This model demonstrates how to compare two sentences",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate mutual information (MI) between each hidden layer and the input/desired output, to construct the IP. For instance, hidden layers with many neurons require MI estimators with robustness towards the high dimensionality associated with such layers. MI estimators should also be able to naturally handle convolutional layers, while at the same time being computationally tractable to scale to large networks. None of the existing IP methods to date have been able to study truly deep Convolutional Neural Networks (CNNs), such as the e.g.\ VGG-16. In this paper, we propose an IP analysis using the new matrix--based R\'enyi's entropy coupled with tensor kernels over convolutional layers, leveraging the power of kernel methods to represent properties of the probability distribution independently of the dimensionality of the data. The obtained results shed new light on the previous literature concerning small-scale DNNs, however using a completely new approach. Importantly, the new framework enables us to provide the first comprehensive IP analysis of contemporary large-scale DNNs and CNNs, investigating the different training phases and providing new insights into the training dynamics of large-scale neural networks. Although Deep Neural Networks (DNNs) are at the core of most state-of-the art systems in computer vision, the theoretical understanding of such networks is still not at a satisfactory level (Shwartz-Ziv & Tishby, 2017) . In order to provide insight into the inner workings of DNNs, the prospect of utilizing the Mutual Information (MI), a measure of dependency between two random variables, has recently garnered a significant amount of attention (Cheng et al., 2018; Noshad et al., 2019; Saxe et al., 2018; Shwartz-Ziv & Tishby, 2017; Yu et al., 2018; . Given the input variable X and the desired output Y for a supervised learning task, a DNN is viewed as a transformation of X into a representation that is favorable for obtaining a good prediction of Y . By treating the output of each hidden layer as a random variable T , one can model the MI I(X; T ) between X and T . Likewise, the MI I(T ; Y ) between T and Y can be modeled. The quantities I(X; T ) and I(T ; Y ) span what is referred to as the Information Plane (IP). Several works have demonstrated that one may unveil interesting properties of the training dynamics by analyzing DNNs in the form of the IP Goldfeld et al., 2019; Noshad et al., 2019; Chelombiev et al., 2019) . Figure 1 , produced using our proposed estimator, illustrates one such insight that is similar to the observations of Shwartz-Ziv & Tishby (2017) , where training can be separated into two distinct phases, the fitting phase and the compression phase. This claim has been highly debated as subsequent research has linked the compression phase to saturation of neurons (Saxe et al., 2018) or clustering of the hidden representations (Goldfeld et al., 2019) . Contributions We propose a novel approach for estimating MI, wherein a kernel tensor-based estimator of Rényi's entropy allows us to provide the first analysis of large-scale DNNs as commonly found in state-of-the-art methods. We further highlight that the multivariate matrix-based approach, proposed by , can be viewed as a special case of our approach. However, our proposed method alleviates numerical instabilities associated with the multivariate matrixbased approach, which enables estimation of entropy for high-dimensional multivariate data. Further, using the proposed estimator, we investigate the claim of Cheng et al. (2018) that the entropy H(X) ≈ I(T ; X) and H(Y ) ≈ I(T ; Y ) in high dimensions (in which case MI-based analysis would be meaningless) and illustrate that this does not hold for our estimator. Finally, our results indicate that the compression phase is apparent mostly for the training data, particularly for more challenging datasets. By utilizing a technique such as early-stopping, a common technique to avoid overfitting, training tends to stop before the compression phase occurs (see Figure 1 ). This may indicate that the compression phase is linked to the overfitting phenomena. Figure 1 : IP obtained using our proposed estimator for a small DNN averaged over 5 training runs. The solid black line illustrates the fitting phase while the dotted black line illustrates the compression phase. The iterations at which early stopping would be performed assuming a given patience parameter are highlighted. Here, patience denotes the number of iterations that need to pass without progress on a validation set before training is stopped to avoid overfitting. It can be observed that for low patience values, training will stop before the compression phase. For the benefit of the reader, the bottom right corner displays a magnified version of the first four layers. In this work, we propose a novel framework for analyzing DNNs from a MI perspective using a tensor-based estimate of the Rényi's α-order entropy. Our experiments illustrate that the proposed approach scales to large DNNs, which allows us to provide insights into the training dynamics. We observe that the compression phase in neural network training tends to be more prominent when MI is estimated on the training set and that commonly used early-stopping criteria tend to stop training before or at the onset of the compression phase. This could imply that the compression phase is linked to overfitting. Furthermore, we showed that, for our tensor-based approach, the claim that H(X) ≈ I(T ; X) and H(Y ) ≈ I(T ; Y ) does not hold. We believe that our proposed approach can provide new insight and facilitate a more theoretical understanding of DNNs.",First comprehensive information plane analysis of large scale deep neural networks using matrix based entropy and tensor kernels.,Yu et al. ; Convolutional Neural Networks ; IP ; I(T ; Saxe ; two ; Chelombiev et al. ; first ; Deep Neural Networks ; Shwartz-Ziv & Tishby,patience ; no means ; the observations ; our tensor-based approach ; a MI perspective ; the IP ; the Mutual Information (MI ; early-stopping ; the data ; the existing IP methods,Yu et al. ; Convolutional Neural Networks ; IP ; I(T ; Saxe ; two ; Chelombiev et al. ; first ; Deep Neural Networks ; Shwartz-Ziv & Tishby,"An IP analysis using R\'enyi's entropy coupled with tensor kernels over convolutional layers is being used to examine deep neural networks (DNNs) via information plane (IP) theory. However, it is not clear how to estimate mutual information between hidden layers and input/desired output, to construct the IP. MI estimators should be robust towards the high dimensionality associated with such layers, while also being computationally tractable to scale to large networks. In order to provide insight into deep Convolutional Neural Networks (CNNs) and their training phases, we propose an IP analysis utilizing the new matrix",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"We propose a framework to model the distribution of sequential data coming from
 a set of entities connected in a graph with a known topology. The method is
 based on a mixture of shared hidden Markov models (HMMs), which are trained
 in order to exploit the knowledge of the graph structure and in such a way that the
 obtained mixtures tend to be sparse. Experiments in different application domains
 demonstrate the effectiveness and versatility of the method. Hidden Markov models (HMMs) are a ubiquitous tool for modelling sequential data. They started by being applied to speech recognition systems and from there they have spread to almost any application one can think of, encompassing computational molecular biology, data compression, and computer vision. In the emerging field of cognitive radars BID12 , for the task of opportunistic usage of the spectrum, HMMs have been recently used to model the occupancy of the channels by primary users BID21 .When the expressiveness of an HMM is not enough, mixtures of HMM have been adopted. Roughly speaking, mixtures of HMMs can be interpreted as the result of the combination of a set of independent standard HMMs which are observed through a memoryless transformation BID5 BID8 BID22 BID13 .In many real-life settings one does not have a single data stream but an arbitrary number of network connected entities that share and interact in the same medium and generate data streams in real-time. The streams produced by each of these entities form a set of time series with both intra and inter relations between them. In neuroimaging studies, the brain can be regarded as a network: a connected system where nodes, or units, represent different specialized regions and links, or connections, represent communication pathways. From a functional perspective, communication is coded by temporal dependence between the activities of different brain areas BID6 . Also team sports intrinsically involve fast, complex and interdependent events among a set of entities (the players), which interact as a team BID24 BID23 . Thus, understanding a player's behavior implies understanding the behavior of his teammates and opponents over time.The extraction of knowledge from these streams to support the decision-making process is still challenging and the adaptation of HMM to this scenario is immature at best. BID9 proposed a hybrid approach combining the Self-Organizing Map (SOM) and the HMM with applications in clustering, dimensionality reduction and visualization of large-scale sequence spaces. Note that the model at each node is limited to a simple HMM. Wireless local area networks have also been modeled with Markov-based approaches. For instance, BID1 use HMMs for outlier detection in 802.11 wireless access points. However, the typical approaches include a common HMM model for all nodes (with strong limited flexibility) and a HMM model per node, independent of the others (not exploring the dependencies between nodes). BID4 built a sparse coupled hidden Markov model (SCHMM) framework to parameterize the temporal evolution of data acquired with functional magnetic resonance imaging (fMRI). The coupling is captured in the transition matrix, which is assumed to be a function of the activity levels of all the streams; the model per node is still restricted to a simple HMM.In general, in networked data streams, the stream observed in each sensor is often modeled by HMMs but the intercorrelations between sensors are seldom explored. The proper modeling of the intercorrelations has the potential to improve the learning process, acting as a regularizer in the learning process. In here we propose to tackle this void, by proposing as observation model at each node a sparse mixture of HMMs, where the dependencies between nodes are used to promote the sharing of HMM components between similar nodes. In this work we propose a method to model the generative distribution of sequential data coming from nodes connected in a graph with a known fixed topology. The method is based on a mixture of HMMs where its coefficients are regularized during the learning process in such a way that affine nodes will tend to have similar coefficients, exploiting the known graph structure. We also prove that the proposed regularizer promotes sparsity in the mixtures, which is achieved through a fully differentiable loss function (i.e. with no explicit L 0 penalty term). We evaluate the method's performance in two completely different tasks (anomaly detection in Wi-Fi networks and human motion forecasting), showing its effectiveness and versatility.For future work, we plan to extend/evaluate the usage of SpaMHMM for sequence clustering. This is an obvious extension that we did not explore thoroughly in this work, since its main focus was modeling the generative distribution of data. In this context, extending the idea behind SpaMHMM to mixtures of more powerful generative distributions is also in our plans. As is known, HMMs have limited expressiveness due to the strong independence assumptions they rely on. Thus, we plan to extend these ideas to develop an architecture based on more flexible generative models for sequence modeling, like those attained using deep recurrent architectures. After building the usual variational lower bound for the log-likelihood and performing the E-step, we get the following well-known objective:Jpθ, θ -q "" ÿ z,H ppX, z, H|y, θ -q log ppX, z, H|y, θq,which we want to maximize with respect to θ and where θ -are the model parameters that were kept fixed in the E-step. Some of the parameters in the model are constrained to represent valid probabilities, yielding the following Lagrangian: DISPLAYFORM0 Clearly, J r pθq´V r pθ, qq "" 1 Nˆl og ppX|y, θq´E z,H""q "" log ppX, z, H|y, θq qpz, Hq",A method to model the generative distribution of sequences coming from graph connected entities.,H|y ; two ; Markov ; SpaMHMM ; log ppX ; the Self-Organizing Map ; Lagrangian ; HMM.In ; HMM ; J r pθq´V,human motion forecasting ; Experiments ; outlier detection ; the following well-known objective ; the strong independence assumptions ; a ubiquitous tool ; two ; similar coefficients ; that ; the spectrum,H|y ; two ; Markov ; SpaMHMM ; log ppX ; the Self-Organizing Map ; Lagrangian ; HMM.In ; HMM ; J r pθq´V,"We propose a framework to model sequential data from a set of entities connected in a graph with a known topology. This framework is based on a mixture of shared hidden Markov models (HMMs), which are trained to exploit the knowledge of the graph structure and in such a way that mixtures tend to be sparse. Experiments in different application domains demonstrate the effectiveness and versatility of the method. Hidden Markov model (HMM) are a ubiquitous tool for modelling sequential data. They started by being applied to speech recognition systems and from there they have spread to almost any application one can think of, encompassing computational molecular biology,",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Despite remarkable empirical success, the training dynamics of generative adversarial networks (GAN), which involves solving a minimax game using stochastic gradients, is still poorly understood. In this work, we analyze last-iterate convergence of simultaneous gradient descent (simGD) and its variants under the assumption of convex-concavity, guided by a continuous-time analysis with differential equations. First, we show that simGD, as is, converges with stochastic sub-gradients under strict convexity in the primal variable. Second, we generalize optimistic simGD to accommodate an optimism rate separate from the learning rate and show its convergence with full gradients. Finally, we present anchored simGD, a new method, and show convergence with stochastic subgradients. Training of generative adversarial networks (GAN) (Goodfellow et al., 2014) , solving a minimax game using stochastic gradients, is known to be difficult. Despite the remarkable empirical success of GANs, further understanding the global training dynamics empirically and theoretically is considered a major open problem (Goodfellow, 2016; Radford et al., 2016; Metz et al., 2017; Mescheder et al., 2018; Odena, 2019) . The local training dynamics of GANs is understood reasonably well. Several works have analyzed convergence assuming the loss functions have linear gradients and assuming the training uses full (deterministic) gradients. Although the linear gradient assumption is reasonable for local analysis (even though the loss functions may not be continuously differentiable due to ReLU activation functions) such results say very little about global convergence. Although the full gradient assumption is reasonable when the learning rate is small, such results say very little about how the randomness affects the training. This work investigates global convergence of simultaneous gradient descent (simGD) and its variants for zero-sum games with a convex-concave cost using using stochastic subgradients. We specifically study convergence of the last iterates as opposed to the averaged iterates. Organization. Section 2 presents convergence of simGD with stochastic subgradients under strict convexity in the primal variable. The goal is to establish a minimal sufficient condition of global convergence for simGD without modifications. Section 3 presents a generalization of optimistic simGD , which allows an optimism rate separate from the learning rate. We prove the generalized optimistic simGD using full gradients converges, and experimentally demonstrate that the optimism rate must be tuned separately from the learning rate when using stochastic gradients. However, it is unclear whether optimistic simGD is theoretically compatible with stochastic gradients. Section 4 presents anchored simGD, a new method, and presents its convergence with stochastic subgradients. Anchoring represents what we consider to be the strongest contribution of this work. The presentation and analyses of Sections 2, 3, and 4 are guided by continuous-time firstorder ordinary differential equations (ODE). In particular, we interpret optimism and anchoring as discretizations of certain regularized dynamics. Section 5 experimentally demonstrates the benefit of optimism and anchoring for training GANs in some setups. Prior work. There are several independent directions for improving the training of GANs such as designing better architectures, choosing good loss functions, or adding appropriate regularizers (Radford et al., 2016; Sønderby et al., 2017; Gulrajani et al., 2017; Wei et al., 2018; Roth et al., 2017; Mescheder et al., 2018; 2017; Miyato et al., 2018) . In this work, we accept these factors as a given and focus on how to train (optimize) the model effectively. Optimism is a simple modification to remedy the cycling behavior of simGD, which can occur even under the bilinear convex-concave setup Daskalakis & Panageas, 2018; Mertikopoulos et al., 2019; Gidel et al., 2019a; Liang & Stokes, 2019; Mokhtari et al., 2019; Peng et al., 2019) . These prior work assume the gradients are linear and use full gradients. Although the recent name 'optimism' originates from its use in online optimization (Chiang et al., 2012; Rakhlin & Sridharan, 2013a; b; Syrgkanis et al., 2015) , the idea dates back to Popov's work in the 1980s (Popov, 1980) and has been studied independently in the mathematical programming community (Malitsky & Semenov, 2014; Malitsky, 2015; Malitsky & Tam, 2018; Malitsky, 2019; Csetnek et al., 2019) . We note that there are other mechanisms similar to optimism and anchoring such as ""prediction"" (Yadav et al., 2018) , ""negative momentum"" (Gidel et al., 2019b) , and ""extragradient"" (Korpelevich, 1976; Tseng, 2000; Chavdarova et al., 2019) . In this work, we focus on optimism and anchoring. Classical literature analyze convergence of the Polyak-averaged iterates (which assigns less weight to newer iterates) when solving convex-concave saddle point problems using stochastic subgradients (Bruck, 1977; Nemirovski & Yudin, 1978; Nemirovski et al., 2009; Juditsky et al., 2011; Gidel et al., 2019a) . For GANs, however, last iterates or exponentially averaged iterates (Yazıcı et al., 2019) (which assigns more weight to newer iterates) are used in practice. Therefore, the classical work with Polyak averaging do not fully explain the empirical success of GANs. We point out that we are not the first to utilize classical techniques for analyzing the training of GANs. In particular, the stochastic approximation technique (Heusel et al., 2017; Duchi & Ruan, 2018) , control theoretic techniques (Heusel et al., 2017; Nagarajan & Kolter, 2017) , ideas from variational inequalities and monotone operator theory (Gemp & Mahadevan, 2018; Gidel et al., 2019a) , and continuous-time ODE analysis (Heusel et al., 2017; Csetnek et al., 2019) have been utilized for analyzing GANs. In this work, we analyzed the convergence of SSSGD, Optimistic simGD, and Anchored SSSGD. Under the assumption that the cost L is convex-concave, Anchored SSSGD provably converges under the most general setup. Through experiments, we showed that the practical GAN training benefits from optimism and anchoring in some (but not all) setups. Generalizing these results to accommodate projections and proximal operators, analogous to projected and proximal gradient methods, is an interesting direction of future work. Weight clipping and spectral normalization (Miyato et al., 2018) A FURTHER DISCUSSION ON THE CONVERGENCE RESULTS Theorems 1, 2, 3, and 4 use related but different notions of convergence. Theorems 1 and 4 are asymptotic (has no rate) while Theorems 2 and 3 are non-asymptotic (has a rate). Theorems 1 and 3 respectively show almost sure and L 2 convergence of the iterates. Theorems 2 and 3 show convergence of the squared gradient norm for the best and last iterates, respectively. We did not make these choices. The choices were dictated by what we can prove based on the analysis. The discrete-time analysis of SimGD-O of Theorem 2 bounds the squared gradient norm of the best iterate, while the continuous-time analysis bounds the squared gradient norm of the ""last iterate"" (at terminal time). The discrepancy comes from the fact that while we have monotonic decrease of g(t) in continuous-time, we have no analogous monotonicity condition on g k in discrete-time. To the best of our knowledge, there is no result establishing a O(1/k) rate on the squared gradient norm of the last iterate for SimGD-O or the related ""extragradient method"" Korpelevich (1976) . Theorem 3 is the first result showing a rate close to O(1/k) on the last literate. For SimGD-O and Corollary 1, the parameter choices are almost optimal. The optimal choices that minimize the bound of Theorem 2 are α = 0.124897/R and β = 1.94431α; they provide a factor of 135.771, a very small improvement over the factor 136 of Corollary 1. For SimGD-A and Theorem 3, there is a discrepancy in the rate between the continuous time analysis O(1/t 2 ) and the discrete time rate O(1/k 2−2p ) for p ∈ (1/2, 1), which is slightly slower than O(1/k). In discretizing the continuous-time calculations to obtain a discrete proof, errors accumulate and prevent the rate from being better than O(1/k). This is not an artifact of the proof. Simple tests on bilinear examples show divergence when p < 1/2. SSSGD-A and Theorem 4 involves the parameter ε. While the proof requires ε > 0, we believe this is an artifact of the proof. In particular, we conjecture that Lemma 17 holds with o(s/τ ) rather than O(s/τ ), and, if so, it is possible to establish convergence with ε = 0. In Figure 2 , it seems that that the choice ε = 0 and p = 2/3 is optimal for SSSGD-A. While we do not have a theoretical explanation for this, we point out that this is not surprising as p = 2/3 is known to be optimal in stochastic convex minimization (Moulines & Bach, 2011; Taylor & Bach, 2019) . Theorems 2, 3, and 4 extend to monotone operators (Ryu & Boyd, 2016; Bauschke & Combettes, 2017) without any modification to their proofs. In infinite dimensional setups (which is of interest in the field of monotone operators) Theorem 4 establishes strong convergence, while many convergence results (including Theorems 2 and 3) establish weak convergence. However, Theorem 1 does not extend to monotone operators, as the use of the LaSalle-Krasnovskii principle is particular to convex-concave saddle functions.",Convergence proof of stochastic sub-gradients method and variations on convex-concave minimax problems,zero ; SSSGD ; Metz et al. ; Radford ; Roth ; Peng et al. ; First ; Odena ; Malitsky ; Nagarajan & Kolter,Prior work ; the stochastic approximation technique ; Gemp & Mahadevan ; the bilinear convex-concave setup ; the last literate ; ODE ; global convergence ; Sønderby ; its variants ; the learning rate,zero ; SSSGD ; Metz et al. ; Radford ; Roth ; Peng et al. ; First ; Odena ; Malitsky ; Nagarajan & Kolter,"The training dynamics of generative adversarial networks (GAN) involves solving a minimax game using stochastic gradients, is poorly understood. In this work, we analyze last-iterate convergence of simultaneous gradient descent (simGD) and its variants under the assumption of convex-concavity, guided by a continuous-time analysis with differential equations. First, we show that simGD, as is, converges with stochiastic sub-gradients under strict convexity in the primal variable. Second, we generalize optimistic simGD to accommodate an optimism rate separate from learning rate and show its convergence",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"This work addresses the long-standing problem of robust event localization in the presence of temporally of misaligned labels in the training data. We propose a novel versatile loss function that generalizes a number of training regimes from standard fully-supervised cross-entropy to count-based weakly-supervised learning. Unlike classical models which are constrained to strictly fit the annotations during training, our soft localization learning approach relaxes the reliance on the exact position of labels instead. Training with this new loss function exhibits strong robustness to temporal misalignment of labels, thus alleviating the burden of precise annotation of temporal sequences. We demonstrate state-of-the-art performance against standard benchmarks in a number of challenging experiments and further show that robustness to label noise is not achieved at the expense of raw performance. Figure 1: Temporal localization under label misalignment. Models are trained with noisy labels that differ from the actual ground-truth, while the final inference objective is the precise localization of events. The surge of deep neural networks Schmidhuber, 2015) has accentuated the evergrowing need for large corpora of data (Banko & Brill, 2001; Halevy et al., 2009) . The main bottleneck for the efficient creation of datasets remains the annotation process. Over the years, while new labeling paradigms have emerged to alleviate this issue (e.g., crowdsourcing (Deng et al., 2009) or external information sources (Abu-El-Haija et al., 2016) ), these methods have also highlighted, and emphasized, the prevalence of label noise. Deep neural networks are unfortunately not immune to these perturbations as their intrinsic ability to memorize and learn label noise (Zhang et al., 2017) can be the cause of training robustness issues and poor generalization performance. In this context, the development of models robust to label noise is essential. This work tackles the problem of precise temporal localization of events (i.e., determining when and which events occur) in sequential data (e.g. time series, video or audio sequences) despite only having access to poorly aligned annotations for training (see Figure 1 ). This task is characterized by the discrepency between the precision required of the predictions during inference and the noisiness of the training labels. Indeed, while models are trained on inaccurate data, they are evaluated on their ability to predict event occurences as precisely as possible with respect to the ground-truth. In such a setting, effective models have to infer event locations more accurately than the labels they relied on for training. This requirement is particularly challenging for most classical approaches that are designed to learn localization by strictly mimicking the provided annotations. Indeed, as the training labels themselves do not accurately reflect the event location, focusing on replicating these unreliable patterns is incompatible with the overall objective of learning the actual ground-truth. These challenges highlight the need for more relaxed learning approaches that are less dependent on the exact location of labels for training. The presence of temporal noise in localization tasks is ubiquitous given the continuous nature of the perturbation, in contrast to classification noise where only a fraction of the samples are misclassified. Temporal labeling is further characterized by an inevitable trade-off between annotation precision and time investment. For instance, while a coarse manual transcription of a minute of complex piano music might be achieved within a moderate time frame, a millisecond precision requirement -a common assumption for deep learning models -significantly increases the annotation burden. In this respect, models alleviating the need for costly annotations are key for a wide and efficient deployment of deep learning models in temporal localization applications. This work introduces a novel model-agnostic loss function that relaxes the reliance of the learning process on the exact temporal location of the annotations. This softer learning approach inherently makes the model more robust to temporally misaligned labels. Contributions This work: a) proposes a novel loss function for robust temporal localization under label misalignment, b) presents a succinct analysis of the loss' properties, c) evaluates the robustness of state-of-the-art localization models to label misalignment, and d) demonstrates the effectiveness of the proposed approach in various experiments. In this work, we have shown how relaxing annotation requirements (i.e., weakening the model's reliance on the exact location of events) not only has the practical benefit of alleviating annotation efforts but, more importantly, leads to a model that is robust to temporal noise without compromising performance on clean training data. This contrasts with traditional approaches which attempt to strictly mimic the annotations, leading to poor predictions when training with noisy labels. We have demonstrated these claims on a number of classical challenging tasks, in which our SoftLoc loss exhibits state-of-the-art performance. The proposed loss function is agnostic to the underlying network and hence can be used as a loss replacement in almost any recurrent architecture. The versatility of the model can find applications in a wide array of tasks, even beyond temporal localization.",This work introduces a novel loss function for the robust training of temporal localization DNN in the presence of misaligned labels.,Schmidhuber ; Banko & Brill ; Halevy ; al. ; the years ; Deng et al. ; Abu-El-Haija ; Zhang et al. ; transcription of a minute ; SoftLoc,Models ; classical challenging tasks ; labels ; training regimes ; datasets ; the precision ; Abu-El-Haija ; precise annotation ; the exact location ; the model's reliance,Schmidhuber ; Banko & Brill ; Halevy ; al. ; the years ; Deng et al. ; Abu-El-Haija ; Zhang et al. ; transcription of a minute ; SoftLoc,"This work addresses the long-standing issue of robust event localization in the presence of misaligned labels in training data. A versatile loss function is proposed that generalizes a number of training regimes from standard fully-supervised cross-entropy to count-based weakly-vised learning. Unlike classical models which are constrained to strictly fit the annotations during training, our soft localization learning approach relaxes the reliance on the exact position of labels instead. Training with this new loss function exhibits strong robustness to temporal misalignment of labels and alleviates the burden of precise annotation of temporal sequences. This work demonstrates state-of-the-",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Many real-world data sets are represented as graphs, such as citation links, social media, and biological interaction. The volatile graph structure makes it non-trivial to employ convolutional neural networks (CNN's) for graph data processing. Recently, graph attention network (GAT) has proven a promising attempt by combining graph neural networks with attention mechanism, so as to achieve massage passing in graphs with arbitrary structures. However, the attention in GAT is computed mainly based on the similarity between the node content, while the structures of the graph remains largely unemployed (except in masking the attention out of one-hop neighbors). In this paper, we propose an `````````````````````````````""ADaptive Structural Fingerprint"" (ADSF) model to fully exploit both topological details of the graph and  content features of the nodes. The key idea is to contextualize each node with a weighted, learnable receptive field  encoding rich and diverse local graph structures. By doing this, structural interactions between the nodes can  be inferred accurately, thus improving subsequent attention layer as well as the convergence of learning. Furthermore, our model provides a useful platform  for different subspaces of node features and various scales of graph structures to ``cross-talk'' with each other through the learning of multi-head attention, being particularly useful in handling complex real-world data.   Encouraging performance is observed on a number of benchmark data sets in node classification. Many real-world data set are represented naturally as graphs. For example, citation networks specify the citation links among scientific papers; social media often need to explore the significant amount of connections between users; biological processes typically involve complex interactions such as protein-protein-interaction (PPI). In these scenarios, the complex structures such as the graph topology or connectivities encode crucial domain-specific knowledge for the learning and prediction tasks. Examples include node embedding or classification, graph classification, and so on. The complexity of graph-structured data makes it non-trivial to employ traditional convolutional neural networks (CNN's). The CNN architecture was originally designed for images whose pixels are located on a uniform grids, and so the convolutional filters can be reused everywhere without having to accommodate local structure changes (LeCun & Kavukcuoglu, 2010) . More recently, CNN was used in natural language processing where the words of a sentence can be considered as a uniform chain, and showed great power in extracting useful semantic features (Kim, 2014) . However, extending CNN to deal with arbitrary structured graphs beyond uniform grids or chains can be quite non-trivial. To solve this problem, graph neural networks (GNN) were early proposed by Gori et al. (2005) and Sperduti (1997) , which adopt an iterative process and propagate the state of each node, followed by a neural network module to generate the output of each node, until an equilibrium state is reached. Recent development of GNN can be categorized into spectral and nonspectral approaches. Spectral approaches employ the tools in signal processing and transform the convolutional operation in the graph domain to much simpler operations of the Laplacian spectrum (Bruna et al., 2014) , and various approaches have been proposed to localize the convolution in either the graph or spectral domain (Henaff et al., 2015; Defferrard et al., 2016; Kipf & Welling, 2017) . Non-spectral approaches define convolutions directly on the graph within spatially close nodes. As a result, varying node structures have to be accommodated through various processing steps such as fixed-neighborhood size sampling (Hamilton et al., 2017) , neighborhood normalization (Niepert et al., 2016) , and learning a weight matrix for each node degree (Duvenaud et al., 2015) or neighborhood size (Hamilton et al., 2017) . More recently, the highway connection in residual network is further introduced in graph neural networks to improve the performance on graph data processing (Zhang & Meng, 2019) . Recently, graph attention network (GAT) proves a promising framework by combining graph neural networks with attention mechanism in handling graphs with arbitrary structures (Velickovic et al., 2017) . The attention mechanism allows dealing with variable sized input while focusing on the most relevant parts, and has been widely used in sequence modelling (Bahdanau et al., 2015; Devlin et al., 2019; Vaswani et al., 2017) , machine translation (Luong et al., 2015) , and visual processing (Xu et al., 2015) . The GAT model further introduces attention module into graphs, where the hidden representation of the nodes are computed by repeatedly attending over their neighbors' features, and the weighting coefficients are calculated inductively based on a self-attention strategy. State-of-theart performance has been obtained on tasks of node embedding and classification. The attention in GAT is computed mainly based on the content of the nodes; the structures of the graph, on the other hand, are simply used to mask the attention, e.g., only one-hop neighbors will be attended. However, we believe that rich structural information such as the topology or ""shapes"" of local edge connections should provide a more valuable guidance on learning node representations. For example, in social networks or biological networks, a community or pathway is oftentimes composed of nodes that are densely inter-connected with each other but several hops away. Therefore, it can be quite beneficial if a node can attend high-order neighbors from the same community, even if they show no direct connections. To achieve this, simply checking k-hop neighbors would seem insufficient and a thorough exploration of structural landscapes of the graph becomes necessary. In order to fully exploit rich, high-order structural details in graph attention networks, we propose a new model called ""adaptive structural fingerprints"". The key idea is to contextualize each node within a local receptive field composed of its high-order neighbors. Each node in the neighborhood will be assigned a non-negative, closed-form weighting based on local information propagation procedures, and so the domain (or shape) of the receptive field will adapt automatically to local graph structures and the learning task. We call this weighted, tunable receptive field for each node its ""structural fingerprint"". We then define interactions between two structural fingerprints, which will be used in conjunction with node feature similarities to compute a final attention layer. Furthermore, our approach provides a useful platform for different subspaces of the node features and various scales of local graph structures to coordinate with each other in learning multi-head attention, being particularly beneficial in handling complex real-world graph data sets. The rest of the paper is organized as follows. In Section 2, we introduce the proposed method, including limitation of content-based graph attention, construction of the adaptive structural fingerprints, and the whole algorithm workflow. In Section 3, we discuss related work. Section 4 reports empirical evaluations and the last section concludes the paper. In this work, we proposed an adaptive structural fingerprint model to encode complex topological and structural information of the graph to improve learning hidden representations of the nodes through attention. There are a number of interesting future directions. First, we will consider varying fingerprint parameters (such as decay profile) instead of sharing them across all the nodes; second, we will also consider applying the structural fingerprints in problems of graph partitioning and community detection, where node features might be unavailable and graph structures will be the main information for decisions; third, we will extend our approach to challenging problems of graph-level classification where node-types shall be taken into account in constructing structural fingerprint; finally, on the theoretical side, we will borrow existing tools in semi-supervised learning and study the generalization performance of our approach on semi-supervised node embedding and classification. Figure 7 : Performance of GAT (percent accuracy) for different neighborhood sizes.","Exploiting rich strucural details in graph-structued data via adaptive ""strucutral fingerprints''",al. ; one ; CNN ; Vaswani ; ADSF ; Gori et al. ; second ; Kim ; LeCun & Kavukcuoglu ; Hamilton,a neural network module ; the weighting coefficients ; massage ; the convolution ; our approach ; graph ; Examples ; Defferrard et al. ; subsequent attention layer ; multi-head attention,al. ; one ; CNN ; Vaswani ; ADSF ; Gori et al. ; second ; Kim ; LeCun & Kavukcuoglu ; Hamilton,"The volatile graph structure makes it non-trivial to employ convolutional neural networks (CNN's) for graph data processing. However, graph attention network (GAT) has proven a promising attempt by combining graph neural networks with attention mechanism to achieve massage passing in graphs with arbitrary structures. The attention in GAT is computed based on the similarity between node content and structures, while structures of the graph remains largely unemployed except in masking the attention out of one-hop neighbors). In this paper, we propose an ``````````````",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"To improve how neural networks function it is crucial to understand their learning process. The information bottleneck theory of deep learning proposes that neural networks achieve good generalization by compressing their representations to disregard information that is not relevant to the task. However, empirical evidence for this theory is conflicting, as compression was only observed when networks used saturating activation functions. In contrast, networks with non-saturating activation functions achieved comparable levels of task performance but did not show compression. In this paper we developed more robust mutual information estimation techniques, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions. Using these adaptive estimation techniques, we explored compression in networks with a range of different activation functions. With two improved methods of estimation, firstly, we show that saturation of the activation function is not required for compression, and the amount of compression varies between different activation functions. We also find that there is a large amount of variation in compression between different network initializations. Secondary, we see that L2 regularization leads to significantly increased compression, while preventing overfitting. Finally, we show that only compression of the last layer is positively correlated with generalization. Although deep learning (reviewed by BID15 ) has produced astonishing advances in machine learning BID17 , a rigorous statistical explanation for the outstanding performance of deep neural networks (DNNs) is still to be found.According to the information bottleneck (IB) theory of deep learning BID18 BID16 ) the ability of DNNs to generalize can be seen as a type of representation compression. The theory proposes that DNNs use compression to eliminate noisy and task-irrelevant information from the input, while retaining information about the relevant segments BID1 . The information bottleneck method BID19 quantifies the relevance of information by considering an intermediate representation T between the original signal X and the salient data Y . T is the most relevant representation of X, and is said to be an information bottleneck, when it maximally compresses the input, retaining only the most relevant information, while maximizing the information it shares with the target variable Y . Formally, the information bottleneck minimizes the Lagrangian: DISPLAYFORM0 where I(·) is mutual information. In this Lagrangian β is the Lagrange multiplier, determining the trade-off between compression and retention of information about the target. In the context of deep learning, T is a layer's hidden activity represented as a single variable, X is a data set and Y is the set of labels. Compression for a given layer is signified by a decrease in I(T, X) value, while I(T, Y ) is increasing during training. Fitting behaviour refers to both values increasing. BID16 visualized the dynamic of training a neural network by plotting the values of I(T, X) and I(T, Y ) against each other. This mapping was named the information plane. According to IB theory the learning trajectory should move the layer values to the top left of this plane. In fact what was observed was that a network with tanh activation function had two distinct phases: fitting and compression. The paper and the associated talks 1 show that the compression phase leads to layers stabilizing on the IB bound. When this study was replicated by BID14 with networks using ReLU BID12 activation function instead of tanh, the compression phase did not happen, and the information planes only showed fitting throughout the whole training process. This behaviour required more detailed study, as a constant increase in mutual information between the network and its input implies increasing memorization, an undesired trait that is linked to overfitting and poor generalization BID11 .Measuring differential mutual information in DNNs is an ill-defined task, as the training process is deterministic BID14 . Mutual information of hidden activity T with input X is: DISPLAYFORM1 If we consider the hidden activity variable T to be deterministic then entropy is: DISPLAYFORM2 However, if T is continuous then the entropy formula is: DISPLAYFORM3 In the case of deterministic DNNs, hidden activity T is a continuous variable and p(T |X) is distributed as the delta function. For the delta function : DISPLAYFORM4 Thus, the true mutual information value I(T, X) is in fact infinite. However, to observe the dynamics of training in terms of mutual information, finite values are needed. The simplest way to avoid trivial infinite mutual information values, is to add noise to hidden activity.Two ways of adding noise have been explored previously by BID16 and BID14 . One way is to add noise Z directly to T and get a noisy variableT = T + Z. Then H(T |X) = H(Z) and mutual information is I(T , X) = H(T ) + H(Z). When the additive noise is Gaussian, the mutual information can be approximated using kernel density estimation (KDE), with an assumption that the noisy variable is distributed as a Gaussian mixture BID9 . The second way to add noise is to discretize the continuous variables into bins. To estimate mutual information , BID16 and BID14 primarily relied on binning hidden activity. The noise comes embedded with the discretization that approximates the probability density function of a random variable. In context of neural networks , adding noise can be done by binning hidden activity and approximating H(T ) as a discrete variable. In this case H(T |X) = 0 since the mapping is deterministic and I(T, X) = H(T ).Generally, when considering mutual information in DNNs, the analyzed values are technically the result of the estimation process and, therefore, are highly sensitive to it. For this reason it is vital to maintain consistency when estimating mutual information. The problem is not as acute when working with DNNs implemented with saturating activation functions, since all hidden activity is bounded. However, with non-saturating functions, and the resulting unbounded hidden activity, the level of noise brought by the estimation procedure has to be proportional and consistent, adapting to the state of every layer of the network at a particular epoch.In the next section adaptive estimation schemes are presented, both for the binning and KDE estimators. It is shown that for networks with unbounded activation functions in their hidden layers, the estimates of information change drastically. Moreover, the adaptive estimators are better able to evaluate different activation functions in a way that allows them to be compared. This approach shows considerable variation in compression for different activation functions. It also shows that L2 regularization leads to more compression and clusters all layers to the same value of mutual information. When compression in hidden layers is quantified with a compression metric and compared with generalization, no significant correlation is observed. However, compression of the last softmax layer is correlated with generalization. In this paper we proposed adaptive approaches to estimating mutual information in the hidden layers of DNNs. These adaptive approaches allowed us to compare behaviour of different activation functions and to observe compression in DNNs with non-saturating activation functions. However, unlike saturating activation functions, compression is not always present and is sensitive to initialization. This may be due to the minimal size of the network architecture that was tested. Experiments with larger convolutional neural networks could be used to explore this possibility.Different non-saturating activation functions compress information at different rates. While saturation plays a role in compression rates, we show that its absence does not imply absence of compression. Even seemingly similar activation functions, such as softplus and centered softplus, gave different compression scores. Compression does not always happen in later stages of training, but can happen from initialization. Further work is needed to understand the other factors contributing to compression.We also found that DNNs implemented with L2 regularization strongly compress information, forcing layers to forget information about the input. The clustering of mutual information to a single point on the information plane has never been reported previously. This result could lay the ground for further research to optimize the regularization to stabilize the layers on the information bottleneck bound to achieve better generalization BID0 , as well as linking information compression to memorization in neural networks BID20 .There are a few limitations to the analysis presented here. Principally , for tractability, the networks we explored were much smaller and more straightforward than many state of the art networks used for practical applications. Furthermore , our methods for computing information, although adaptive for any distribution of network activity, were not rigorously derived. Finally, our compression metric is ad-hoc. However, overall we have three main observations: first, compression is not restricted to saturating activation functions, second, L2 regularization induces compression, and third, generalization accuracy is positively correlated with the degree of compression only in the last layer and is not significantly affected by compression of hidden layers.",We developed robust mutual information estimates for DNNs and used them to observe compression in networks with non-saturating activation functions,ReLU ; firstly ; Gaussian ; I(T ; first ; third ; Lagrangian ; two ; Secondary ; One,activation functions ; estimation ; ReLU ; consistency ; The paper ; many state ; this paper ; DNNs ; a single point ; H(Z,ReLU ; firstly ; Gaussian ; I(T ; first ; third ; Lagrangian ; two ; Secondary ; One,"The information bottleneck theory of deep learning proposes that neural networks achieve good generalization by compressing representations to disregard information that is not relevant to the task. However, empirical evidence for this theory is conflicting. In contrast, networks with saturating activation functions achieved comparable levels of task performance but did not show compression. In this paper, we developed more robust mutual information estimation techniques, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions. We also explored compression in networks with a range of different activation functions. With two improved methods of estimation, we show that saturation of the",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Routing models, a form of conditional computation where examples are routed through a subset of components in a larger network, have shown promising results in recent works. Surprisingly, routing models to date have lacked important properties, such as architectural diversity and large numbers of routing decisions. Both architectural diversity and routing depth can increase the representational power of a routing network. In this work, we address both of these deficiencies. We discuss the significance of architectural diversity in routing models, and explain the tradeoffs between capacity and optimization when increasing routing depth. In our experiments, we find that adding architectural diversity to routing models significantly improves performance, cutting the error rates of a strong baseline by 35% on an Omniglot setup. However, when scaling up routing depth, we find that modern routing techniques struggle with optimization. We conclude by discussing both the positive and negative results, and suggest directions for future research. Modern neural networks process each input in the exact same way. This static paradigm is rigid compared to how brains process sensory inputs. Brains can utilize different subnetworks to process different categories of objects, such as face-specific processing in the fusiform face area BID27 BID17 . While static neural networks are empirically effective, it remains an open question whether neural networks with input-dependent processing can improve performance. Input-dependent processing holds the promise of offering better parameter efficiency and reduced computation due to the specialization of processing.Input-dependent processing has been underexplored in comparison with the wealth of work on static networks. Much of the work exploring input-dependent processing has taken the form of per-example routing within a network BID44 BID12 BID35 BID41 , which is form of conditional computation BID3 . In per-example routing, different examples are processed by different subcomponents, or experts BID23 , inside a larger model, or supernetwork BID12 . Only a subset of experts in the supernetwork are active for any given example. This paradigm enables the experts, each of which has its own set of parameters, to specialize to subsets of the input domain. The process of routing each example, which determines the experts that are used, is learned jointly with the parameters of the experts.Routing models to date have been relatively small, homogeneous networks. Typically, the same architectural unit (e.g., a fully connected layer of the same width) is used for every expert. The experts differ only in the parameters. Intuitively, the diversity of input examples is best handled by a diversity of architectural units with varying properties, implying that the usage of homogeneous experts is limiting. Furthermore, the number of routing decisions made in prior routing network works has typically been five or fewer. More routing decisions increase the number of distinct paths in the network, which may increase representational power. Making static networks deeper reliably improves performance, so we suspect that the representational power of routing networks is limited when only a few routing decisions are made.In this work, we address these two deficiencies in routing models. Since we aim to increase the representational capacities of routing models, we first introduce a simple trick that reduces overfitting.We then show how routing models with architectural diversity represent a broad family of models that generalize a number of powerful models. We also discuss the tradeoffs of scaling up the number of routing decisions with respect to optimization difficulty. In our experiments, we demonstrate that architecturally diverse routing models beat the best baselines with a 35% improvement in error rate on an Omniglot setup. By ablating the architectural diversity, we show that diversity plays a key role in achieving strong performance. We then scale up the number of decisions in routing models on CIFAR-10 and demonstrate that while competitive performance can be achieved, the accuracy drops as the number of decisions increase due to optimization challenges. Finally, we discuss our both our positive and negative findings and suggest future research directions for routing models. In this work, we introduced diversity to routing models and experimented with increasing routing depth. We believe that these two ideas are both intuitive and simple for researchers to implement. In our experiments, we found that architectural diversity can have a big impact in final performance. However, the impact of routing depth remains uncertain due to optimization difficulties faced by current methods.While routing models are a promising direction of research, practitioners still prefer static models due to their simplicity and reliable performance. For the use of routing models to become widespread, there must be a successful application of routing models on a domain where static models struggle. We believe that large scale problems fit this criteria, since they play into the theoretical scaling strengths of routing models. While architectural diversity will help improve routing models on large scale tasks, the routing depth optimization problem will continue to impede success. We encourage researchers to develop methods that will enable routing methods to effectively scale on large scale tasks. We remain optimistic that routing models will play an important role in the neural networks of the future.","Per-example routing models benefit from architectural diversity, but still struggle to scale to a large number of routing decisions.",Omniglot ; five ; two ; first,the wealth ; the parameters ; depth ; a network ; powerful models ; the specialization ; different categories ; conditional computation ; optimization ; an Omniglot setup,Omniglot ; five ; two ; first,"Routing models, a form of conditional computation where examples are routed through a subset of components in a larger network, have shown promising results in recent works. However, they lacked important properties, such as architectural diversity and routing depth, which can increase representational power of a routing network. In this work, we discuss the significance of architectural diversity in routing models, and explain the tradeoffs between capacity and optimization when increasing routing depth. In our experiments, we find that adding architectural diversity significantly improves performance, cutting error rates by 35% on an Omniglot setup, while modern routing techniques struggle with optimization. In addition, we highlight",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"We investigate the robustness properties of image recognition models equipped with two features inspired by human vision, an explicit episodic memory and a shape bias, at the ImageNet scale. As reported in previous work, we show that an explicit episodic memory improves the robustness of image recognition models against small-norm adversarial perturbations under some threat models. It does not, however, improve the robustness against more natural, and typically larger, perturbations. Learning more robust features during training appears to be necessary for robustness in this second sense. We show that features derived from a model that was encouraged to learn global, shape-based representations (Geirhos et al., 2019) do not only improve the robustness against natural perturbations, but when used in conjunction with an episodic memory, they also provide additional robustness against adversarial perturbations. Finally, we address three important design choices for the episodic memory: memory size, dimensionality of the memories and the retrieval method. We show that to make the episodic memory more compact, it is preferable to reduce the number of memories by clustering them, instead of reducing their dimensionality. ImageNet-trained deep neural networks (DNNs) are state of the art models for a range of computer vision tasks and are currently also the best models of the human visual system and primate visual systems more generally (Schrimpf et al., 2018 ). Yet, they have serious deficiencies as models of human and primate visual systems: 1) they are extremely sensitive to small adversarial perturbations imperceptible to the human eye (Szegedy et al., 2013) , 2) they are much more sensitive than humans to larger, more natural perturbations (Geirhos et al., 2018) , 3) they rely heavily on local texture information in making their predictions, whereas humans rely much more on global shape information (Geirhos et al., 2019; , 4) a fine-grained, image-by-image analysis suggests that images that ImageNet-trained DNNs find hard to recognize do not match well with the images that humans find hard to recognize . Here, we add a fifth under-appreciated deficiency: 5) human visual recognition has a strong episodic component lacking in DNNs. When we recognize a coffee mug, for instance, we do not just recognize it as a mug, but as this particular mug that we have seen before or as a novel mug that we have not seen before. This sense of familiarity/novelty comes automatically, involuntarily, even when we are not explicitly trying to judge the familiarity/novelty of an object we are seeing. More controlled psychological experiments also confirm this observation: humans have a phenomenally good longterm recognition memory with a massive capacity even in difficult one-shot settings (Standing, 1973; Brady et al., 2008) . Standard deep vision models, on the other hand, cannot perform this kind of familiarity/novelty computation naturally or automatically, since this information is available to a trained model only indirectly and implicitly in its parameters. What does it take to address these deficiencies and what are the potential benefits, if any, of doing so other than making the models more human-like in their behavior? In this paper, we address these questions. We show that a minimal model incorporating an explicit key-value based episodic memory does not only make it psychologically more realistic, but also reduces the sensitivity to small adversarial perturbations. It does not, however, reduce the sensitivity to larger, more natural perturbations and it does not address the heavy local texture reliance issue. In the episodic memory, using features from DNNs that were trained to learn more global shape-based representations (Geirhos et al., 2019) addresses these remaining issues and moreover provides additional robustness against adversarial perturbations. Together, these results suggest that two basic ideas motivated and inspired by human vision, a strong episodic memory and a shape bias, can make image recognition models more robust to both natural and adversarial perturbations at the ImageNet scale.","systematic study of large-scale cache-based image recognition models, focusing particularly on their robustness properties",two ; ImageNet ; second ; Geirhos ; al. ; three ; fifth ; one ; Brady,a phenomenally good longterm recognition memory ; a shape bias ; serious deficiencies ; its parameters ; the other hand ; previous work ; Geirhos ; familiarity/novelty ; Geirhos et al ; more robust features,two ; ImageNet ; second ; Geirhos ; al. ; three ; fifth ; one ; Brady,"We examine the robustness properties of image recognition models equipped with two features inspired by human vision, an explicit episodic memory and shape bias, at the ImageNet scale. We show that the explicit memory improves robustness against small-norm adversarial perturbations under threat models, while the shape-based representations enhance robustness. Learning more robust features during training, such as memory size, dimensionality, and retrieval method, can help in this second sense. By clustering memories, these features can help improve robustness in this sense. In contrast, human visual recognition has a strong episodic component lacking in DNNs.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"The ability to synthesize realistic patterns of neural activity is crucial for studying neural information processing. Here we used the Generative Adversarial Networks (GANs) framework to simulate the concerted activity of a population of neurons.
 We adapted the Wasserstein-GAN variant to facilitate the generation of unconstrained neural population activity patterns while still benefiting from parameter sharing in the temporal domain.
 We demonstrate that our proposed GAN, which we termed Spike-GAN, generates spike trains that match accurately the first- and second-order statistics of datasets of tens of neurons and also approximates well their higher-order statistics. We applied Spike-GAN to a real dataset recorded from salamander retina and showed that it performs as well as state-of-the-art approaches based on the maximum entropy and the dichotomized Gaussian frameworks. Importantly, Spike-GAN does not require to specify a priori the statistics to be matched by the model, and so constitutes a more flexible method than these alternative approaches.
 Finally, we show how to exploit a trained Spike-GAN  to construct 'importance maps' to detect the most relevant statistical structures present in a spike train. 
 Spike-GAN provides a powerful, easy-to-use technique for generating realistic spiking neural activity and for describing the most relevant features of the large-scale neural population recordings studied in modern systems neuroscience.
 Understanding how to generate synthetic spike trains simulating the activity of a population of neurons is crucial for systems neuroscience. In computational neuroscience, important uses of faithfully generated spike trains include creating biologically consistent inputs needed for the simulation of realistic neural networks, generating large datasets to be used for the development and validation of new spike train analysis techniques, and estimating the probabilities of neural responses in order to extrapolate the information coding capacity of neurons beyond what can be computed from the neural data obtained experimentally BID14 BID29 . In experimental systems neuroscience, the ability to develop models that produce realistic neural population patterns and that identify the key sets of features in these patterns is fundamental to disentangling the encoding strategies used by neurons for sensation or behavior and to design closed-loop experiments BID16 in which synthetic patterns, representing salient features of neural information, are fed to systems of electrical micro-stimulation BID44 or patterned light optogenetics BID3 for naturalistic intervention on neural circuits.One successful way to generate realistic spike trains is that of using a bottom-up approach, focusing explicitly on replicating selected low-level aspects of spike trains statistics. Popular methods include renewal processes BID42 ; BID10 ), latent variable models BID23 BID22 and maximum entropy approaches BID43 BID40 BID39 , which typically model the spiking activity under the assumption that only first and second-order correlations play a relevant role in neural coding (but see BID4 ; BID18 ; BID32 ). Other methods model spike train responses assuming linear stimulus selectivity and generating single trial spike trains using simple models of input-output neural nonlinearities and neural noise BID15 BID35 BID19 . These methods have had a considerable success in modeling the activity of populations of neurons in response to sensory stimuli BID35 . Nevertheless, these models are not completely general and may fail to faithfully represent spike trains in many situations. This is because neural variability changes wildly across different cortical areas BID24 due to the fact that responses, especially in higher-order areas and in behaving animals, have complex non-linear tuning to many parameters and are affected by many behavioral variables (e.g. the level of attention BID9 ).An alternative approach is to apply deep-learning methods to model neural activity in response to a given set of stimuli using supervised learning techniques BID26 . The potential advantage of this type of approach is that it does not require to explicitly specify any aspect of the spike train statistics. However , applications of deep networks to generate faithful spike patterns have been rare. Here, we explore the applicability of the Generative Adversarial Networks (GANs) framework BID12 to this problem. Three aspects of GANs make this technique a good candidate to model neural activity. First , GANs are an unsupervised learning technique and therefore do not need labeled data (although they can make use of labels BID31 BID5 ). This greatly increases the amount of neural data available to train them. Second , recently proposed modifications of the original GANs make them good at fitting distributions presenting multiple modes BID13 . This is an aspect that is crucial for neural data because the presentation of even a single stimulus can elicit very different spatio-temporal patterns of population activity BID7 BID28 . We thus need a method that generates sharp realistic samples instead of producing samples that are a compromise between two modes (which is typical, for instance, of methods seeking to minimize the mean squared error between the desired output and the model's prediction BID11 BID20 ). Finally , using as their main building block deep neural networks, GANs inherit the capacity of scaling up to large amounts of data and therefore constitute a good candidate to model the ever growing datasets provided by experimental methods like chronic multi-electrode and optical recording techniques.In the present work we extend the GAN framework to synthesize realistic neural activity. We adapt the recently proposed Wasserstein-GAN (WGAN) which has been proven to stabilize training, by modifying the network architecture to model invariance in the temporal dimension while keeping dense connectivity across the modeled neurons. We show that the proposed GAN, which we called Spike-GAN, is able to produce highly realistic spike trains matching the first and second-order statistics of a population of neurons. We further demonstrate the applicability of Spike-GAN by applying it to a real dataset recorded from the salamander retina and comparing the activity patterns the model generates to those obtained with a maximum entropy model BID46 and with a dichotomized Gaussian method BID22 . Finally, we describe a new procedure to detect, in a given activity pattern, those spikes participating in a specific feature characteristic of the probability distribution underlying the training dataset. We explored the application of the Generative Adversarial Networks framework BID12 to synthesize neural responses that approximate the statistics of the activity patterns of a Figure 4 : A) An example pattern showing the different packets highlighted with different colors and sorted to help visualization. The probability of each type of packet to occur was set to 0.1. Packets of the same type do not overlap in time. B) Realistic neural population pattern (gray spikes do not participate in any packet). C) Examples of activity patterns (grayscale panels) in which only one type of packet is usually present (one or two times) during a period of time from 16 to 32 ms. Packets are highlighted as white spikes. Heatmaps: importance maps showing the change that disrupting specific spikes has on the critic's output. Note that packet spikes normally show higher values. We used a sliding window of 8 ms (with a step size of 2 ms) to selectively shuffle the activity of each neuron at different time periods. The Spike-GAN used to obtain these importance maps was trained for 50000 iterations on 8192 samples. D) Average of 200 randomly selected importance maps across the neurons dimension, yielding importance as a function of time. E) Average of the same 200 randomly selected importance maps across the time dimension, yielding importance as a function of neurons. Errorbars correspond to standard error. population of neurons. For this purpose, we put forward Spike-GAN, by adapting the WGAN variant proposed by to allow sharing weights across time while maintaining a densely connected structure across neurons. We found that our method reproduced to an excellent approximation the spatio-temporal statistics of neural activity on which it was trained. Importantly, it does so without the need for these statistics to be handcrafted in advance, which avoids making a priori assumptions about which features of the external world make neurons fire.Recently, BID33 have proposed a deep learning method, LFADS (Latent Factor Analysis via Dynamical Systems), to model the activity of a population of neurons using a variational autoencoder (in which the encoder and decoder are recurrent neural networks). LFADS allows inferring the trial-by-trial population dynamics underlying the modeled spike train patterns and thus can be seen as a complementary method to Spike-GAN, which does not explicitly provide the latent factors governing the response of the neurons. Regarding the application of the GANs framework to the field of neuroscience, BID1 proposed a GAN-based approach for fitting network models to experimental data consisting of a set of tuning curves extracted from a population of neurons. However, to the best of our knowledge our work is the first to use GANs to directly produce realistic neural patterns simulating the activity of populations of tenths of neurons.Building on the work by BID47 , we showed how to use Spike-GAN to visualize the particular features that characterize the training dataset. Specifically, Spike-GAN can be used to obtain importance maps that highlight the spikes that participate in generating activity motifs that are most salient in the spike trains. This can be useful for unsupervised identification of highly salient low-dimensional representations of neural activity, which can then be used to describe and interpret experimental results and discover the key units of neural information used for functions such as sensation and behavior.A further and promising application of importance maps is that of designing realistic patterns of stimulation that can be used to perturb populations of neurons using electrical or optical neural stimulation techniques BID44 BID8 . The ability of Spike-GAN to generate realistic neural activity including its temporal dynamics and to identify its most salient features suggests that it may become a very relevant tool to design perturbations. In FIG1 we provide a more detailed description of a potential application of Spike-GAN, in which importance maps may allow inferring the set of neurons participating in the encoding of the information about a given set of stimuli FIG1 ) and the spatio-temporal structure of the packets elicited by each stimulus FIG1 .We have compared Spike-GAN with two alternative methods based on the maximum entropy and the dichotomized Gaussian frameworks. These methods offer the possibility of computing the sample probabilities (MaxEnt model) and separately specifying the signal and noise correlations present in the generated samples (DG model). Spike-GAN does not have these features; nevertheless, it does have important advantages over the mentioned methods. First, Spike-GAN is more flexible than the MaxEnt and DG models, being able to fit any type of spatio-temporal structure present in the data. Further, it does not require making a priori assumptions about which statistical properties of a dataset are relevant and thus need to be matched. Finally, Spike-GAN is based on the deep neural network framework, and is therefore able to directly benefit from the engineering advances emerging in this rapidly-growing field. Conceivably, this will enable Spike-GAN, or methods derived from it, to make in the future better and better use of the datasets of ever increasing size that are produced by the experimental neuroscience community.",Using Wasserstein-GANs to generate realistic neural activity and to detect the most relevant features present in neural population patterns.,GAN ; Wasserstein ; two ; Three ; fed ; tens ; the Generative Adversarial Networks ; second ; tenths ; first,the ever growing datasets ; salamander retina ; Wasserstein-GAN ; the signal and noise correlations ; important uses ; complex non-linear tuning ; This ; realistic spike trains ; neural responses ; the critic's output,GAN ; Wasserstein ; two ; Three ; fed ; tens ; the Generative Adversarial Networks ; second ; tenths ; first,The ability to synthesize realistic patterns of neural activity is crucial for neural information processing. We used the Generative Adversarial Networks (GANs) framework to simulate the concerted activity of a population of neurons. The Wasserstein-GAN variant facilitates the generation of unconstrained neural population activity patterns while still benefiting from parameter sharing in the temporal domain. Our proposed GAN generates spike trains that match accurately the first- and second-order statistics of datasets of tens of neurons and also approximates well their higher-order data. We applied Spike-GAN to a real dataset recorded from salamander retina and showed that it performs,/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Blind document deblurring is a fundamental task in the field of document processing and restoration, having wide enhancement applications in optical character recognition systems, forensics, etc. Since this problem is highly ill-posed, supervised and unsupervised learning methods are well suited for this application. Using various techniques, extensive work has been done on natural-scene deblurring. However, these extracted features are not suitable for document images. We present SVDocNet, an end-to-end trainable U-Net based spatial recurrent neural network (RNN) for blind document deblurring where the weights of the RNNs are determined by different convolutional neural networks (CNNs). This network achieves state of the art performance in terms of both quantitative measures and qualitative results. With the advent of digitization, document and text based images have become very prominent in one's quotidian lifestyle, spanning over reports, certificates, receipts, handwritten documents, etc. During image acquisition, numerous unavoidable factors such as camera shake, focusing errors, and noise may corrupt the image, leading to loss of valuable information. Hence, image post-processing became mandatory. This step is especially vital in automated information retrieval and optical character recognition systems. The process of image degradation in single image deblurring is modelled as where y is the observed image, x is the original image, and k is the unknown blurring kernel, also known as the point spread function (PSF), and n denotes uncorrelated additive noise. Blind deconvolution is the method of obtaining the original image and, in some cases, the PSF, from the observed image. The problem of blind image deblurring is highly ill-posed and non-convex. Many techniques have been used for deblurring of text-based images. Early on, statistical and learning based methods were prominent for blur kernel estimation. With the emergence of deep learning, using CNN based approaches were proposed as function approximators to predict the deblurred image. Although these methods have proven to give admirable results, they still have certain pitfalls. We assume that the function modelled by the CNN for image restoration is a spatially invariant function, whereas this may not be true, as in the case of dynamic scenes [12] . Also, deconvolution of different types of blur kernels would inevitably increase the model parameters and computational expenses. Hence, model adjustment based on the PSF and the need for spatial variance became necessary. We propose SVDocNet, an end to end trainable spatially variant network based on the well known U-Net encoder-decoder architecture, consisting of recurrent layers in the skip connections between the encoder-decoder blocks. Additionally, we have three auxiliary networks that do not contribute to any intermediary features or outputs, but learn the internal adjustments that must be customized to each image in the form of the primary network's weights to guide the propagation of features. We evaluate the model on benchmark datasets and compare the results with state of the art solutions. We proposed SVDocNet, an end-to-end trainable spatially variant U-Net based architecture for blind document deblurring, replacing the skip connections between the encoder and decoder blocks with alternating convolutional and recurrent layers for efficient feature extraction. Three auxiliary U-Net networks are present to predict suitable weights for the recurrent layers by examining the input blurred image. We demonstrated the potency of this system both quantitatively and qualitatively.","We present SVDocNet, an end-to-end trainable U-Net based spatial recurrent neural network (RNN) for blind document deblurring.",SVDocNet ; U-Net ; RNN ; quotidian lifestyle ; PSF ; CNN ; three,n denotes ; dynamic scenes ; these extracted features ; blind image deblurring ; the model ; the original image ; focusing errors ; three ; state ; the advent,SVDocNet ; U-Net ; RNN ; quotidian lifestyle ; PSF ; CNN ; three,"In the field of document processing and restoration, blind document deblurring is a crucial task, having wide enhancement applications in optical character recognition systems, forensics, etc. supervised and unsupervised learning methods are well suited for this application. However, these extracted features are not suitable for document images. SVDocNet is an end-to-end trainable U-Net based spatial recurrent neural network (RNN) for blind document denlurring where weights of the RNNs are determined by different convolutional neural networks (CNNs). This network achieves state of the art performance in both quantitative measures and qualitative results",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Computations for the softmax function in neural network models are expensive when the number of output classes is large. This can become a significant issue in both training and inference for such models. In this paper, we present Doubly Sparse Softmax (DS-Softmax), Sparse Mixture of Sparse of Sparse Experts, to improve the efficiency for softmax inference. During training, our method learns a two-level class hierarchy by dividing entire output class space into several partially overlapping experts. Each expert is responsible for a learned subset of the output class space and each output class only belongs to a small number of those experts. During inference, our method quickly locates the most probable expert to compute small-scale softmax. Our method is learning-based and requires no knowledge of the output class partition space a priori. We empirically evaluate our method on several real-world tasks and demonstrate that we can achieve significant computation reductions without loss of performance. Deep learning models have demonstrated impressive performance in many classification problems BID15 . In many of these models, the softmax function/layer is commonly used to produce categorical distributions over the output space. Due to its linear complexity, the computation for the softmax layer can become a bottleneck with large output dimensions, such as language modeling BID3 , neural machine translation BID1 and face recognition BID33 . In some models, softmax contributes to more than 95% computation. This becomes more of an issue when the computational resource is limited, like mobile devices BID13 .Many methods have been proposed to reduce softmax complexity for both training and inference phases. For training, the goal is to reduce the training time. Sampling based BID11 and hierarchical based methods BID9 BID25 were introduced. D-Softmax BID6 and Adaptive-Softmax BID10 , construct two levelhierarchies for the output classes based on the unbalanced word distribution for training speedup. The hierarchies used in these methods are either pre-defined or constructed manually, which can be unavailable or sub-optimal. Unlike training , in inference, our goal is not to computing the exact categorical distribution over the whole vocabulary, but rather to search for top-K classes accurately and efficiently. Existing work BID31 BID30 BID37 on this direction focus on designing efficient approximation techniques to find the top-K classes given a trained model. Detailed discussions of related works are to be found in Section 4.Our work aims to improve the inference efficiency of the softmax layer. We propose a novel Doubly Sparse softmax (DS-Softmax) layer. The proposed method is motivated by BID29 , and it learns a two-level overlapping hierarchy using sparse mixture of sparse experts. Each expert is trained to only contain a small subset of entire output class space, while each class is permitted to belong to more than one expert. Given a set of experts and an input vector , the DS-Softmax first selects the top expert that is most related to the input (in contrast to a dense mixture of experts), and then the chosen expert could return a scored list of most probable classes in it sparse subset. This method can reduce the linear complexity in original softmax significantly since it does not need to consider the whole vocabulary.We conduct experiments in different real tasks, ranging from language modeling to neural machine translation. We demonstrate our method can reduce softmax computation dramatically without loss of prediction performance. For example, we achieved more than 23x speedup in language modeling and 15x speedup in translation with similar performances. Qualitatively, we demonstrate learned two-level overlapping hierarchy is semantically meaningful on natural language modeling tasks.2 DS-SOFTMAX: SPARSE MIXTURE OF SPARSE EXPERTS 2.1 BACKGROUND Before introducing our method, we first provide an overview of the background.Hierarchical softmax. Hierarchical softmax uses a tree to organize output space where a path represents a class BID25 . There are a few ways to construct such hierarchies. Previous work BID25 BID6 BID10 focus on building hierarchies with prior knowledge. Other approaches, like BID24 , performed clustering on embeddings to construct a hierarchy. Our work aims to learn a two-level hierarchy while the major difference is that we allow overlapping in the learned hierarchy.Sparsely-gated mixture-of-experts. BID29 designed a sparsely gated mixture of experts model so that outrageously large networks can achieve significantly better performance in language modeling and translation. They borrowed conditional computation idea to keep similar computation even though the number of parameters increases dramatically. Their proposed sparsely-gated Mixture of Experts (MoE) only use a few experts selected by the sparsely gating network for computation on each example. The original MoE cannot speedup softmax computation but serves as an inspiration for our model design.Group lasso. Group lasso has been commonly used to reduce effective features in linear model BID7 BID21 . Recently, it has been applied in a neural network for regularization BID28 and convolutional deep neural network speedup BID36 . It has been demonstrated as an effective method to reduce the number of nodes in the neural network. In this work, we use group lasso to sparsify the experts. In this paper, we present doubly sparse: sparse mixture of sparse experts for efficient softmax inference. Our method is trained end-to-end. It learns a two-level overlapping class hierarchy. Each expert is learned to be only responsible for a small subset of the output class space. During inference, our method first identifies the responsible expert and then perform a small scale softmax computation just for that expert. Our experiments on several real-world tasks have demonstrated the efficacy of our proposed method.","We present doubly sparse softmax, the sparse mixture of sparse of sparse experts, to improve the efficiency for softmax inference through exploiting the two-level overlapping hierarchy.",Sparse Softmax ; Sparse Mixture of Sparse of Sparse Experts ; two ; more than one ; first ; Group ; linear,many classification problems ; the neural network ; several partially overlapping experts ; (DS-Softmax ; an issue ; the inference efficiency ; linear ; a small number ; language modeling ; The original MoE,Sparse Softmax ; Sparse Mixture of Sparse of Sparse Experts ; two ; more than one ; first ; Group ; linear,"The softmax function in neural network models is expensive when the number of output classes is large. This can become a significant issue in both training and inference for such models. In this paper, we present Doubly Sparse Softmax (DS-Softmax), Sparse Mixture of Sparse Of Sparse Experts, to improve the efficiency for softmax inference. In training, each expert responsible for a learned subset of output class space and each output class belongs to a small number of those experts. In inference, the method quickly locates the most probable expert to compute small-scale softmax. Our method is learning-based",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"The lack of crisp mathematical models that capture the structure of real-world
 data sets is a major obstacle to the detailed theoretical understanding of deep
 neural networks. Here, we first demonstrate the effect of structured data sets
 by experimentally comparing the dynamics and the performance of two-layer
 networks trained on two different data sets: (i) an unstructured synthetic data
set containing random i.i.d. inputs, and (ii) a simple canonical data set such
 as MNIST images. Our analysis reveals two phenomena related to the dynamics of
 the networks and their ability to generalise that only appear when training on
 structured data sets. Second, we introduce a generative model for data sets,
 where high-dimensional inputs lie on a lower-dimensional manifold and have
 labels that depend only on their position within this manifold. We call it the
 *hidden manifold model* and we experimentally demonstrate that training
 networks on data sets drawn from this model reproduces both the phenomena seen
 during training on MNIST. A major impediment for understanding the effectiveness of deep neural networks is our lack of mathematical models for the data sets on which neural networks are trained. This lack of tractable models prevents us from analysing the impact of data sets on the training of neural networks and their ability to generalise from examples, which remains an open problem both in statistical learning theory (Vapnik, 2013; Mohri et al., 2012) , and in analysing the average-case behaviour of algorithms in synthetic data models (Seung et al., 1992; Engel & Van den Broeck, 2001; Zdeborová & Krzakala, 2016) . Indeed, most theoretical results on neural networks do not model the structure of the training data, while some works build on a setup where inputs are drawn component-wise i.i.d. from some probability distribution, and labels are either random or given by some random, but fixed function of the inputs. Despite providing valuable insights, these approaches are by construction blind to key structural properties of real-world data sets. Here, we focus on two types of data structure that can both already be illustrated by considering the simple canonical problem of classifying the handwritten digits in the MNIST database using a neural network N ( LeCun & Cortes, 1998) . The input patterns are images with 28 × 28 pixels, so a priori we work in the high-dimensional R 784 . However, the inputs that may be interpreted as handwritten digits, and hence constitute the ""world"" of our problem, span but a lower-dimensional manifold within R 784 which is not easily defined. Its dimension can nevertheless be estimated to be around D ≈ 14 based on the neighbourhoods of inputs in the data set (Grassberger & Procaccia, 1983; Costa & Hero, 2004; Levina & Bickel, 2004; Facco et al., 2017; Spigler et al., 2019) . The intrinsic dimension being lower than the dimension of the input space is a property expected to be common to many real data sets used in machine leanring. We should not consider presenting N with an input that is outside of its world (or maybe we should train it to answer that the ""input is outside of my world"" in such cases). We will call inputs structured if they are concentrated on a lower-dimensional manifold and thus have a lower-dimensional latent representation. The second type of the structure concerns the function of the inputs that is to be learnt, which we will call the learning task. We will consider two models: the teacher task, where the label is obtained as a function of the high-dimensional input; and the latent task, where the label is a function of only the lower-dimensional latent representation of the input. structured inputs inputs that are concentrated on a fixed, lower-dimensional manifold in input space latent representation for a structured input, its coordinates in the lower-dimensional manifold task the function of the inputs to be learnt latent task for structured inputs, labels are given as a function of the latent representation only teacher task for all inputs, labels are obtained from a random, but fixed function of the high-dimensional input without explicit dependence on the latent representation, if it exists MNIST task discriminating odd from even digits in the MNIST database vanilla teacher-student setup Generative model due to Gardner & Derrida (1989) , where data sets consist of component-wise i.i.d. inputs with labels given by a fixed, but random neural network acting directly on the input hidden manifold model (HMF) Generative model introduced in Sec. 4 for data sets consisting of structured inputs (Eq. 6) with latent labels (Eq. 7) Table 1 : Several key concepts used/introduced in this paper. We begin this paper by comparing neural networks trained on two different problems: the MNIST task, where one aims to discriminate odd from even digits in the in the MNIST data set; and the vanilla teacher-student setup, where inputs are drawn as vectors with i.i.d. component from the Gaussian distribution and labels are given by a random, but fixed, neural network acting on the high-dimensional inputs. This model is an example of a teacher task on unstructured inputs. It was introduced by Gardner & Derrida (1989) and has played a major role in theoretical studies of the generalisation ability of neural networks from an average-case perspective, particularly within the framework of statistical mechanics (Seung et al., 1992; Watkin et al., 1993; Engel & Van den Broeck, 2001; Zdeborová & Krzakala, 2016; Advani & Saxe, 2017; Aubin et al., 2018; Barbier et al., 2019; Goldt et al., 2019; Yoshida et al., 2019) , and also in recent statistical learning theory works, e.g. (Ge et al., 2017; Li & Y., 2017; Mei & Montanari, 2019; Arora et al., 2019) . We choose the MNIST data set because it is the simplest widely used example of a structured data set on which neural networks show significantly different behaviour than when trained on synthetic data of the vanilla teacher-student setup. Our reasoning then proceeds in two main steps: 1. We experimentally identify two key differences between networks trained in the vanilla teacherstudent setup and networks trained on the MNIST task (Sec. 3). i) Two identical networks trained on the same MNIST task, but starting from different initial conditions, will achieve the same test error on MNIST images, but they learn globally different functions. Their outputs coincide in those regions of input space where MNIST images tend to lie -the ""world"" of the problem, but differ significantly when tested on Gaussian inputs. In contrast, two networks trained on the teacher task learn the same functions globally to within a small error. ii) In the vanilla teacher-student setup, the test error of a network is stationary during long periods of training before a sudden drop-off. These plateaus are well-known features of this setup (Saad & Solla, 1995; Engel & Van den Broeck, 2001 ), but are not observed when training on the MNIST task nor on other datasets used commonly in machine learning.",We demonstrate how structure in data sets impacts neural networks and introduce a generative model for synthetic data sets that reproduces this impact.,MNIST ; Mohri ; Spigler et al. ; Advani & Saxe ; Watkin ; Zdeborová & Krzakala ; first ; Eq ; Broeck ; Seung et al.,a teacher task ; one ; a structured input ; Arora et ; an unstructured synthetic data ; These plateaus ; this manifold ; Engel & Van den ; the structure ; handwritten digits,MNIST ; Mohri ; Spigler et al. ; Advani & Saxe ; Watkin ; Zdeborová & Krzakala ; first ; Eq ; Broeck ; Seung et al.,"The lack of crisp mathematical models that capture the structure of real-world data sets is a major impediment to the detailed theoretical understanding of deep neural networks. To illustrate the effect of structured data sets on the performance of two-layer neural networks trained on two different data sets: an unstructured synthetic data set containing random i.i.d. inputs, and a simple canonical data set such as MNIST images. Our analysis reveals two phenomena related to the dynamics of the networks and their ability to generalise that only appear when training on structured data set. Firstly, we introduce a generative model for data sets, where high",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Determining the optimal order in which data examples are presented to Deep Neural Networks during training is a non-trivial problem. However, choosing a non-trivial scheduling method may drastically improve convergence. In this paper, we propose a Self-Paced Learning (SPL)-fused Deep Metric Learning (DML) framework, which we call Learning Embeddings for Adaptive Pace (LEAP). Our method parameterizes mini-batches dynamically based on the \textit{easiness} and \textit{true diverseness} of the sample within a salient feature representation space. In LEAP, we train an \textit{embedding} Convolutional Neural Network (CNN) to learn an expressive representation space by adaptive density discrimination using the Magnet Loss. The \textit{student} CNN classifier dynamically selects samples to form a mini-batch based on the \textit{easiness} from cross-entropy losses and \textit{true diverseness} of examples from the representation space sculpted by the \textit{embedding} CNN. We evaluate LEAP using deep CNN architectures for the task of supervised image classification on MNIST, FashionMNIST, CIFAR-10, CIFAR-100, and SVHN. We show that the LEAP framework converges faster with respect to the number of mini-batch updates required to achieve a comparable or better test performance on each of the datasets. The standard method to train Deep Neural Networks (DNNs) is stochastic gradient descent (SGD) which employs backpropagation to compute gradients. It typically relies on fixed-size mini-batches of random samples drawn from a finite dataset. However, the contribution of each sample during model training varies across training iterations and configurations of the model's parameters BID15 . This raises the importance of data scheduling for training DNNs, that is, searching for an optimal ordering of training examples which are presented to the model. Previous studies on Curriculum Learning (Bengio et al., 2009, CL) show that organizing training samples based on the ascending order of difficulty can favour model training. However, in CL, the curriculum remains fixed over the iterations and is determined without any knowledge or introspection of the model's learning. Self-Paced Learning BID14 ) presents a method for dynamically generating a curriculum by biasing samples based on their easiness under the current model parameters. This can lead to a highly imbalanced selection of samples, i.e. very few instances of some classes are chosen, which negatively affects the training process due to overfitting. BID19 propose a simple batch selection strategy based on the loss values of training data for speeding up neural network training. However, their results are limited and the approach is time-consuming, as it achieves high performance on MNIST, but fails on CIFAR-10. Their work reveals that selecting the examples to present to a DNN is non-trivial, yet the strategy of uniformly sampling the training data set is not necessarily the optimal choice. BID12 show that partitioning the data into groups with respect to diversity and easiness in their Self-Paced Learning with Diversity (SPLD) framework, can have substantial effect on training. Rather than constraining the model to limited groups and areas, they propose to spread the sample selection as wide as possible to obtain diverse samples of similar easiness. However, their use of K-Means and Spectral Clustering to partition the data into groups can lead to sub-optimal clustering results when learning non-linear feature representations. Therefore, learning an appropriate metric by which to capture similarity among arbitrary groups of data is of great practical importance. Deep Metric Learning (DML) approaches have recently attracted considerable attention and have been the focus of numerous studies BID1 ; BID23 ). The most common methods are supervised, in which a feature space in which distance corresponds to class similarity is obtained. The Magnet Loss BID21 presents state-of-the-art performance on fine-grained classification tasks. BID26 show that it achieves state-of-the-art on clustering and retrieval tasks. This paper makes two key contributions toward scheduling data examples in the mini-batch setting:• We propose a general sample selection framework called Learning Embeddings for Adap- tive Pace (LEAP) that is independent of model architecture or objective, and learns when to introduce certain samples to the DNN during training.• To our knowledge, we are the first to leverage metric learning to improve self-paced learning. We exploit a new type of knowledge -similar instance-level samples are discovered through an embedding network trained by DML in concert with the self-paced learner.2 RELEVANT WORK An important finding is that fusing a salient non-linear representation space with a dynamic learning strategy can help a DNN converge towards an optimal solution. A random curriculum or a dynamic learning strategy without a good representation space was found to achieve a lower test accuracy or converge more slowly than LEAP. Biasing samples based on the easiness and true diverseness to select mini-batches shows improvement in convergence to achieve classification performance comparable or better than the baselines, Random and SPLD. As shown in TAB2 , the student CNN models show increased accuracy on MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100 and SVHN with our LEAP sampling method. It is to be noted that the LEAP framework improves the performance of complex convolutional architectures which already leverage regularization techniques such as batch normalization, dropout, and data augmentation. We see that the improvements on coarse-grain datasets such as MNIST, Fashion-MNIST, CIFAR-10, and SVHN are between 0.11 and 0.81 percentage points. On a fine-grained dataset like CIFAR-100, it is more challenging to obtain a high classification accuracy. This is because there are a 100 fine-grained classes but the number of training instances for each class is small. We have only 500 training images and 100 testing images per class. In addition, the dataset contains images of low quality and images where only part of the object is visible (i.e. for a person, only head or only body). However, we show that with LEAP, we can attain a significant increase in accuracy by 4.50 and 3.72 percentage points over the baselines SPLD and Random, respectively. The mix of easy and diverse samples from a more accurate representation space of the data helps select appropriate samples during different stages of training and guide the network to achieve a higher classification accuracy, especially for more difficult fine-grained classifcation tasks. Experimental results across all datasets (MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100, and SVHN) and sampling methods (LEAP, SPLD, and Random). The test accuracy (%) results are averaged over five runs, with the exception of CIFAR-100 and SVHN which had four runs each. ""*"" indicates that no data augmentation scheme was applied on the dataset.In cases where the classification dataset is balanced and the classes are clearly identifiable, we showed that our end-to-end LEAP training protocol is practical. An interesting line of work would be to apply LEAP on more complex real-world classification datasets such as iNaturalist BID8 , where there are imbalanced classes with a lot of diversity and require fine-grained visual recognition. Another interesting area of application would be learning representations using DML for different computer vision tasks (e.g. human pose estimation, human activity recognition, semantic segmentation, etc.) and fusing a representative SPL strategy to train the student CNN. We introduced LEAP, an end-to-end representation learning SPL strategy for adaptive mini-batch formation. Our method uses an embedding CNN for learning an expressive representation space through a DML technique called the Magnet Loss. The student CNN is a classifier which can exploit this new knowledge from the representation space to place the true diverseness and easiness as sample importance priors during online mini-batch selection. The computational overhead of training two CNNs can be mitigated by training the embedding CNN and student CNN in parallel. LEAP achieves good convergence speed and higher test performance on MNIST, FashionMNIST, CIFAR-10, CIFAR-100 and SVHN using a combination of two deep CNN architectures. We hope this will help foster progress of end-to-end SPL fused DML strategies for DNN training, where a number of potentially interesting directions can be considered for further exploration. Our framework is implemented in PyTorch and will be released as open-source on GitHub following the review process.",LEAP combines the strength of adaptive sampling with that of mini-batch online learning and adaptive representation learning to formulate a representative self-paced strategy in an end-to-end DNN training protocol.,LEAP ; Pace ; DML ; Deep Metric Learning ; CNN ; Curriculum Learning ; Deep Neural Networks ; Learning Embeddings for ; CL ; Learning Embeddings,groups ; a salient non-linear representation space ; batches ; sub-optimal clustering results ; The test accuracy (%) results ; the examples ; the ascending order ; Bengio ; MNIST ; stochastic gradient descent,LEAP ; Pace ; DML ; Deep Metric Learning ; CNN ; Curriculum Learning ; Deep Neural Networks ; Learning Embeddings for ; CL ; Learning Embeddings,"Determining the optimal order in which data examples are presented to Deep Neural Networks during training is a non-trivial problem. However, choosing a nontrivial scheduling method may drastically improve convergence. In this paper, we propose a Self-Paced Learning (SPL)-fused Deep Metric Learning (DML) framework, which we call Learning Embeddings for Adaptive Pace (LEAP). The method parameterizes mini-batches dynamically based on the \textit{easiness} and \textits{true diverseness} of the sample within a salient feature representation space. In LEAP,",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Machine learning algorithms designed to characterize, monitor, and intervene on human health (ML4H) are expected to perform safely and reliably when operating at scale, potentially outside strict human supervision. This requirement warrants a stricter attention to issues of reproducibility than other fields of machine learning. In this work, we conduct a systematic evaluation of over 100 recently published ML4H research papers along several dimensions related to reproducibility we identified. We find that the field of ML4H compares poorly to more established machine learning fields, particularly concerning data accessibility and code accessibility.   Finally, drawing from success in other fields of science, we propose recommendations to data providers, academic publishers, and the ML4H research community in order to promote reproducible research moving forward. Science requires reproducibility, but many sub-fields of science have recently experienced a reproducibility crisis, eroding trust in processes and results and potentially influencing the rising rates of scientific retractions [1, BID4 BID43 . Reproducibility is also critical for machine learning research, whose goal is to develop algorithms to reliably solve complex tasks at scale, with limited or no human supervision. Failure of a machine learning system to consistently replicate an intended behavior in a context different from which that behavior was defined may result in dramatic, even fatal, consequences BID26 . Ranking prominently among machine learning applications that may put human lives at stake are those related to Machine Learning for Health (ML4H). In a field where applications are meant to directly affect human health, findings should undergo heavy scrutiny along the validation pipeline from research findings to applications deployed in the wild. For example, in 2018, 12 AI tools using ML4H algorithms to inform medical diagnosis and treatment were cleared by Food and Drug Administration (FDA) and will be marketed to and potentially used by millions of Americans BID30 . Verifying the reproducibility of the claims put forward by the device manufacturer should thus be a main priority of regulatory bodies BID35 , extending the need for reproducible ML4H results beyond the machine learning research community.Unfortunately, several factors relating to the availability, quality, and consistency of clinical or biomedical data make reproducibility especially challenging in ML4H applications. In this work, * Equal Contribution we make several contributions. First, we present a taxonomy of reproducibility tailored to ML4H applications, and designed to capture reproducibility goals more broadly. Second, we use this taxonomy to define several metrics geared towards quantifying the particular challenges in reproducibility faced within ML4H, and conduct a comprehensive review of the published literature to support our claims and compare ML4H to machine learning more generally. Finally, we build on this analysis by exploring promising areas of further research for reproducibility in ML4H. In this work, we have framed the question of reproducibility in ML4H around three foundational lenses: technical, statistical, and conceptual replicability. In each of these areas, we argue both qualitatively and quantitatively, through a manual, extensive review of the literature, that ML4H performs worse than other machine learning fields in several reproducibility metrics we have identified. While keeping in mind the intrinsic challenges of data acquisition and use that plague the field, we highlight several areas of opportunities for the future, focused around improving access to data, expanding our trajectory of statistical rigor, and increasing the use of multi-source data to better enable conceptual reproducibility. * indicates all publiclyaccessible papers published were used.Potential Biases This selection and annotation procedure allowed us to analyze a large number of papers, but has several possible biases. In particular, our annotation questions were all of these were designed to be determinable via quick, scanning techniques and as a result this task took on average between 45 seconds and 3 minutes per paper. In such a limited time, some losses are unavoidable. We recognize several sources of possible bias worth mentioning.Firstly, some papers may, for example, release datasets or code products external to the paper and not mention it in the actual text. We will omit these associated products. If such effects induce a notable bias in our results, however, we must question why as a field we are comfortable releasing our code/data without any mention in the associated paper.Secondly, not all papers intended to be analyzed were publicly accessible. Similarly, the versions of papers we analyzed could have been different from the version presented at the actual conference venue, or there could exist updated versions of papers we analyzed in different repositories. Our analysis technique will miss these effects.Thirdly, some papers naturally fit into multiple categories (e.g., a work focused on medical named entity recognition would be both a ML4H work and an NLP work). In the interest of ensuring our comparison classes were as pure as possible, we omitted all clearly multi-domain works, but allowed works that centered primarily in a single domain to remain.Lastly, different fields present different kinds of works, and not all works fit into our framework. Largely theoretical works, for example, often have no real datasets or public experiments. Similarly, presenting variance is a different question for works focused principally around computational efficiency rather than predictive accuracy. We handled these issues by attempting to answer these questions as best we could, and flagging any papers that overtly did not fit our scheme and excluding them from our analyses.","By analyzing more than 300 papers in recent machine learning conferences, we found that Machine Learning for Health (ML4H) applications lag behind other machine learning fields in terms of reproducibility metrics.",NLP ; Secondly ; Equal Contribution ; First ; millions ; three ; AI ; FDA ; Second ; Firstly,these issues ; works ; whose goal ; reproducibility goals ; code accessibility ; First ; academic publishers ; access ; different repositories ; Health,NLP ; Secondly ; Equal Contribution ; First ; millions ; three ; AI ; FDA ; Second ; Firstly,"Machine learning algorithms designed to monitor, monitor, and intervene on human health (ML4H) are expected to perform safely and reliably when operating at scale, potentially outside strict human supervision. This requirement warrants stricter attention to issues of reproducibility than other fields of machine learning. In this work, we examine over 100 recently published ML4H research papers along several dimensions, including data accessibility and code accessibility, and propose recommendations to data providers, academic publishers, and the ML4 H research community to promote reproducible research moving forward.    These recommendations are based on success in other fields, such as computer science and medicine.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"In an explanation generation problem, an agent needs to identify and explain the reasons for its decisions to another agent. Existing work in this area is mostly confined to planning-based systems that use automated planning approaches to solve the problem. In this paper, we approach this problem from a new perspective, where we propose a general logic-based framework for explanation generation. In particular, given a knowledge base $KB_1$ that entails a formula $\phi$ and a second knowledge base $KB_2$ that does not entail $\phi$, we seek to identify an explanation $\epsilon$ that is a subset of $KB_1$ such that the union of $KB_2$ and $\epsilon$ entails $\phi$. We define two types of explanations, model- and proof-theoretic explanations, and use cost functions to reflect preferences between explanations. Further, we present our algorithm implemented for propositional logic that compute such explanations and empirically evaluate it in random knowledge bases and a planning domain. With increasing proliferation and integration of AI systems in our daily life, there is a surge of interest in explainable AI, which includes the development of AI systems whose actions can be easily understood by humans. Driven by this goal, machine learning (ML) researchers have begun to classify commonly used ML algorithms according to different dimensions of explainability (Guidotti et al. 2018) ; improved the explainability of existing ML algorithms BID3 BID0 BID3 ; as well as proposed new ML algorithms that trade off accuracy for increasing explainability (Dong et al. 2017; BID1 . 1 While the term interpretability is more commonly used in the ML literature and can be used interchangeably with explainability, we use the latter term as it is more commonly used broadly across different subareas of AI.In contrast, researchers in the automated planning community have mostly taken a complementary approach. While there is some work on adapting planning algorithms to find easily explainable plans 2 (i.e., plans that are easily understood and accepted by a human user) BID0 , most work has focused on the explanation generation problem (i.e., the problem of identifying explanations of plans found by planning agents that when presented to users, will allow them to understand and accept the proposed plan) (Langley 2016; Kambhampati 1990) . Within this context, researchers have tackled the problem where the model of the human user may be (1) inconsistent with the model of the planning agent (Chakraborti et al. 2017b) ; (2) must be learned BID0 ; and (3) a different form or abstraction than that of the planning agent BID0 Tian et al. 2016) . However, a common thread across most of these works is that they, not surprisingly, employ mostly automated planning approaches. For example, they often assume that the models of both the agent and human are encoded in PDDL format.In this paper, we approach the explanation generation problem from a different perspective -one based on knowledge representation and reasoning (KR). We propose a general logic-based framework for explanation generation, where given a knowledge base KB 1 (of an agent) that entails a formula φ and a knowledge base KB 2 (of a human user) that does not entail φ, the goal is to identify an explanation ⊆ KB 1 such that KB 2 ∪ entails φ. We define two types of explanations, model-and proof-theoretic explanations, and use cost functions to reflect preferences between explanations. Further, we present an algorithm, implemented for propositional logic, that computes such explanations and evaluate its performance experimentally in random knowledge bases as well as in a planning domain.In addition to providing an alternative approach to solve the same explanation generation problem tackled thus far by the automated planning community, our approach has the merit of being more generalizable to other problems beyond planning problems as long as they can be modeled using a logical KR language. There is a very large body of work related to the very broad area of explainable AI. We have briefly discussed some of them from the ML literature in Section . We refer readers to surveys by BID0 and (Dosilovic et al. 2018 ) for more in-depth discussions of this area. We focus below on related work from the KR and planning literature only since we employ KR techniques to solve explainable planning problems in this paper.Related Work from the KR Literature: We note that the notion of an explanation proposed in this paper might appear similar to the notion of a diagnosis that has been studied extensively in the last several decades (e.g., (Reiter 1987)) as both aim at explaining something to an agent. Diagnosis focuses on identifying the reason for the inconsistency of a theory whereas an mor p-explanation aims at identifying the support for a formula. The difference lies in that a diagnosis is made with respect to the same theory and m-or p-explanation is sought for the second theory.Another earlier research direction that is closely related to the proposed notion of explanation is that of developing explanation capabilities of knowledge-based systems and decision support systems, which resulted in different notions of explanation such as trace, strategic, deep, or reasoning explanations (see review by BID3 for a discussion of these notions). All of these types of explanations focus on answering why certain rules in a knowledge base are used and how a conclusion is derived. This is not our focus in this paper. The present development differs from earlier proposals in that m-or p-explanations are identified with the aim of explaining a given formula to a second theory. Furthermore, the notion of an optimal explanation with respect to the second theory is proposed.There have been attempts to using argumentation for explanation (Cyras et al. 2017; Cyras et al. 2019) because of the close relation between argumentation and explanation. For example, argumentation was used by (Cyras et al. 2019) to answer questions such as why a schedule does (does not) satisfy a criteria (e.g., feasibility, efficiency, etc.); the approach was to develop for each type of inquiry, an abstract argumentation framework (AF) that helps explain the situation by extracting the attacks (non-attacks) from the corresponding AF. Our work differs from these works in that it is more general and does not focus on a specific question.It is worth to pointing out that the problem of computing a most preferred explanation for ϕ from KB 1 to KB 2 might look similar to the problem of computing a weakest sufficient condition of ϕ on KB 1 under KB 2 as described by BID3 . As it turns out, the two notions are quite different. Given that KB 1 = {p, q} and KB 2 = {p}. It is easy to see that q is the unique explanation for q from KB 1 to KB 2 . On the other hand, the weakest sufficient condition of q on KB 1 under KB 2 is ⊥ (Proposition 8, BID3 ).Related Work from the Planning Literature: In human-aware planning, the (planning) agent must have knowledge of the human model in order to be able to contemplate the goals of the humans as well as foresee how its plan will be perceived by them. This is of the highest importance in the context of explainable planning since an explanation of a plan cannot be onesided (i.e., it must incorporate the human's beliefs of the planner). In a plan generation process, a planner performs argumentation over a set of different models (Chakraborti et al. 2017a ); these models usually are the model of the agent incorporating the planner, the model of the human in the loop, the model the agent thinks the human has, the model the human thinks the agent has, and the agent's approximation of the latter.Therefore, the necessity for plan explanations arises when the model of the agent and the model the human thinks the agent has diverge so that the optimal plans in the agent's model are inexplicable to the human. During a collaborative activity, an explainable planning agent BID1 ) must be able to account for such model differences and maintain an explanatory dialogue with the human so that both of them agree on the same plan. This forms the nucleus of explanation generation of an explainable planning agent, and is referred to as model reconciliation (Chakraborti et al. 2017b) . In this approach , the agent computes the optimal plan in terms of his model and provides an explanation of that plan in terms of model differences. Essentially, these explanations can be viewed as the agent's attempt to move the human's model to be in agreement with its own. Further, for computing explanations using this approach the following four requirements are considered:• Completeness -No better solution exists. This is achieved by enforcing that the plan being explained is optimal in the updated human model.• Conciseness -Explanations should be easily understandable to the human.• Monotonicity -The remaining model differences cannot change the completeness of an explanation.• Computability -Explanations should be easy to compute (from the agent's perspective).As our work is motivated by these ideas , we now identify some similarities and connections with our proposed approach. First, it is easy to see that we implicitly enforce the first three requirements when computing an explanation -the notions of completeness and conciseness are captured through the use of our cost functions. We do not claim to satisfy the computability requirement as it is more subjective and is more domain dependent.In a nutshell, the model reconciliation approach works by providing a model update such that the optimal plan is feasible and optimal in the updated model of the human. This is similar to our definition of the explanation generation problem where we want to identify an explanation ⊆ KB 1 (i.e., a set of formulae) such that KB 2 ∪ |= φ. In addition, the ⊆-minimal support in Definition 1 is equivalent to minimally complete explanations (MCEs) (the shortest explanation). The -general support can be viewed as similar to the minimally monotonic explanations (MMEs) (the shortest explanation such that no further model updates invalidate it), with the only difference being that in the general support scenario, the explanations are such that all subsuming are also valid supports.In contrast, model patch explanations (MPEs) (includes all the model updates) are trivial explanations and are equivalent to our definition that KB 1 itself serves as an m-explanation for KB 2 . Note that, in our approach, we do not allow for explanations on ""mistaken"" expectations in the human model, as it can be inferred from Proposition 1 (monotonic language L). From the model reconciliation perspective, such restriction is relaxed and allowed. However, a similar property can be seen if the mental model is not known and, therefore, by taking an ""empty"" model as starting point explanations can only add to the human's understanding but not mend mistaken ones. Explanation generation is an important problem within the larger explainable AI thrust. Existing work on this problem has been done in the context of automated planning domains, where researchers have primarily employed, unsurprisingly, automated planning approaches. In this paper, we approach the problem from the perspective of KR, where we propose a general logic-based framework for explanation generation. We further define two types of explanations, model-and proof-theoretic explanations, and use cost functions to reflect preferences between explanations. Our empirical results with algorithms implemented for propositional logic on both random knowledge bases as well as a planning domain demonstrate the generality of our approach beyond planning problems. Future work includes investigating more complex scenarios, such as one where an agent needs to persuade another that its knowledge base is incorrect.",A general framework for explanation generation using Logic.,Kambhampati ; an explanation $\epsilon$ ; $ ; the Planning Literature: In ; \phi$. ; ML ; First ; model- ; two ; KR,"these types ; a general logic-based framework ; KB ; an ""empty"" model ; random knowledge bases ; an explanation ⊆ KB ; human ; (i.e., plans ; the development ; a plan",Kambhampati ; an explanation $\epsilon$ ; $ ; the Planning Literature: In ; \phi$. ; ML ; First ; model- ; two ; KR,"In an explanation generation problem, an agent needs to identify and explain its decisions to another agent. Existing work in this area is confined to planning-based systems that use automated planning approaches to solve the problem. In this paper, we propose a general logic-based framework for explanation generation. In particular, given a knowledge base $KB_1$ that entails a formula $\phi and a second knowledge base (KB_2$ that does not entail $\phi), we seek to identify an explanation $\epsilon$ that is a subset of $KB-1$ such that the union of $\KB_3$ and $\",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Neural networks trained only to optimize for training accuracy can often be fooled by adversarial examples --- slightly perturbed inputs misclassified with high confidence. Verification of networks enables us to gauge their vulnerability to such adversarial examples. We formulate verification of piecewise-linear neural networks as a mixed integer program. On a representative task of finding minimum adversarial distortions, our verifier is two to three orders of magnitude quicker than the state-of-the-art. We achieve this computational speedup via tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available. The computational speedup allows us to verify properties on convolutional and residual networks with over 100,000 ReLUs --- several orders of magnitude more than networks previously verified by any complete verifier. In particular, we determine for the first time the exact adversarial accuracy of an MNIST classifier to perturbations with bounded l-∞ norm ε=0.1: for this classifier, we find an adversarial example for 4.38% of samples, and a certificate of robustness to norm-bounded perturbations for the remainder. Across all robust training procedures and network architectures considered, and for both the MNIST and CIFAR-10 datasets, we are able to certify more samples than the state-of-the-art and find more adversarial examples than a strong first-order attack. Neural networks trained only to optimize for training accuracy have been shown to be vulnerable to adversarial examples: perturbed inputs that are very similar to some regular input but for which the output is radically different BID14 . There is now a large body of work proposing defense methods to produce classifiers that are more robust to adversarial examples. However, as long as a defense is evaluated only via heuristic attacks (such as the Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2015) or BID6 's attack (CW)), we have no guarantee that the defense actually increases the robustness of the classifier produced. Defense methods thought to be successful when published have often later been found to be vulnerable to a new class of attacks. For instance, multiple defense methods are defeated in BID5 by constructing defense-specific loss functions and in BID0 by overcoming obfuscated gradients.Fortunately, we can evaluate robustness to adversarial examples in a principled fashion. One option is to determine (for each test input) the minimum distance to the closest adversarial example, which we call the minimum adversarial distortion BID7 . Alternatively, we can determine the adversarial test accuracy BID1 , which is the proportion of the test set for which no perturbation in some bounded class causes a misclassification. An increase in the mean minimum adversarial distortion or in the adversarial test accuracy indicates an improvement in robustness. 1 We present an efficient implementation of a mixed-integer linear programming (MILP) verifier for properties of piecewise-linear feed-forward neural networks. Our tight formulation for nonlinearities and our novel presolve algorithm combine to minimize the number of binary variables in the MILP problem and dramatically improve its numerical conditioning. Optimizations in our MILP implementation improve performance by several orders of magnitude when compared to a naïve MILP implementation, and we are two to three orders of magnitude faster than the state-of-the-art Satisfiability Modulo Theories (SMT) based verifier, Reluplex BID7 We make the following key contributions:• We demonstrate that, despite considering the full combinatorial nature of the network, our verifier can succeed at evaluating the robustness of larger neural networks, including those with convolutional and residual layers.• We identify why we can succeed on larger neural networks with hundreds of thousands of units. First , a large fraction of the ReLUs can be shown to be either always active or always inactive over the bounded input domain. Second , since the predicted label is determined by the unit in the final layer with the maximum activation, proving that a unit never has the maximum activation over all bounded perturbations eliminates it from consideration. We exploit both phenomena, reducing the overall number of non-linearities considered.• We determine for the first time the exact adversarial accuracy for MNIST classifiers to perturbations with bounded l ∞ norm . We are also able to certify more samples than the state-of-the-art and find more adversarial examples across MNIST and CIFAR-10 classifiers with different architectures trained with a variety of robust training procedures.Our code is available at https://github.com/vtjeng/MIPVerify.jl. This paper presents an efficient complete verifier for piecewise-linear neural networks.While we have focused on evaluating networks on the class of perturbations they are designed to be robust to, defining a class of perturbations that generates images perceptually similar to the original remains an important direction of research. Our verifier is able to handle new classes of perturbations (such as convolutions applied to the original image) as long as the set of perturbed images is a union of polytopes in the input space.We close with ideas on improving verification of neural networks. First, our improvements can be combined with other optimizations in solving MILPs. For example, BID4 DISPLAYFORM0 We consider two cases.Recall that a is the indicator variable a = 1 x≥0 .When a = 0, the constraints in Equation FORMULA0 This formulation for rectified linearities is sharp BID15 if we have no further information about x. This is the case since relaxing the integrality constraint on a leads to (x, y) being restricted to an area that is the convex hull of y = max(x, 0). However , if x is an affine expression x = w T z + b, the formulation is no longer sharp, and we can add more constraints using bounds we have on z to improve the problem formulation.","We efficiently verify the robustness of deep neural models with over 100,000 ReLUs, certifying more samples than the state-of-the-art and finding more adversarial examples than a strong first-order attack.",two ; Goodfellow et al. ; two to three ; SMT ; hundreds of thousands ; FGSM ; Second ; One ; the Fast Gradient Sign Method ; first,bounds ; rectified linearities ; an MNIST classifier ; the closest adversarial example ; the-art ; MILP ; a novel presolve ; the full combinatorial nature ; the input space ; inputs,two ; Goodfellow et al. ; two to three ; SMT ; hundreds of thousands ; FGSM ; Second ; One ; the Fast Gradient Sign Method ; first,"Neural networks trained to optimize for training accuracy can often be fooled by adversarial examples. Verification of networks enables us to gauge their vulnerability to such examples. We formulate verification of piecewise-linear neural networks as a mixed integer program. On a representative task of finding minimum adversarial distortions, our verifier is two to three orders of magnitude quicker than the state-of-the-art. We achieve this computational speedup via tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available. The computational speed up allows us to verify properties on convolutional and",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Reinforcement learning and evolutionary algorithms can be used to create sophisticated control solutions. Unfortunately explaining how these solutions work can be difficult to due to their ""black box"" nature. In addition, the time-extended nature of control algorithms often prevent direct applications of explainability techniques used for standard supervised learning algorithms. This paper attempts to address explainability of blackbox control algorithms through six different techniques: 1) Bayesian rule lists, 2) Function analysis, 3) Single time step integrated gradients, 4) Grammar-based decision trees, 5) Sensitivity analysis combined with temporal modeling with LSTMs, and 6) Explanation templates. These techniques are tested on a simple 2d domain, where a simulated rover attempts to navigate through obstacles to reach a goal. For control, this rover uses an evolved multi-layer perception that maps an 8d field of obstacle and goal sensors to an action determining where it should go in the next time step. Results show that some simple insights in explaining the neural network are possible, but that good explanations are difficult. Explanation of machine learning algorithms is a challenging and important field of research. Most techniques to date have focused on supervised learning algorithms, such as image processing, text processing and medical diagnosis BID10 BID5 . Instead of supervised learning, this paper focuses on reward based machine learning such as reinforcement learning and evolutionary algorithms, where rewards are given to measure performance instead of using examples of what is correct. The nature of reward learning and supervised learning is different in both problem domains and learning tools used to solve these problems. In this paper we look at explainability techniques that have been designed for supervised learning problems and apply them to reward learning problems. Reinforcement learning and evolutionary algorithms can be used to automatically learn high performance control systems for complex problems BID4 BID3 BID1 . This is particularly the case in the context of autonomy where control may involve many variables and need to dynamically adapt to different environments and situations.A common form of machine learning is to train a set of weights of a neural network-based control policy. Based on inputs (such as sensors) the control policy can command control actions (such as speed and direction of a vehicle). Training is typically done with a simulator, where the learning algorithm attempts to improve the performance of the control policy through a long series of trials. The goal of this training process is to produce a high-performance non-linear control policy that takes inputs and produces controls.While a successful training will produce a control policy that achieves high performance in simulation, how the control policy actually works will typically be unclear to its programmers, let alone its end-users. Due to this fact, machine learning algorithms are often referred to as ""blackbox"": their inputs and outputs can be viewed, but there is no knowledge of their internal workings.Even when machine learning achieves high performance, it can be difficult to trust for two reasons: 1) coverage, and 2) generalizability. In terms of coverage, while an algorithm may have performed well in scenarios that were tested, there may be other likely scenarios where it would have performed very poorly. In addition since coverage of machine learning algorithms is largely dependent on the data set, the user may not even be aware of the algorithm's coverage and can easily overlook large gaps in the data sets. In terms of generalizability, while the algorithm performed well in the simulator it may not perform well in the real world or in environments that are slightly different than the simulated one. These problems can be exacerbated by the blackbox nature of these learning algorithms, where reward hacking, poorly defined utility functions or simple errors in the simulator can lead to unrealistically high levels of performance that cannot be achieved when deployed. In addition, machine learning algorithms have many unintuitive parameters that have no obvious relation to the underlying control problem, such as number of hidden nodes and learning rates. Yet poor choices of these parameters can lead to poor generalization.Improving explainability of these blackbox algorithms can help improve trust that they will behave as expected when deployed (Gunning ) . If a control decision is backed up by a meaningful and understandable rationale, then one can trust that the decision is not made ""by chance"", and therefore the system can be expected to behave well in other similar circumstances. Additionally, if we understand a learned control algorithm, we can see if there are any clear gaps in coverage, or if there are any obvious flaws that would prevent it from generalizing outside of the simulated environment. On the other hand, what constitutes a meaningful, understandable explanation?Providing explanations of machine learning is a very active research field. Several approaches have been proposed for standard supervised learning algorithms. Despite this fact, it is still unclear what types of explanations may be suitable in practice. Control further complicates the picture, because control strategies develop over time, and are typically not evaluated over snapshots. How can such strategies be captured in explanations and what type of explanations would those be?To address this problem, we have experimented with a variety of techniques to provide explanations in the context of a very simple machine learning algorithm that we developed for navigating a rover towards a goal while avoiding obstacles. We decided to build the algorithm from scratch in order to evaluate the pitfalls and errors that may occur in developing such systems, as well as how/what explanations may assist in detecting those. We used six techniques in order to develop explanations: 1) Bayesian rule lists, 2) Function analysis, 3) Single time step integrated gradients , 4) Grammarbased decision trees, 5) Sensitivity analysis combined with temporal modeling with LSTMs, and 6) Explanation templates. This set of techniques was chosen as it represents a diverse set of explanations that could be readily applied to control data. In particular, it includes both local and global explanations . These local attempt to explain a single control action in a particular state. To form a big picture of a control policy with local explanations , we would want many local explanations covering many different states. In contrast global explanations try to explain an overall action policy over all states.The remainder of the paper is organized as follows. We first present the example obstacle avoidance problem we use throughout the paper. Then we describe the neural network controller and the Monte Carlo algorithm used to determine the weights of the neural network. We subsequently discuss the need for explainability and how simple analysis of algorithm performance may be insufficient. To address this we present six different explainability algorithms applied to the example problem and discuss their relative merits. While several explanation algorithms have been successfully used on supervised learning problems, direct application to reward based controls learning is somewhat illusive. A large part of this is due to the time-extended property of control policies. An action taken at a particular time step may seem sub-optimal at that particular time step but has benefits for future time steps. This limits a lot of direct application of supervised learning explanation as these explanations will tend to explain the superficial benefit of the action for the immediate time step and will likely miss the explanations of the future benefits. Our use of grammar-Based decision trees and temporal modeling attempt to address this issue, but they also lead to another problem: Control policies that need to optimize for future time steps are performing operations that are inherently complex and are difficult to summarize with simple explanations. In our test-domain the explanation algorithms are able to expose a major flaw in the operation of our learned neural network controller. However, it seems unlikely that they would be able to reveal more subtle issues or would be able to scale to more complex learned controllers. In addition the explanations do not seem as convincing or as useful as the explanations the same algorithms provide for their original supervised learning domain. Explaining a control algorithm based on machine learning is difficult due to the black-box nature of machine learning algorithms and the time-extended properties of control problems. In this paper we attempt to explain such a controller used on a simple obstacle avoidance problem: a neural network trained using a Monte Carlo algorithm. We do this by applying a number of explainability algorithms to this problem. These algorithms look at the inputs and outputs of the controller and based on these values attempt to explain what the controller is trying to do. The explanation algorithms proved useful in revealing a potential hazard in the controller, where it tries to head towards an obstacle and then turn to avoid it. However beyond this flaw it was difficult to gain deep insights into these explanations.",Describes a series of explainability techniques applied to a simple neural network controller used for navigation.,six ; Bayesian ; Grammar ; two ; first ; Monte Carlo,the blackbox nature ; what explanations ; Explanation ; poor generalization ; another problem ; generalizability ; controls ; the time-extended nature ; supervised learning problems ; different environments,six ; Bayesian ; Grammar ; two ; first ; Monte Carlo,"Reinforcement learning and evolutionary algorithms can be used to create sophisticated control solutions. However, time-extended control algorithms often prevent direct applications of explainability techniques used for standard supervised learning algorithms. This paper addresses explainability of blackbox control algorithms through six different techniques: Bayesian rule lists, Function analysis, 3) Single time step integrated gradients, 4) Sensitivity analysis combined with temporal modeling with LSTMs, and 6) Explanation templates. These techniques are tested on a simple 2d domain, where a simulated rover attempts to navigate through obstacles to reach a goal. For control, this rover uses an evolved multi",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Answering questions about data can require understanding what parts of an input X influence the response Y. Finding such an understanding can be built by testing relationships between variables through a machine learning model. For example, conditional randomization tests help determine whether a variable relates to the response given the rest of the variables. However, randomization tests require users to specify test statistics. We formalize a class of proper test statistics that are guaranteed to select a feature when it provides information about the response even when the rest of the features are known. We show that f-divergences provide a broad class of proper test statistics. In the class of f-divergences, the KL-divergence yields an easy-to-compute proper test statistic that relates to the AMI. Questions of feature importance can be asked at the level of an individual sample.   We show that estimators from the same AMI test can also be used to find important features in a particular instance. We provide an example to show that perfect predictive models are insufficient for instance-wise feature selection. We evaluate our method on several simulation experiments, on a genomic dataset, a clinical dataset for hospital readmission, and on a subset of classes in ImageNet. Our method outperforms several baselines in various simulated datasets, is able to identify biologically significant genes, can select the most important predictors of a hospital readmission event, and is able to identify distinguishing features in an image-classification task. Model interpretation techniques aim to select features important for a response by reducing models (sometimes locally) to be human interpretable. However, the phrase model interpretation can be a bit of a misnomer. Any interpretation of a model must be imbued to the model by the population distribution that provides the data to train the model. In this sense, interpreting a model should be viewed as understanding the population distribution of data through the lens of a model. Existing methods for understanding the population distributions only work with particular models fit to the population, particular choices of test statistic, or particular auxiliary models for interpretation (Ribeiro et al., 2016; Lundberg and Lee, 2017) . Such structural restrictions limit the applicability of these methods to a smaller class of population distributions. To be able to work in a black-box manner, feature selection methods can use models but must not require a particular structure in models used in selection processes. Understanding the population distribution can be phrased as assessing whether a response is independent of a feature given the rest of the features; this test is called a conditional randomization test (Candes et al., 2018) . Conditional randomization tests require test statistics. Test statistics like linear model coefficients (Barber et al., 2015) or correlation may miss dependence between the response and outcome. To avoid missing relationships between variables, we develop the notion of a proper test statistic. Proper test statistics are those whose power increases to one as the amount of data increases. Conditional independence implies the conditional-joint factorizes into conditionalmarginals. Measuring the divergence between these distributions yields a proper test statistic. Of the class of integral probability metrics (Müller, 1997) and f -divergences (Csiszár, 1964) , the KLdivergence simplifies estimation and allows for reuse of the model structures and code from the standard task of predicting the response from the features. Using the KL-divergence in this context has a natural interpretation; it is a measure of the additional information each feature provides about the outcome over the rest. This measure of information is known as the additional mutual information (AMI) . Our proposed procedure is called the additional mutual information conditional randomization test (AMI-CRT). AMI-CRT uses regressions to simulate data from the null for each feature and compares the additional mutual information (AMI) of the original data to the AMI of the simulations from Beyond understanding the population distribution, some tasks require interpreting a population distribution on the level of an individual datapoint. Methods that test for conditional independence work under distributional notions of feature selection, but are not designed to identify the relevant features for a particular sample. To address this issue of ""instance-wise feature selection,"" several methods have been proposed, including local perturbations (Simonyan et al., 2013; Sundararajan et al., 2017; Ribeiro et al., 2016) and fitting simpler auxiliary models to explain the predictions of a large model (Chen et al., 2018; Lundberg and Lee, 2017; Yoon et al., 2019; Turner, 2016; Štrumbelj and Kononenko, 2014; Shrikumar et al., 2017) . Our instance-wise work is most similar to that of Burns et al. (2019) , who repurpose the HRT framework to perform instance-wise feature selection, or Gimenez and Zou (2019) , who define a conditional randomization test (CRT) procedure for subsets of the feature space. In general, however, the conditions under which instance-wise feature selection with predictive models may be possible are not well developed. We address this issue by first identifying a set of sufficient conditions under which instance-wise feature selection is always possible. We then show how estimators used in AMI-CRT can be repurposed for use in an instance-wise setting, yielding a procedure called the AMI-IW. We develop AMI-CRT for testing for conditional independence of each feature x j ⊥ y | x −j from a finite sample from the population distribution. AMI-CRT uses the KL-divergence to cast independence testing as regression and allows for the reuse of code from building the original model from the features to the response. We develop FAST-AMI-CRT which requires less computation than AMI-CRT and is robust to poor estimation of the null conditional. We define sufficient conditions under which to perform instance-wise feature selection and develop the AMI-IW, an instance-wise feature selection method built from the pieces of FAST-AMI-CRT. AMI-CRT, FAST-AMI-CRT, and AMI-IW all outperform several popular methods. in various simulated tasks, in identifying biologically significant genes, selecting the most indicative features to predict hospital readmissions, and in identifying distinguishing features in an image classification task. where Lβ is an log-likelihood estimate using q (k,m) β end end Let x be a dataset such that x −j = x −j , and x j is randomly sampled from q θ ( Let x (k) be a dataset such that x −j = x −j , and x j is randomly sampled from and . We first list the assumptions here: 3. The cumulative distribution functions of t(D N ) and t( D j,N ) are both continuous everywhere. 4. We have access to complete conditionals q( Proof. We prove that t is a proper test statistic if and only if t(E n ) is a consistent estimator of We do this by showing t yields p-values that are zero under the alternate hypothesis and uniformly distributed under the null. Recall that the p-value for our test is: Under the alternate hypothesis Consider the case where whereq j − → indicates a convergence in probability. Since x j ⊥ y | x −j , notice also that Therefore, the term inside the expectation in the p j (D N ) above is always 0, yielding a p-value of 0 in the limit of N . Since these p-values converge in probability to a single point, the p-values converge in distribution to a delta mass at 0. Under the null hypothesis In the case where x j ⊥ y | x −j , the samples in q N (y, x) and q j,N (y, x j , x −j ) are both sampled from the same distribution q =q j . Therefore, the distribution of t(D N ) as a function of q N , is the same as that of t( D j,N ) as a function ofq j,N . Let F N be the cumulative distribution function of t( D j,N ) which in this case is the same as that of t(D N ). We rewrite the p-value expression as p N (·) be the generalized inverse cumulative distribution function which exists because F N is a continuous everywhere function. With this, we derive the distribution of the p-value: Discontinuities could occur when the event t(D N ) = t( D j,N ) occurs with some non-zero probability c. This means that the p-value does not take all the values in [0, 1]. To see this, note that To remedy this, we can replace the indicator function in our test-statistic with the following function: where Uniform([0, 1]) is a continuous uniform random variable. ,N ) , the distribution of the p-value is the same as the uniform random variable : N ) ) is continuous everywhere in its support because t(D N ) = t( D j,N ) occurs with zero probability.","We develop a simple regression-based model-agnostic feature selection method to interpret data generating processes with FDR control, and outperform several popular baselines on several simulated, medical, and image datasets.",ImageNet ; Turner ; Kononenko ; Csiszár ; Shrikumar ; KL ; Barber et al. ; Lundberg ; FAST-AMI-CRT ; Simonyan,Barber et al ; Ribeiro et al ; variables ; the divergence ; The cumulative distribution functions ; a particular structure ; a misnomer ; KLdivergence ; the class ; an example,ImageNet ; Turner ; Kononenko ; Csiszár ; Shrikumar ; KL ; Barber et al. ; Lundberg ; FAST-AMI-CRT ; Simonyan,"Testing relationships between variables through a machine learning model, such as conditional randomization tests, can help determine whether a variable relates to the response given the rest of the variables. However, these tests require users to specify test statistics. F-divergences provide a broad class of proper test statistics, while the KL-Divergence yields an easy-to-compute proper test statistic. Questions of feature importance can also be asked at the level of an individual sample. Model interpretation techniques aim to select features important for a response by reducing models to be human interpretable, reducing models (sometimes locally) to make them human interpret",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"This paper extends the proof of density of neural networks in the space of continuous (or even measurable) functions on Euclidean spaces to functions on compact sets of probability measures.
 By doing so the work parallels a more then a decade old results on mean-map embedding of probability measures in reproducing kernel Hilbert spaces.  
 The work has wide practical consequences for multi-instance learning, where it theoretically justifies some recently proposed constructions.
 The result is then extended to Cartesian products, yielding universal approximation theorem for tree-structured domains, which naturally occur in data-exchange formats like JSON, XML, YAML, AVRO, and ProtoBuffer. This has important practical implications, as it enables to automatically create an architecture of neural networks for processing structured data (AutoML paradigms), as demonstrated by an accompanied library for JSON format. This work has been motivated by recently proposed solutions to multi-instance learning BID28 ; BID19 ; BID5 and by mean-map embedding of probability measures BID23 . It generalizes the universal approximation theorem of neural networks to compact sets of probability measures over compact subsets of Euclidean spaces. Therefore, it can be seen as an adaptation of the mean-map framework to the world of neural networks, which is important for comparing probability measures and for multi-instance learning, and it proves the soundness of the constructions of BID19 ; BID5 .The universal approximation theorem is extended to inputs with a tree schema (structure) which, being the basis of many data exchange formats like JSON, XML, ProtoBuffer, Avro, etc., are nowadays ubiquitous. This theoretically justifies applications of (MIL) neural networks in this setting.As the presented proof relies on the Stone-Weierstrass theorem, it restricts non-linear functions in neural networks to be continuous in all but the last non-linear layer. Although this does not have an impact on practical applications (all commonly use nonlinear functions within neural networks are continuous) it would be interesting to generalize the result to non-continuous non-linearities, as has been done for feed-forward neural networks in BID14 .",This paper extends the proof of density of neural networks in the space of continuous (or even measurable) functions on Euclidean spaces to functions on compact sets of probability measures.,Euclidean ; decade ; Hilbert ; Cartesian ; JSON ; XML ; YAML ; ProtoBuffer ; Avro,this setting ; the soundness ; multi-instance learning ; mean-map ; ProtoBuffer ; compact sets ; Cartesian ; the presented proof ; data-exchange formats ; an adaptation,Euclidean ; decade ; Hilbert ; Cartesian ; JSON ; XML ; YAML ; ProtoBuffer ; Avro,"The proof of density of neural networks in the space of continuous (or even measurable) functions on Euclidean spaces to compact sets of probability measures is extended to tree-structured domains. The work parallels a more then a decade old results on mean-map embedding probability measures in reproducing kernel Hilbert spaces. This work has wide practical implications for multi-instance learning, where it enables to automatically create neural networks for processing structured data, as demonstrated by an accompanied library for JSON format. The universal approximation theorem is further extended to inputs with tree schema (structure) which naturally occur in data exchange formats like JSON, XML,",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Neural networks are vulnerable to small adversarial perturbations. Existing literature largely focused on understanding and mitigating the vulnerability of learned models. In this paper, we demonstrate an intriguing phenomenon about the most popular robust training method in the literature, adversarial training: Adversarial robustness, unlike clean accuracy, is sensitive to the input data distribution. Even a semantics-preserving transformations on the input data distribution can cause a significantly different robustness for the adversarial trained model that is both trained and evaluated on the new distribution. Our discovery of such sensitivity on data distribution is based on a study which disentangles the behaviors of clean accuracy and robust accuracy of the Bayes classifier. Empirical investigations further confirm our finding. We construct semantically-identical variants for MNIST and CIFAR10 respectively, and show that standardly trained models achieve comparable clean accuracies on them, but adversarially trained models achieve significantly different robustness accuracies. This counter-intuitive phenomenon indicates that input data distribution alone can affect the adversarial robustness of trained neural networks, not necessarily the tasks themselves. Lastly, we discuss the practical implications on evaluating adversarial robustness, and make initial attempts to understand this complex phenomenon. Neural networks have been demonstrated to be vulnerable to adversarial examples BID22 BID3 . Since the first discovery of adversarial examples, great progress has been made in constructing stronger adversarial attacks BID12 BID18 BID17 BID6 . In contrast, defenses fell behind in the arms race BID5 BID1 . Recently a line of works have been focusing on understanding the difficulty in achieving adversarial robustness from the perspective of data distribution. In particular, BID24 demonstrated the inevitable tradeoff between robustness and clean accuracy in some particular examples. BID20 showed that the sample complexity of ""learning to be robust"" learning could be significantly higher than that of ""learning to be accurate"".In this paper, we contribute to this growing literature from a new angle, by studying the relationship between adversarial robustness and the input data distribution. We focus on the adversarial training method, arguably the most popular defense method so far due to its simplicity, effectiveness and scalability BID12 BID13 BID15 BID17 BID8 . Our main contribution is the finding that adversarial robustness is highly sensitive to the input data distribution:A semantically-lossless shift on the data distribution could result in a drastically different robustness for adversarially trained models.Note that this is different from the transferability of a fixed model that is trained on one data distribution but tested on another distribution. Even retraining the model on the new data distribution may give us a completely different adversarial robustness on the same new distribution. This is also in sharp contrast to the clean accuracy of standard training, which, as we show in later sections, is insensitive to such shifts. To our best knowledge, our paper is the first work in the literature that demonstrates such sensitivity.Our investigation is motivated by the empirical observations on the MNIST dataset and the CIFAR10 dataset. In particular , while comparable SOTA clean accuracies (the difference is less than 3%) are achieved by MNIST and CIFAR10 BID10 , CIFAR10 suffers from much lower achievable robustness than MNIST in practice. 1 Results of this paper consist of two parts. First in theory , we start with analyzing the difference between the regular Bayes error and the robust error, and show that the regular Bayes error is invariant to invertible transformations of the data distribution, but the robust error is not. We further prove that if the input data is uniformly distributed, then the perfect decision boundary cannot be robust. However, we also manage to find a robust model for the binarized MNIST dataset (semantically almost identical to MNIST, later described in Section 3). The certification method by BID26 guarantees that this model achieves at most 3% robust error. Such a sharp contrast suggests the important role of the data distribution in adversarial robustness, and leads to our second contribution on the empirical side: we design a series of augmented MNIST and CIFAR10 datasets to demonstrate the sensitivity of adversarial robustness to the input data distribution.Our finding of such sensitivity raises the question of how to properly evaluate adversarial robustness. In particular, the sensitivity of adversarial robustness suggests that certain datasets may not be sufficiently representative when benchmarking different robust learning algorithms. It also raises serious concerns about the deployment of believed-to-be-robust training algorithm in a real product. In a standard development procedure , various models (for example different network architectures) would be prototyped and measured on the existing data. However, the sensitivity of adversarial robustness makes the truthfulness of the performance estimations questionable, as one would expect future data to be slightly shifted. We illustrate the practical implications in Section 4 with two practical examples: 1) the robust accuracy of PGD trained model is sensitive to gamma values of gamma-corrected CIFAR10 images. This indicates that image datasets collected under different light conditions may have different robustness properties; 2) both as a ""harder"" version of MNIST, the fashion-MNIST BID27 and edge-fashion-MNIST (an edge detection variant described in Section 4.2) exhibit completely different robustness characteristics. This demonstrates that different datasets may give completely different evaluations for the same algorithm.Finally, our finding opens up a new angle and provides novel insights to the adversarial vulnerability problem, complementing several recent works on the issue of data distributions' influences on robustness. BID24 hypothesize that there is an intrinsic tradeoff between clean accuracy and adversarial robustness. Our studies complement this result, showing that there are different levels of tradeoffs depending on the characteristics of input data distribution, under the same learning settings (training algorithm, model and training set size). BID20 show that different data distributions could have drastically different properties of adversarially robust generalization, theoretically on Bernoulli vs mixtures of Gaussians, and empirically on standard benchmark datasets. From the sensitivity perspective, we demonstrate that being from completely different distributions (e.g. binary vs Gaussian or MNIST vs CIFAR10) may not be the essential reason for having large robustness difference. Gradual semantics-preserving transformations of data distribution can also cause large changes to datasets' achievable robustness. We make initial attempts in Section 5 to further understand this sensitivity. We investigated perturbable volume and inter-class distance as the natural causes of the sensitivity; model capacity and sample complexity as the natural remedies. However, the complexity of the problem has so far defied our efforts to give a definitive answer. In this paper we provided theoretical analyses to show the significance of input data distribution in adversarial robustness, which further motivated our systematic experiments on MNIST and CI-FAR10 variants. We discovered that, counter-intuitively, robustness of adversarial trained models are sensitive to semantically-preserving transformations on data. We demonstrated the practical implications of our finding that the existence of such sensitivity questions the reliability in evaluating robust learning algorithms on particular datasets. Finally, we made initial attempts to understand this sensitivity. DISPLAYFORM0 Then we apply Markov's inequality, for all real number t > 0: DISPLAYFORM1 Finally, we observe that the longest (in terms of 2 norm) such ∞ attacks vector to HP 2 are parallel to the normal vector 1 to HP 2 . They have 2 distance √ d. The set these attacks cover is characterized by {x DISPLAYFORM2 Let t = 2 d, we have: DISPLAYFORM3 In the case of zero-one loss, RR DISPLAYFORM4 A.2 PROOF FOR THEOREM 2.1Proof. (First Inequality for Cube) The proof here follows that of BID16 , but we track of the tight constants so as to give tighter adversarial robustness calculations.Let Φ be one dimensional standard normal cumulative distribution function and let µ d denote d dimensional Gaussian measures. Consider the map T : DISPLAYFORM5 T pushes forward µ d defined on R d into a probability measure P on (0, 1) d : DISPLAYFORM6 for A ⊂ (0, 1) d . Next we have the following Gaussian isoperimetric/concentration inequality BID16 : DISPLAYFORM7 Now for A ⊂ (0, 1) d , we have: DISPLAYFORM8 where the first inequality follows from that T has Lipschitz constant DISPLAYFORM9 , and thus T −1 has Lipschitz constant √ 2π; and the second one follows from Gaussian isoperimetric inequality. DISPLAYFORM10 Additionally, the inequality Φ(x) ≥ 1 − e x 2 2 implies the last inequality in the theorem.","Robustness performance of PGD trained models are sensitive to semantics-preserving transformation of image datasets, which implies the trickiness of evaluation of robust learning algorithms in practice.",second ; first ; Gaussians ; two ; zero-one ; Lipschitz ; MNIST ; Bayes ; one ; PGD,the natural causes ; the sensitivity perspective ; completely different robustness characteristics ; Even a semantics-preserving transformations ; comparable clean accuracies ; the first inequality ; great progress ; second ; Our discovery ; the regular Bayes error,second ; first ; Gaussians ; two ; zero-one ; Lipschitz ; MNIST ; Bayes ; one ; PGD,"Neural networks are vulnerable to small adversarial perturbations due to their sensitivity to the input data distribution. In this paper, we show that adversarial robustness is sensitive to input data distributions, and even semantics-preserving transformations can cause significantly different robustness for the adversarial trained model that is both trained and evaluated on the new distribution. Our discovery of such sensitivity on data distribution is based on a study which disentangles the behaviors of clean accuracy and robust accuracy of Bayes classifier. Empirical investigations further confirm our finding. We construct semantically-identical variants for MNIST and CIFAR10",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"We recently observed that convolutional filters initialized
 farthest apart from each other using offthe-
 shelf pre-computed Grassmannian subspace
 packing codebooks performed surprisingly well
 across many datasets. Through this short paper,
 we’d like to disseminate some initial results in this
 regard in the hope that we stimulate the curiosity
 of the deep-learning community towards considering
 classical Grassmannian subspace packing
 results as a source of new ideas for more efficient
 initialization strategies.","Initialize weights using off-the-shelf Grassmannian codebooks, get  faster training and better accuracy",Grassmannian,"this short paper ; results ; Grassmannian ; the curiosity ; We ; offthe-
 shelf pre-computed Grassmannian subspace
 packing codebooks ; classical Grassmannian subspace ; convolutional filters ; many datasets ; this
 regard",Grassmannian,Classical Grassmannian subspace packing codebooks performed surprisingly well across many datasets. We’d like to disseminate some initial results in this regard in the hope that we stimulate the curiosity of the deep-learning community towards considering classical grassmannian packing code books as a source of new ideas for more efficient and efficient initialization strategies.,/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"One of the big challenges in machine learning applications is that training data can be different from the real-world data faced by the algorithm. In language modeling, users’ language (e.g. in private messaging) could change in a year and be completely different from what we observe in publicly available data. At the same time, public data can be used for obtaining general knowledge (i.e. general model of English). We study approaches to distributed fine-tuning of a general model on user private data with the additional requirements of maintaining the quality on the general data and minimization of communication costs. We propose a novel technique that significantly improves prediction quality on users’ language compared to a general model and outperforms gradient compression methods in terms of communication efficiency. The proposed procedure is fast and leads to an almost 70% perplexity reduction and 8.7 percentage point improvement in keystroke saving rate on informal English texts. Finally, we propose an experimental framework for evaluating differential privacy of distributed training of language models and show that our approach has good privacy guarantees. Two common problems arising after deployment of a machine learning model on user devices are discrepancy between training data and actual data stored on user devices, and the need of regular model updates. In the case of language modeling, it corresponds to the difference between language and style of the training corpus mined in the Internet and messages of the user, which account for most of the text generated on the device. Even if the training corpus includes a substantial part of informal texts (tweets, forum threads, etc.), real user data can be very different. This is a challenge for word prediction algorithms in software keyboard applications. The most general approach to improvement of customer experience in typing is integrating a separate user language model trained on device in an on-line fashion. In the simplest case it is a smoothed n-gram (e.g. Kneser-Ney n-gram model BID6 )).In BID26 continuously learned personalized language model based on LSTM was proposed but as far as each user generates only a small portion of textual data, such data by itself cannot be used for updates of the general model. Thus , for a model update, a collection of potentially sensitive data from many users is needed. As shown in , collecting data for training may be avoided. We propose a similar approach for distributed fine-tuning of language models on private data. In this sense our method can be considered as ""federated fine-tuning"" but we prefer to take more traditional term. In this setting we start with a language model trained on a large text corpus representing the general language. This model G will be updated continuously on user devices but with an additional requirement that the model must not go too far from the general language model, i.e. we don't overfit on user data.We pursue two goals: 1) to develop an algorithm of distributed fine-tuning that is fast, communication efficient and doesn't need collecting sensitive user data; and 2) to prevent the language model from forgetting ""general English"". Besides , we provide analysis of possibility of privacy violation After each round the server model G t+1 is sent to the next K elements. in our model. BID8 ) demonstrated an attack on distributed training algorithm leading to information leakage. This means that privacy analysis in necessary for such algorithms.Our main contributions are: 1) we propose an efficient procedure of distributed fine-tuning of language models immune to the problem of catastrophic forgetting BID3 ), 2) we provide experimental evaluation of on-device training time, communication costs and convergence rates of the general language model in realistic conditions, 3) we compare two most popular strategies of improving communication efficiency in the context of distributed learning, and 4) we propose an experimental framework for evaluation of differential privacy of distributed training of language models, and using this framework, we evaluate privacy guarantees of our approach.In our research we are focused on improvement of keystroke saving rate (see section 2.4) because this metric reflects customer typing experience more directly than perplexity or BLEU. We use LSTM architecture for our language model as described in BID27 and evaluate ondevice training time for this architecture. We show that the on-device training time is reasonably small, thus demonstrating the feasibility of the whole approach. We have presented our results in distributed fine-tuning of neural language models. We paid special attention to preventing a catastrophic forgetting of the general language after a model fine-tuning on the user devices. Our experiments showed that the performance of an initial model of the general English on user data can be improved significantly almost without a performance degradation on the standard English training data. We found that a combination of on-device training with random rehearsal and server-side model averaging provides the best performance for such distributed finetuning. Users' models were trained for the whole epoch that reduced communication costs while at the same time being quite fast -it took less than 3 minutes with a realistic assessment of volume of the available user data. Finally, we provided an experimental evaluation of differential privacy of our method and showed that the method has a reasonable level of differential privacy compared to other solutions. We still have to note that we provided an empirical estimation of differential privacy which holds with some high probability but not almost surely.This statistic doesn't converge to the Kolmogorov distribution as shown in W. Lilliefors (1969) . It converges to the distribution with smaller critical values at the same significance levels because we overfit on the sample data when the estimator r is plugged in. We chose a 5% significance level and critical value for it is 1.08. In 19 cases out of 20 the Lilliefors test failed to reject the null hypothesis at a 5% significance level. TAB4 provides exact values obtained during the application of the statistical test. Relying on these values along with data visualization in 3 we can state that random variable c(s) has tails that decrease like the Pareto distribution tails.The hypothesis that we accepted suggests that the cumulative distribution function of c(s) is given by the formula (8). It means that the tail distribution function for all x > x 0 is given by DISPLAYFORM0 We chose x 0 = c (k) n , so F (x 0 ) is just the ratio k/n. Thus, C can be estimated by DISPLAYFORM1 Values of C are given in the TAB4 . Finally, from formula (11) and proposition 1 it is easy to derive that (ε, δ)-differential privacy is provided by the values ε, δ that satisfy DISPLAYFORM2",We propose a method of distributed fine-tuning of language models on user devices without collection of private data,Lilliefors ; Kolmogorov ; One ; W. Lilliefors ; Pareto ; Two ; c(s ; BLEU ; English ; F,a model fine-tuning ; user private data ; the general language ; some high probability ; the whole epoch ; communication costs ; Kolmogorov ; LSTM architecture ; language ; distributed training,Lilliefors ; Kolmogorov ; One ; W. Lilliefors ; Pareto ; Two ; c(s ; BLEU ; English ; F,"Machine learning applications can be different from real-world data. In language modeling, users' language could change in a year and be completely different from what we observe in publicly available data. However, public data can be used for obtaining general knowledge (i.e. general model of English). We study approaches to distributed fine-tuning of a general model on user private data with the additional requirements of maintaining the quality on the general data and minimization of communication costs. We propose a novel technique that significantly improves prediction quality on users’ language and outperforms gradient compression methods in communication efficiency. The proposed procedure is fast and leads",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Recent advances in learning from demonstrations (LfD) with deep neural networks have enabled learning complex robot skills that involve high dimensional perception such as raw image inputs. 
 LfD algorithms generally assume learning from single task demonstrations. In practice, however, it is more efficient for a teacher to demonstrate a multitude of tasks without careful task set up, labeling, and engineering. Unfortunately in such cases, traditional imitation learning techniques fail to represent the multi-modal nature of the data, and often result in sub-optimal behavior. In this paper we present an LfD approach for learning multiple modes of behavior from visual data. Our approach is based on a stochastic deep neural network (SNN), which represents the underlying intention in the demonstration as a stochastic activation in the network. We present an efficient algorithm for training SNNs, and for learning with vision inputs, we also propose an architecture that associates the intention with a stochastic attention module.
 We demonstrate our method on real robot visual object reaching tasks, and show that
 it can reliably learn the multiple behavior modes in the demonstration data. Video results are available at https://vimeo.com/240212286/fd401241b9. A key problem in robotic control is to simplify the problem of programming a complex behavior. Traditional control engineering approaches, which rely on accurate manual modeling of the system environment, are very challenging to apply in modern robotic applications where most sensory inputs come from images and other high-dimensional signals such as tactile feedback.In contrast, imitation learning, or learning from demonstration (LfD) approaches BID31 aim to directly learn a control policy from mentor or expert demonstrations. The key advantages of LfD are simplicity and data-efficiency, and indeed, LfD has been successfully used for learning complex robot skills such as locomotion BID32 , driving BID27 BID30 , flying BID0 , and manipulation BID21 BID4 BID25 . Recently, advances in deep representation learning BID12 have facilitated LfD methods with high dimensional perception, such as mapping raw images directly to controls BID9 . These advances are capable of learning generalizable skills BID18 , and offer a promising approach for modern industrial challenges such as pick and place tasks BID5 .One challenge in LfD, however, is learning different modes of the same task. For example, consider learning to pick up an object from a pile. The demonstrator can choose to pick up a different object each time, yet we expect LfD to understand that these are similar demonstrations of the same pick-up skill, only with a different intention in mind. Moreover , we want the learned robot behavior to display a similar multi-modal 1 nature.Standard approaches for LfD with image inputs, such as learning with deep neural networks (NNs) BID27 BID9 BID18 , are not suitable for learning multimodal behaviors. In their essence, NNs learn a deterministic mapping from observation to control, which cannot represent the inherently multi-modal latent intention in the demonstrations. In practice , this manifests as an 'averaging' of the different modes in the data BID3 , leading to an undesirable policy.A straightforward approach for tackling the multi-modal problem in LfD is to add a label for each mode in the data. Thus, in the pick-up task above, the demonstrator would also explicitly specify the object she intends to pick-up beforehand. Such an approach has several practical shortcomings: it requires the demonstrator to record more data, and requires the possible intentions to be specified in advance, making it difficult to use the same recorded data for different tasks. More importantly , such a solution is conceptually flawed -it solves an algorithmic challenge by placing additional burden on the client.In this work, we propose an approach for LfD with multi-modal demonstrations that does not require any additional data labels. Our method is based on a stochastic neural network model, which represents the latent intention as a random activation in the network. We propose a novel and efficient learning algorithm for training stochastic networks, and present a network architecture suitable for LfD with raw image inputs, where the intention takes the form of a stochastic attention over features in the image.We show that our method can reliably reproduce behavior with multiple intentions in real-robot object reaching tasks. Moreover, in scenarios where multiple intentions exist in the demonstration data, the stochastic neural networks perform better than their deterministic counterparts. We presented an approach for learning from demonstrations that contain multiple modes of performing the same task. Our method is based on stochastic neural networks, and represents the mode Figure 2 : Comparison of IDS and SNN algorithms. We plot three different errors during training (on the training data), for the same model trained using IDS and SNN algorithm. Left: the respective training loss for each method. Since the max in IDS upper bounds the softmax in SNN, the loss plot for IDS lower bounds SNN. Middle: the IDS loss on the training data, for both models. Since the SNN is trained on a different loss function (softmax), its performance is worse. This shows an important point: if, at test time, we use optimistic sampling to sample z from best samples during training, we should expect IDS to perform better than SNN. Right: the average log-likelihood loss during training. The SNN wins here, since the softmax encourages to increase the likelihood of 'incorrect' z values. This provides additional motivation for using optimistic sampling.of performing the task by a stochastic vector -the intention, which is given as input to a feedforward neural network. We presented a simple and efficient algorithm for training our models, and a particular implementation suitable for vision-based inputs. As we demonstrated in real-robot experiments, our method can reliably learn to reproduce the different modes in the demonstration data, and outperforms standard approaches in cases where such different modes exist.In future work we intend to investigate the extension of this approach to more complex manipulation tasks such as grasping and assembly, and domains with a very large number of objects in the scene. An interesting point in our model is tying the features to the intention by an attention mechanism, and we intend to further investigate recurrent attention mechanisms (Xu et al., 2015) that could offer better generalization at inference time. F (Q, θ) =E Q log P (u 1:T , z|x 1:T ; θ) Q(z|x 1:T , u 1:T ) ≤E P (·|x 1:T ,u 1:T ;θ) [log P (y 1:T |x 1:T ,θ)] DISPLAYFORM0 where F is the Kullback Liebler divergence between P (u 1:T |z, x 1:T ; θ) and Q(z|u 1:T , x 1:T ) given as follows:F (Q, θ) = −D KL (Q||P (·|x 1:T , u 1:T ; θ)) + log P (y 1:T |x 1:T ,θ). Most importantly, it has also been shown in Theorem 2 of BID23 that if Q and θ form a pair of local maximizer to F , then θ is also a local maximum of the original likelihood maximization problem. To maximize F w.r.t Q, one has the closed form solution based on Bayes theorem: Q * (z|u 1:T , x 1:T ; θ old ) =P (z|y 1:T , x 1:T , θ) DISPLAYFORM1 Here, {z 1 , . . . , z N } is a sequence of latent random variables sampled i.i.d. from the distribution P (z).Given parameter θ, denoted by θ old , immediately the posterior distribution Q that maximizes F is given by: Q * (z|x 1:T , u 1:T ) = P (z|x 1:T , u 1:T ; θ old ). In this case, the above loss function is equivalent to the complete data log-likelihood * (θ, θ old ) := E P (·|u 1:T ,x 1:T ;θold) log P (x 1:T , z|u 1:T ; θ) P (z|x 1:T , u 1:T ; θ old ) , which is a lower bound of the log likelihood. Furthermore , if θ = θ old , then clearly * (θ old , θ old ) is equal to the log-likelihood log P (y 1:T |x 1:T ,θ old ).Tang & Salakhutdinov (2013) present a generalized EM algorithm to train a SNN. In the E-step, the following approximate posterior distribution is used:Q(z|u 1:T , x 1:T ; θ old ) :=r(z; x 1:T ,y 1:T , θ old )P (z), wherer (z; x 1:T ,y 1:T , θ old ) = r(z; x 1:T , u 1: DISPLAYFORM2 r(z i ; x 1:T , u 1:T , θ old ) is the the importance sampling weight.Recall that for our distribution model, r(z; x 1:T , u 1:T , θ old ) ∝ exp(−d(f (x, z; θ), u)), therefore we obtain that the importance weights correspond to a soft-max over the prediction error.In the M-step, the θ parameters are updated with the gradient vector with respect to the following optimization: θ ∈ arg max θ∈Θˆ (θ, θ old ), wherê DISPLAYFORM3 (z i ; x 1:T ,y 1:T , θ old ) log P (y 1:T , z i |x 1:T , θ)is the empirical expected log likelihood, andQ is the posterior distribution from the E-step. Here we drop the last term in F because in our case Q that does not depend on θ. Correspondingly, the gradient estimate is given by: DISPLAYFORM4 (z i )∇ θ log r(z i ; x 1:T , u 1:T , θ), the equality is due to the facts that log P (y 1:T , z|x 1:T , θ) = log r(z; x 1:T ,y 1:T , θ) + log P (z) and distribution P (z) is independent of θ.To better understand this estimator, we will analyze the bias and variance of the gradient estimator. Based on the construction of importance sampling weight, immediately the gradient estimator is consistent. Furthermore, under certain regular assumptions, the bias is O(N −1/2 ). (This means the gradient estimator is asymptotically unbiased.) Furthermore, the variance of this estimator is given by DISPLAYFORM5 where the integrand is given by v(z; θ) =r(z; x 1:T ,y 1:T , θ old ) · (∇ θ log r(z; x 1:T , u 1:T , θ)) 2 ≥ 0.",multi-modal imitation learning from unstructured demonstrations using stochastic neural network modeling intention.,IDS ; F w.r.t Q ; SNN ; Standard ; F ; Bayes ; LfD ; max ; .Tang & Salakhutdinov ; ∇,a label ; a different loss function ; techniques ; -it ; F (Q ; the inherently multi-modal latent intention ; a promising approach ; Bayes ; the intention ; a complex behavior,IDS ; F w.r.t Q ; SNN ; Standard ; F ; Bayes ; LfD ; max ; .Tang & Salakhutdinov ; ∇,"Learning from demonstrations (LfD) with deep neural networks has enabled learning complex robot skills that involve high dimensional perception such as raw image inputs. However, traditional imitation learning techniques often fail to represent the multi-modal nature of the data, and often result in sub-optimal behavior. In this paper, we present an LfD approach for learning multiple modes of behavior from visual data. Our approach is based on stochastic deep neural network (SNN), which represents the underlying intention in the demonstration, and is associated with stochastically activated attention modules. This approach is similar to imitation learning, but it is",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Formulating the reinforcement learning (RL) problem in the framework of probabilistic inference not only offers a new perspective about RL, but also yields practical algorithms that are more robust and easier to train. While this connection between RL and probabilistic inference has been extensively studied in the single-agent setting, it has not yet been fully understood in the multi-agent setup. In this paper, we pose the problem of multi-agent reinforcement learning as the problem of performing inference in a particular graphical model. We model the environment, as seen by each of the agents, using separate but related Markov decision processes. We derive a practical off-policy maximum-entropy actor-critic algorithm that we call Multi-agent Soft Actor-Critic (MA-SAC) for performing approximate inference in the proposed model using variational inference. MA-SAC can be employed in both cooperative and competitive settings. Through experiments, we demonstrate that MA-SAC outperforms a strong baseline on several multi-agent scenarios. While MA-SAC is one resultant multi-agent RL algorithm that can be derived from the proposed probabilistic framework, our work provides a unified view of maximum-entropy algorithms in the multi-agent setting. The traditional reinforcement learning (RL) paradigm, that formalizes learning based on trial and error, has primarily been developed for scenarios where a single trainable agent is learning in an environment. In this setup, although the agent changes its behavior as learning progresses, the environment dynamics themselves do not change. Thus, the environment appears to be stationary from the point of view of the learning agent. However, in a setting where several agents are learning in the same environment simultaneously (multi-agent setting), this is not true as a change in one agent's behavior manifests itself as a change in environment dynamics from the point of view of other agents (Busoniu et al., 2008) . It has been established that stability issues can arise if each agent is independently trained using standard single-agent RL methods (Tan, 1993) . While, in theory, it is possible to treat a collection of multiple agents as a single centralized metaagent to be trained, in practice, this approach becomes infeasible as the action space of the centralized meta-agent grows exponentially with the number of agents. Moreover, executing the resultant centralized policy is not always possible due to various reasons like geographic separation between agents, communication overhead and so on (Foerster et al., 2018b) . Even if these issues are taken care of, when the agents are competitive, designing a reward function for the centralized meta-agent is very challenging and thus, in general, such a setup cannot be used with competitive agents. There are numerous practical scenarios that require several intelligent agents to function together (either cooperatively or competitively). Consider, for instance, a soccer game between two teams: agents within a team must cooperate while being competitive with the opponents. Considering that traditional single-agent RL methods cannot satisfactorily handle problems from the multi-agent domain, completely new RL algorithms that explicitly acknowledge and exploit the presence of other intelligent agents in the environment are required. In this paper, we pose the multi-agent reinforcement learning problem as the problem of performing probabilistic inference in a particular graphical model. While such a formulation is well known in the single-agent RL setting (Levine, 2018) , its extension to the multi-agent setup is non-trivial especially in the general case where the agents may be cooperative and/or competitive. We model the environment as seen by each of the agents using separate but related Markov Decision Processes (MDPs). Each agent then tries to maximize the expected return it gets from the environment under its own MDP (Section 4). Using our framework, we derive an off-policy maximum entropy actor-critic algorithm that generalizes the Soft Actor-Critic (SAC) algorithm (Haarnoja et al., 2018a ) to a multi-agent setup. We refer to this algorithm as Multi-agent Soft Actor-Critic (MA-SAC). Like SAC, it is a maximum entropy algorithm, i.e., the learned policies try to maximize the rewards while at the same time maximizing entropy of the stochastic actor. Such algorithms are known to be more stable and easier to train (Haarnoja et al., 2018a) . MA-SAC follows the centralized training, decentralized execution paradigm. As we demonstrate in Section 4, each agent learns its own policy while being actively aware of the presence of other agents. The learned policy of any given agent only utilizes its local observation at test time. Thus, MA-SAC avoids the pitfalls of both independent training of agents (being unaware of other agents leads to non-stationarity and hence instability) and training a centralized agent (centralized policies are hard to execute) as described above. By setting a tunable temperature parameter (Section 4.3) to zero, MA-SAC yields an algorithm that is very similar to the Multi-agent Deep Deterministic Policy Gradients algorithm (MADDPG) (Lowe et al., 2017) apart from a minor change in updating the actor. The utility of this modification is clearly reflected in our derivation of the inference procedure. When the temperature parameter is non-zero, agents trained using MA-SAC outperform agents trained using MADDPG on multiple cooperative and competitive tasks as we demonstrate in Section 5.3. Our main contributions are: (i) we present a probabilistic view of the multi-agent reinforcement learning problem where each agent models the environment using a separate but related MDP; (ii) we derive an off-policy maximum entropy actor-critic algorithm (MA-SAC) by performing structured variational inference in the proposed model; (iii) we empirically demonstrate that MA-SAC performs well in practice and highlight different ways in which our framework can utilize ideas from other existing approaches in multi-agent RL; and (iv) although we only present an actor-critic algorithm in this paper, our framework allows derivation of maximum-entropy variants of other reinforcement learning algorithms in the multi-agent setting. In this paper we posed the multi-agent RL problem as the problem of performing probabilistic inference in a graphical model where each agent views the environment as a separate MDP. We derived an off policy maximum entropy actor-critic algorithm based on the centralized training, decentralized execution paradigm using our proposed model. Our experimental results show that the proposed algorithm outperforms a strong baseline (MADDPG) on several cooperative and competitive tasks. As noted in Section 5.4, various existing ideas for parameterizing Q-functions (Yang et al., 2018; Rashid et al., 2018; Iqbal & Sha, 2019) can be naturally integrated with MA-SAC to improve its scalability as the number of agents increases. Our framework can also be used for deriving maximum-entropy variants of other RL algorithms in the multi-agent setting. We leave these ideas for future work.",A probabilistic framework for multi-agent reinforcement learning,Markov Decision Processes ; Markov ; RL ; the Soft Actor-Critic ; Levine ; al. ; Multi ; MA-SAC ; MA ; one,a separate but related MDP ; the single-agent setting ; completely new RL algorithms ; Sha ; a separate MDP ; the problem ; Busoniu ; The utility ; scenarios ; variational inference,Markov Decision Processes ; Markov ; RL ; the Soft Actor-Critic ; Levine ; al. ; Multi ; MA-SAC ; MA ; one,"The reinforcement learning (RL) problem in the framework of probabilistic inference provides a new perspective about RL, while it has not been fully understood in the single-agent setting. This is due to the complexity of performing inference in a particular graphical model, where each agent model the environment using separate but related Markov decision processes. In this paper, we derive a practical off-policy maximum-entropy actor-critic algorithm that we call Multi-agent Soft Actor-Critic (MA-SAC) for performing approximate inference in the proposed model using variational inference. This approach can be employed in both cooperative and competitive",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"In one-class-learning tasks, only the normal case can be modeled with data, whereas the variation of all possible anomalies is too large to be described sufficiently by samples. Thus, due to the lack of representative data, the wide-spread discriminative approaches cannot cover such learning tasks, and rather generative models, which attempt to learn the input density of the normal cases, are used. However, generative models suffer from a large input dimensionality (as in images) and are typically inefficient learners. We propose to learn the data distribution more efficiently with a multi-hypotheses autoencoder. Moreover, the model is criticized by a discriminator, which prevents artificial data modes not supported by data, and which enforces diversity across hypotheses. This consistency-based anomaly detection (ConAD) framework allows the reliable identification of outof- distribution samples. For anomaly detection on CIFAR-10, it yields up to 3.9% points improvement over previously reported results. On a real anomaly detection task, the approach reduces the error of the baseline models from 6.8% to 1.5%. Anomaly detection classifies a sample as normal or abnormal. In many applications, however, it must be treated as a one-class-learning problem, since the abnormal class cannot be defined sufficiently by samples. Samples of the abnormal class can be extremely rare, or they do not cover the full space of possible anomalies. For instance, in an autonomous driving system, we may have a test case with a bear or a kangaroo on the road. For defect detection in manufacturing, new, unknown production anomalies due to critical changes in the production environment can appear. In medical data analysis, there can be unknown deviations from the healthy state. In all these cases, the well-studied discriminative models, where decision boundaries of classifiers are learned from training samples of all classes, cannot be applied. The decision boundary learning of discriminative models will be dominated by the normal class, which will negatively influence the classification performance.Anomaly detection as one-class learning is typically approached by generative, reconstruction-based methods BID30 . They approximate the input distribution of the normal cases by parametric models, which allow them to reconstruct input samples from this distribution. At test time, the data log-likelihood serves as an anomaly-score. In the case of high-dimensional inputs, such as images, learning a representative distribution model of the normal class is hard and requires many samples.Typically, an autoencoder-based approach such as the variational autoencoder BID21 BID13 ) is used. Autoencoders tend to produce blurry reconstructions, since they regress the conditional mean, and cannot model multi-modal distributions; see FIG0 for an example on a Metal Anomaly dataset. Due to multiple modes in the actual distribution, the approximation with the mean predicts high probabilities in areas not supported by samples. The blurry reconstructions in FIG0 should have a low probability and be classified as anomalies, but they have the highest likelihood under the learned autoencoder.Multiple-hypotheses networks could give the model more expressive power BID23 , BID5 , BID11 , BID2 . In conjunction with autoencoders, the multiple hypotheses can be realized with a multi-headed decoder. Concretely, each network head may predict a Gaussian density estimate. gives the network more expressive power with a multi-headed decoder (also known as multiple-hypotheses networks). The resulting anomaly scores are hence much clearer in our framework ConAD. In this work, we propose to employ multiple-hypotheses networks for learning data distributions for anomaly detection tasks. Hypotheses are meant to form clusters in the data space and can easily capture model uncertainty not encoded by the latent code. multiple-hypotheses networks can provide a more fine-grained description of the data distribution and therefore enable also a more fine-grained anomaly detection. Furthermore, to reduce support of artificial data modes by hypotheses learning, we propose using a discriminator D as a critic. The combination of multiple-hypotheses learning with D aims to retain the consistency of estimated data modes w.r.t. the real data distribution. Further, D encourage diversity across hypotheses with hypotheses discrimination. Our framework allows the model to identify out-of-distribution samples reliably.For the anomaly detection task on CIFAR-10, our proposed model results in up to 3.9% points improvement over previously reported results. On a real anomaly detection task, the approach reduces the error of the baseline models from 6.8% to 1.5%.",We propose an anomaly-detection approach that combines modeling the foreground class via multiple local densities with adversarial training.,one ; anomaly detection ; ConAD ; Anomaly ; Metal Anomaly ; Gaussian ; the anomaly detection task,all these cases ; Autoencoders ; the normal cases ; The decision boundary learning ; our framework ; medical data analysis ; possible anomalies ; the anomaly detection task ; artificial data modes ; it,one ; anomaly detection ; ConAD ; Anomaly ; Metal Anomaly ; Gaussian ; the anomaly detection task,"In one-class-learning tasks, only the normal case can be modeled with data, whereas the variation of all possible anomalies is too large to be described sufficiently by samples. Hence, wide-spread discriminative approaches cannot cover such learning tasks, and generative models, which attempt to learn the input density of normal cases, are typically inefficient learners. The model is criticized by a discriminator, which prevents artificial data modes not supported by data, and enforces diversity across hypotheses. This consistency-based anomaly detection (ConAD) framework enables reliable identification of outof-distribution samples. For anomaly detection on CIFAR",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"We propose a solution for evaluation of mathematical expression. However, instead of designing a single end-to-end model we propose a Lego bricks style architecture. In this architecture instead of training a complex end-to-end neural network, many small networks can be trained independently each accomplishing one specific operation and acting a single lego brick. More difficult or complex task can then be solved using a combination of these smaller network. In this work we first identify 8 fundamental operations that are commonly used to solve arithmetic operations (such as 1 digit multiplication, addition, subtraction, sign calculator etc). These fundamental operations are then learned using simple feed forward neural networks. We then shows that different operations can be designed simply by reusing these smaller networks. As an example we reuse these smaller networks to develop larger and a more complex network to solve n-digit multiplication, n-digit division, and cross product. This bottom-up strategy not only introduces reusability, we also show that it allows to generalize for computations involving n-digits and we show results for up to 7 digit numbers. Unlike existing methods, our solution also generalizes for both positive as well as negative numbers. The success of feed-forward Artificial Neural Network (ANN) lies in their ability to learn that allow an arbitrarily connected network to develop an internal structure appropriate for a particular task. This learning is dependent on the data provided to the network during the training process. It has been commonly observed that almost all ANNs lack generalization and their performance drastically degrades on unseen data. This includes degradation of performance on data containing the seen categories but acquired under from a different setup (location, lighting, view point, size, ranges etc) . Although there are techniques such as Domain Adaptation to address these generalization issues, however this behaviour indicates that the learning process in neural network is primarily based on memorization and they lack understanding of inherent rules. Thus the decision making process in ANN is lacking quantitative reasoning, numerical extrapolation or systematic abstraction. However when we observe other living species, numerical extrapolation and quantitative reasoning is their fundamental capability what makes them intelligent beings. For e.g. if we observe the learning process among children, they can memorize single digit arithmetic operation and then extrapolate it to higher digits. More specifically our ability to +, −, × and ÷ higher digit number is based on understanding how to reuse the examples that we have memorized for single digits. This indicates that the key to generalization is in understanding to reuse what has been memorized. Furthermore, complex operations are usually combination of several simple function. Thus complex numerical extrapolation and quantitative reasoning among ANNs can be developed by identifying and learning the fundamental operations that can be reused to develop complex functions. Inspired from the methodology of learning adopted by humans, in this work we first identify several fundamental operations that are commonly used to solve arithmetic operations (such as 1 digit multiplication, addition, subtraction, merging of two number based on their place value, learning to merge sign +/− etc). These fundamental operations are then learned using simple feed forward neural networks. We then reuse these smaller networks to develop larger and a more complex network to solve various problems like n-digit multiplication, n-digit division, cross product etc. To the best of our knowledge this is the first work that proposed a generalized solution for these arith-metic operations. Furthermore, unlike exiting methods ( Hornik et al. (1989) ; Siu & Roychowdhury (1992); Peddada (2015) ; Sharma (2013) ; Trask et al. (2018) ) ours is also the first solution that works for both positive as well as negative numbers. In this paper we show that many complex tasks can be divided into smaller sub-tasks, furthermore many complex task share similar sub-tasks. Thus instead of training a complex end-to-end neural network, many small networks can be trained independently each accomplishing one specific operation. More difficult or complex task can then be solved using a combination of these smaller network. In this work we first identify several fundamental operations that are commonly used to solve arithmetic operations (such as 1 digit multiplication, addition, subtraction, place value shifter etc). These fundamental operations are then learned using simple feed forward neural networks. We then reuse these smaller networks to develop larger and a more complex network to solve various problems like n-digit multiplication and n-digit division. One of the limitation of the proposed work is the use of float operation in the tokenizer which limits the end-to-end training of complex networks. However, since we are only using pre-trained smaller network representing fundamental operations, this does not creates any hinderance in our current work. However, we aim to resolve this issue in future. We have also designed a cross product network using the same strategy and we are currently testing its accuracy. As a future work we aim to develop a point cloud segmentation algorithm by using a larger number of identical smaller network (i.e. cross product) that can compute a normal vector using 3 3D points as input.","We train many small networks each for a specific operation, these are then combined to perform complex operations","Trask ; ANN ; Lego ; Siu & Roychowdhury ; Peddada ; Artificial Neural Network ; Sharma ; +, −, × ; first ; one",unseen data ; their fundamental capability ; one specific operation ; a different setup ; various problems ; almost all ANNs ; our solution ; two ; sign calculator ; these smaller networks,"Trask ; ANN ; Lego ; Siu & Roychowdhury ; Peddada ; Artificial Neural Network ; Sharma ; +, −, × ; first ; one","We propose a solution for evaluation of mathematical expression. Instead of designing a single end-to-end model, we propose a Lego bricks style architecture. Many small networks can be trained independently each accomplishing one specific operation and acting a single lego brick. More difficult or complex tasks can then be solved using a combination of these smaller networks. In this work, we identify 8 fundamental operations that are commonly used to solve arithmetic operations. These fundamental operations are then learned using simple feed forward neural networks. The success of feed-forward Artificial Neural Network (ANN) lies in their ability to learn that allow an arbitrarily connected network to develop an",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"To gain high rewards in muti-agent scenes, it is sometimes necessary to understand other agents and make corresponding optimal decisions. We can solve these tasks by first building models for other agents and then finding the optimal policy with these models. To get an accurate model, many observations are needed and this can be sample-inefficient. What's more, the learned model and policy can overfit to current agents and cannot generalize if the other agents are replaced by new agents. In many practical situations, each agent we face can be considered as a sample from a population with a fixed but unknown distribution. Thus we can treat the task against some specific agents as a task sampled from a task distribution. We apply meta-learning method to build models and learn policies. Therefore when new agents come, we can adapt to them efficiently. Experiments on grid games show that our method can quickly get high rewards. Applying Reinforcement Learning (RL) to multi-agent scenes requires carefully consideration about the influence of other agents. We cannot simply treat other agents as part of the environment and apply independent RL methods BID8 if the actions of them has impact on the payoff of the agent to be trained. For example, consider the two-person ultimatum bargaining game, where two players take part in. One player propose a deal to split a fixed amount of money for them two and the other player decides to accept it or not. If the second player accepts the proposal, they split the money, but if the proposal is refused, they both get zero. Experimental results BID4 show that in actual life, the second player makes the decision according to whether he or she judge the final result fair, rather than makes the obvious rational decision. Thus, the first player needs to predict how the second player will react so as to make the proposal acceptable.In order to exploit the other agents and find the corresponding optimal policy, we need to understand these agents. Here in this paper, we call all the other agents ""opponents"" to distinguish our agent from them, even if they may have cooperative relationship with our agent. For simplicity, we only consider tasks with only one opponent. Extension to tasks with more opponents is straightforward. A general way to exploit an opponent is to build a model for it from observations. This model can characterize any needed feature of the opponent, such as next action or the final goal. Such a model can make predictions for the opponent and thus turns the two-agent task into a simple-agent decision making problem. Then we can apply various RL methods to solve this problem.It is necessary that we need to have an accurate model for the opponent to help make decision. Previous works BID6 BID12 propose some methods to model the opponent. Generally, it requires many observations to get a precise model for the opponent. This may cost many iterations to act with the opponent. What's more, even if we can precisely model the opponent, there exists a main drawback of above process that the performance of the learned policy has no guarantee for any other opponent. Things are even worse if opponents have their private types which are unknown for us. New opponents with different types can have different policies or even different payoffs. Therefore, it seems that when a new opponent came, we have to learn a policy from the beginning. In some practical situations, the whole opponents follow a distributions over all these possible types. Let's come back to the ultimatum bargaining game. BID0 shows that people with different ethnicity may have different standards for fairness. Thus if we assume the type for player 2 to be its judgment for fairness, there can be a distribution for types dependent on the ethnic distribution. Given that opponents follows a distribution, it is possible that we can employ some given opponents to help us speed up the process of opponent modeling and policy improving for the current opponent.If we consider the policy learning against a specific opponent as a task, our goal can be considered as training a policy on various tasks so that it can efficiently adapt to a good policy on a new task with few training samples. This is exactly a meta-learning problem. We employ Model-Agnostic MetaLearning (MAML) BID1 to conduct meta-learning. BID11 applied meta-learning to understand opponents, but this work doesn't address the policy improvement for the agent to be trained. We apply meta-learning to opponent modeling and policy learning separately while training the two meta-learners jointly. Then we use the meta-learners to initialize the model and policy for the new opponent. Experimental results show that the agent can adapt to the new opponent with a small number of interactions with the opponent. In the face of other agents, it is beneficial to build models for opponents and find a corresponding good policy. This method can be sample-inefficient since it costs many observations to build models and learn a policy then. We propose a method that can employ the information learned from experiences with other opponents to speed up the learning process for the current opponents. This method is suitable for many practical situations where the opponent population has a relative stable distribution over their policies. We apply meta-learning to jointly train the opponent modeling and policy improving process. Experimental results show that our method can be sample-efficient.",Our work applies meta-learning to multi-agent Reinforcement Learning to help our agent efficiently adapted to new coming opponents.,first ; Applying Reinforcement Learning ; two ; One ; second ; zero ; only one ; Model-Agnostic MetaLearning ; MAML,policy improving process ; a corresponding good policy ; carefully consideration ; more opponents ; a sample ; only one ; a task ; Things ; the policy improvement ; money,first ; Applying Reinforcement Learning ; two ; One ; second ; zero ; only one ; Model-Agnostic MetaLearning ; MAML,"To gain high rewards in muti-agent scenes, it is necessary to understand other agents and make corresponding optimal decisions. To get an accurate model, many observations are needed and the learned model and policy can overfit to current agents and cannot generalize if the other agents are replaced by new agents. In practical situations, each agent we face can be considered a sample from a population with a fixed but unknown distribution. Thus we can treat the task against some specific agents as a task sampled from a task distribution. We apply meta-learning method to build models and learn policies, and when new agents come, we can adapt to them",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Recent visual analytics systems make use of multiple machine learning models to better fit the data as opposed to traditional single, pre-defined model systems. However, while multi-model visual analytic systems can be effective, their added complexity poses usability concerns, as users are required to interact with the parameters of multiple models. Further, the advent of various model algorithms and associated hyperparameters creates an exhaustive model space to sample models from. This poses complexity to navigate this model space to find the right model for the data and the task. In this paper, we present Gaggle, a multi-model visual analytic system that enables users to interactively navigate the model space. Further translating user interactions into inferences, Gaggle simplifies working with multiple models by automatically finding the best model from the high-dimensional model space to support various user tasks. Through a qualitative user study, we show how our approach helps users to find a best model for a classification and ranking task. The study results confirm that Gaggle is intuitive and easy to use, supporting interactive model space navigation and automated model selection without requiring any technical expertise from users. Visual analytic (VA) techniques continue to leverage machine learning (ML) to provide people effective systems for gaining insights into data [27] . Systems such as Interaxis [36] help domain experts combine their knowledge and reasoning skills about a dataset or domain with the computational prowess of machine learning. These systems are traditionally designed with a pre-defined single ML model that has a carefully chosen learning algorithm and hyperparameter settings. Various combination of learning algorithms and hyperparameters give rise to a vast number of different model types (see Table 1 ). These different models constitute an exhaustive model space from which unique models can be sampled using a distinct combination of a learning algorithm and associated hyperparameters. For example, support vector machine (SVM) models have many options for kernel functions (i.e., linear, poly or radial) and hyperparameters (i.e., C (regularization parameter), γ (kernel coefficient), etc. ). When a model is correctly chosen for the phenomena, task, data distribution, or question users try to answer, existing VA techniques can effectively support users in exploration and analysis. However, in cases where the right model to use for a problem is not known a priori, one needs to navigate this model space to find a fitting model for the task or the problem. To combat this, recent VA systems use multiple ML models to support a diverse set of user tasks (e.g., Regression, Clustering , etc. [15, 21, 17, 63] ). For example, the VA system Clustervision [39] allows users to inspect multiple clustering models and select one based on quality and preference. Similarly, Snowcat [16] allows inspecting multiple ML models across a diverse set of tasks, such as classification, regression, time-series forecasting, etc. However, these multimodel systems are often more complex to use compared to single-model alternatives (e.g, in Clustervision users need to be well-versed with cluster model metrics and shown models.) We refer to this complex combination of parameter and hyperparameter settings as model space, as there are a large number of models that can be instantiated in this hyperdimensional space. Further, the interactive exploration of different parameter and hyperparameter combinations can be referred to as model space navigation. Our definition of model space is related to the work by Brown et al. [14] where they presented a tool called ModelSpace to analyze how the model parameters have changed over time during data exploration. In this paper we present Gaggle, a visual analytic tool that provides the user experience of a single-model system, yet leverages multiple models to support data exploration. Gaggle constructs multiple classification and ranking models, and then using a bayesian optimization hyperparameter selection technique, automatically finds a classification and ranking model for users to inspect, thus simplifying the search for an optimal model. Furthermore, our technique utilises simple user interactions for model space navigation to find the right model for the task. For example, users can drag data items into specific classes to record classification task's user preferences. Similarly, users can demonstrate that specific data items should be higher or lower in rank within a class by dragging them on top of each other. Gaggle uses ML to help users in data exploration or data structuring tasks, e.g, grouping data in self-defined categories, and ranking the members of the group based on their representativeness to the class. For example, a professor may want to use a tool to help categorize new student applications in different sets, and then rank the students in each set. Similarly, a salesman may want to cluster and rank potential clients in various groups. These problems fall under classification tasks in ML; however, unlike a conventional classification problem, our use case specifically supports interactive data exploration or data structuring, the models constructed are not meant to predict labels for unseen data items in future. Using this workflow, we expect our technique guards against possible model overfitting incurred due to adjusting the models confirm to specified user preferences. Furthermore, Gaggle addresses a common problem of datasets that either lack adequate ground truth, or do not have it [61, 72, 49] . To resolve this problem, Gaggle allows users to iteratively define classes and add labels. On each iteration, users add labels to data items and then build a classifier. We conducted a qualitative user study of Gaggle to collect user feedback on the system design and usability. The results of our study indicate that users found the workflow in Gaggle intuitive, and they were able to perform classification and ranking tasks effectively. Further, users confirmed that Gaggle incorporated their feedback into the interactive model space navigation technique to find the right model for the task. Overall, the contributions of this paper include: • A model space navigation technique facilitated by a Bayesian optimization hyperparameter tuning and automated model selection approach. • A VA tool Gaggle, that allows interactive model space navigation supporting classification and ranking tasks using simple demonstration-based user interactions. • The results of a user study testing Gaggle's effectiveness. Large Model Search Space: Searching models by combining different learning algorithms and hyperparameters leads to an extremely large search space. As a result, a small set of constraints on the search process would not sufficiently reduce the space, leading to a large number of sub-constrained and ill-defined solutions. Thus, how many interactions are considered optimal for a given model space? In this work, we approached this challenge by using Bayesian optimization for ranking models. However, larger search spaces may pose scalability issues while too many user constraints may ""over-constrain"" models leading to poor results. In this paper, we present an interactive model space navigation approach for helping people perform classification and ranking tasks. Current VA techniques rely on a pre-selected model for a designated task or problem. However, these systems may fail if the selected model does not suit the task or the user's goals. As a solution, our technique helps users find a model suited to their goals by interactively navigating the high-dimensional model space. Using this approach, we prototyped Gaggle, a VA system to facilitate classification and ranking of data items. Further, with a qualitative user study, we collected and analyzed user feedback to understand the usability and effectiveness of Gaggle. The study results show that users agree that Gaggle is easy to use, intuitive, and helps them interactively navigate the model space to find an optimal classification and ranking model.","Gaggle, an interactive visual analytic system to help users interactively navigate model space for classification and ranking tasks.",ModelSpace ; Snowcat ; Gaggle ; Brown ; e.g ; linear ; Bayesian ; one ; SVM ; Large Model Search,user feedback ; our technique guards ; self-defined categories ; this model space ; a classification ; the user's goals ; their knowledge ; Snowcat ; a multi-model visual analytic system ; single-model alternatives,ModelSpace ; Snowcat ; Gaggle ; Brown ; e.g ; linear ; Bayesian ; one ; SVM ; Large Model Search,"Multi-model visual analytic systems can be effective, but their added complexity creates an exhaustive model space to sample models from. Additionally, the advent of various model algorithms and associated hyperparameters creates a unique model space for sample models. This creates complexity to navigate this model space, making it difficult to find the right model for the data and the task. Gaggle, a multi-model graphical analytic system, enables users to interactively navigate the model space by automatically finding the best model from the high-dimensional model space. This approach simplifies working with multiple models, simplifies user interactions, and enables automated model selection. ",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Deep latent variable models have become a popular model choice due to the scalable learning algorithms introduced by (Kingma & Welling 2013, Rezende et al. 2014). These approaches maximize a variational lower bound on the intractable log likelihood of the observed data. Burda et al. (2015) introduced a multi-sample variational bound, IWAE, that is at least as tight as the standard variational lower bound and becomes increasingly tight as the number of samples increases. Counterintuitively, the typical inference network gradient estimator for the IWAE bound performs poorly as the number of samples increases (Rainforth et al. 2018, Le et al. 2018). Roeder et a. (2017) propose an improved gradient estimator, however, are unable to show it is unbiased. We show that it is in fact biased and that the bias can be estimated efficiently with a second application of the reparameterization trick. The doubly reparameterized gradient (DReG) estimator does not suffer as the number of samples increases, resolving the previously raised issues. The same idea can be used to improve many recently introduced training techniques for latent variable models. In particular, we show that this estimator reduces the variance of the IWAE gradient, the reweighted wake-sleep update (RWS) (Bornschein & Bengio 2014), and the jackknife variational inference (JVI) gradient (Nowozin 2018). Finally, we show that this computationally efficient, drop-in estimator translates to improved performance for all three objectives on several modeling tasks. Following the influential work by BID20 BID30 , deep generative models with latent variables have been widely used to model data such as natural images BID29 BID14 , speech and music time-series BID8 BID11 BID22 , and video BID1 BID15 BID9 . The power of these models lies in combining learned nonlinear function approximators with a principled probabilistic approach, resulting in expressive models that can capture complex distributions. Unfortunately, the nonlinearities that empower these model also make marginalizing the latent variables intractable, rendering direct maximum likelihood training inapplicable. Instead of directly maximizing the marginal likelihood, a common approach is to maximize a tractable lower bound on the likelihood such as the variational evidence lower bound (ELBO) BID19 BID3 . The tightness of the bound is determined by the expressiveness of the variational family. For tractability, a factorized variational family is commonly used, which can cause the learned model to be overly simplistic. BID5 introduced a multi-sample bound, IWAE, that is at least as tight as the ELBO and becomes increasingly tight as the number of samples increases. Counterintuitively, although the bound is tighter, BID28 theoretically and empirically showed that the standard inference network gradient estimator for the IWAE bound performs poorly as the number of samples increases due to a diminishing signal-to-noise ratio (SNR). This motivates the search for novel gradient estimators. BID31 proposed a lower-variance estimator of the gradient of the IWAE bound. They speculated that their estimator was unbiased, however, were unable to prove the claim. We show that it is in fact biased, but that it is possible to construct an unbiased estimator with a second application of the reparameterization trick which we call the IWAE doubly reparameterized gradient (DReG) estimator. Our estimator is an unbiased, computationally efficient drop-in replacement, and does not suffer as the number of samples increases, resolving the counterintuitive behavior from previous work BID28 . Furthermore, our insight is applicable to alternative multisample training techniques for latent variable models: reweighted wake-sleep (RWS) BID4 and jackknife variational inference (JVI) BID27 .In this work, we derive DReG estimators for IWAE, RWS, and JVI and demonstrate improved scaling with the number of samples on a simple example. Then , we evaluate DReG estimators on MNIST generative modeling, Omniglot generative modeling, and MNIST structured prediction tasks. In all cases, we demonstrate substantial unbiased variance reduction, which translates to improved performance over the original estimators. In this work, we introduce doubly reparameterized estimators for the updates in IWAE, RWS, and JVI. We demonstrate that across tasks they provide unbiased variance reduction, which leads to improved performance. Furthermore, DReG estimators have the same computational cost as the original estimators. As a result, we recommend that DReG estimators be used instead of the typical gradient estimators.Variational Sequential Monte Carlo BID24 BID26 and Neural Adapative Sequential Monte Carlo BID13 extend IWAE and RWS to sequential latent variable models, respectively. It would be interesting to develop DReG estimators for these approaches as well.We found that a convex combination of IWAE-DReG and RWS-DReG performed best, however, the weighting was task dependent. In future work, we intend to apply ideas from BID2 to automatically adapt the weighting based on the data.Finally, the form of the IWAE-DReG estimator (Eq. 7) is surprisingly simple and suggests that there may be a more direct derivation that is applicable to general MCOs.",Doubly reparameterized gradient estimators provide unbiased variance reduction which leads to improved performance.,three ; Le ; JVI ; RWS ; RWS-DReG ; Eq ; second ; Kingma & Welling ; et a. ; ELBO,music time-series ; a lower-variance estimator ; Rezende et al ; complex distributions ; the updates ; data ; the IWAE gradient ; three ; Rainforth ; We,three ; Le ; JVI ; RWS ; RWS-DReG ; Eq ; second ; Kingma & Welling ; et a. ; ELBO,"Deep latent variable models have become a popular model choice due to scalable learning algorithms. These approaches maximize a variational lower bound on the intractable log likelihood of observed data. Burda et al. (2015) introduced a multi-sample variational bound, IWAE, that is at least as tight as the standard variational upper bound and becomes tighter as the number of samples increases. However, the typical inference network gradient estimator does not perform as well as the typical one, resulting in improved performance for all three objectives on several modeling tasks. This approach can be used to improve many recently introduced training techniques for latent",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Training conditional generative latent-variable models is challenging in scenarios where the conditioning signal is very strong and the decoder is expressive enough to generate a plausible output given only the condition; the generative model tends to ignore the latent variable, suffering from posterior collapse.  We find, and empirically show, that one of the major reasons behind posterior collapse is rooted in the way that generative models are conditioned, i.e., through concatenation of the latent variable and the condition . To mitigate this problem, we propose to explicitly make the latent variables depend on the condition by unifying the conditioning and latent variable sampling, thus coupling them so as to prevent the model from discarding the root of variations . To achieve this, we develop a conditional Variational Autoencoder architecture that learns a distribution not only of the latent variables, but also of the condition, the latter acting as prior on the former . Our experiments on the challenging tasks of conditional human motion prediction and image captioning demonstrate the effectiveness of our approach at avoiding posterior collapse . Video results of our approach are anonymously provided in http://bit.ly/iclr2020",We propose a conditional variational autoencoder framework that mitigates the posterior collapse in scenarios where the conditioning signal strong enough for an expressive decoder to generate a plausible output from it.,,the model ; the decoder ; the condition ; this ; variations ; posterior collapse ; scenarios ; them ; this problem ; only the condition,,"Training conditional generative latent-variable models is challenging in scenarios where the conditioning signal is strong and the decoder is expressive enough to generate plausible output given only the condition. The generative model tends to ignore the latent variable, suffering from posterior collapse. To mitigate this issue, we develop a conditional Variational Autoencoder architecture that learns a distribution not only of latent variables, but also of the condition, the latter acting as prior on the former. Our experiments on the challenging tasks of conditional human motion prediction and image captioning demonstrate the effectiveness of our approach at avoiding posterior collapse, and the video results are anonymously provided in http",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Electronic Health Records (EHR) comprise of longitudinal clinical observations portrayed with sparsity, irregularity, and high-dimensionality which become the major obstacles in drawing reliable downstream outcome. Despite greatly numbers of imputation methods are being proposed to tackle these issues, most of the existing methods ignore correlated features or temporal dynamics and entirely put aside the uncertainty. In particular, since the missing values estimates have the risk of being imprecise, it motivates us to pay attention to reliable and less certain information differently. In this work, we propose a novel variational-recurrent imputation network (V-RIN), which unified imputation and prediction network, by taking into account the correlated features, temporal dynamics, and further utilizing the uncertainty to alleviate the risk of biased missing values estimates. Specifically, we leverage the deep generative model to estimate the missing values based on the distribution among variables and a recurrent imputation network to exploit the temporal relations in conjunction with utilization of the uncertainty. We validated the effectiveness of our proposed model with publicly available real-world EHR dataset, PhysioNet Challenge 2012, and compared the results with other state-of-the-art competing methods in the literature. Electronic Health Records (EHR) store longitudinal data comprising of patient's clinical observations in the intensive care unit (ICU). Despite the surge of interest in clinical research on EHR, it still holds diverse challenging issues to be tackled with, such as high-dimensionality, temporality, sparsity, irregularity, and bias (Cheng et al., 2016; Lipton et al., 2016; Yadav et al., 2018; Shukla & Marlin, 2019) . Specifically, sequences of the medical events are recorded irregularly in terms of variables and time, due to various reasons such as lack of collection or documentation, or even recording fault (Wells et al., 2013; Cheng et al., 2016) . In fact, since it carries essential information regarding the patient's health status, improper handling of missing values might draw an unintentional bias (Wells et al., 2013; Beaulieu-Jones et al., 2017) yielding unreliable downstream analysis and verdict. Complete-case analysis is one approach to draw the clinical outcome by disregarding the missing values and relying only on the observed values. However, excluding the missing data shows poor performance at high missing rates and also requires modeling separately for different dataset. In fact, the missing values and their patterns are correlated with the target labels (Che et al., 2018) . Thus, we resort to the imputation approach to improve clinical outcomes prediction as the downstream task. There exist numerous proposed strategies in imputing missing values in the literature. Brick & Kalton (1996) classified the imputation methods of being deterministic or stochastic in terms of the utilization of the randomness. While deterministic methods such as mean (Little & Rubin, 1987) and median filling (Acuña & Rodriguez, 2004) produced only one possible value, it is desirable to generate samples by considering the data distribution, thus leading to stochastic-based imputation methods. Moreover, since we are dealing with multivariate time series, an adequate imputation model should reflect several properties altogether, namely, 1) temporal relations, 2) correlations across variables, and additionally 3) offering a probabilistic interpretation for uncertainty estimation (Fortuin et al., 2019) . Recently, the rise of the deep learning models offers potential solutions in accommodating aforementioned conditions. Variational autoencoders (VAEs) (Kingma & Welling, 2014) and generative adversarial networks (GANs) (Goodfellow et al., 2014) exploited the latent distribution of highdimensional incomplete data and generated comparable data points as the approximation estimates for the missing or corrupted values (Nazabal et al., 2018; Luo et al., 2018; Jun et al., 2019) . However, even though these models employed the stochastic approach in inferring and generating samples, they scarcely utilized the uncertainty. In addition, such deep generative models are insufficient in estimating the missing values of multivariate time series, due to their nature of ignoring temporal relations between a span of time points. Hence, it requires additional approaches to model the temporal dynamics, such as Gaussian process (Fortuin et al., 2019) or recurrent neural network (RNNs) (Luo et al., 2018; Jun et al., 2019) . On the other hand, by the virtue of RNNs which have proved a remarkable performance in modeling the sequential data, we can estimate the complete data by taking into account the temporal characteristics. GRU-D (Che et al., 2018) proposed a modified gated-recurrent unit (GRU) cell to model missing patterns in the form of masking vector and temporal delay. Likewise, BRITS (Cao et al., 2018) modeled the temporal relations by bi-directional dynamics, and also considered features correlation by regression layers in estimating the missing values. However, they didn't take into account the uncertainty in estimating the missing values. That is, since the imputation estimates are not thoroughly accurate, we may introduce their fidelity score denoted by the uncertainty, which enhances the task performance by emphasizing the reliable or less uncertain information and vice versa (He, 2010; Gemmeke et al., 2010; Jun et al., 2019) . In this work, we define our primary task as prediction of in-hospital mortality on EHR data. However, since the data are characterized by sparse and irregularly-sampled, we devise an effective imputation model as the secondary problem but major concern in this work. We propose a novel variational-recurrent imputation network (V-RIN), which unified imputation and prediction network for multivariate time series EHR data, governing both correlations among variables and temporal relations. Specifically, given the sparse data, an inference network of VAE is employed to capture data distribution in the latent space. From this, we employ a generative network to obtain the reconstructed data as the imputation estimates for the missing values as well as the uncertainty indicating the imputation fidelity score. Then, we integrate the temporal and feature correlations into a combined vector and feed it into a novel uncertainty-aware GRU in the recurrent imputation network. Finally, we obtain the mortality prediction as a clinical verdict from the complete imputed data. In general, our main contributions in this paper are as follows: • We estimate the missing values by utilizing deep generative model combined with recurrent imputation network to capture both features correlations and the temporal dynamics jointly, yielding the uncertainty. • We effectively incorporate the uncertainty with the imputation estimates in our novel uncertainty-aware GRU cell for better prediction result. • We evaluated the effectiveness of the proposed models by training the imputation and prediction networks jointly using the end-to-end manner, achieving the superior performance among other state-of-the-art competing methods on real-world multivariate time series EHR data. In this paper, we proposed a novel unified framework comprising of imputation and prediction network for sparse high-dimensional multivariate time series. It combined deep generative model with recurrent model to capture features correlations and temporal relations in estimating the missing values and yielding uncertainty. We utilized the uncertainties as the fidelity of our estimation and incorporated them for clinical outcome prediction. We evaluated the effectiveness of proposed model with PhysioNet 2012 Challenge dataset as the real-world EHR multivariate time series data, proving the superiority of our model in the in-mortality prediction task, compared to other state-of-the-art comparative models in the literature.","Our variational-recurrent imputation network (V-RIN) takes into account the correlated features, temporal dynamics, and further utilizes the uncertainty to alleviate the risk of biased missing values estimates.",Yadav ; Luo et al. ; Lipton ; Cheng et al. ; Wells et al. ; Che et al. ; Electronic Health Records ; al. ; Acuña & Rodriguez ; Nazabal,the utilization ; Luo et al ; features ; multivariate time series ; the complete imputed data ; clinical outcomes prediction ; collection ; Brick & Kalton ; the distribution ; the results,Yadav ; Luo et al. ; Lipton ; Cheng et al. ; Wells et al. ; Che et al. ; Electronic Health Records ; al. ; Acuña & Rodriguez ; Nazabal,"EHR comprises longitudinal clinical observations portrayed with sparsity, irregularity, and high-dimensionality which become the major obstacles in drawing reliable downstream outcome. Despite greatly numbers of imputation methods being proposed to tackle these issues, most of the existing methods ignore correlated features or temporal dynamics and entirely put aside the uncertainty. This motivates us to pay attention to reliable and less certain information differently. In this work, we propose a novel variational-recurrent imputation network (V-RIN), which unified imputation and prediction network, by taking into account correlated features, temporal dynamics, and further utilizing the uncertainty to alleviate the risk",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to down-stream tasks, several model compression techniques on pre-trained language representations have been proposed (Sun et al., 2019; Sanh, 2019). However, surprisingly,  the simple baseline of just pre-training and fine-tuning compact models has been overlooked. In this paper, we first show that pre-training remains important in the context of smaller architectures, and fine-tuning pre-trained compact models can be competitive to more elaborate methods proposed in concurrent work. Starting with pre-trained compact models, we then explore transferring task knowledge from large fine-tuned models through standard knowledge distillation. The resulting simple, yet effective and general algorithm, Pre-trained Distillation, brings further improvements. Through extensive experiments, we more generally explore the interaction between pre-training and distillation under two variables that have been under-studied: model size and properties of unlabeled task data. One surprising observation is that they have a compound effect even when sequentially applied on the same data. To accelerate future research, we will make our 24 pre-trained miniature BERT models publicly available. Self-supervised learning on a general-domain text corpus followed by end-task learning is the twostaged training approach that enabled deep-and-wide Transformer-based networks (Vaswani et al., 2017) to advance language understanding (Devlin et al., 2018; Yang et al., 2019b; Sun et al., 2019b; . However, state-of-the-art models have hundreds of millions of parameters, incurring a high computational cost. Our goal is to realize their gains under a restricted memory and latency budget. We seek a training method that is well-performing, general and simple and can leverage additional resources such as unlabeled task data. Before considering compression techniques, we start with the following research question: Could we directly train small models using the same two-staged approach? In other words, we explore the idea of applying language model (LM) pre-training and task fine-tuning to compact architectures directly. This simple baseline has so far been overlooked by the NLP community, potentially based on an underlying assumption that the limited capacity of compact models is capitalized better when focusing on the end task rather than a general language model objective. Concurrent work to ours proposes variations of the standard pre-training+fine-tuning procedure, but with limited generality (Sun et al., 2019a; Sanh, 2019) . We make the surprising finding that pre-training+fine-tuning in its original formulation is a competitive method for building compact models. For further gains, we additionally leverage knowledge distillation (Hinton et al., 2015) , the standard technique for model compression. A compact student is trained to recover the predictions of a highly accurate teacher. In addition to the posited regularization effect of these soft labels (Hinton et al., 2015) , distillation provides a means of producing pseudo-labels for unlabeled data. By regarding LM pre-training of compact models as a student initialization strategy, we can take advantage of both methods. The resulting algorithm is a sequence of three standard training operations: masked LM (MLM) pre-training (Devlin et al., 2018) , task-specific distillation, and optional fine-tuning. From here on, we will refer to it as Pre-trained Distillation (PD) ( Figure 1 ). As we will show in Get loss L ← − y P Ω (y|x) log P θ (y|x) In a controlled study following data and model architecture settings in concurrent work (Section 4), we show that Pre-trained Distillation outperforms or is competitive with more elaborate approaches which use either more sophisticated distillation of task knowledge (Sun et al., 2019a) or more sophisticated pre-training from unlabeled text (Sanh, 2019) . The former distill task knowledge from intermediate teacher activations, starting with a heuristically initialized student. The latter fine-tune a compact model that is pre-trained on unlabeled text with the help of a larger LM teacher. One of the most noteworthy contributions of our paper are the extensive experiments that examine how Pre-trained Distillation and its baselines perform under various conditions. We investigate two axes that have been under-studied in previous work: model size and amount/quality of unlabeled data. While experimenting with 24 models of various sizes (4m to 110m parameters) and depth/width trade-offs, we observe that pre-trained students can leverage depth much better than width; in contrast, this property is not visible for randomly-initialized models. For the second axis, we vary the amount of unlabeled data, as well as its similarity to the labeled set. Interestingly, Pretrained Distillation is more robust to these variations in the transfer set than standard distillation. Finally, in order to gain insight into the interaction between LM pre-training and task-specific distillation, we sequentially apply these operations on the same dataset. In this experiment, chaining the two operations performs better than any one of them applied in isolation, despite the fact that a single dataset was used for both steps. This compounding effect is surprising, indicating that pre-training and distillation are learning complementary aspects of the data. Given the effectiveness of LM pre-training on compact architectures, we will make our 24 pretrained miniature BERT models publicly available in order to accelerate future research.",Studies how self-supervised learning and knowledge distillation interact in the context of building compact models.,two ; first ; NLP ; hundreds of millions ; Devlin et al. ; Transformer ; Sanh ; BERT ; P θ ; Sun,the fact ; Pretrained Distillation ; Pre-trained Distillation ; architectures ; pre-trained language representations ; al. ; we ; these soft labels ; pre-trained students ; a general-domain text corpus,two ; first ; NLP ; hundreds of millions ; Devlin et al. ; Transformer ; Sanh ; BERT ; P θ ; Sun,"Natural language representations have been dominated by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Several model compression techniques have been proposed. However, surprisingly, the simple baseline of just Pre-training and fine-tuning compact models has been overlooked by the NLP community, potentially leading to more elaborate methods proposed in concurrent work. To accelerate future research, we will make our 24 pre-trained miniature BERT models publicly available. This approach involves learning small models using the same two-staged training approach, which enables deep-and-wide Transformer-based networks to advance",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Human perception of 3D shapes goes beyond reconstructing them as a set of points or a composition of geometric primitives: we also effortlessly understand higher-level shape structure such as the repetition and reflective symmetry of object parts. In contrast, recent advances in 3D shape sensing focus more on low-level geometry but less on these higher-level relationships. In this paper, we propose 3D shape programs, integrating bottom-up recognition systems with top-down, symbolic program structure to capture both low-level geometry and high-level structural priors for 3D shapes. Because there are no annotations of shape programs for real shapes, we develop neural modules that not only learn to infer 3D shape programs from raw, unannotated shapes, but also to execute these programs for shape reconstruction. After initial bootstrapping, our end-to-end differentiable model learns 3D shape programs by reconstructing shapes in a self-supervised manner. Experiments demonstrate that our model accurately infers and executes 3D shape programs for highly complex shapes from various categories. It can also be integrated with an image-to-shape module to infer 3D shape programs directly from an RGB image, leading to 3D shape reconstructions that are both more accurate and more physically plausible. Given the table in Figure 1 , humans are able to instantly recognize its parts and regularities: there exist sharp edges, smooth surfaces, a table top that is a perfect circle, and two lower, square layers. Beyond these basic components, we also perceive higher-level, abstract concepts: the shape is bilateral symmetric; the legs are all of equal length and laid out on the opposite positions of a 2D grid. Knowledge like this is crucial for visual recognition and reasoning (Koffka, 2013; Dilks et al., 2011) .Recent AI systems for 3D shape understanding have made impressive progress on shape classification, parsing, reconstruction, and completion BID9 BID19 , many making use of large shape repositories like ShapeNet (Chang et al., 2015) . Popular shape representations include voxels BID23 , point clouds BID9 , and meshes BID21 . While each has its own advantages, these methods fall short on capturing the strong shape priors we just described, such as sharp edges and smooth surfaces.A few recent papers have studied modeling 3D shapes as a collection of primitives BID19 , with simple operations such as addition and subtraction BID15 . These representations have demonstrated success in explaining complex 3D shapes. In this paper, we go beyond them to capture the high-level regularity within a 3D shape, such as symmetry and repetition.In this paper, we propose to represent 3D shapes as shape programs. We define a domain-specific language (DSL) for shapes, containing both basic shape primitives for parts with their geometric and semantic attributes, as well as statements such as loops to enforce higher-level structural priors.Inverse procedural graphics. The problem of inferring programs from voxels is closely related to inverse procedural graphics, where a procedural graphics program is inferred from an image or declarative specification BID11 Št'ava et al., 2010) . Where the systems have been most successful, however, are when they leverage a large shape-component library (Chaudhuri et al., 2011; BID14 or are applied to a sparse solution space BID20 . Kulkarni et al. (2015a) approached the problem of inverse graphics as inference in a probabilistic program for generating 2D images, or image contours, from an underlying 3D model. They demonstrated results on several different applications using parametric generative models for faces, bodies, and simple multi-part objects based on generalized cylinders. In this work, we extend the idea of inverse procedural graphics to 3-D voxel representations, and show how this idea can apply to large data sets like ShapeNet. We furthermore do not have to match components to a library of possible shapes, instead using a neural network to directly infer shapes and their parameters.A few recent papers have explored the use of simple geometric primitives to describe shapes BID19 Zou et al., 2017; , putting the classic idea of generalized cylinders BID12 Binford, 1971) or geons (Biederman, 1987) in the modern context of deep learning. In particular, BID15 extended these papers and addressed the problem of inferring 3-D CAD programs from perceptual input. We find this work inspiring, but also feel that a key goal of 3-D program inference is to reconstruct a program in terms of semantically meaningful parts and their spatial regularity, which we address here. Some other graphics papers also explore regularity, but without using programs BID5 BID24 BID7 .Work in the HCI community has also addressed the problem of inferring parametric graphics primitives from perceptual input. For example, BID6 proposed to learn to instantiate procedural primitives for an interactive modeling system. In our work, we instead learn to instantiate multiple procedural graphics primitives simultaneously, without assistance from a human user. The domain specific language (DSL) for 3D shapes. Semantics depends on the types of objects that are modeled, i.e., semantics for vehicle and furniture should be different. For details of DSL in our experimental setting, please refer to supplementary.Program synthesis. In the AI literature, Ellis et al. (2018) leveraged symbolic program synthesis techniques to infer 2D graphics programs from images, extending their earlier work by using neural nets for faster inference of low-level cues such as strokes (Ellis et al., 2015) . Here, we show how a purely end-to-end network can recover 3D graphics programs from voxels, conceptually relevant to RobustFill (Devlin et al., 2017) , which presents a purely end-to-end neural program synthesizer for text editing. The very recent SPIRAL system (Ganin et al., 2018) also takes as its goal to learn structured program-like models from (2D) images. An important distinction from our work here is that SPIRAL explains an image in terms of paint-like ""brush strokes"", whereas we explain 3D voxels in terms of high-level objects and semantically meaningful parts of objects, like legs or tops. Learning to execute programs. Neural Program Interpreters (NPI) have been extensively studied for programs that abstract and execute tasks such as sorting, shape manipulation, and grade-school arithmetic BID10 Cai et al., 2017; Bošnjak et al., 2017) . In NPI BID10 , the key insight is that a program execution trace can be decomposed into predefined operations that are more primitive; and at each step, an NPI learns to predict what operation to take next depending on the general environment, domain specific state , and previous actions. Cai et al. (2017) improved the generalization of NPIs by adding recursion . Johnson et al. (2017) learned to execute programs for visual question and answering. In this paper, we also learn a 3D shape program executor that renders 3D shapes from programs as a component of our model. We have introduced 3D shape programs as a new shape representation. We have also proposed a model for inferring shape programs, which combines a neural program synthesizer and a neural executor. Experiments on ShapeNet show that our model successfully explains shapes as programs and generalizes to shapes outside training categories. Further experiments on Pix3D show our model can be extended to infer shape programs and reconstruct 3D shapes directly from color images. We now discuss key design choices and future work.Analyzing the neural program executor. We look deep into the intermediate representation of the neural program executor, which is a 64-dimensional vector output by the LSTM (see Figure 3) . We manipulate individual dimensions and visualize the generated voxels. FIG5 shows that these dimensions capture interpretable geometric features (e.g., height, radius, and number of repetitions).Design of the DSL. Our design of the DSL for shape programs makes certain semantic commitments. A DSL with these semantics has advantages and disadvantages: it naturally supports semantic correspondences across shapes and enables better in-class reconstructions; on the other hand, it may limit the ability to generalize to shapes outside training classes. Our current instantiation focuses on the semantics of furniture (a superclass, whose subclasses share similar semantics). Within this superclass, our model generalizes well: trained on chairs and tables, it generalizes to new furniture categories such as beds. In future work , we are interested in learning a library of shape primitives directly from data, which will allow our approach to adapt automatically to new superclasses or domains of shape.Structure search vs. amortized inference. For our program synthesis task, we use neural nets for amortized inference rather than structure search, due to the large search space and our desire to return a shape interpretation nearly instantaneously, effectively trading neural net training time for fast inference at test time. Our model takes 5 ms to infer a shape program with a Titan X GPU. We also considered various possible approaches for structured search over the space of shape programs, but decided that these would most likely be too our slow for our goals. One approach to structured search is constraint solving. Ellis et al. (2015) used the performant Z3 SMT solver (De Moura & Bjørner, 2008) to infer 2D graphics programs, taking 5-20 minutes for problems arguably simpler than our 3D shape programs. Other approaches could be based on stochastic search, such as MCMC in the space of programs. For the related problem of inverse graphics from 2D images, MCMC, like constraint solving, takes too long for perception at a glance BID1 . Efficient integration of discrete search and amortized inference, however, is a promising future research direction.","We propose 3D shape programs, a structured, compositional shape representation. Our model learns to infer and execute shape programs to explain 3D shapes.",Ellis ; Dilks et al. ; RGB ; Titan ; AI ; Chaudhuri ; Bošnjak ; Chang et al. ; ShapeNet ; Ganin,"the systems ; The domain specific language ; top-down, symbolic program structure ; faces ; a neural executor ; higher-level structural priors ; Chang et al ; RobustFill ; an RGB image ; two lower, square layers",Ellis ; Dilks et al. ; RGB ; Titan ; AI ; Chaudhuri ; Bošnjak ; Chang et al. ; ShapeNet ; Ganin,"Human perception of 3D shapes goes beyond reconstructing them as a set of points or a composition of geometric primitives. We also effortlessly understand higher-level structures such as repetition and reflective symmetry of object parts. However, recent advances in 3D shape sensing focus more on low-level geometry and high-level structural priors. In this paper, we introduce bottom-up recognition systems, integrating top-down, symbolic program structure to capture both lower-level and high level structures. This approach is crucial for visual recognition and reasoning, as well as the ability to understand more complex shapes like sharp edges and smooth surfaces. In addition",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Recent efforts in Dialogue State Tracking (DST) for task-oriented dialogues have progressed toward open-vocabulary or generation-based approaches where the models can generate slot value candidates from the dialogue history itself. These approaches have shown good performance gain, especially in complicated dialogue domains with dynamic slot values. However, they fall short in two aspects: (1) they do not allow models to explicitly learn signals across domains and slots to detect potential dependencies among \textit{(domain, slot)} pairs; and (2) existing models follow auto-regressive approaches which incur high time cost when the dialogue evolves over multiple domains and multiple turns. In this paper, we propose a novel framework of Non-Autoregressive Dialog State Tracking (NADST) which can factor in potential dependencies among domains and slots to optimize the models towards better prediction of dialogue states as a complete set rather than separate slots. In particular, the non-autoregressive nature of our method not only enables decoding in parallel to significantly reduce the latency of DST for real-time dialogue response generation, but also detect dependencies among slots at token level in addition to slot and domain level. Our empirical results show that our model achieves the state-of-the-art joint accuracy across all domains on the MultiWOZ 2.1 corpus, and the latency of our model is an order of magnitude lower than the previous state of the art as the dialogue history extends over time. In task-oriented dialogues, a dialogue agent is required to assist humans for one or many tasks such as finding a restaurant and booking a hotel. As a sample dialogue shown in Table 1 , each user utterance typically contains important information identified as slots related to a dialogue domain such as attraction-area and train-day. A crucial part of a task-oriented dialogue system is Dialogue State Tracking (DST), which aims to identify user goals expressed during a conversation in the form of dialogue states. A dialogue state consists of a set of (slot, value) pairs e.g. (attraction-area, centre) and (train-day, tuesday). Existing DST models can be categorized into two types: fixed-and open-vocabulary. Fixed vocabulary models assume known slot ontology and generate a score for each candidate of (slot,value) Lee et al., 2019) . Recent approaches propose open-vocabulary models that can generate the candidates, especially for slots such as entity names and time, from the dialogue history (Lei et al., 2018; Wu et al., 2019) . Most open-vocabulary DST models rely on autoregressive encoders and decoders, which encode dialogue history sequentially and generate token t i of individual slot value one by one conditioned on all previously generated tokens t [1:i−1] . For downstream tasks of DST that emphasize on low latency (e.g. generating real-time dialogue responses), auto-regressive approaches incur expensive time cost as the ongoing dialogues become more complex. The time cost is caused by two major components: length of dialogue history i.e. number of turns, and length of slot values. For complex dialogues extended over many turns and multiple domains, the time cost will increase significantly in both encoding and decoding phases. Similar problems can be seen in the field of Neural Machine Translation (NMT) research where a long piece of text is translated from one language to another. Recent work has tried to improve the We proposed NADST, a novel Non-Autoregressive neural architecture for DST that allows the model to explicitly learn dependencies at both slot-level and token-level to improve the joint accuracy rather than just individual slot accuracy. Our approach also enables fast decoding of dialogue states by adopting a parallel decoding strategy in decoding components. Our extensive experiments on the well-known MultiWOZ corpus for large-scale multi-domain dialogue systems benchmark show that our NADST model achieved the state-of-the-art accuracy results for DST tasks, while enjoying a substantially low inference latency which is an order of magnitude lower than the prior work. A APPENDIX","We propose the first non-autoregressive neural model for Dialogue State Tracking (DST), achieving the SOTA accuracy (49.04%) on MultiWOZ2.1 benchmark, and reducing inference latency by an order of magnitude.",Lee et al. ; DST ; al. ; train-day ; Non-Autoregressive ; APPENDIX ; Non-Autoregressive Dialog ; Dialogue State Tracking ; two ; Lei et al.,an order ; Existing DST models ; the dialogue history ; humans ; length ; our model ; a complete set ; Lee et al. ; domains ; auto-regressive approaches,Lee et al. ; DST ; al. ; train-day ; Non-Autoregressive ; APPENDIX ; Non-Autoregressive Dialog ; Dialogue State Tracking ; two ; Lei et al.,"Dialogue State Tracking (DST) for task-oriented dialogues have progressed toward open-vocabulary or generation-based approaches where the models can generate slot value candidates from the dialogue history itself. However, they fall short in two aspects: (1) they do not allow models to explicitly learn signals across domains and slots to detect potential dependencies among \textit{(domain, slot)} pairs; and (2) existing models follow auto-regressive approaches which incur high time cost when the dialogue evolves over multiple domains and multiple turns. In this paper, we propose a novel framework of Non-Autoregressive Dialog State",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Learning when to communicate and doing that effectively is essential in multi-agent tasks. Recent works show that continuous communication allows efficient training with back-propagation in multi-agent scenarios, but have been restricted to fully-cooperative tasks. In this paper, we present Individualized Controlled Continuous Communication Model (IC3Net) which has better training efficiency than simple continuous communication model, and can be applied to semi-cooperative and competitive settings along with the cooperative settings. IC3Net controls continuous communication with a gating mechanism and uses individualized rewards foreach agent to gain better performance and scalability while fixing credit assignment issues. Using variety of tasks including StarCraft BroodWars explore and combat scenarios, we show that our network yields improved performance and convergence rates than the baselines as the scale increases. Our results convey that IC3Net agents learn when to communicate based on the scenario and profitability. Communication is an essential element of intelligence as it helps in learning from others experience, work better in teams and pass down knowledge. In multi-agent settings, communication allows agents to cooperate towards common goals. Particularly in partially observable environments, when the agents are observing different parts of the environment, they can share information and learnings from their observation through communication.Recently, there have been a lot of success in the field of reinforcement learning (RL) in playing Atari Games BID22 to playing Go BID28 , most of which have been limited to the single agent domain. However, the number of systems and applications having multi-agents have been growing BID16 BID23 ; where size can be from a team of robots working in manufacturing plants to a network of self-driving cars. Thus, it is crucial to successfully scale RL to multi-agent environments in order to build intelligent systems capable of higher productivity. Furthermore, scenarios other than cooperative, namely semi-cooperative (or mixed) and competitive scenarios have not even been studied as extensively for multi-agent systems.The mixed scenarios can be compared to most of the real life scenarios as humans are cooperative but not fully-cooperative in nature. Humans work towards their individual goals while cooperating with each other. In competitive scenarios, agents are essentially competing with each other for better rewards. In real life, humans always have an option to communicate but can choose when to actually communicate. For example, in a sports match two teams which can communicate, can choose to not communicate at all (to prevent sharing strategies) or use dishonest signaling (to misdirect opponents) BID18 in order to optimize their own reward and handicap opponents; making it important to learn when to communicate.Teaching agents how to communicate makes it is unnecessary to hand code the communication protocol with expert knowledge BID29 BID14 . While the content of communication is important, it is also important to know when to communicate either to increase scalability and performance or to increase competitive edge. For example, a prey needs to learn when to communicate to avoid communicating its location with predators. BID29 showed that agents communicating through a continuous vector are easier to train and have a higher information throughput than communication based on discrete symbols. Their continuous communication is differentiable, so it can be trained efficiently with back-propagation. However, their model assumes full-cooperation between agents and uses average global rewards. This restricts the model from being used in mixed or competitive scenarios as full-cooperation involves sharing hidden states to everyone; exposing everything and leading to poor performance by all agents as shown by our results. Furthermore, the average global reward for all agents makes the credit assignment problem even harder and difficult to scale as agents don't know their individual contributions in mixed or competitive scenarios where they want themselves to succeed before others.To solve above mentioned issues, we make the following contributions:1. We propose Individualized Controlled Continuous Communication Model (IC3Net), in which each agent is trained with its individualized reward and can be applied to any scenario whether cooperative or not. 2. We empirically show that based on the given scenario-using the gating mechanism-our model can learn when to communicate. The gating mechanism allows agents to block their communication; which is useful in competitive scenarios. 3. We conduct experiments on different scales in three chosen environments including StarCraft and show that IC3Net outperforms the baselines with performance gaps that increase with scale. The results show that individual rewards converge faster and better than global rewards. In this work, we introduced IC3Net which aims to solve multi-agent tasks in various cooperation settings by learning when to communicate. Its continuous communication enables efficient training by backpropagation, while the discrete gating trained by reinforcement learning along with individual rewards allows it to be used in all scenarios and on larger scale.Through our experiments, we show that IC3Net performs well in cooperative, mixed or competitive settings and learns to communicate only when necessary. Further, we show that agents learn to stop communication in competitive cases. We show scalability of our network by further experiments. In future, we would like to explore possibility of having multi-channel communication where agents can decide on which channel they want to put their information similar to communication groups but dynamic. It would be interesting to provide agents a choice of whether to listen to communication from a channel or not.","We introduce IC3Net, a single network which can be used to train agents in cooperative, competitive and mixed scenarios. We also show that agents can learn when to communicate using our model.",Individualized Controlled Continuous Communication Model ; StarCraft ; two ; three,systems ; two teams ; individualized rewards ; manufacturing plants ; everything ; RL ; partially observable environments ; Recent works ; convergence rates ; profitability,Individualized Controlled Continuous Communication Model ; StarCraft ; two ; three,"Learning when to communicate and doing that effectively is essential in multi-agent tasks. Recent works show that continuous communication allows efficient training with back-propagation, but have been restricted to fully-cooperative tasks. In this paper, we present Individualized Controlled Continuous Communication Model (IC3Net) which has better training efficiency than simple continuous communication model and can be applied to semi-cooper and competitive settings along with cooperative settings. IC3Net controls continuous communication with a gating mechanism and uses individualized rewards foreach agents to gain better performance and scalability while fixing credit assignment issues. Using variety of tasks including StarCraft Bro",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Informed and robust decision making in the face of uncertainty is critical for robots that perform physical tasks alongside people. We formulate this as a Bayesian Reinforcement Learning problem over latent Markov Decision Processes (MDPs). While Bayes-optimality is theoretically the gold standard, existing algorithms do not scale well to continuous state and action spaces. We propose a scalable solution that builds on the following insight: in the absence of uncertainty, each latent MDP is easier to solve. We split the challenge into two simpler components. First, we obtain an ensemble of clairvoyant experts and fuse their advice to compute a baseline policy. Second, we train a Bayesian residual policy to improve upon the ensemble's recommendation and learn to reduce uncertainty. Our algorithm, Bayesian Residual Policy Optimization (BRPO), imports the scalability of policy gradient methods as well as the initialization from prior models. BRPO significantly improves the ensemble of experts and drastically outperforms existing adaptive RL methods. Robots operating in the real world must resolve uncertainty on a daily basis. Often times, a robot is uncertain about how the world around it evolves. For example, a self-driving car must drive safely around unpredictable actors like pedestrians and bicyclists. A robot arm must reason about occluded objects when reaching into a cluttered shelf. On other occasions, a robot is uncertain about the task it needs to perform. An assistive home robot must infer a human's intended goal by interacting with them. Both examples of uncertainty require simultaneous inference and decision making, which can be framed as Bayesian reinforcement learning (RL) over latent Markov Decision Processes (MDPs). Agents do not know which latent MDP they are interacting with, preventing them from acting optimally with respect to that MDP. Instead, Bayes optimality only requires that agents be optimal with respect to their current uncertainty over latent MDPs. The Bayesian RL problem can be viewed as solving a large continuous belief MDP, which is computationally infeasible to solve directly (Ghavamzadeh et al., 2015) . We build upon a simple yet recurring observation (Osband et al., 2013; Kahn et al., 2017; Choudhury et al., 2018) : while solving the belief MDP may be hard, solving individual latent MDPs is much more tractable. Given exact predictions for all actors, the self-driving car can invoke a motion planner to find a collision-free path. The robot arm can employ an optimal controller to dexterously retrieve an object given exact knowledge of all objects. Once the human's intended goal is discovered, the robot can provide assistance. Hence, the overall challenge boils down to solving two (perhaps) simpler sub-challenges: solving the latent MDPs and combining these solutions to solve the belief MDP. Let's assume we can approximately solve the latent MDPs to create an ensemble of policies as shown in Figure 1 . We can think of these policies as clairvoyant experts, i.e., experts that think they know the latent MDP and offer advice accordingly. A reasonable strategy is to weigh these policy proposals by the belief and combine them into a single recommendation to the agent. While this recommendation is good for some regimes, it can be misleading when uncertainty is high. The onus then is on the agent to disregard the recommendation and explore the space effectively to collapse uncertainty. This leads to our key insight. Learning Bayesian corrections on top of clairvoyant experts is a scalable strategy for solving complex reinforcement learning problems. While learning corrections echoes the philosophy of boosting (Freund & Schapire, 1999) , our agent goes one step beyond: it learns to take uncertainty-reducing actions that highlight which expert to boost. Our algorithm, Bayesian Residual Policy Optimization (BRPO), augments a belief-space batch policy optimization algorithm (Lee et al., 2019) with clairvoyant experts (Figure 1 ). The agent observes the experts' recommendation, belief over the latent MDPs, and state. It returns a correction over the expert proposal, including uncertainty-reducing sensing actions that experts never need to take. Our key contribution is the following: • We propose a scalable Bayesian RL algorithm to solve problems with complex latent rewards and dynamics. • We experimentally demonstrate that BRPO outperforms both the ensemble of experts and existing adaptive RL algorithms. In the real world, robots must deal with uncertainty, either due to complex latent dynamics or task specifics. Because uncertainty is an inherent part of these tasks, we can at best aim for optimality under uncertainty, i.e., Bayes optimality. Existing BRL algorithms or POMDP solvers do not scale well to problems with complex latent MDPs or a large (continuous) set of MDPs. We decompose BRL problems into two parts: solving each latent MDP and being Bayesian over the solutions. Our algorithm, Bayesian Residual Policy Optimization, operates on the residual belief-MDP space given an ensemble of experts. BRPO focuses on learning to explore, relying on the experts for exploitation. BRPO is capable of solving complex problems, outperforming existing BRL algorithms and improving on the original ensemble of experts. Although out of scope for this work, a few key challenges remain. First is an efficient construction of an ensemble of experts, which becomes particularly important for continuous latent spaces with infinitely many MDPs. Infinitely many MDPs do not necessarily require infinite experts, as many may converge to similar policies. An important future direction is subdividing the latent space and computing a qualitatively diverse set of policies (Liu et al., 2016) . Another challenge is developing an efficient Bayes filter, which is an active research area. In certain occasions, the dynamics of the latent MDPs may not be accessible, which would require a learned Bayes filter. Combined with a tractable, efficient Bayes filter and an efficiently computed set of experts, we believe that BRPO will provide an even more scalable solution for BRL problems. As discussed in Section 3.1, Bayesian reinforcement learning and posterior sampling address quite different problems. We present a toy problem to highlight the distinction between them. Consider a deterministic tree-like MDP ( Figure 6 ). Reward is received only at the terminal leaf states: one leaf contains a pot of gold (R = 100) and all others contain a dangerous tiger (R = −10). All non-leaf states have two actions, go left (L) and go right (R). The start state additionally has a sense action (S), which is costly (R = −0.1) but reveals the exact location of the pot of gold. Both algorithms are initialized with a uniform prior over the N = 2 d possible MDPs (one for each possible location of the pot of gold). To contrast the performance of the Bayes-optimal policy and posterior sampling, we consider the multi-episode setting where the agent repeatedly interacts with the same MDP. The MDP is sampled once from the uniform prior, and agents interact with it for T episodes. This is the setting typically considered by posterior sampling (PSRL) (Osband et al., 2013) . Before each episode, PSRL samples an MDP from its posterior over MDPs, computes the optimal policy, and executes it. After each episode, it updates the posterior and repeats. Sampling from the posterior determinizes the underlying latent parameter. As a result, PSRL will never produce sensing actions to reduce uncertainty about that parameter because the sampled MDP has no uncertainty. More concretely, the optimal policy for each tree MDP is to navigate directly to the gold without sensing; PSRL will never take the sense action. Thus, PSRL makes an average of N −1 2 mistakes before sampling the correct pot of gold location and the cumulative reward over T episodes is In the first episode, the Bayes-optimal first action is to sense. All subsequent actions in this first episode navigate toward the pot of gold, for an episode reward of −0.1 + 100. In the subsequent T − 1 episodes, the Bayes-optimal policy navigates directly toward the goal without needing to sense, for a cumulative reward of 100T − 0.1. The performance gap between the Bayes-optimal policy and posterior sampling grows exponentially with depth of the tree d. Practically, a naïve policy gradient algorithm (like BPO) would struggle to learn the Bayes-optimal policy: it would need to learn to both sense and navigate the tree to the sensed goal. BRPO can take advantage of the set of experts, which each navigate to their designated leaf. During training, the BRPO agent only needs to learn to balance sensing with navigation. As mentioned in Section 3.1, PSRL is an online learning algorithm and is designed to address domains where the posterior naturally updates as a result of multiple episodes of interactions with the latent MDP. PSRL is more focused on improving the performance over episodes, which is different from the average performance or zero-shot performance that we consider in this work.",We propose a scalable Bayesian Reinforcement Learning algorithm that learns a Bayesian correction over an ensemble of clairvoyant experts to solve problems with complex latent rewards and dynamics.,RL ; Bayes ; Bayesian Reinforcement Learning ; BRL ; MDP ; Bayesian ; one ; First ; two ; Choudhury,this recommendation ; the original ensemble ; a scalable solution ; Schapire ; T episodes ; the latent MDP ; individual latent MDPs ; al. ; a robot ; a cluttered shelf,RL ; Bayes ; Bayesian Reinforcement Learning ; BRL ; MDP ; Bayesian ; one ; First ; two ; Choudhury,"Informed and robust decision making in the face of uncertainty is critical for robots that perform physical tasks alongside people. Bayesian Reinforcement Learning (RL) over latent Markov Decision Processes (MDPs) is the gold standard for Bayes-optimality. However, existing algorithms do not scale well to continuous state and action spaces, making each latent MDP easier to solve. We split the challenge into two simpler components. First, we obtain an ensemble of clairvoyant experts and fuse their advice to compute a baseline policy. Second, we train a Bayesian residual policy to improve upon the ensemble's recommendation and learn",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Pointwise localization allows more precise localization and accurate interpretability, compared to bounding box, in applications where objects are highly unstructured such as in medical domain. In this work, we focus on  weakly supervised localization (WSL) where a model is trained to classify an image and localize regions of interest at pixel-level using only global image annotation. Typical convolutional attentions maps are prune to high false positive regions. To alleviate this issue, we propose a new deep learning method for WSL, composed of a localizer and a classifier, where the localizer is constrained to determine relevant and irrelevant regions using conditional entropy (CE) with the aim to reduce false positive regions. Experimental results on a public medical dataset and two natural datasets, using Dice index, show that, compared to state of the art WSL methods, our proposal can provide significant improvements in terms of image-level classification and pixel-level localization (low false positive) with robustness to overfitting. A public reproducible PyTorch implementation is provided. Pointwise localization is an important task for image understanding, as it provides crucial clues to challenging visual recognition problems, such as semantic segmentation, besides being an essential and precise visual interpretability tool. Deep learning methods, and particularly convolutional neural networks (CNNs), are driving recent progress in these tasks. Nevertheless, despite their remarkable performance, their training requires large amounts of labeled data, which is time consuming and prone to observer variability. To overcome this limitation, weakly supervised learning (WSL) has emerged recently as a surrogate for extensive annotations of training data (Zhou, 2017) . WSL involves scenarios where training is performed with inexact or uncertain supervision. In the context of pointwise localization or semantic segmentation, weak supervision typically comes in the form of image level tags (Kervadec et al., 2019; Kim et al., 2017; Pathak et al., 2015; Teh et al., 2016; Wei et al., 2017) , scribbles (Lin et al., 2016; Tang et al., 2018) or bounding boxes (Khoreva et al., 2017) . Current state-of-the-art WSL methods rely heavily on pixelwise activation maps produced by a CNN classifier at the image level, thereby localizing regions of interest (Zhou et al., 2016) . Furthermore, this can be used as an interpretation of the model's decision (Zhang & Zhu, 2018) . The recent literature abounds of WSL works that relax the need of dense and prohibitively time consuming pixel-level annotations (Rony et al., 2019) . Bottom-up methods rely on the input signal to locate regions of interest, including spatial pooling techniques over activation maps (Durand et al., 2017; Oquab et al., 2015; Sun et al., 2016; Zhang et al., 2018b; Zhou et al., 2016) , multi-instance learning (Ilse et al., 2018) and attend-and-erase based methods (Kim et al., 2017; Li et al., 2018; Pathak et al., 2015; Singh & Lee, 2017; Wei et al., 2017) . While these methods provide pointwise localization, the models in (Bilen & Vedaldi, 2016; Kantorov et al., 2016; Shen et al., 2018; Tang et al., 2017; Wan et al., 2018 ) predict a bounding box instead, i.e., perform weakly supervised object detection. Inspired by human visual attention, top-down methods rely on the input signal and a selective backward signal to determine the corresponding region of interest. This includes special feedback layers (Cao et al., 2015) , backpropagation error (Zhang et al., 2018a) and Grad-CAM (Chattopadhyay et al., 2018; Selvaraju et al., 2017) . In many applications, such as in medical imaging, region localization may require high precision such as cells, boundaries, and organs localization; regions that have an unstructured shape, and different scale that a bounding box may not be able to localize precisely. In such cases, a pointwise localization can be more suitable. The illustrative example in Fig.1 (bottom row) shows a typical case where using a bounding box to localize the glands is clearly problematic. This motivates us to consider predicting a mask instead of a bounding box. Consequently, our latter choice of evaluation datasets is constrained by the availability of both global image annotation for training and pixel-level annotation for evaluation. In this work, we focus on the case where there is one object of interest in the image. Often, within an agnostic-class setup, input image contains the object of interest among other irrelevant parts (noise, background). Most the aforementioned WSL methods do not consider such prior, and feed the entire image to the model. In such scenario, (Wan et al., 2018) argue that there is an inconsistency between the classification loss and the task of WSL; and that typically the optimization may reach sub-optimal solutions with considerable randomness in them, leading to high false positive localization. False positive localization is aggravated when a class appears in different and random shape/structure, or may have relatively similar texture/color to the irrelevant parts driving the model to confuse between both parts. False positive regions can be problematic in critical domains such as medical applications where interpretability plays a central role in trusting and understanding an algorithm's prediction. To address this important issue, and motivated by the importance of using prior knowledge in learning to alleviate overfitting when training using few samples (Belharbi et al., 2017; Krupka & Tishby, 2007; Mitchell, 1980; Yu et al., 2007) , we propose to use the aforementioned prior in order to favorite models with low false positive localization. To this end, we constrain the model to learn to localize both relevant and irrelevant regions simultaneously in an end-to-end manner within a WSL scenario, where only image-level labels are used for training. We model the relevant (discriminative) regions as the complement of the irrelevant (non-discriminative) regions (Fig.1) . Our model is composed of two sub-models: (1) a localizer that aims to localize both types of regions by predicting a latent mask, (2) and a classifier that aims to classify the visible content of the input image through the latent mask. The localizer is driven through CE (Cover & Thomas, 2006) to simultaneously identify (1) relevant regions where the classifier has high confidence with respect to the image label, (2) and irrelevant regions where the classifier is being unable to decide which image label to assign. This modeling allows the discriminative regions to pop out and be used to assign the corresponding image label, while suppressing non-discriminative areas, leading to more reliable predictions. In order to localize complete discriminative regions, we extend our proposal by training the localizer to recursively erase discriminative parts during training only. To this end, we propose a consistent recursive erasing algorithm that we incorporate within the backpropagation. At each recursion, and within the backpropagation, the algorithm localizes the most discriminative region; stores it; then erases it from the input image. At the end of the final recursion, the model has gathered a large extent of the object of interest that is fed next to the classifier. Thus, our model is driven to localize complete relevant regions while discarding irrelevant regions, resulting in more reliable region localization. Moreover, since the discriminative parts are allowed to be extended over different instances, our proposal handles multi-instances intrinsically. The main contribution of this paper is a new deep learning framework for WSL at pixel level. The framework is composed of two sequential sub-networks where the first one localizes regions of interest, whereas the second classifies them. Based on CE, the end-to-end training of the framework allows to incorporate prior knowledge that, an image is more likely to contain relevant and irrelevant regions. Throughout the CE measured at the classifier level, the localizer is driven to localize relevant regions (with low CE) and irrelevant regions (with high CE). Such localization is achieved with the main goal of providing a more interpretable and reliable regions of interest with low false positive localization. This paper also contributes a consistent recursive erasing algorithm that is incorporated within backpropagation, along with a practical implementation in order to obtain complete discriminative regions. Finally, we conduct an extensive series of experiments on three public image datasets (medical and natural), where the results show the effectiveness of the proposed approach in terms of pointwise localization (measured with Dice index) while maintaining competitive accuracy for image-level classification. In this work, we present a novel approach for WSL at pixel-level where we impose learning relevant and irrelevant regions within the model with the aim to reduce false positive localization. Evaluated on three datasets, and compared to state of the art WSL methods, our approach shows its effectiveness in accurately localizing regions of interest with low false positive while maintaining a competitive classification error. This makes our approach more reliable in term of interpetability. As future work, we consider extending our approach to handle multiple classes within the image. Different constraints can be applied over the predicted mask, such as texture properties, shape, or other region constraints. Predicting bounding boxes instead of heat maps is considered as well since they can be more suitable in some applications where pixel-level accuracy is not required. Our recursive erasing algorithm can be further improved by using a memory-like mechanism that provides spatial information to prevent forgetting the previously spotted regions and promote localizing the entire region (Sec.B.3).",A deep learning method for weakly-supervised pointwise localization that learns using image-level label only. It relies on conditional entropy to localize relevant and irrelevant regions aiming to minimize false positive regions.,Khoreva ; Zhang et al. ; PyTorch ; Zhou et al. ; Wan et al. ; Tang ; al. ; localizer ; Kervadec et al. ; Kim et al.,bottom row ; Sec ; Wei et al. ; shape ; a new deep learning method ; the image level ; Lin et al ; these tasks ; a public medical dataset ; models,Khoreva ; Zhang et al. ; PyTorch ; Zhou et al. ; Wan et al. ; Tang ; al. ; localizer ; Kervadec et al. ; Kim et al.,"Pointwise localization enables more precise localization and accurate interpretability, compared to bounding box, in applications where objects are unstructured. In this work, we focus on weakly supervised localization (WSL) where a model is trained to classify an image and localize regions of interest at pixel-level using only global image annotation. However, typical convolutional attentions maps are prune to high false positive regions. To alleviate this issue, a new deep learning method for WSL, composed of a localizer and a classifier, where the localizer is constrained to determine relevant and irrelevant regions using conditional entropy (CE",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"We introduce a novel method for converting text data into abstract image representations, which allows image-based processing techniques (e.g. image classification networks) to be applied to text-based comparison problems. We apply the technique to entity disambiguation of inventor names in US patents. The method involves converting text from each pairwise comparison between two inventor name records into a 2D RGB (stacked) image representation. We then train an image classification neural network to discriminate between such pairwise comparison images, and use the trained network to label each pair of records as either matched (same inventor) or non-matched (different inventors), obtaining highly accurate results (F1: 99.09%, precision: 99.41%, recall: 98.76%). Our new text-to-image representation method could potentially be used more broadly for other NLP comparison problems, such as disambiguation of academic publications, or for problems that require simultaneous classification of both text and images. Databases of patent applications and academic publications can be used to investigate the process of research and innovation. For example, patent data can be used to identify prolific inventors (Gay et al., 2008) or to investigate whether mobility increases inventor productivity (Hoisl, 2009 ). However, the names of individuals in large databases are rarely distinct, hence individuals in such databases are not uniquely identifiable. For example, an individual named ""Chris Jean Smith"" may have patents under slightly different names such as ""Chris Jean Smith"", ""Chris J. Smith"", ""C J Smith"", etc. . . There may also be different inventors with patents under the same or similar names, such as ""Chris Jean Smith"", ""Chris J. Smith"", ""Chris Smith"", etc. . . Thus it is ambiguous which names (and hence patents) should be assigned to which individuals. Resolving this ambiguity and assigning unique identifiers to individuals -a process often referred to as named entity disambiguation -is important for research that relies on such databases. Machine learning algorithms have been used increasingly in recent years to perform automated disambiguation of inventor names in large databases (e.g. Li et al. (2014) ; Ventura et al. (2015) ; Kim et al. (2016) ). See Ventura et al. (2015) for a review of supervised, semi-supervised, and unsupervised machine learning approaches to disambiguation. These more recent machine learning approaches have often out-performed more traditional rule-and threshold-based methods, but they have generally used feature vectors containing several pre-selected measures of string similarity as input for their machine learning algorithms. That is, the researcher generally pre-selects a number of string similarity measures which they believe may be useful as input for the machine learning algorithm to make discrimination decisions. Here we introduce a novel approach of representing text-based data, which enables image classifiers to perform text classification. This new representation enables a supervised machine learning algorithm to learn its own features from the data, rather than selecting from a number of pre-defined string similarity measures chosen by the researcher. To do this, we treat the name disambiguation problem primarily as a classification problem -i.e. we assess pairwise comparisons between records as either matched (same inventor) or non-matched (different inventors) (Trajtenberg et al., 2006; Miguélez & Gómez-Miguélez, 2011; Li et al., 2014; Ventura et al., 2015; Kim et al., 2016) . Then, for a given pairwise comparison between two inventor records, our text-to-image representa-tion method converts the associated text strings into a stacked 2D colour image (or, equivalently, a 3D tensor) which represents the underlying text data. We describe our text-to-image representation method in detail in Section 4.1 (see Figure 1 in that section for an example of text-to-image conversion). We also test a number of alternative representations in Section 5.4. Our novel method of representing text-based records as abstract images enables image processing algorithms (e.g. image classification networks), to be applied to textbased natural language processing (NLP) problems involving pairwise comparisons (e.g. named entity disambiguation). We demonstrate this by combining our text-to-image conversion method with a commonly used convolutional neural network (CNN) (Krizhevsky et al., 2012) , obtaining highly accurate results (F1: 99.09%, precision: 99.41%, recall: 98.76%). Our name disambiguation algorithm provides a novel way of combining image processing with NLP, allowing image classifiers to perform text classification. We demonstrated this with the AlexNet CNN, producing highly accurate results (F1 score: 99.09%). We also analysed several variants of alternative string-maps, and found that the accuracy of the disambiguation algorithm was quite robust to such variation. Our disambiguation algorithm could easily be adapted to other NLP problems requiring text matching of multiple strings (e.g. academic author name disambiguation or record linkage problems). The algorithm could also potentially be modified to process records that contain both text and image data, by combining each record's associated image with the abstract image representation of the record's text, in a single comparison-map.","We introduce a novel text representation method which enables image classifiers to be applied to text classification problems, and apply the method to inventor name disambiguation.",two ; al. ; Kim et al ; Chris J. Smith ; Miguélez & Gómez-Miguélez ; US ; Chris Jean Smith ; NLP ; Li ; recent years,et al ; a novel method ; Chris Smith ; the record's text ; US patents ; Gay et al ; example ; disambiguation ; an image classification neural network ; this,two ; al. ; Kim et al ; Chris J. Smith ; Miguélez & Gómez-Miguélez ; US ; Chris Jean Smith ; NLP ; Li ; recent years,"We introduce a novel method for converting text data into abstract image representations, which allows image-based processing techniques (e.g. image classification networks) to be used for text-based comparison problems. The technique involves converting each pairwise comparison between two inventor name records into a 2D RGB (stacked) image representation, and train an image classification neural network to discriminate between such pairwise comparisons images. The trained network label each pair of records as either matched (same inventor) or non-matched (different inventors), obtaining highly accurate results. The text-to-image representation method can be used more broadly for other N",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"We explore the concept of co-design in the context of neural network verification. Specifically, we aim to train deep neural networks that not only are robust to adversarial perturbations but also whose robustness can be verified more easily. To this end, we identify two properties of network models - weight sparsity and so-called ReLU stability - that turn out to significantly impact the complexity of the corresponding verification task. We demonstrate that improving weight sparsity alone already enables us to turn computationally intractable verification problems into tractable ones. Then, improving ReLU stability leads to an additional 4-13x speedup in verification times. An important feature of our methodology is its ""universality,"" in the sense that it can be used with a broad range of training procedures and verification approaches.
 Deep neural networks (DNNs) have recently achieved widespread success in image classification BID17 , face and speech recognition BID27 , and game playing BID23 BID8 . This success motivates their application in a broader set of domains, including more safety-critical environments. This thrust makes understanding the reliability and robustness of the underlying models, let alone their resilience to manipulation by malicious actors, a central question. However, predictions made by machine learning models are often brittle. A prominent example is the existence of adversarial examples BID26 : imperceptibly modified inputs that cause state-of-the-art models to misclassify with high confidence.There has been a long line of work on both generating adversarial examples, called attacks BID2 BID13 BID0 BID13 BID9 , and training models robust to adversarial examples, called defenses BID10 BID21 BID19 BID14 . However, recent research has shown that most defenses are ineffective BID2 BID0 . Furthermore, even for defenses such as that of BID19 that have seen empirical success against many attacks, we are unable to conclude yet with certainty that they are robust to all attacks that we want these models to be resilient to.This state of affairs gives rise to the need for verification of networks, i.e., the task of formally proving that no small perturbations of a given input can cause it to be misclassified by the network model. Although many exact verifiers 1 have been designed to solve this problem, the verification process is often intractably slow. For example, when using the Reluplex verifier of , even verifying a small MNIST network turns out to be computationally infeasible. Thus, addressing this intractability of exact verification is the primary goal of this work. In this paper, we use the principle of co-design to develop training methods that emphasize verification as a goal, and we show that they make verifying the trained model much faster. We first demonstrate that natural regularization methods already make the exact verification problem significantly more tractable. Subsequently, we introduce the notion of ReLU stability for networks, present a method that improves a network's ReLU stability, and show that this improvement makes verification an additional 4-13x faster. Our method is universal, as it can be added to any training procedure and should speed up any exact verification procedure, especially MILP-based methods.Prior to our work, exact verification seemed intractable for all but the smallest models. Thus, our work shows progress toward reliable models that can be proven to be robust, and our techniques can help scale verification to even larger networks.Many of our methods appear to compress our networks into more compact, simpler forms. We hypothesize that the reason that regularization methods like RS Loss can still achieve very high accuracy is that most models are overparametrized in the first place. There exist clear parallels between our methods and techniques in model compression BID12 BID6 ) -therefore, we believe that drawing upon additional techniques from model compression can further improve the ease-of-verification of networks. We also expect that there exist objectives other than weight sparsity and ReLU stability that are important for verification speed. If so, further exploring the principle of co-design for those objectives is an interesting future direction. Exact verification and certification are two related approaches to formally verifying properties of neural networks, such as adversarial robustness. In both cases, the end goal is formal verification. Certification methods, which solve an easier-to-solve relaxation of the exact verification problem, are important developments because exact verification previously appeared computationally intractable for all but the smallest models.For the case of adversarial robustness, certification methods exploit a trade-off between provable robustness and speed. They can fail to provide certificates of robustness for some inputs that are actually robust, but they will either find or fail to find certificates of robustness quickly. On the other hand, exact verifiers will always give the correct answer if given enough time, but exact verifiers can sometimes take many hours to formally verify robustness on even a single input.In general, the process of training a robust neural network and then formally verifying its robustness happens in two steps.•Step 1: Training BID22 and BID20 , propose a method for step 2 (the certification step), and then propose a training objective in step 1 that is directly related to their method for step 2. We call this paradigm ""co-training."" In BID22 , they found that using their step 2 on a model trained using Wong and Kolter (2018)'s step 1 resulted in extremely poor provable robustness (less than 10%), and the same was true when using Wong and Kolter (2018)'s step 2 on a model trained using their step 1.We focus on MILP-based exact verification as our step 2, which encompasses the best current exact verification methods. The advantage of using exact verification for step 2 is that it will be accurate, regardless of what method is used in step 1. The disadvantage of using exact verification for step 2 is that it could be extremely slow. For our step 1, we used standard robust adversarial training. In order to significantly speed up exact verification as step 2, we proposed techniques that could be added to step 1 to induce weight sparsity and ReLU stability.In general, we believe it is important to develop effective methods for step 1, given that step 2 is exact verification. However, ReLU stability can also be beneficial for tightening the relaxation of certification approaches like that of and , as unstable ReLUs are the primary cause of the overapproximation that occurs in the relaxation step. Thus, our techniques for inducing ReLU stability can be useful for certification as well.Finally, in recent literature on verification and certification, most works have focused on formally verifying the property of adversarial robustness of neural networks. However, verification of other properties could be useful, and our techniques to induce weight sparsity and ReLU stability would still be useful for verification of other properties for the exact same reasons that they are useful in the case of adversarial robustness.",We develop methods to train deep neural models that are both robust to adversarial perturbations and whose robustness is significantly easier to verify.,two ; ReLU ; Reluplex ; first ; RS Loss ; many hours ; Wong ; Kolter,this paradigm ; An important feature ; progress ; other properties ; effective methods ; a broad range ; Wong ; enough time ; the-art ; more safety-critical environments,two ; ReLU ; Reluplex ; first ; RS Loss ; many hours ; Wong ; Kolter,"We explore the concept of co-design in neural network verification, which aims to train deep neural networks that are robust to adversarial perturbations but whose robustness can be verified more easily. We identify two properties of network models - weight sparsity and so-called ReLU stability - that impact the complexity of the corresponding verification task. This approach enables us to turn computationally intractable verification problems into tractable ones, resulting in an additional 4-13x speedup in verification times. An important feature of our methodology is its ""universality"" in the sense that it can be used with a broad range of",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Recent improvements in large-scale language models have driven progress on automatic generation of syntactically and semantically consistent text for many real-world applications. Many of these advances leverage the availability of large corpora. While training on such corpora encourages the model to understand long-range dependencies in text, it can also result in the models internalizing the social biases present in the corpora. This paper aims to quantify and reduce biases exhibited by language models. Given a conditioning context (e.g. a writing prompt) and a language model, we analyze if (and how) the sentiment of the generated text is affected by changes in values of sensitive attributes (e.g. country names, occupations, genders, etc.) in the conditioning context, a.k.a. counterfactual evaluation. We quantify these biases by adapting individual and group fairness metrics from the fair machine learning literature. Extensive evaluation on two different corpora (news articles and Wikipedia) shows that state-of-the-art Transformer-based language models exhibit biases learned from data. We propose embedding-similarity and sentiment-similarity regularization methods that improve both individual and group fairness metrics without sacrificing perplexity and semantic similarity---a positive step toward development and deployment of fairer language models for real-world applications. Text representation learning methods (word and sentence encoders) trained on large unlabeled corpora are widely used in the development of natural language processing systems (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018; Devlin et al., 2018) . Progress in this area has led to consistent improvements of model performances on many downstream tasks. However, recent studies have found that both context-free and context-dependent word embedding models contain human-like semantic biases, including gender and race (Bolukbasi et al., 2016; Caliskan et al., 2017; Zhao et al., 2019) . Zhao et al. (2018a) provide an insight into this phenomenon by showing that web corpora contain biases (e.g., gender) which are inherited by models trained on these datasets. In this work, we focus on language models which have been shown to exhibit systematic biases (Lu et al., 2018; Bordia & Bowman, 2019; Qian et al., 2019) . We train a Transformer-based language model (Vaswani et al., 2017; on two large corpora: Wikipedia articles from Wikitext-103 (Merity et al., 2016) and news articles from the English-language news corpus from . 1 We analyze systematic variations in sentiment scores of the text generated by the language model given a conditioning context, under different instantiations of control variables (e.g. country names, occupations, and person names) in the context. In a counterfactual experiment, we find that sentiment scores for the text generated by this language model vary substantially as we change the control variables in the context. We propose two approaches to reduce counterfactual sentiment biases based on the concept of embedding similarity or sentiment similarity. In the first method, we encourage hidden states of the conditioning context to be similar irrespective of the instantiations of the control variables in the context. In the second method, we regularize the difference between sentiment scores of various instantiations of the control variables. Experiments with counterfactual conditioning demonstrate that both of these methods reduce sentiment biases while retaining the generation capability of the language model, as measured by perplexity and semantic similarity. While specifying optimal model fairness behavior is difficult, our method provides a framework to address various fairness specifications and an important step toward the deployment of fairer language models. Our main contributions in this paper are: • We demonstrate systematic counterfactual sentiment biases in large-scale language models. • We present methods to quantify these biases by adopting individual and group fairness metrics from the fair machine learning literature. • We propose embedding and sentiment similarity-based methods for training language models to be invariant to certain transformations of their inputs. • We empirically demonstrate the efficacy of these methods to reduce counterfactual sentiment biases of language models. We use a sentiment classifier as a proxy to measure biases in this paper. We note that the classifier itself is not perfect and might exhibit some biases. We leave investigations of an unbiased evaluator to future work. As large-scale language models are increasingly deployed for real-world applications, developing methods for assessing and mitigating bias with respect to sensitive attributes may be an increasingly important area of inquiry for facilitating pro-social outcomes. Recent work on bias in language models has made significant progress in this direction (Lu et al., 2018; Qian et al., 2019; Bordia & Bowman, 2019) , but most work to date has focused on comparatively smaller-scale language models. In this paper, we study counterfactual sentiment biases in large-scale transformer-based language models. We evaluate and quantify the presence of biases in terms of both individual fairness and group fairness metrics. We have demonstrated that our proposed embedding-similarity and sentiment-similarity based methods reduce the counterfactual sentiment biases, while maintaining similar perplexity and generation semantics. While specifying optimal model fairness behavior is difficult, our method provides a framework to address various fairness specifications and an important step toward the deployment of fairer language models. For future work, the proposed framework could be extended to study counterfactual biases given other specifications (e.g. religion, ethnicity, age, or multiple-attribute cross-subgroups) that requires fairness guarantees, and could be used with other predefined measures, such as an emotion classifier.",We reduce sentiment biases based on counterfactual evaluation of text generation using language models.,Wikipedia ; two ; Zhao et ; Zhao ; Devlin ; al. ; corpora ; Pennington ; first ; Caliskan,age ; large-scale transformer-based language models ; sensitive attributes ; Zhao et al ; long-range dependencies ; Peters ; the corpora ; a counterfactual experiment ; an increasingly important area ; group,Wikipedia ; two ; Zhao et ; Zhao ; Devlin ; al. ; corpora ; Pennington ; first ; Caliskan,"Recent advances in large-scale language models have driven progress on automatic generation of syntactically and semantically consistent text for real-world applications. These advances leverage the availability of large corpora. However, training on such corpora can lead to the models internalizing social biases. This paper aims to quantify and reduce biases exhibited by language models. Given a conditioning context (e.g. a writing prompt) and a language model, we analyze if (and how) the sentiment of the generated text is affected by changes in values of sensitive attributes in the conditioning context, a.k.a. counterfactual evaluation. We",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"A structured understanding of our world in terms of objects, relations, and hierarchies is an important component of human cognition. Learning such a structured world model from raw sensory data remains a challenge. As a step towards this goal, we introduce Contrastively-trained Structured World Models (C-SWMs). C-SWMs utilize a contrastive approach for representation learning in environments with compositional structure. We structure each state embedding as a set of object representations and their relations, modeled by a graph neural network. This allows objects to be discovered from raw pixel observations without direct supervision as part of the learning process. We evaluate C-SWMs on compositional environments involving multiple interacting objects that can be manipulated independently by an agent, simple Atari games, and a multi-object physics simulation. Our experiments demonstrate that C-SWMs can overcome limitations of models based on pixel reconstruction and outperform typical representatives of this model class in highly structured environments, while learning interpretable object-based representations. Compositional reasoning in terms of objects, relations, and actions is a central ability in human cognition (Spelke & Kinzler, 2007) . This ability serves as a core motivation behind a range of recent works that aim at enriching machine learning models with the ability to disentangle scenes into objects, their properties, and relations between them (Chang et al., 2016; Battaglia et al., 2016; Watters et al., 2017; van Steenkiste et al., 2018; Kipf et al., 2018; Sun et al., 2018; 2019b; Xu et al., 2019) . These structured neural models greatly facilitate predicting physical dynamics and the consequences of actions, and provide a strong inductive bias for generalization to novel environment situations, allowing models to answer counterfactual questions such as ""What would happen if I pushed this block instead of pulling it?"". Arriving at a structured description of the world in terms of objects and relations in the first place, however, is a challenging problem. While most methods in this area require some form of human annotation for the extraction of objects or relations, several recent works study the problem of object discovery from visual data in a completely unsupervised or self-supervised manner (Eslami et al., 2016; Greff et al., 2017; Nash et al., 2017; van Steenkiste et al., 2018; Kosiorek et al., 2018; Janner et al., 2019; Xu et al., 2019; Burgess et al., 2019; Greff et al., 2019; Engelcke et al., 2019) . These methods follow a generative approach, i.e., they learn to discover object-based representations by performing visual predictions or reconstruction and by optimizing an objective in pixel space. Placing a loss in pixel space requires carefully trading off structural constraints on latent variables vs. accuracy of pixel-based reconstruction. Typical failure modes include ignoring visually small, but relevant features for predicting the future, such as a bullet in an Atari game (Kaiser et al., 2019) , or wasting model capacity on visually rich, but otherwise potentially irrelevant features, such as static backgrounds. To avoid such failure modes, we propose to adopt a discriminative approach using contrastive learning, which scores real against fake experiences in the form of state-action-state triples from an experience buffer (Lin, 1992) , in a similar fashion as typical graph embedding approaches score true facts in the form of entity-relation-entity triples against corrupted triples or fake facts. We introduce Contrastively-trained Structured World Models (C-SWMs), a class of models for learning abstract state representations from observations in an environment. C-SWMs learn a set of abstract state variables, one for each object in a particular observation. Environment transitions are modeled using a graph neural network (Scarselli et al., 2009; Li et al., 2015; Kipf & Welling, 2016; Gilmer et al., 2017; Battaglia et al., 2018 ) that operates on latent abstract representations. This paper further introduces a novel object-level contrastive loss for unsupervised learning of object-based representations. We arrive at this formulation by adapting methods for learning translational graph embeddings (Bordes et al., 2013; Wang et al., 2014) to our use case. By establishing a connection between contrastive learning of state abstractions (François-Lavet et al., 2018; and relational graph embeddings (Nickel et al., 2016a) , we hope to provide inspiration and guidance for future model improvements in both fields. In a set of experiments, where we use a novel ranking-based evaluation strategy, we demonstrate that C-SWMs learn interpretable object-level state abstractions, accurately learn to predict state transitions many steps into the future, demonstrate combinatorial generalization to novel environment configurations and learn to identify objects from scenes without supervision. Structured world models offer compelling advantages over pure connectionist methods, by enabling stronger inductive biases for generalization, without necessarily constraining the generality of the model: for example, the contrastively trained model on the 3-body physics environment is free to store identical representations in each object slot and ignore pairwise interactions, i.e., an unstructured world model still exists as a special case. Experimentally, we find that C-SWMs make effective use of this additional structure, likely because it allows for a transition model of significantly lower complexity, and learn object-oriented models that generalize better to unseen situations. We are excited about the prospect of using C-SWMs for model-based planning and reinforcement learning in future work, where object-oriented representations will likely allow for more accurate counterfactual reasoning about effects of actions and novel interactions in the environment. We further hope to inspire future work to think beyond autoencoder-based approaches for object-based, structured representation learning, and to address some of the limitations outlined in this paper.",Contrastively-trained Structured World Models (C-SWMs) learn object-oriented state representations and a relational model of an environment from raw pixel input.,first ; Spelke & Kinzler ; Structured World Models ; Lin ; Chang et al. ; van Steenkiste ; Gilmer ; al. ; Battaglia ; Kosiorek,translational graph embeddings ; hierarchies ; These methods ; example ; state transitions ; a challenge ; Watters ; each object ; Our experiments ; a set,first ; Spelke & Kinzler ; Structured World Models ; Lin ; Chang et al. ; van Steenkiste ; Gilmer ; al. ; Battaglia ; Kosiorek,"Structured world models are crucial components of human cognition. Learning such a structured world model from raw sensory data remains a challenge. Contrastively-trained Structured World Models (C-SWMs) are a contrastive approach for representation learning in environments with compositional structure. They structure each state embedding as a set of object representations and their relations, modeled by a graph neural network. This allows objects to be discovered from raw pixel observations without direct supervision as part of the learning process. These structured world models can overcome limitations and outperform typical representatives of this model class in highly structured environments, while learning interpretable object-based representations",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Existing methods for AI-generated artworks still struggle with generating high-quality stylized content, where high-level semantics are preserved, or separating fine-grained styles from various artists. We propose a novel Generative Adversarial Disentanglement Network which can disentangle two complementary factors of variations when only one of them is labelled in general, and fully decompose complex anime illustrations into style and content in particular. Training such model is challenging, since given a style, various content data may exist but not the other way round. Our approach is divided into two stages, one that encodes an input image into a style independent content, and one based on a dual-conditional generator. We demonstrate the ability to generate high-fidelity anime portraits with a fixed content and a large variety of styles from over a thousand artists, and vice versa, using a single end-to-end network and with applications in style transfer. We show this unique capability as well as superior output to the current state-of-the-art. Computer generated art (Hertzmann, 2018) has become a topic of focus lately, due to revolutionary advancements in deep learning. Neural style transfer (Gatys et al., 2016) is a groundbreaking approach where high-level styles from artwork can be re-targeted to photographs using deep neural networks. While there has been numerous works and extensions on this topic, there are deficiencies in existing methods. For complex artworks, the methods that rely on matching neural network features and feature statistics, do not sufficiently capture the concept of style at the semantic level. Methods based on image-toimage translation (Isola et al., 2017) are able to learn domain specific definitions of style, but do not scale well to a large number of styles. In addressing these challenges, we found that style transfer can be formulated as a particular instance of a general problem, where the dataset has two complementary factors of variation, with one of the factors labelled, and the goal is to train a generative network where the two factors can be fully disentangled and controlled independently. For the style transfer problem, we have labelled style and unlabelled content. Based on various adversarial training techniques, we propose a solution to the problem and call our method Generative Adversarial Disentangling Network. Our approach consists of two main stages. First, we train a style-independent content encoder, then we introduce a dual-conditional generator based on auxiliary classifier GANs. We demonstrate the disentanglement performance of our approach on a large dataset of anime portraits with over a thousand artist-specific styles, where our decomposition approach outperforms existing methods in terms of level of details and visual quality. Our method can faithfully generate portraits with proper style-specific shapes and appearances of facial features, including eyes, mouth, chin, hair, blushes, highlights, contours, as well as overall color saturation and contrast. To show the generality of our method, we also include results on the NIST handwritten digit dataset where we can disentangle between writer identity and digit class when only the writer is labelled, or alternatively when only the digit is labelled. We introduced a Generative Adversarial Disentangling Network which enables true semanticlevel artwork synthesis using a single generator. Our evaluations and ablation study indicate that style and content can be disentangled effectively through our a two-stage framework, where first a style independent content encoder is trained and then, a content and styleconditional GANs is used for synthesis. While we believe that our approach can be extended to a wider range of artistic styles, we have validated our technique on various styles within the context of anime illustrations. In particular, this techniques is applicable, as long as we disentangle two factors of variation in a dataset and only one of the factors is labelled and controlled. Compared to existing methods for style transfer, we show significant improvements in terms of modeling high-level artistic semantics and visual quality. In the future, we hope to extend our method to styles beyond anime artworks, and we are also interested in learning to model entire character bodies, or even entire scenes. In the top two rows, in each column are two samples from the training dataset by the same artist. In each subsequent group of three rows, the leftmost image is from the training dataset. The images to the right are style transfer results generated by three different methods, from the content of the left image in the group and from the style of the top artist in the column. In each group, first row is our method, second row is StarGAN and third row is neural style. For neural style, the style image is the topmost image in the column. As stated in (Gatys et al., 2016) , which is based on an earlier work on neural texture synthesis (Gatys et al., 2015) , the justification for using Gram matrices of neural network features as a representation of style is that it captures statistical texture information. So, in essence, ""style"" defined as such is a term for ""texture statistics"", and the style transfer is limited to texture statistics transfer. Admittedly, it does it in smart ways, as in a sense the content features are implicitly used for selecting which part of the style image to copy the texture statistics from. As discussed in section 2 above, we feel that there is more about style than just feature statistics. Consider for example the case of caricatures. The most important aspects of the style would be what facial features of the subjects are exaggerated and how they are exaggerated. Since these deformations could span a long spatial distance, they cannot be captured by local texture statistics alone. Another problem is domain dependency. Consider the problem of transferring or preserving color in style transfer. If we have a landscape photograph taken during the day and want to change it to night by transferring the style from another photo taken during the night, or if we want to change the season from spring to autumn, then color would be part of the style we want to transfer. But if we have a still photograph and want to make it an oil painting, then color is part of the content, we may want only the quality of the strokes of the artwork but keep the colors of our original photo. People are aware of this problem and in (Gatys et al., 2017) , two methods, luminance-only style transfer and color histogram matching, are developed to optionally keep the color of the content image. However, color is only one aspect of the image for which counting it as style vs. content could be an ambiguity. For more complicated aspects, the option to keep or to transfer may not be easily available. We make two observations here. First, style must be more than just feature statistics. Second, the concept of ""style"" is inherently domain-dependent. In our opinion, ""style"" means different ways of presenting the same subject. In each different domain, the set of possible subjects is different and so is the set of possible ways to present them. So, we think that any successful style transfer method must be adaptive to the intended domain and the training procedure must actively use labelled style information. Simple feature based methods will never work in the general setting. This includes previous approaches which explicitly claimed to disentangle style and content, such as in (Kazemi et al., 2019) which adopts the method in the original neural style transfer for style and content losses, and also some highly accomplished methods like (Liao et al., 2017) . As a side note, for these reasons we feel that some previous methods made questionable claims about style. In particular, works like (Huang et al., 2018) and StyleGAN (Karras et al., 2018 ) made reference to style while only being experimented on collections of real photographs. By our definition, in such dataset, without a careful definition and justification there is only one possibly style, that is, photorealistic, so the distinction between style and content does not make sense, and calling a certain subset of factors ""content"" and others ""style"" could be an arbitrary decision. This is also why we elect to not test our method on more established GAN datasets like CelebA or LSUN, which are mostly collections of real photos.","An adversarial training-based method for disentangling two complementary sets of variations in a dataset where only one of them is labelled, tested on style vs. content in anime illustrations.",over a thousand ; the season from spring to autumn ; only one ; two ; Gatys et al. ; First ; AI ; one ; NIST ; the day,mouth ; the left image ; complex anime illustrations ; only the digit ; part ; visual quality ; a general problem ; thousand ; superior output ; (Gatys,over a thousand ; the season from spring to autumn ; only one ; two ; Gatys et al. ; First ; AI ; one ; NIST ; the day,"AI-generated artworks still struggle with preserving high-level semantics and separating fine-grained styles from various artists. A novel Generative Adversarial Disentanglement Network can disentangle two complementary factors of variations when only one of them is labelled in general, and fully decompose complex anime illustrations into style and content in particular. However, training such model is challenging, since given a style, various content data may exist but not the other way round. Our approach is divided into two stages, one that encodes an input image into a style independent content, and one based on a dual-conditional generator.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Most approaches to learning action planning models heavily rely on a significantly large volume of training samples or plan observations. In this paper, we adopt a different approach based on deductive learning from domain-specific knowledge, specifically from logic formulae that specify constraints about the possible states of a given domain. The minimal input observability required by our approach is a single example composed of a full initial state and a partial goal state. We will show that exploiting specific domain knowledge enables to constrain the space of possible action models as well as to complete partial observations, both of which turn out helpful to learn good-quality action models. The learning of action models in planning has been typically addressed with inductive learning data-intensive approaches. From the pioneer learning system ARMS BID13 ) to more recent ones BID8 Zhuo and Kambhampati 2013; Kucera and Barták 2018) , all of them require thousands of plan observations or training samples, i.e., sequences of actions as evidence of the execution of an observed agent, to obtain and validate an action model. These approaches return the statistically significant model that best explains the plan observations by minimizing some error metric. A model explains an observation if a plan containing the observed actions is computable with the model and the states induced by this plan also include the possibly partially observed states. The limitation of posing model learning and validation as optimization tasks over a set of observations is that it neither guarantees completeness (the model may not explain all the observations) nor correctness (the states induced by the execution of the plan generated with the model may contain contradictory information).Differently , other approaches rely on symbolic-via learning. The Simultaneous Learning and Filtering (SLAF) approach BID2 exploits logical inference and builds a complete explanation through a CNF formula that represents the initial belief state, and a plan observation. The formula is updated with every action and state of Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. the observation, thus representing all possible transition relations consistent with it. SLAF extracts all satisfying models of the learned formula with a SAT solver although the algorithm cannot effectively learn the preconditions of actions. A more recent approach addresses the learning of action models from plan observations as a planning task which searches the space of all possible action models BID0 . A plan here is conceived as a series of steps that determine the preconditions and effects of the action models plus other steps that validate the formed actions in the observations. The advantage of this approach is that it only requires input samples of about a total of 50 actions.This paper studies the impact of using mixed input data, i.e, automatically-collected plan observations and humanencoded domain-specific knowledge, in the learning of action models. Particularly, we aim to stress the extreme case of having a single observation sample and answer the question to whether the lack of training samples can be overcome with the supply of domain knowledge. The question is motivated by (a) the assumption that obtaining enough training observations is often difficult and costly, if not impossible in some domains (Zhuo 2015); (b) the fact that although the physics of the real-world domain being modeled are unknown, the user may know certain pieces of knowledge about the domain; and (c) the desire for correct action models that are usable beyond their fitness to a set of testing observations. To this end, we opted for checking our hypothesis in the framework proposed in BID0 since this planning-based satisfiability approach allows us to configure additional constraints in the compilation scheme, it is able to work under a minimal set of observations and uses an off-the-shelf planner 1 . Ultimately, we aim to compare the informational power of domain observations (information quantity) with the representational power of domainspecific knowledge (information quality). Complementarily, we restrict our attention to solely observations over fluents as in many applications the actual actions of an agent may not be observable BID11 .Next section summarizes basic planning concepts and outlines the baseline learning approach BID0 ). Then we formalize our one-shot learning task with domain knowledge and subsequently we explain the task-solving process. Section 5 presents the experimental evaluation and last section concludes. We present an approach to learn action models that builds upon a former compilation-to-planning learning system BID0 . Our proposal studies the gains of using domain-specific knowledge when the availability (amount and observability) of learning examples is very limited. Introducing domain knowledge encoded as schematic mutexes allows to narrow down the search space of the learning task and improve overall the performance of the learning system to the point that it offsets the lack of learning examples. In a theoretical work that analyzes the relation between the number of observed trajectory plans and the guarantee for a learned action model to achieve the goal BID12 , authors conclude that the number of trajectories needed scales gracefully and the guarantee grows linearly with the number of predicates and quasi-linearly with the number of actions. This evidences that learning accurate models is heavily dependent on the number and quality (observability) of the learning examples. In this sense, our proposal comes to alleviate this dependency by relying on easily deducible domain knowledge. It is not only capable of learning from a single non-fully observable learning example but also proves that learning from a 30%-observable example with domain-specific knowledge is comparable to learning from a complete plan observation.",Hybrid approach to model acquisition that compensates a lack of available data with domain specific knowledge provided by experts,SAT ; Copyright ; Kucera ; Association for the Advancement of Artificial Intelligence ; i.e ; Filtering ; Zhuo ; CNF ; The Simultaneous Learning ; one,"the availability ; specific domain knowledge ; automatically-collected plan observations ; accurate models ; observed trajectory plans ; which ; Copyright ; basic planning concepts ; the states ; i.e., sequences",SAT ; Copyright ; Kucera ; Association for the Advancement of Artificial Intelligence ; i.e ; Filtering ; Zhuo ; CNF ; The Simultaneous Learning ; one,"Learning action planning models rely on a significantly large volume of training samples or plan observations. In this paper, we use deductive learning from domain-specific knowledge, where logic formulae specify constraints about possible states of a given domain. The single example composed of a full initial state and a partial goal state is exploited to constrain the space of possible action models as well as complete partial observations. These approaches help to learn good-quality action models. In planning, the learning of action models in planning has been typically addressed with inductive learning data-intensive approaches. From the pioneer learning system ARMS BID13 to more recent",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"In this paper, we study deep diagonal circulant neural networks, that is deep neural networks in which weight matrices are the product of diagonal and circulant ones.
 Besides making a theoretical analysis of their expressivity, we introduced principled techniques for training these models: we devise an initialization scheme and proposed a smart use of non-linearity functions in order to train deep diagonal circulant networks. 
 Furthermore, we show that these networks outperform recently introduced deep networks with other types of structured layers. We conduct a thorough experimental study to compare the performance of deep diagonal circulant networks with state of the art models based on structured matrices and with dense models. We show that our models achieve better accuracy than other structured approaches while required 2x fewer weights as the next best approach. Finally we train deep diagonal circulant networks to build a compact and accurate models on a real world video classification dataset with over 3.8 million training examples. The deep learning revolution has yielded models of increasingly large size. In recent years, designing compact and accurate neural networks with a small number of trainable parameters has been an active research topic, motivated by practical applications in embedded systems (to reduce memory footprint (Sainath & Parada, 2015) ), federated and distributed learning (to reduce communication (Konečný et al., 2016) ), derivative-free optimization in reinforcement learning (to simplify the computation of the approximated gradient (Choromanski et al., 2018) ). Besides a number of practical applications, it is also an important research question whether or not models really need to be this big or if smaller results can achieve similar accuracy (Ba & Caruana, 2014) . Structured matrices are at the very core of most of the work on compact networks. In these models, dense weight matrices are replaced by matrices with a prescribed structure (e.g. low rank matrices, Toeplitz matrices, circulant matrices, LDR, etc.). Despite substantial efforts (e.g. Cheng et al. (2015) ; ), the performance of compact models is still far from achieving an acceptable accuracy motivating their use in real-world scenarios. This raises several questions about the effectiveness of such models and about our ability to train them. In particular two main questions call for investigation: Q1 How to efficiently train deep neural networks with a large number of structured layers? Q2 What is the expressive power of structured layers compared to dense layers? In this paper, we provide principled answers to these questions for the particular case of deep neural networks based on diagonal and circulant matrices (a.k.a. Diagonal-circulant networks or DCNNs). The idea of using diagonal and circulant matrices together comes from a series of results in linear algebra by Müller-Quade et al. (1998) and . The most recent result from Huhtanen & Perämäki demonstrates that any matrix A in C n⇥n can be decomposed into the product of 2n 1 alternating diagonal and circulant matrices. The diagonal-circulant decomposition inspired to design the AFDF structured layer, which is the building block of DCNNs. However, were not able to train deep neural networks based on AFDF. To answer Q1, we first describe a theoretically sound initialization procedure for DCNN which allows the signal to propagate through the network without vanishing or exploding. Furthermore, we provide a number of empirical insights to explain the behaviour of DCNNs, and show the impact of the number of the non-linearities in the network on the convergence rate and the accuracy of the network. By combining all these insights, we are able (for the first time) to train large and deep DCNNs. We demonstrate the good performance of DCNNs on a large scale application (the YouTube-8M video classification problem) and obtain very competitive accuracy. To answer Q2, we propose an analysis of the expressivity of DCNNs by extending the results by . We introduce a new bound on the number of diagonal-circulant required to approximate a matrix that depends on its rank. Building on this result, we demonstrate that a DCNN with bounded width and small depth can approximate any dense networks with ReLU activations. Outline of the paper: We present in Section 2 the related work on structured neural networks and several compression techniques. Section 3 introduces circulant matrices, our new result extending the one from . Section 4 proposes an theoretical analysis on the expressivity on DCNNs. Section 5 describes two efficient techniques for training deep diagonal circulant neural networks. Finally, Section 6 presents extensive experiments to compare the performance of deep diagonal circulant neural networks in different settings w.r.t. other state of the art approaches. Section 7 provides a discussion and concluding remarks. This paper deals with the training of diagonal circulant neural networks. To the best of our knowledge, training such networks with a large number of layers had not been done before. We also endowed this kind of models with theoretical guarantees, hence enriching and refining previous theoretical work from the literature. More importantly, we showed that DCNNs outperform their competing structured alternatives, including the very recent general approach based on LDR networks. Our results suggest that stacking diagonal circulant layers with non linearities improves the convergence rate and the final accuracy of the network. Formally proving these statements constitutes the future directions of this work. As future work, we would like to generalize the good results of DCNNs to convolutions neural networks. We also believe that circulant matrices deserve a particular attention in deep learning because of their strong ties with convolutions: a circulant matrix operator is equivalent to the convolution operator with circular paddings (as shown in [5]). This fact makes any contribution to the area of circulant matrices particularly relevant to the field of deep learning with impacts beyond the problem of designing compact models. As future work, we would like to generalize our results to deep convolutional neural networks.","We train deep neural networks based on diagonal and circulant matrices, and show that this type of networks are both compact and accurate on real world applications.",Toeplitz ; Sainath & Parada ; two ; Ba & Caruana ; first ; ReLU ; al. ; recent years ; Konečný ; LDR,the training ; non-linearity functions ; The most recent result ; deep learning ; diagonal circulant layers ; other types ; Toeplitz ; several compression techniques ; communication ; the next best approach,Toeplitz ; Sainath & Parada ; two ; Ba & Caruana ; first ; ReLU ; al. ; recent years ; Konečný ; LDR,"In this paper, we examine deep diagonal circulant neural networks, which are deep neural networks where weight matrices are the product of diagonal and circulants. We introduce principled techniques for training these models, such as a smart use of non-linearity functions in order to train them. Furthermore, we show that these networks outperform recently introduced deep networks with other types of structured layers. We conduct a thorough experimental study to compare the performance of deep diagonalcirculant networks with state of the art models based on structured matrices and with dense models. We show that our models achieve better accuracy than other structured approaches while required",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"There is no consensus yet on the question whether adaptive gradient methods like Adam are easier to use than non-adaptive optimization methods like SGD. In this work, we fill in the important, yet ambiguous concept of ‘ease-of-use’ by defining an optimizer’s tunability:  How easy is it to find good hyperparameter configurations using automatic random hyperparameter search? We propose a practical and universal quantitative measure for optimizer tunability that can form the basis for a fair optimizer benchmark.   Evaluating a variety of optimizers on an extensive set of standard datasets and architectures, we find  that Adam is the most tunable for the majority of problems, especially with a low budget for hyperparameter tuning. With the ubiquity of deep learning in various applications, a multitude of first-order stochastic optimizers (Robbins & Monro, 1951) have been in vogue. They have varying algorithmic components like momentum (Sutskever et al., 2013 ) and adaptive learning rates (Tieleman & Hinton, 2012; Duchi et al., 2011; Kingma & Ba, 2015) . With all these choices, picking the optimizer is among the most important design decisions for machine learning practitioners. For this decision, the best possible generalization performance is certainly an important characteristic to be taken into account. However, we argue that in practice, an even more important characteristic is whether the best possible performance can be reached with the available resources. The performance of optimizers strongly depends on the choice of hyperparameter values such as the learning rate. In the machine learning research community, the sensitivity of models to hyperparameters has been of great debate recently, where in multiple cases, reported model advances did not stand the test of time because they can be explained by better hyperparameter tuning (Lucic et al., 2018; Melis et al., 2018; Henderson et al., 2018) . This has led to calls for using automatic hyperparameter optimization methods with a fixed budget for a fairer comparison of models (Sculley et al., 2018; Feurer & Hutter, 2019; Eggensperger et al., 2019) . For industrial applications, automated machine learning (AutoML, , which has automatic hyperparameter optimization as one of its key concepts, is becoming increasingly more important. In both cases, an optimization algorithm that achieves good performances with relatively little tuning effort is arguably substantially more useful than an optimization algorithm that achieves top performances, but reaches it only with a lot of careful tuning effort. Hence, we advocate that the performance obtained by an optimizer is not only the best performance obtained when using that optimizer, but also has to account for the cost of tuning its hyperparameters to obtain that performance, thus being dichotomous. We term this concept tunability in this paper. Despite the importance of this concept, there is no standard way of measuring tunability. Works that propose optimization techniques show their performance on various tasks as depicted in Table 1 . It is apparent that the experimental settings, as well as the network architectures tested, widely vary, hindering a fair comparison. The introduction of benchmarking suites like DEEPOBS (Schneider et al., 2019) have standardized the tested architectures, however, this does not fix the problem of selecting the hyperparameters themselves, and the effort expended in doing so. Previous studies treat tunability to be the best performance obtained on varying a hyperparameter (Schneider et al., 2019) or by measuring the improvement in performance by tuning a hyperparameter (Probst et al., 2019 ), but do not take any cognizance to the intermediate performance during the tuning process. Table 1 : Experimental settings shown in the original papers of popular optimizers. The large differences in test problems and tuning methods make them difficult to compare. γ denotes learning rate, µ denotes momentum, λ is the weight decay coefficient. Our work proposes a new notion of tunability for optimizers that takes into account the tuning efforts of an HPO. The results of our experiments support the hypothesis that adaptive gradient methods are easier to tune than non-adaptive methods: In a setting with low budget for hyperparameter tuning, tuning only Adam optimizer's learning rate is likely to be a very good choice; it doesn't guarantee the best possible performance, but it is evidently the easiest to find well-performing hyperparameter configurations for. While SGD yields the best performance in some cases, its best configuration is tedious to find, and Adam often performs close to it. We, thus, state that the substantial value of the adaptive gradient methods, specifically Adam, is its amenability to hyperparameter search. This is in contrast to the findings of Wilson et al. (2017) who observe no advantage in tunabilty for adaptive gradient methods, and thus deem them to be of 'marginal value'. Unlike them, we base our experiments on a standard hyperparameter optimization method that allows for an arguably fairer comparison. Our study is certainly not exhaustive: We do not study the effect of the inclusion of a learning rate schedule, or using a different HPO algorithm on the results. However, their inclusion would result in a large increase the number of experiments, and constitutes our future work. We hope that this paper encourages other researchers to conduct future studies on the performance of optimizers from a more holistic perspective, where the cost of the hyperparameter search is included.",We provide a method to benchmark optimizers that is cognizant to the hyperparameter tuning process.,Duchi ; Feurer & Hutter ; Adam ; Sculley ; first ; Melis ; al. ; Robbins & Monro ; Tieleman & Hinton ; SGD,a very good choice ; a practical and universal quantitative measure ; that performance ; their inclusion ; no advantage ; a variety ; Kingma & Ba ; the tuning efforts ; the most important design decisions ; a multitude,Duchi ; Feurer & Hutter ; Adam ; Sculley ; first ; Melis ; al. ; Robbins & Monro ; Tieleman & Hinton ; SGD,"There is no consensus on whether adaptive gradient methods are easier to use than non-adaptive optimization methods like SGD. In this work, we define an optimizer's tunability by defining it using automatic random hyperparameter search. The practical and universal quantitative measure for optimizer tunability can form the basis for a fair optimizer benchmark.   Evaluating a variety of optimizers on an extensive set of standard datasets and architectures, we find  that Adam is the most tunable optimizer for the majority of problems, especially with a low budget for hyperparameters tuning. This is due to the ubiquity of deep",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Parameter pruning is a promising approach for CNN compression and acceleration by eliminating redundant model parameters with tolerable performance loss. Despite its effectiveness, existing regularization-based parameter pruning methods usually drive weights towards zero with large and constant regularization factors, which neglects the fact that the expressiveness of CNNs is fragile and needs a more gentle way of regularization for the networks to adapt during pruning. To solve this problem, we propose a new regularization-based pruning method (named IncReg) to incrementally assign different regularization factors to different weight groups based on their relative importance, whose effectiveness is proved on popular CNNs compared with state-of-the-art methods. Recently, deep Convolutional Neural Networks (CNNs) have made a remarkable success in computer vision tasks by leveraging large-scale networks learning from big amount of data. However, CNNs usually lead to massive computation and storage consumption, thus hindering their deployment on mobile and embedded devices. To solve this problem, many research works focus on compressing the scale of CNNs. Parameter pruning is a promising approach for CNN compression and acceleration, which aims at eliminating redundant model parameters at tolerable performance loss. To avoid hardware-unfriendly irregular sparsity, structured pruning is proposed for CNN acceleration BID0 BID22 . In the im2col implementation BID1 BID3 of convolution, weight tensors are expanded into matrices, so there are generally two kinds of structured sparsity, i.e. row sparsity (or filter-wise sparsity) and column sparsity (or shape-wise sparsity) BID24 BID23 .There are mainly two categories of structured pruning. One is importance-based methods, which prune weights in groups based on some established importance criteria BID17 BID19 BID23 . The other is regularization-based methods, which add group regularization terms to learn structured sparsity BID24 BID16 BID8 . Existing group regularization approaches mainly focus on the regularization form (e.g. Group LASSO BID26 ) to learn structured sparsity, while ignoring the influence of regularization factor. In particular , they tend to use a large and constant regularization factor for all weight groups in the network BID24 BID16 , which has two problems. Firstly, this 'one-size-fit-all' regularization scheme has a hidden assumption that all weights in different groups are equally important, which however does not hold true, since weights with larger magnitude tend to be more important than those with smaller magnitude. Secondly, few works have noticed that the expressiveness of CNNs is so fragile BID25 during pruning that it cannot withstand a large penalty term from beginning, especially for large pruning ratios and compact networks (like ResNet BID7 ). AFP BID6 was proposed to solve the first problem, while ignored the second one. In this paper , we propose a new regularization-based method named IncReg to incrementally learn structured sparsity. We propose a new structured pruning method based on an incremental way of regularization, which helps CNNs to transfer their expressiveness to the rest parts during pruning by increasing the regularization factors of unimportant weight groups little by little. Our method is proved to be comparably effective on popular CNNs compared with state-of-the-art methods, especially in face of large pruning ratios and compact networks.",we propose a new regularization-based pruning method (named IncReg) to incrementally assign different regularization factors to different weight groups based on their relative importance.,second ; two ; CNN ; Firstly ; Secondly ; Convolutional Neural Networks ; first ; One ; IncReg ; zero,few works ; redundant model parameters ; filter-wise sparsity ; their expressiveness ; the first problem ; its effectiveness ; Convolutional Neural Networks ; all weights ; a new regularization-based pruning method ; the expressiveness,second ; two ; CNN ; Firstly ; Secondly ; Convolutional Neural Networks ; first ; One ; IncReg ; zero,"Parameter pruning is a promising approach for CNN compression and acceleration by eliminating redundant model parameters with tolerable performance loss. However, existing regularization-based parameter pruning methods usually drive weights towards zero with large and constant regularization factors, which neglects the fact that the expressiveness of CNNs is fragile and needs a more gentle way of regularization for the networks to adapt during pruning. To address this problem, we propose a regularization based pruning method (incReg) to incrementally assign different regularization factor to different weight groups based on their relative importance, which is proved on popular CNNs compared with state",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Generative deep learning has sparked a new wave of Super-Resolution (SR) algorithms that enhance single images with impressive aesthetic results, albeit with imaginary details. Multi-frame Super-Resolution (MFSR) offers a more grounded approach to the ill-posed problem, by conditioning on multiple low-resolution views. This is important for satellite monitoring of human impact on the planet -- from deforestation, to human rights violations -- that depend on reliable imagery. To this end, we present HighRes-net, the first deep learning approach to MFSR that learns its sub-tasks in an end-to-end fashion: (i) co-registration, (ii) fusion, (iii) up-sampling, and (iv) registration-at-the-loss. Co-registration of low-res views is learned implicitly through a reference-frame channel, with no explicit registration mechanism. We learn a global fusion operator that is applied recursively on an arbitrary number of low-res pairs. We introduce a registered loss, by learning to align the SR output to a ground-truth through ShiftNet. We show that by learning deep representations of multiple views, we can super-resolve low-resolution signals and enhance Earth observation data at scale. Our approach recently topped the European Space Agency's MFSR competition on real-world satellite imagery. Multiple low-resolution images collectively contain more information than any individual lowresolution image, due to minor geometric displacements, e.g. shifts, rotations, atmospheric turbulence, and instrument noise. Multi-Frame Super-Resolution (MFSR) (Tsai, 1984) aims to reconstruct hidden high-resolution details from multiple low-resolution views of the same scene. Single Image Super-Resolution (SISR), as a special case of MFSR, has attracted much attention in the computer vision, machine learning and deep learning communities in the last 5 years, with neural networks learning complex image priors to upsample and interpolate images (Xu et al., 2014; Srivastava et al., 2015; He et al., 2016) . However, in the meantime not much work has explored the learning of representations for the more general problem of MFSR to address the additional challenges of co-registration and fusion of multiple low-resolution images. This paper explores how Multi-Frame Super-Resolution (MFSR) can benefit from recent advances in learning representations with neural networks. To the best of our knowledge, this work is the first to introduce a deep-learning approach that solves the co-registration, fusion and registration-at-theloss problems in an end-to-end learning framework. Prompting this line of research is the increasing drive towards planetary-scale Earth observation to monitor the environment and human rights violations. Such observation can be used to inform policy, achieve accountability and direct on-the-ground action, e.g. within the framework of the Sustainable Development Goals (Jensen & Campbell, 2019) . Nomenclature Registration is the problem of estimating the relative geometric differences between two images (e.g. due to shifts, rotations, deformations). Fusion, in the MFSR context, is the problem of mapping multiple low-res representations into a single representation. By coregistration, we mean the problem of registering all low-resolution views to improve their fusion. By registration-at-the-loss, we mean the problem of registering the super-resolved reconstruction to the high-resolution ground-truth prior to computing the loss. This gives rise to the notion of a registered loss. Co-registration of multiple images is required for longitudinal studies of land change and environmental degradation. The fusion of multiple images is key to exploiting cheap, high-revisit-frequency satellite imagery, but of low-resolution, moving away from the analysis of infrequent and expensive high-resolution images. Finally, beyond fusion itself, super-resolved generation is required throughout the technical stack: both for labeling, but also for human oversight (Drexler, 2019) demanded by legal context (Harris et al., 2018) . In this paper, we presented HighRes-net -the first deep learning approach to multi-frame superresolution that learns typical sub-tasks of MFSR in an end-to-end fashion: (i) co-registration, (ii) fusion, (iii) up-sampling, and (iv) registration-at-the-loss. It recursively fuses a variable number of low-resolution views by learning a global fusion operator. The overall fusion also aligns all low-resolution views with an implicit co-registration mechanism through the reference channel. We also introduced ShiftNet-Lanczos, a network that learns to register and align the super-resolved output of HighRes-net with a high-resolution ground-truth. Registration is important both to align multiple low-resolution inputs (co-registration) and to compute similarity metrics between shifted signals. Our experiments suggest that an end-to-end cooperative setting (HighRes-net + ShiftNet-Lanczos) helps with training and test performance. By design, our approach is fast to train, faster to test, and low in terms of memory-footprint by doing the bulk of the computational work (co-registration + fusion) on multiple images while maintaining their low-resolution height & width. There is an ongoing proliferation of low-resolution yet high-revisit low-cost satellite imagery, but they often lack the detailed information of expensive high-resolution imagery. We believe MFSR can raise its potential to NGOs and non-profits that contribute to the UN Sunstainable Development Goals. A APPENDIX","The first deep learning approach to MFSR to solve registration, fusion, up-sampling in an end-to-end manner.",HighRes ; MFSR ; Xu et al. ; two ; Drexler ; Super-Resolution ; ShiftNet ; Earth ; the Sustainable Development Goals ; Multi-Frame Super-Resolution,"low-resolution yet high-revisit low-cost satellite imagery ; shifted signals ; registration, (ii) fusion ; neural networks ; conditioning ; the European Space Agency's ; the planet ; the-ground ; a variable number ; deep representations",HighRes ; MFSR ; Xu et al. ; two ; Drexler ; Super-Resolution ; ShiftNet ; Earth ; the Sustainable Development Goals ; Multi-Frame Super-Resolution,"Super-Resolution (SR) algorithms enhance single images by conditioning on multiple low-resolution views. This approach is important for satellite monitoring of human impact on the planet, such as deforestation, to human rights violations. HighRes-net, the first deep learning approach to MFSR, learns sub-tasks in an end-to-end fashion, including co-registration, fusion, up-sampling, and registration-at-the-loss. It demonstrates that by learning deep representations of multiple views, we can super-resolve lower-resolution signals and enhance Earth observation data at scale. The concept of",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"We study the problem of learning permutation invariant representations that can capture containment relations. We propose training a model on a novel task: predicting the size of the symmetric difference between pairs of multisets, sets which may contain multiple copies of the same object. With motivation from fuzzy set theory, we formulate both multiset representations and how to predict symmetric difference sizes given these representations. We model multiset elements as vectors on the standard simplex and multisets as the summations of such vectors, and we predict symmetric difference as the l1-distance between multiset representations. We demonstrate that our representations more effectively predict the sizes of symmetric differences than DeepSets-based approaches with unconstrained object representations. Furthermore, we demonstrate that the model learns meaningful representations, mapping objects of different classes to different standard basis vectors. Tasks for which the input is an unordered collection, i.e. a set, are ubiquitous and include multipleinstance learning Ilse et al. (2018) , point-cloud classification Zaheer et al. (2017) ; Qi et al. (2017) , estimating cosmological parameters Zaheer et al. (2017) ; Ravanbakhsh et al. (2016) , collaborative filtering Hartford et al. (2018) , and relation extraction Verga et al. (2017) ; Rossiello et al. (2019) . Recent work has demonstrated the benefits of permutation invariant models that have inductive biases well aligned with the set-based input of the tasks (Ilse et al., 2018; Qi et al., 2017; Zaheer et al., 2017; Lee et al., 2019) . The containment relationship between sets -and intersection more generally -is often considered as a measure of relatedness. For instance, when comparing the keywords for two documents, we may wish to model that {currency, equilibrium} describes a more specific set of topics than (i.e. is ""contained"" in) {money, balance, economics}. The containment order is a natural partial order on sets. However, we are often interested not in sets, but multisets, which may contain multiple copies of the same object; examples include bags-of-words, geo-location data over a time period, and data in any multiple-instance learning setting (Ilse et al., 2018) . The containment order can be extended to multisets. Learning to represent multisets in a way that respects this partial order is a core representation learning challenge. Note that this may require modeling not just exact containment, but relations that consider the relatedness of individual objects. We may want to learn representations of the multisets' elements which induce the desired multiset relations. In the aforementioned example, we may want money ≈ currency and balance ≈ equilibrium. Previous work has considered modeling hierarchical relationships or orderings between pairs of individual items (Ganea et al., 2018; Lai and Hockenmaier, 2017; Nickel and Kiela, 2017; Suzuki et al., 2019; Vendrov et al., 2015; Vilnis et al., 2018; Vilnis and McCallum, 2015; Li et al., 2019; Athiwaratkun and Wilson, 2018) . However, this work does not naturally extend from representing individual items to modeling relations between multisets via the elements' learned representations. Furthermore, we may want to consider richer information about the relationship between two multisets beyond containment, such as the size of their intersection. In this paper, we present a measure-theoretic definition of multisets, which lets us formally define the ""flexible containment"" notion exemplified above. The theory lets us derive method for learning representations of multisets and their elements, given the relationships between pairs of multisets -in particular, we propose to use the sizes of their symmetric differences or of their intersections. We learn these representations with the goal of predicting the relationships between unseen pairs of multisets (whose elements may themselves have been unseen during training). We prove that this allows us to predict containment relations between unseen pairs of multisets. We show empirically that the theoretical basis of our model is important for being able to capture these relations, comparing our approach to DeepSets-based approaches (Zaheer et al., 2017) with unconstrained item representations. Furthermore, we demonstrate that our model learns ""meaningful"" representations. 2 RELATED WORK 2.1 SET REPRESENTATION Qi et al. (2017) and Zaheer et al. (2017) both explore learning functions on sets. Importantly, they arrive at similar theoretical statements about the approximation of such functions, which rely on permutation invariant pooling functions. In particular, Zaheer et al. (2017) show that any set function f (A) can be approximated by a model of the form ρ a∈A φ(a) for some learned ρ and φ, which they call DeepSets. They note that the sum can be replaced by a max-pool (which is essentially the formulation of Qi et al. (2017) ), and observe empirically that this leads to better performance. 1 More recently, there has been some very interesting work on leveraging the relationship between sets. Probst (2018) proposes a set autoencoder, while Skianis et al. (2019) learn set representations with a network that compares the input set to trainable ""hidden sets."" However, both these approaches require solving computationally expensive matching problems at each iteration. Vendrov et al. (2015) and Ganea et al. (2018) seek to model partial orders on objects via geometric relationships between their embeddings -namely, using cones in Euclidean space and hyperbolic space, respectively. Nickel and Kiela (2017) use a similar idea to embed hierarchical network structures in hyperbolic space, simply using the hyperbolic distance between embeddings. These approaches are unified under the framework of ""disk embeddings"" by Suzuki et al. (2019) . The idea is to map each object to the product space X × R, where X is a (pseudo-)metric space. This mapping can be expressed as A → (f (A), r(A)), and it is trained with the objective that A B if and only if d X (f (A), f (B)) ≤ r(B) − r(A). An equivalent statement can be made for multisets (see Proposition 3.2.4) . We propose a novel task: predicting the size of either the symmetric difference of the intersection between pairs of multisets. We motivate this construction via a measure-theoretic notion of ""flexible containment."" We demonstrate the utility of this idea, developing a theoretically-motivated model that given only the sizes of symmetric differences between pairs of multisets, learns representations of such multisets and their elements. These representations allow us to predict containment relations with extremely high accuracy. Our model learns to map each type of object to a standard basis vector, thus essentially performing semi-supervised clustering. One interesting area for future theoretical work is understanding a related problem: clustering n objects given multiset difference sizes. As a first step, we show in Appendix H that n − 1 specific multiset comparisons are sufficient to recover the clusters. We would also be curious to see if one can learn the latent multiset space U. Following similar reasoning, we can convince ourselves that multiset union should be defined as It is important to differentiate this from ""multiset addition,"" which simply combines two multisets directly: A + B = {1, 1, 1, 1, 1, 2, 2, 2, 3} for our example above, and in general m A+B = m A (x) + m B (x). Multiset difference is a little harder to define. The main problem is that we cannot rely on a notion of ""complement"" for multisets. Instead, let us again try to reason by example. For our example multisets above, we have A \ B = {1, 2}. To arrive at this result, we remove from A each copy of an element which also appears in B. Note that if B had more of a certain element than A, that element would not appear in the final result. In other words, we are performing a subtraction of counts which is ""glued"" to a minimum value of zero. That is, m A\B (x) = max{m A (x) − m B (x), 0}. We can further convince ourselves of the correctness of this expression by noting that we recover the identity Finally, symmetric multiset difference can be defined using our expression for multiset difference, combined with either multiset addition or union. In particular, note that A B = (A\B)+(B \A) = (A \ B) ∪ (B \ A) -addition and union both work because (A \ B) and (B \ A) are necessarily disjoint. This gives us: (The equation still holds if we replace the addition with a maximum.)","Based on fuzzy set theory, we propose a model that given only the sizes of symmetric differences between pairs of multisets, learns representations of such multisets and their elements.",φ(a ; U. Following ; Rossiello ; first ; Li ; al. ; Qi et al. ; Ganea ; Suzuki et al ; Kiela,Previous work ; some very interesting work ; containment relations ; this paper ; future theoretical work ; the desired multiset relations ; relatedness ; Lai ; such vectors ; mapping objects,φ(a ; U. Following ; Rossiello ; first ; Li ; al. ; Qi et al. ; Ganea ; Suzuki et al ; Kiela,"The problem of learning permutation invariant representations that can capture containment relations is addressed by training a model on a novel task: predicting the size of the symmetric difference between pairs of multisets, sets containing multiple copies of the same object. With motivation from fuzzy set theory, we formulate both multiset representations and how to predict asymmetric difference sizes given these representations. These representations are modeled as vectors on the standard simplex and multisaets as the summations of such vectors, and they predict symmetric differences as the l1-distance. The model learns meaningful representations, mapping objects of different classes to different standard basis",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Zeroth-order optimization is the process of minimizing an objective $f(x)$, given oracle access to evaluations at adaptively chosen inputs $x$. In this paper, we present two simple yet powerful GradientLess Descent (GLD) algorithms that do not rely on an underlying gradient estimate and are numerically stable. We analyze our algorithm from a novel geometric perspective and we show that for {\it any monotone transform} of a smooth and strongly convex objective with latent dimension $k \ge n$, we present a novel analysis that shows convergence within an $\epsilon$-ball of the optimum in $O(kQ\log(n)\log(R/\epsilon))$ evaluations, where the input dimension is $n$, $R$ is the diameter of the input space and $Q$ is the condition number. Our rates are the first of its kind to be both 1) poly-logarithmically dependent on dimensionality and 2) invariant under monotone transformations. We further leverage our geometric perspective to show that our analysis is optimal. Both monotone invariance and its ability to utilize a low latent dimensionality are key to the empirical success of our algorithms, as demonstrated on synthetic and MuJoCo benchmarks.
 We consider the problem of zeroth-order optimization (also known as gradient-free optimization, or bandit optimization), where our goal is to minimize an objective function f : R n → R with as few evaluations of f (x) as possible. For many practical and interesting objective functions, gradients are difficult to compute and there is still a need for zeroth-order optimization in applications such as reinforcement learning (Mania et al., 2018; Salimans et al., 2017; Choromanski et al., 2018) , attacking neural networks Papernot et al., 2017) , hyperparameter tuning of deep networks (Snoek et al., 2012) , and network control (Liu et al., 2017) . The standard approach to zeroth-order optimization is, ironically, to estimate the gradients from function values and apply a first-order optimization algorithm (Flaxman et al., 2005) . Nesterov & Spokoiny (2011) analyze this class of algorithms as gradient descent on a Gaussian smoothing of the objective and gives an accelerated O(n √ Q log((LR 2 + F )/ )) iteration complexity for an LLipschitz convex function with condition number Q and R = x 0 − x * and F = f (x 0 ) − f (x * ). They propose a two-point evaluation scheme that constructs gradient estimates from the difference between function values at two points that are close to each other. This scheme was extended by (Duchi et al., 2015) for stochastic settings, by (Ghadimi & Lan, 2013) for nonconvex settings, and by (Shamir, 2017) for non-smooth and non-Euclidean norm settings. Since then, first-order techniques such as variance reduction (Liu et al., 2018) , conditional gradients (Balasubramanian & Ghadimi, 2018) , and diagonal preconditioning (Mania et al., 2018) have been successfully adopted in this setting. This class of algorithms are also known as stochastic search, random search, or (natural) evolutionary strategies and have been augmented with a variety of heuristics, such as the popular CMA-ES (Auger & Hansen, 2005) . These algorithms, however, suffer from high variance due to non-robust local minima or highly non-smooth objectives, which are common in the fields of deep learning and reinforcement learn-ing. Mania et al. (2018) notes that gradient variance increases as training progresses due to higher variance in the objective functions, since often parameters must be tuned precisely to achieve reasonable models. Therefore, some attention has shifted into direct search algorithms that usually finds a descent direction u and moves to x + δu, where the step size is not scaled by the function difference. The first approaches for direct search were based on deterministic approaches with a positive spanning set and date back to the 1950s (Brooks, 1958) . Only recently have theoretical bounds surfaced, with Gratton et al. (2015) giving an iteration complexity that is a large polynomial of n and Dodangeh & Vicente (2016) giving an improved O(n 2 L 2 / ). Stochastic approaches tend to have better complexities: Stich et al. (2013) uses line search to give a O(nQ log(F/ )) iteration complexity for convex functions with condition number Q and most recently, Gorbunov et al. (2019) uses importance sampling to give a O(nQ log(F/ )) complexity for convex functions with average condition numberQ, assuming access to sampling probabilities. Stich et al. (2013) notes that direct search algorithms are invariant under monotone transforms of the objective, a property that might explain their robustness in high-variance settings. In general, zeroth order optimization suffers an at least linear dependence on input dimension n and recent works have tried to address this limitation when n is large but f (x) admits a low-dimensional structure. Some papers assume that f (x) depends only on k coordinates and Wang et al. (2017) applies Lasso to find the important set of coordinates, whereas Balasubramanian & Ghadimi (2018) simply change the step size to achieve an O(k(log(n)/ ) 2 ) iteration complexity. Other papers assume more generally that f (x) = g(P A x) only depends on a k-dimensional subspace given by the range of P A and Djolonga et al. (2013) apply low-rank approximation to find the low-dimensional subspace while Wang et al. (2013) use random embeddings. Hazan et al. (2017) assume that f (x) is a sparse collection of k-degree monomials on the Boolean hypercube and apply sparse recovery to achieve a O(n k ) runtime bound. We will show that under the case that f (x) = g(P A x), our algorithm will inherently pick up any low-dimensional structure in f (x) and achieve a convergence rate that depends on k log(n). This initial convergence rate survives, even if we perturb f (x) = g(P A x) + h(x), so long as h(x) is sufficiently small. We will not cover the whole variety of black-box optimization methods, such as Bayesian optimization or genetic algorithms. In general, these methods attempt to solve a broader problem (e.g. multiple optima), have weaker theoretical guarantees and may require substantial computation at each step: e.g. Bayesian optimization generally has theoretical iteration complexities that grow exponentially in dimension, and CMA-ES lacks provable complexity bounds beyond convex quadratic functions. In addition to the slow runtime and weaker guarantees, Bayesian optimization assumes the success of an inner optimization loop of the acquisition function. This inner optimization is often implemented with many iterations of a simpler zeroth-order methods, justifying the need to understand gradient-less descent algorithms within its own context.",Gradientless Descent is a provably efficient gradient-free algorithm that is monotone-invariant and fast for high-dimensional zero-th order optimization.,two ; non-Euclidean ; CMA-ES ; Lasso ; Auger & Hansen ; f(x)$ ; al. ; Papernot ; Gorbunov ; Nesterov & Spokoiny,a low-dimensional structure ; monotone transforms ; the function difference ; Wang et al. ; our algorithm ; This inner optimization ; $Q$ ; first ; Bayesian ; the difference,two ; non-Euclidean ; CMA-ES ; Lasso ; Auger & Hansen ; f(x)$ ; al. ; Papernot ; Gorbunov ; Nesterov & Spokoiny,"Zeroth-order optimization is the process of minimizing an objective $f(x)$, given oracle access to evaluations at adaptively chosen inputs $x. It is a simple yet powerful GradientLess Descent (GLD) algorithm that does not rely on an underlying gradient estimate and are numerically stable. We analyze our algorithm from a novel geometric perspective and show that for a smooth and strongly convex objective with latent dimension $k \ge n$, we can achieve convergence within an $\epsilon$-ball of the optimum in $O(kQ\log(n)\log(R/\ep",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"We explore the behavior of a standard convolutional neural net in a setting that introduces classification tasks sequentially and requires the net to master new tasks while preserving mastery of previously learned tasks.   This setting corresponds to that which human learners face as they acquire domain expertise, for example, as an individual reads a textbook chapter-by-chapter. Through simulations involving sequences of 10 related tasks, we find reason for optimism that nets will scale well as they advance from having a single skill to becoming domain experts. We observed two key phenomena. First, forward facilitation---the accelerated learning of task n+1 having learned n previous tasks---grows with n. Second, backward interference---the forgetting of the n previous tasks when learning task n+1---diminishes with n.  Forward facilitation is the goal of research on metalearning, and reduced backward interference is the goal of research on ameliorating catastrophic forgetting. We find that both of these goals are attained simply through broader exposure to a domain. We explored the behavior of a standard convolutional neural net for classification tasks in a setting that introduces tasks sequentially and requires the net to master new tasks while preserving mastery of previously learned tasks. This setting corresponds to that which human learners face as they become experts in a domain, for example, as they read a textbook chapter by chapter. Our network exhibits six interesting properties:1. Forward facilitation is observed once the net has acquired sufficient expertise in the domain, as evidenced by requiring less training to learn new tasks as a function of the number of related tasks learned (see highlighted black curve in FIG2 BID8 BID8 . 5. Training performance improves according to a power function of the number of tasks learned, controlling for experience on a task (the slope of the curves in FIG2 , and also according to a power function of the amount of training a given task has received, controlling for number of tasks learned (the slope of the curves in FIG2 ). Power-law learning is a robust characteristic of human skill acquisition, observed on a range of behavioral measures BID20 BID7 . 6. Catastrophic forgetting is evidenced primarily for task 1 when task 2 is learned-the canonical case studied in the literature. However, the model becomes more robust as it acquires sufficient domain experience, and eventually the relearning effort becomes negligible (see copper curves in FIG2 ,f). The anomalous behavior of task 2 is noteworthy, yielding a transition behavior that is perhaps analogous to the ""zero one infinity"" rule coined by Willem van der Poel.We are able to identify these interesting phenomena because our simulations examined scaling behavior and not just effects of one task on a second-the typical case for studying catastrophic forgetting-or the effects of many tasks on a subsequent task-the typical case for metalearning and few-shot learning. Studying the entire continuum from the first task to the n'th is quite revealing.We found strong evidence for improved learning performance with broader domain expertise, and further investigation is merited. We are beginning investigations that examine how similar tasks must be to facilitate one another: how does scaling behavior change when the tasks dimensions switch across successive episodes (e.g., from color to shape to texture)? Our preliminary results suggest that the domain knowledge acquired is quite general and extends to other dimensions of the images. We are also examining the scaling properties of metalearning methods that are explicitly designed to facilitate transfer. The results presented in this article can serve as a baseline to measure the magnitude of facilitation that the specialized methods offer. A holy grail of sorts would be to identify methods that demonstrate backward facilitation, where training on later tasks improves performance on earlier tasks, and compositional generalization BID11 BID10 BID17 , where learning the interrelationship among earlier tasks allows new tasks to be performed on the first trial. Humans demonstrate the former under rare conditions BID1 BID12 ; the latter is common in human behavior, as when individuals are able to perform a task immediately from instruction . 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329",We study the behavior of a CNN as it masters new tasks while preserving mastery for previously learned tasks,two ; First ; n. Second ; six ; zero ; one ; Willem van der ; Poel ; second,the number ; a transition behavior ; corresponds ; a task ; Our preliminary results ; the domain ; metalearning ; Willem van der ; sequences ; domain experts,two ; First ; n. Second ; six ; zero ; one ; Willem van der ; Poel ; second,"We explored the behavior of a standard convolutional neural net for classification tasks in a setting that introduces tasks sequentially and requires the net to master new tasks while preserving mastery of previously learned tasks. This setting corresponds to that which human learners face as they acquire domain expertise, for example, as an individual reads a textbook chapter-by-chapter. Through simulations involving sequences of 10 related tasks, we observed two key phenomena: forward facilitation, backward interference, and backward interference. Forward facilitation is the goal of research on metalearning, and reduced backward interference are the goals for ameliorating catastrophic forgetting. These goals are",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"A central challenge in multi-agent reinforcement learning is the induction of coordination between agents of a team. In this work, we investigate how to promote inter-agent coordination using policy regularization and discuss two possible avenues respectively based on inter-agent modelling and synchronized sub-policy selection. We test each approach in four challenging continuous control tasks with sparse rewards and compare them against three baselines including MADDPG, a state-of-the-art multi-agent reinforcement learning algorithm. To ensure a fair comparison, we rely on a thorough hyper-parameter selection and training methodology that allows a fixed hyper-parameter search budget for each algorithm and environment. We consequently assess both the hyper-parameter sensitivity, sample-efficiency and asymptotic performance of each learning method. Our experiments show that the proposed methods lead to significant improvements on cooperative problems. We further analyse the effects of the proposed regularizations on the behaviors learned by the agents. Multi-Agent Reinforcement Learning (MARL) refers to the task of training an agent to maximize its expected return by interacting with an environment that contains other learning agents. It represents a challenging branch of Reinforcement Learning (RL) with interesting developments in recent years (Hernandez-Leal et al., 2018) . A popular framework for MARL is the use of a Centralized Training and a Decentralized Execution (CTDE) procedure (Lowe et al., 2017; Iqbal & Sha, 2019; Foerster et al., 2019; Rashid et al., 2018) . It is typically implemented by training critics that approximate the value of the joint observations and actions, which are used to train actors restricted to the observation of a single agent. Such critics, if exposed to coordinated joint actions leading to high returns, can steer the agents' policies toward these highly rewarding behaviors. However, these approaches depend on the agents luckily stumbling on these actions in order to grasp their benefit. Thus, it might fail in scenarios where coordination is unlikely to occur by chance. We hypothesize that in such scenarios, coordination-promoting inductive biases on the policy search could help discover coordinated behaviors more efficiently and supersede task-specific reward shaping and curriculum learning. In this work, we explore two different priors for successful coordination and use these to regularize the learned policies. The first avenue, TeamReg, assumes that an agent must be able to predict the behavior of its teammates in order to coordinate with them. The second, CoachReg, supposes that coordinating agents individually recognize different situations and synchronously use different subpolicies to react to them. In the following sections we show how to derive practical regularization terms from these premises and meticulously evaluate them 1 . Our contributions are twofold. First, we propose two novel approaches that aim at promoting coordination in multi-agent systems. Our methods augment CTDE MARL algorithms with additional multi-agent objectives that act as regularizers and are optimized jointly with the main return-maximization objective. Second, we design two new sparse-reward cooperative tasks in the multi-agent particle environment (Mordatch & Abbeel, 2018) . We use them along with two standard multi-agent tasks to present a detailed evaluation of our approaches against three different baselines. Finally, we validate our methods' key components by performing an ablation study. Our experiments suggest that our TeamReg objective provides a dense learning signal that helps to guide the policy towards coordination in the absence of external reward, eventually leading it to the discovery of high performing team strategies in a number of cooperative tasks. Similarly, by enforcing synchronous sub-policy selections, CoachReg enables to fine-tune a sub-behavior for each recognized situation yielding significant improvements on the overall performance. The proposed methods offer a way to incorporate new inductive biases in CTDE multi-agent policy search algorithms. In this work, we evaluate them by extending MADDPG, a state of the art algorithm widely used in the MARL litterature. We compare against vanilla MADDPG as well as two of its variations in the four cooperative multi-agent tasks described in Section 5. The first variation (DDPG) is the single-agent counterpart of MADDPG (decentralized training). The second (MADDPG + sharing) shares the policy and value-function models across agents. To offer a fair comparison between all methods, the hyper-parameter search routine is the same for each algorithm and environment (see Appendix D.1). For each search-experiment (one per algorithm per environment), 50 randomly sampled hyper-parameter configurations each using 3 training seeds (total of 150 runs) are used to train the models for 15, 000 episodes. For each algorithm-environment pair, we then select the best hyper-parameter configuration for the final comparison and retrain them on 10 seeds for twice as long. We give more details about the training setup and model selection in Appendix B and D.2. The results of the hyperparameter searches are given in Appendix D.5. In this work we introduced two policy regularization methods to promote multi-agent coordination within the CTDE framework: TeamReg, which is based on inter-agent action predictability and CoachReg that relies on synchronized behavior selection. A thorough empirical evaluation of these methods showed that they significantly improve asymptotic performances on cooperative multiagent tasks. Interesting avenues for future work would be to study the proposed regularizations on other policy search methods as well as to combine both incentives and investigate how the two coordinating objectives interact. Finally, a limitation of the current formulation is that it relies on single-step metrics, which simplifies off-policy learning but also limits the longer-term coordination opportunities. A promising direction is thus to explore model-based planning approaches to promote long-term multi-agent interactions. A TASKS DESCRIPTIONS SPREAD (Figure 3a ): In this environment, there are 3 agents (small orange circles) and 3 landmarks (bigger gray circles). At every timestep, agents receive a team-reward r t = n − c where n is the number of landmarks occupied by at least one agent and c the number of collisions occurring at that timestep. To maximize their return, agents must therefore spread out and cover all landmarks. Initial agents' and landmarks' positions are random. Termination is triggered when the maximum number of timesteps is reached. BOUNCE (Figure 3b ): In this environment, two agents (small orange circles) are linked together with a spring that pulls them toward each other when stretched above its relaxation length. At episode's mid-time a ball (smaller black circle) falls from the top of the environment. Agents must position correctly so as to have the ball bounce on the spring towards the target (bigger beige circle), which turns yellow if the ball's bouncing trajectory passes through it. They receive a team-reward of r t = 0.1 if the ball reflects towards the side walls, r t = 0.2 if the ball reflects towards the top of the environment, and r t = 10 if the ball reflects towards the target. At initialisation, the target's and ball's vertical position is fixed, their horizontal positions are random. Agents' initial positions are also random. Termination is triggered when the ball is bounced by the agents or when the maximum number of timesteps is reached. COMPROMISE (Figure 3c ): In this environment, two agents (small orange circles) are linked together with a spring that pulls them toward each other when stretched above its relaxation length. They both have a distinct assigned landmark (light gray circle for light orange agent, dark gray circle for dark orange agent), and receive a reward of r t = 10 when they reach it. Once a landmark is reached by its corresponding agent, the landmark is randomly relocated in the environment. Initial positions of agents and landmark are random. Termination is triggered when the maximum number of timesteps is reached. CHASE (Figure 3d ): In this environment, two predators (orange circles) are chasing a prey (turquoise circle). The prey moves with respect to a scripted policy consisting of repulsion forces from the walls and predators. At each timestep, the learning agents (predators) receive a teamreward of r t = n where n is the number of predators touching the prey. The prey has a greater max speed and acceleration than the predators. Therefore, to maximize their return, the two agents must coordinate in order to squeeze the prey into a corner or a wall and effectively trap it there. Termination is triggered when the maximum number of time steps is reached.",We propose regularization objectives for multi-agent RL algorithms that foster coordination on cooperative tasks.,Hernandez-Leal ; − ; Reinforcement Learning ; three ; MARL ; first ; two ; recent years ; CoachReg ; TASKS,Our methods ; the models ; sub-policy selection ; our methods' key components ; two agents ; the following sections ; high returns ; TeamReg ; significant improvements ; other learning agents,Hernandez-Leal ; − ; Reinforcement Learning ; three ; MARL ; first ; two ; recent years ; CoachReg ; TASKS,"Multi-agent reinforcement learning involves the induction of coordination between agents of a team. In this work, we examine how to promote inter-agent coordination using policy regularization and discuss two possible avenues based on inter-agents modelling and synchronized sub-policy selection. We test each approach in four challenging continuous control tasks with sparse rewards and compare them against three baselines including MADDPG, a state-of-the-art multi-agent learning algorithm. To ensure a fair comparison, we rely on a thorough hyper-parameter selection and training methodology that allows a fixed hyperparameter search budget for each algorithm and environment. The hyper",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Interpretability has largely focused on local explanations, i.e. explaining why a model made a particular prediction for a sample. These explanations are appealing due to their simplicity and local fidelity. However, they do not provide information about the general behavior of the model. We propose to leverage model distillation to learn global additive explanations that describe the relationship between input features and model predictions. These global explanations take the form of feature shapes, which are more expressive than feature attributions. Through careful experimentation, we show qualitatively and quantitatively that global additive explanations are able to describe model behavior and yield insights about models such as neural nets. A visualization of our approach applied to a neural net as it is trained is available at https://youtu.be/ErQYwNqzEdc Recent research in interpretability has focused on developing local explanations: given an existing model and a sample, explain why the model made a particular prediction for that sample BID40 . The accuracy and quality of these explanations have rapidly improved, and they are becoming important tools to understand model decisions for individual samples. However, the human cost of examining multiple local explanations can be prohibitive with today's large data sets, and it is unclear whether multiple local explanations can be aggregated without contradicting each other BID41 BID0 .In this paper, we are interested in global explanations that describe the overall behavior of a model. While usually not as accurate as local explanations on individual samples, global explanations provide a different, complementary view of the model. They allow us to clearly visualize trends in feature space, which is useful for key tasks such as understanding which features are important, detecting unexpected patterns in the training data and debugging errors learned by the model.We propose to use model distillation techniques BID7 BID24 to learn global additive explanations of the form DISPLAYFORM0 to approximate the prediction function of the model F (x). Figure 1 illustrates our approach. The output of our approach is a set of p feature shapes {h i } p 1 that can be composed to form an explanation model that can be quantitatively evaluated. Through controlled experiments, we empirically validate that feature shapes provide accurate and interesting insights into the behavior of complex models. In this paper, we focus on interpreting F from fully-connected neural nets trained on tabular data.Our goal is not to replace local explanations nor to explain how the model functions internally. What we claim is that we can complement local explanations with global additive explanations that clearly illustrate the relationship between input features and model predictions. Our contributions are:• We propose to learn global additive explanations for complex, non-linear models such as neural nets.• We leverage powerful generalized additive models in a model distillation setting to learn feature shapes that are more expressive than feature attributions Figure 1 : Given a black box model and unlabeled samples (new unlabeled data or training data with labels discarded), our approach leverages model distillation to learn feature shapes that describe the relationship between input features and model predictions.• We perform a quantitative comparison of feature shapes to other global explanation methods in terms of fidelity to the model being explained, accuracy on independent test data, and interpretability through a user study. We presented a method for ""opening up"" complex models such as neural nets trained on tabular data. The method, based on distillation with high-accuracy additive models, has clear advantages over other approaches that learn additive explanations but not using distillation, and non-additive explanations using distillation. Our global additive explanations do not aim to compete with local explanations or non-additive explanations such as decision trees. Instead, we show that different interpretable representations work well for different tasks, and global additive explanations are valuable for important tasks that require quick understanding of feature-prediction relationships. Although in this paper we focus on explaining FNNs, the method will work with any classification or regression model including random forests and CNNs, but is not designed to work with raw image inputs such as pixels where providing a global explanation in terms of input pixels is not meaningful. One way to address this is to define more meaningful ""features"", e. hi (xi) hi (xi) hi(xi) Figure A1 : Feature shapes for features x 1 to x 9 of F 1 from Section 4.1. Notice how x 9 , which is a noise feature that does not affect F 1 , has been assigned an importance of approximately 0 throughout its range. The feature shape of x 10 , another noise feature, is very similar to x 9 and hence not included here.hi (xi) hi (xi) hi(xi) FIG0 : Feature shapes for features x 1 to x 9 of F 2 from Section 4.1. Notice how x 9 , which is a noise feature that does not affect F 2 , has been assigned an importance of approximately 0 throughout its range. The feature shape of x 10 , another noise feature, is very similar to x 9 and hence not included here. Table A1 : Accuracy and fidelity of global explanation models across 1H and 2H teacher neural nets and datasets. TAB4 is a subset of this table with only 2H neural nets.In general, the lower-capacity 1H neural nets are easier to approximate (i.e. better student-teacher fidelity), but their explanations are less accurate on independent test data. Students of simpler teachers tend to be less accurate even if they are faithful to their (simple) teachers. One exception is the FICO data, where the fidelity of the 2H explanations is better. Our interpretation is that many features in the FICO data have almost linear feature shapes (see Figure A5 for a sample of features), and the 2H model may be able to better capture fine details while being simple enough that it can still be faithfully approximated. The accuracy of the SAT and SAS for 1H and 2H neural nets are comparable, taking into account the confidence intervals.On the Magic data, the fidelity of the gGRAD explanation to the 1H neural net (see * in Table A1 ) is markedly worse than other explanation methods. We investigate the individual gradients of the 1H neural net with respect to each feature ( DISPLAYFORM0 ∂xi in GRAD equation in Section 3). 99% of them have reasonable values (between -5.6 and 6). However, 3 are larger than 1,000 (with none between 6 and 1,000) and 13 are lower than -1,000 (with none between -1,000 and -5.6), resulting in the ensuing gGRAD explanation generating extreme predictions for several samples that are not faithful to the teacher's predictions. Because AUC is a ranking loss, accuracy (AUC) is less affected than fidelity (RMSE) by the presence of these extreme values. This shows that gGRAD explanations may be problematic when individual gradients are arbitrarily large, e.g. in overfitted neural nets. Figure A7 , removing the color and number of samples in each node, to improve readability for the user study.",We propose to leverage model distillation to learn global additive explanations in the form of feature shapes (that are more expressive than feature attributions) for models such as neural nets trained on tabular data.,AUC ; gGRAD ; One ; FICO ; nets.• ; SAS ; https://youtu.be/ErQYwNqzEdc ; F ; SAT ; Magic,the user study ; a ranking loss ; that sample ; a black box model ; model predictions ; FICO ; feature attributions ; quick understanding ; the individual gradients ; key tasks,AUC ; gGRAD ; One ; FICO ; nets.• ; SAS ; https://youtu.be/ErQYwNqzEdc ; F ; SAT ; Magic,"In interpretability, local explanations are appealing due to their simplicity and local fidelity. However, they do not provide information about the general behavior of the model. We propose to leverage model distillation to learn global additive explanations that describe the relationship between input features and model predictions. These global explanations take the form of feature shapes, which are more expressive than feature attributions. Through careful experimentation, we show qualitatively and quantitatively that these global explanations are able to describe model behavior and yield insights about models such as neural nets. A visualization of our approach applied to a neural net as it is trained is available at https://youtu.be",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"The conversion of scanned documents to digital forms is performed using an Optical Character Recognition (OCR) software. This work focuses on improving the quality of scanned documents in order to improve the OCR output. We create an end-to-end document enhancement pipeline which takes in a set of noisy documents and produces clean ones. Deep neural network based denoising auto-encoders are trained to improve the OCR quality. We train a blind model that works on different noise levels of scanned text documents. Results are shown for blurring and watermark noise removal from noisy scanned documents. Scanned documents are stored as images and need to be processed by an Optical Character Recognition (OCR) software to extract the text contents into a digital format such as an ASCII text file. This is an active research area and there are many tools in the market that process a scanned document and extract the content in a digital format. The success with extraction of digital output is heavily dependent on the quality of the scanned document. In practice, however, there is some noise associated with the scanned document. Typical noises seen in scanned documents are blurring, watermarks, fading, and salt & pepper. With the rise of deep learning adoption in computer vision tasks, there are many neural network models available for image denoising and restoration [1] . However, most of the literature focuses on pictures (e.g., images from natural scenes) but not text documents, and the techniques used are not directly applicable due to very different nature of text document images. The designed REDNET was successfully tested on deblurring document images with various levels of intensity as well as removing both gray-level and color watermarks from text image documents. Currently, research on designing a unified network that can remove all noise types from text documents is ongoing.","We designed and tested a REDNET (ResNet Encoder-Decoder) with 8 skip connections to remove noise from documents, including blurring and watermarks, resulting in a high performance deep network for document image cleanup.",OCR ; ASCII ; salt & pepper ; REDNET,This ; watermarks ; order ; text documents ; documents ; which ; scanned text documents ; REDNET ; a set ; clean ones,OCR ; ASCII ; salt & pepper ; REDNET,The conversion of scanned documents to digital forms is performed using an Optical Character Recognition (OCR) software. This work focuses on improving the quality of scanning documents in order to improve the OCR output. Deep neural network based denoising auto-encoders train a blind model that works on different noise levels of scanned text documents. The results for blurring and watermark noise removal from noisy scanned documents are shown. Scanned documents are stored as images and need to be processed by an OCR software to extract the text contents into a digital format. This is an active research area and there are many tools in the market,/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Embedding layers are commonly used to map discrete symbols into continuous embedding vectors that reflect their semantic meanings. Despite their effectiveness, the number of parameters in an embedding layer increases linearly with the number of symbols and poses a critical challenge on memory and storage constraints. In this work, we propose a generic and end-to-end learnable compression framework termed differentiable product quantization (DPQ). We present two instantiations of DPQ that leverage different approximation techniques to enable differentiability in end-to-end learning. Our method can readily serve as a drop-in alternative for any existing embedding layer. Empirically, DPQ offers significant compression ratios (14-238x) at negligible or no performance cost on 10 datasets across three different language tasks. The embedding layer is a basic neural network module which maps a discrete symbol/word into a continuous hidden vector. It is widely used in NLP related applications, including language modeling, machine translation and text classification. With large vocabulary sizes, embedding layers consume large amounts of storage and memory. For example, in the medium-sized LSTM-based model on the PTB dataset (Zaremba et al., 2014) , the embedding table accounts for more than 95% of the total number of parameters. Even with sub-words encoding (e.g. Byte-pair encoding), the size of the embedding layer is still very significant. In addition to words/sub-words models in the text domain (Mikolov et al., 2013; Devlin et al., 2018) , embedding layers are also used in a wide range of applications such as knowledge graphs (Bordes et al., 2013; Socher et al., 2013) and recommender systems (Koren et al., 2009) , where the vocabulary sizes are even larger. Recent efforts to reduce the size of embedding layers have been made (Chen et al., 2018b; Shu and Nakayama, 2017) , where the authors proposed to first learn to encode symbols/words with K-way D-dimensional discrete codes (KD codes, such as 5-1-2-4 for ""cat"" and 5-1-2-3 for ""dog""), and then compose the codes to form the output symbol embedding. However, in Shu and Nakayama (2017) , the discrete codes are fixed before training and are therefore non-adaptive and limited to downstream tasks. Chen et al. (2018b) proposes to learn codes in an end-to-end fashion which leads to better task performance. However, their method employs an expensive embedding composition function to turn KD codes into embedding vectors, and requires a distillation procedure which incorporates a pre-trained embedding table as guidance, in order to match the performance of the full embedding baseline. In this work, we propose a novel differentiable product quantization (DPQ) framework. The proposal is based on the observation that the discrete codes (KD codes) are naturally derived through the process of quantization (product quantization by Jegou et al. (2010) in particular). We also provide two concrete approximation techniques that allow differentiable learning. By making the quantization process differentiable, we are able to learn the KD codes in an end-to-end fashion. Compared to the existing methods (Chen et al., 2018b; Shu and Nakayama, 2017) , our framework 1) brings a new and general perspective on how the discrete codes can be obtained in a differentiable manner; 2) allows more flexible model designs (e.g. distance functions and approximation algorithms), and 3) achieves better task performance as well as compression efficiency (by leveraging the sizes of product keys and values) while avoiding the cumbersome distillation procedure. We conduct experiments on ten different datasets across three tasks, by simply replacing the original embedding layer with DPQ. The results show that DPQ can learn compact discrete embeddings with higher compression ratios than the existing methods, at the same time achieving the same performance as the original full embeddings. Furthermore, our results are obtained from end-to-end training where no extra procedures such as distillation are required. To the best of our knowledge, this is the first work to train compact discrete embeddings in an end-to-end fashion without distillation. In this work, we propose a novel and general differentiable product quantization framework for learning compact embedding layers. We provide two instantiations of our framework, which can readily serve as a drop-in replacement for existing embedding layers. Empirically, we evaluate the proposed method on ten datasets across three different language tasks, and show that our method surpasses existing compression methods and can compress the embedding table up to 238× without suffering a performance loss. In the future, we plan to apply the DPQ framework to a wider range of applications and architectures.",We propose a differentiable product quantization framework that can reduce the size of embedding layer in an end-to-end training at no performance cost.,al. ; Shu ; DPQ ; Chen et al. ; two ; Socher ; NLP ; three ; PTB ; Jegou,sub-words encoding ; training ; that ; Mikolov ; our framework ; Zaremba et al ; NLP related applications ; Devlin ; we ; differentiability,al. ; Shu ; DPQ ; Chen et al. ; two ; Socher ; NLP ; three ; PTB ; Jegou,"Embedding layers are commonly used to map discrete symbols into continuous embedding vectors that reflect their semantic meanings. Despite their effectiveness, they increase linearly with the number of symbols and pose a challenge on memory and storage constraints. We propose a generic and end-to-end learnable compression framework, differentiable product quantization (DPQ), that leverage different approximation techniques to enable differentiability. This approach can readily serve as a drop-in alternative for any existing embedding layer. Empirically, DPQ offers significant compression ratios (14-238x) at negligible or no performance cost on 10 datasets across three different language",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"We introduce bio-inspired artificial neural networks consisting of neurons that are additionally characterized by spatial positions. To simulate properties of biological systems we add the costs penalizing long connections and the proximity of neurons in a two-dimensional space. Our experiments show that in the case where the network performs two different tasks, the neurons naturally split into clusters, where each cluster is responsible for processing a different task. This behavior not only corresponds to the biological systems, but also allows for further insight into interpretability or continual learning. Neurons in the human brain naturally group into interconnected regions, forming the full neural system [1] . In this paper, we would like to construct an analogical mechanism in the case of artificial neural networks. To put this idea into practice, we supply each neuron with spatial coordinates. Motivated by biological neural systems, we impose the cost of signal transmission between connected neurons, which grows linearly with the distance between them. In consequence, we obtain artificial groups specialized in different tasks, each group containing neurons that are placed close to each other. The proposed model is examined in a double classification task, where a single network has to classify examples from two different datasets (MNIST and Fashion-MNIST). At test time, we split the network into two subnetworks based on the structure of weights, where each subnetwork represents one task. The resulting models perform their respective tasks only slightly worse than the original network, in contrast to the large performance drop observed after splitting a standard fully connected network. Our model offers a natural interpretation of neurons' responsibilities and is analogous to biological neural systems. The idea of adding spatial coordinates to each neuron and penalizing long connections was previously introduced by [2] . Although our work is based on a similar premise, the resulting models differ significantly. We use a different, simpler spatial loss function, we investigate networks with multiple hidden layers as opposed to a single hidden layer, and we focus on the cluster-forming properties of spatial networks. There have been multiple approaches to finding clusters of neurons in artificial neural networks, although most of them consider the functional aspects of the network. For instance, [3] compare variance of neuron's activations in different tasks to decide which cluster does it belong to. In another approach, [4] cluster the network by using feature vectors calculated based on correlation between the neuron and the output. In comparison, our approach uses the structure of the network -the spatial placement of the neurons and the strength of connections between the neurons. Our model is also related to continual and multi-task learning [5] and parameter reduction models for deep neural networks [6] . In particular, the effect is slightly similar to the one obtained in [7] . Authors focus on splitting network weights into a set of groups, where each is associated with a class (task). In contrast to our biologically inspired mechanism, [7] use a specialized weight regularization technique and strive towards a different goal. In [8] , a nested sparse network is constructed and different forms of knowledge are learned at each level, enabling solving multiple tasks with a single neural network. Checking the response of different groups in our proposed spatial network could be considered useful for interpreting the network predictions, which is also an open problem [9] . We have presented a connection between neuroscience and machine learning, which to our best knowledge has not yet been explored. Experiments show that our proposed spatial artificial neural network manifests behavior similar to the region-forming processes in the brain. For future work we plan to test our model on a continual learning task. We hypothesize that, since the model is able to create disjoint clusters of neurons responsible for different tasks, learning a new task could be possible without disturbing the previously created clusters. Additional constraints could be added to achieve this, such as restricting the movement of neurons with high potential, while allowing neurons with low potential to move freely. Spatial networks could also be investigated in relation to spiking neural networks. The motivation is that spiking neural networks operate in the temporal dimension, which in the brain is dictated mainly by the spatial structure of neurons. Thus, it is possible that the two types of networks have similar properties, making the spatial network a more interesting model to investigate from the neuroscientific standpoint.","Bio-inspired artificial neural networks, consisting of neurons positioned in a two-dimensional space, are capable of forming independent groups for performing different tasks.",two ; one ; neuron,their respective tasks ; correlation ; each cluster ; a natural interpretation ; a single neural network ; each subnetwork ; each group ; that ; this paper ; each neuron,two ; one ; neuron,"Neurons in the human brain are additionally characterized by spatial positions. To simulate properties of biological systems, we add costs penalizing long connections and proximity of neurons in a two-dimensional space. In the case where the network performs two different tasks, the neurons naturally split into clusters, where each cluster is responsible for processing a different task. This behavior corresponds to biological systems and allows for further insight into interpretability or continual learning. In this paper, we propose an analogical mechanism in the case of artificial neural networks where each neuron is supplied with spatial coordinates and penalized for long connections. The concept of spatial coordinates is similar to",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Stochastic Gradient Descent or SGD is the most popular optimization algorithm for large-scale problems. SGD estimates the gradient by uniform sampling with sample size one. There have been several other works that suggest faster epoch wise convergence by using weighted non-uniform sampling for better gradient estimates. Unfortunately, the per-iteration cost of maintaining this adaptive distribution for gradient estimation is more than calculating the full gradient. As a result, the false impression of faster convergence in iterations leads to slower convergence in time, which we call a chicken-and-egg loop. In this paper, we break this barrier by providing the first demonstration of a sampling scheme, which leads to superior gradient estimation, while keeping the sampling cost per iteration similar to that of the uniform sampling. Such an algorithm is possible due to the sampling view of Locality Sensitive Hashing (LSH), which came to light recently. As a consequence of superior and fast estimation, we reduce the running time of all existing gradient descent algorithms. We demonstrate the benefits of our proposal on both SGD and AdaGrad. In this paper, we proposed a novel LSH-based sampler with a reduction to the gradient estimation variance. We achieved it by sampling with probability proportional to the L 2 norm of the instances gradients leading to an optimal distribution that minimizes the variance of estimation. More remarkably, LSD is as computationally efficient as SGD but achieves faster convergence not only epoch wise but also time wise.Peilin Zhao and Tong Zhang. Stochastic optimization with importance sampling for regularized loss minimization. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pp. 1-9, 2015.A EPOCH PLOTS AND PROOFS Theorem 3. Let S be the bucket that sample x m is chosen from in Algorithm 2. Let p m be the sampling probability associated with sample x m . Suppose we query a sample with ✓ t . Then we have an unbiased estimator of the full gradient: DISPLAYFORM0 Proof. DISPLAYFORM1 Theorem 4. The Trace of the covariance of our estimator is: DISPLAYFORM2 Proof.",We improve the running of all existing gradient descent algorithms.,SGD ; first ; Locality Sensitive Hashing ; LSH ; AdaGrad ; Peilin Zhao ; Tong Zhang ; International Conference on Machine Learning ; Trace,a chicken-and-egg loop ; superior and fast estimation ; the most popular optimization algorithm ; the first demonstration ; estimation ; which ; a result ; LSH ; several other works ; faster epoch wise convergence,SGD ; first ; Locality Sensitive Hashing ; LSH ; AdaGrad ; Peilin Zhao ; Tong Zhang ; International Conference on Machine Learning ; Trace,"Stochastic Gradient Descent (SGD) is the most popular optimization algorithm for large-scale problems. It estimates the gradient by uniform sampling with sample size one. There have been several works that suggest faster epoch wise convergence by using weighted non-uniform sampling for better gradient estimates. However, the per-iteration cost of maintaining this adaptive distribution for gradient estimation is more than calculating the full gradient. This creates a chicken-and-egg loop, leading to slower convergence in time. In this paper, we introduce a sampling scheme, which leads to superior gradient estimation while keeping the sampling cost similar to uniform sampling.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"We review eight machine learning classification algorithms to analyze Electroencephalographic (EEG) signals in order to distinguish EEG patterns associated with five basic educational tasks. There is a large variety of classifiers being used in this EEG-based Brain-Computer Interface (BCI) field. While previous EEG experiments used several classifiers in the same experiments or reviewed different algorithms on datasets from different experiments, our approach focuses on review eight classifier categories on the same dataset, including linear classifiers, non-linear Bayesian classifiers, nearest neighbour classifiers, ensemble methods, adaptive classifiers, tensor classifiers, transfer learning and deep learning. Besides, we intend to find an approach which can run smoothly on the current mainstream personal computers and smartphones.   The empirical evaluation demonstrated that Random Forest and LSTM (Long Short-Term Memory) outperform other approaches. We used a data set which users were conducting five frequently-conduct learning-related tasks, including reading, writing, and typing. Results showed that these best two algorithms could correctly classify different users with an accuracy increase of  5% to 9%, use each task independently. Within each subject, the tasks could be recognized with an accuracy increase of  4% to 7%, compared with other approaches. This work suggests that Random Forest could be a recommended approach (fast and accurate) for current mainstream hardware, while LSTM has the potential to be the first-choice approach when the mainstream computers and smartphones can process more data in a shorter time.",Two Algorithms outperformed eight others on a EEG-based BCI experiment,eight ; Electroencephalographic ; EEG ; five ; Brain-Computer Interface ; linear ; Bayesian ; Random Forest ; two ; first,smartphones ; Random Forest ; eight classifier categories ; users ; Long Short-Term Memory ; a large variety ; five ; which ; non-linear Bayesian classifiers ; eight machine,eight ; Electroencephalographic ; EEG ; five ; Brain-Computer Interface ; linear ; Bayesian ; Random Forest ; two ; first,"We review eight machine learning classification algorithms to analyze Electroencephalographic (EEG) signals in order to distinguish EEG patterns associated with five basic educational tasks. There are a large variety of classifiers being used in this EEG-based Brain-Computer Interface (BCI) field. While previous EEG experiments used several classifiers in the same dataset, our approach focuses on review eight classifier categories on the same datasets, including linear classifiers, non-linear Bayesian classifiers and nearest neighbour classifiers. Additionally, we plan to find an approach which can run smoothly on the current mainstream personal computers and smartphones.   The empirical",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"The unconditional generation of high fidelity images is a longstanding benchmark
 for testing the performance of image decoders. Autoregressive image models
 have been able to generate small images unconditionally, but the extension of
 these methods to large images where fidelity can be more readily assessed has
 remained an open problem. Among the major challenges are the capacity to encode
 the vast previous context and the sheer difficulty of learning a distribution that
 preserves both global semantic coherence and exactness of detail. To address the
 former challenge, we propose the Subscale Pixel Network (SPN), a conditional
 decoder architecture that generates an image as a sequence of image slices of equal
 size. The SPN compactly captures image-wide spatial dependencies and requires a
 fraction of the memory and the computation. To address the latter challenge, we
 propose to use multidimensional upscaling to grow an image in both size and depth
 via intermediate stages corresponding to distinct SPNs. We evaluate SPNs on the
 unconditional generation of CelebAHQ of size 256 and of ImageNet from size 32
 to 128. We achieve state-of-the-art likelihood results in multiple settings, set up
 new benchmark results in previously unexplored settings and are able to generate
 very high fidelity large scale samples on the basis of both datasets. A successful generative model has two core aspects: it produces targets that have high fidelity and it generalizes well on held-out data. Autoregressive (AR) models trained by conventional maximum likelihood estimation (MLE) have produced superior scores on held-out data across a wide range of domains such as text BID16 BID18 , audio BID13 , images and videos BID4 . These scores are a measure of the models' ability to generalize in that setting. From the perspective of sample fidelity, the outputs generated by AR models have also achieved state-of-the-art fidelity in many of the aforementioned domains with one notable exception. In the domain of unconditional large-scale image generation, AR samples have yet to manifest long-range structure and semantic coherence.One source of difficulties impeding high-fidelity image generation is the multi-faceted relationship between the MLE scores achieved by a model and the model's sample fidelity. On the one hand, MLE is a well-defined measure as improvements in held-out scores generally produce improvements in the visual fidelity of the samples. On the other hand, as opposed to for example adversarial methods BID0 , MLE forces the model to support the entire empirical distribution. This guarantees the model's ability to generalize at the cost of allotting capacity to parts of the distribution that are irrelevant to fidelity. A second source of difficulties arises from the high dimensionality of large images. A 256 × 256 × 3 image has a total of 196,608 positions that need to be architecturally connected in order to learn dependencies among them; the representations at each position require sufficient capacity to express their respective surrounding contexts. These requirements translate to large amounts of memory and computation. Figure 1: A representation of Multidimensional Upscaling. Left: depth upscaling is applied to a generated 3-bit 256 × 256 RGB subimage from CelebAHQ to map it to a full 8-bit 256 × 256 RGB image. Right: size upscaling followed by depth upscaling are applied to a generated 3-bit 32 × 32 RGB subimage from ImageNet to map it to the target resolution of the 8-bit 128 × 128 RGB image. We stress that the rightmost column of both figures are true unconditional samples from our model at full 8bit depth.These difficulties notwithstanding, we aim to learn the full distribution over 8-bit RGB images of size up to 256 × 256 well enough so that the samples have high fidelity. We aim to guide the model to focus first on visually more salient bits of the distribution and later on the visually less salient bits. We identify two visually salient subsets of the distribution: first, the subset determined by sub-images (""slices"") of smaller size (e.g. 32 × 32) sub-sampled at all positions from the original image; and secondly, the subset determined by the few (e.g. 3) most significant bits of each RGB channel in the image. We use Multidimensional Upscaling to map from one subset of the distribution to the other one by upscaling images in size or in depth. For example, the generation of a 128 × 128 8-bit RGB image proceeds by first upscaling it in size from a 32 × 32 3-bit RGB image to a 128 × 128 3-bit RGB image; we then upscale the resulting image in depth to the original resolution of the 128 × 128 8-bit RGB image. We thus train three networks: (a) a decoder on the small size, low depth image slices subsampled at every n pixels from the original image with the desired target resolution; (b) a size-upscaling decoder that generates the large size, low depth image conditioned on the small size, low depth image; and (c) a depth-upscaling decoder that generates the large size, high depth image conditioned on the large size, low depth image. Figure 1 illustrates this process.To address the latter difficulties that ensue in the training of decoders (b) and (c), we develop the Subscale Pixel Network (SPN) architecture. The SPN divides an image of size N ×N into sub-images of size N S × N S sliced out at interleaving positions (see FIG1 ), which implicitly also captures a form of size upscaling. The N × N image is generated one slice at a time conditioned on previously generated slices in a way that encodes a rich spatial structure. SPN consists of two networks, a conditioning network that embeds previous slices and a decoder proper that predicts a single target slice given the context embedding. The decoding part of the SPN acts over image slices with the same spatial structure and it can share weights for all of them. The SPN is an independent image decoder with an implicit size upscaling mechanism, but it can also be used as an explicit size upscaling network by initializing the first slice of the SPN input at sampling time with one generated separately during step (a).We extensively evaluate the performance of SPN and the size and depth upscaling methods both quantitatively and from a fidelity perspective on two unconditional image generation benchmarks, CelebAHQ-256 and ImageNet of various sizes up to 256. From a MLE scores perspective, we compare with previous work to obtain state-of-the-art results on CelebAHQ-256, both at full 8-bit resolution and at the reduced 5-bit resolution BID7 , and on ImageNet-64. We also establish MLE baselines for ImageNet-128 and ImageNet-256. From a sample fidelity perspective, we show the strong benefits of multidimensional upscaling as well as the benefits of the SPN. We produce CelebAHQ-256 samples (at full 8-bit resolution) that are of similar visual fidelity to those produced with methods such as GANs that lack however an intrinsic measure of generalization BID10 BID6 . We also produce some of the first successful samples on unconditional ImageNet-128 (also at 8-bit) showing again the striking impact of the SPN and of multidimensional upscaling on sample quality and setting a fidelity baseline for future methods. The problem of whether it is possible to learn the distribution of complex natural images and attain high sample fidelity has been a long-standing one in the tradition of generative models. The SPN and Multidimensional Upscaling model that we introduce accomplishes a large step towards solving this problem, by attaining both state-of-the-art MLE scores on large-scale images from complex domains such as CelebAHQ-256 and ImageNet-128 and by being able to generate high fidelity full 8-bit samples from the resulting learnt distributions without alterations to the sampling process (via e.g. heavy modifications of the temperature of the output distribution). The generated samples show an unprecedented amount of semantic coherence and exactness of details even at the large scale size of full 8-bit 128 × 128 and 256 × 256 images.",We show that autoregressive models can generate high fidelity images.,AR ; three ; the Subscale Pixel Network ; RGB ; SPN ; two ; secondly ; ImageNet ; MLE ; second,the same spatial structure ; previously generated slices ; the small size ; the sheer difficulty ; the computation ; complex domains ; the basis ; One ; detail ; parts,AR ; three ; the Subscale Pixel Network ; RGB ; SPN ; two ; secondly ; ImageNet ; MLE ; second,"The unconditional generation of high fidelity images is a longstanding benchmark for image decoders. Autoregressive image models have been able to generate small images unconditionally, but the extension of these methods to large images where fidelity can be more readily assessed has remained an open problem. The main challenges are the capacity to encode the vast previous context and the difficulty of learning a distribution that preserves both global semantic coherence and exactness of detail. To address the former challenge, we propose the Subscale Pixel Network (SPN), a conditional generation algorithm that generates an image as a sequence of image slices of equal size. The SPN compactly",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"We present a neural architecture search algorithm to construct compact reinforcement learning (RL) policies, by combining ENAS and ES in a highly scalable and intuitive way. By defining the combinatorial search space of NAS to be the set of different edge-partitionings (colorings) into same-weight classes, we represent compact architectures via efficient learned edge-partitionings. For several RL tasks, we manage to learn colorings translating to effective policies parameterized by as few as 17 weight parameters, providing >90 % compression over vanilla policies and 6x compression over state-of-the-art compact policies based on Toeplitz matrices, while still maintaining good reward. We believe that our work is one of the first attempts to propose a rigorous approach to training structured neural network architectures for RL problems that are of interest especially in mobile robotics with limited storage and computational resources. Consider a fixed Markov Decision Process (MDP) M and an agent aiming to maximize its total expected/discounted reward obtained in the environment E governed by M. An agent is looking for a sequence of actions a 0 , ..., a T −1 leading to a series of steps maximizing this reward. One of the approaches is to construct a policy π θ : S → A, parameterized by vector θ, which is a mapping from states to actions. Policy π θ determines actions chosen in states visited by an agent. Such a reinforcement learning (RL) policy is usually encoded as a neural network, in which scenario parameters θ correspond to weights and biases of a neural network. Reinforcement learning policies π θ often consist of thousands or millions of parameters (e.g. when they involve vision as part of the state vector) and therefore training them becomes a challenging high-dimensional optimization problem. Deploying such high-dimensional policies on hardware raises additional concerns in resource constrained settings (e.g. limited storage), emerging in particular in mobile robotics (Gage, 2002) . The main question we tackle in this paper is the following: Are high dimensional architectures necessary for encoding efficient policies and if not, how compact can they be in in practice? We show that finding such compact representations is a nontrivial optimization problem despite recent observations that some hardcoded structured families (Choromanski et al., 2018) provide certain levels of compactification and good accuracy at the same time. We model the problem of finding compact presentations by using a joint objective between the combinatorial nature of the network's parameter sharing profile and the reward maximization of RL optimization. We leverage recent advances in the ENAS (Efficient Neural Architecture Search) literature and theory of pointer networks (Vinyals et al., 2015; Pham et al., 2018; Zoph & Le, 2017) to optimize over the combinatorial component of this objective and state of the art evolution strategies (ES) methods (Choromanski et al., 2018; Salimans et al., 2017; Mania et al., 2018a) to optimize over the RL objective. We propose to define the combinatorial search space to be the the set of different edge-partitioning (colorings) into same-weight classes and construct policies with learned weight-sharing mechanisms. We call networks encoding our policies: chromatic networks. We are inspired by two recent papers: (Choromanski et al., 2018) and (Gaier & Ha, 2019) . In the former one, policies based on Toeplitz matrices were shown to match their unstructured counterparts accuracy-wise, while leading to the substantial reduction of the number of parameters from Figure 1 : On the left: matrix encoding linear Toeplitz policy at time t for the RL task with 6-dimensional state vector and 4-dimensional action vector. On the right: that policy in the vectorized form. As we see, a policy defined by a matrix with 24 entries is effectively encoded by a 9-dimensional vector. thousands (Salimans et al., 2017) to hundreds (Choromanski et al., 2018) . Instead of quadratic (in sizes of hidden layers), those policies use only linear number of parameters. The Toeplitz structure can be thought of as a parameter sharing mechanism, where edge weights along each diagonal of the matrix are the same (see: Fig. 1 ). However, this is a rigid pattern that is not learned. We show in this paper that weight sharing patterns can be effectively learned, which further reduces the number of distinct parameters. For instance, using architectures of the same sizes as those in (Choromanski et al., 2018) , we can train effective policies for OpenAI Gym tasks with as few as 17 distinct weights. The latter paper proposes an extremal approach, where weights are chosen randomly instead of being learned, but the topologies of connections are trained and thus are ultimately strongly biased towards RL tasks under consideration. It was shown in (Gaier & Ha, 2019 ) that such weight agnostic neural networks (WANNs) can encode effective policies for several nontrivial RL problems. WANNs replace conceptually simple feedforward networks with general graph topologies using NEAT algorithm (Stanley & Miikkulainen, 2002) providing topological operators to build the network. Our approach is a middle ground, where the topology is still a feedforward neural network, but the weights are partitioned into groups that are being learned in a combinatorial fashion using reinforcement learning. While (Chen et al., 2015) shares weights randomly via hashing, we learn a good partitioning mechanisms for weight sharing. Our key observation is that ENAS and ES can naturally be combined in a highly scalable but conceptually simple way. To give context, vanilla NAS (Zoph & Le, 2017) for classical supervised learning setting (SL) requires a large population of 450 GPU-workers (child models) all training one-by-one, which results in many GPU-hours of training. ENAS (Pham et al., 2018) uses weight sharing across multiple workers to reduce the time, although it can reduce computational resources at the cost of the variance of the controller's gradient. Our method solves both issues (fast training time and low controller gradient variance) by leveraging a large population of much-cheaper CPU workers (300) increasing the effective batch-size of the controller, while also training the workers simultaneously via ES. This setup is not possible in SL, as single CPUs cannot train large image-based classifiers in practice. Furthermore, this magnitude of scaling by numerous workers can be difficult with policy gradient or Q-learning methods as they can be limited by GPU overhead due to exact-gradient computation. We believe that our work is one of the first attempts to propose a flexible, rigorous approach to training compact neural network architectures for RL problems. Those may be of particular importance in mobile robotics (Gage, 2002) where computational and storage resources are very limited. We also believe that this paper opens several new research directions regarding structured policies for robotics. We presented a principled and flexible algorithm for learning structured neural network architectures for RL policies and encoded by compact sets of parameters. Our architectures, called chromatic networks, rely on partitionings of a small sets of weights learned via ENAS methods. Furthermore, we have also provided a scalable way of performing NAS techniques with RL policies which is not limited to weight-sharing, but can potentially also be used to construct several other combinatorial structures in a flexible fashion, such as node deletions and edge removals. We showed that chromatic networks provide more aggressive compression than their state-of-the-art counterparts while preserving efficiency of the learned policies. We believe that our work opens new research directions, especially from using other combinatorial objects. Detailed analysis of obtained partitionings (see: Appendix C) also shows that learned structured matrices are very different from previously used state-of-the-art (in particular they are characterized by high displacement rank), yet it is not known what their properties are. It would be also important to understand how transferable those learned partitionings are across different RL tasks (see: Appendix D). We set LSTM hidden layer size to be 64, with 1 hidden layer. The learning rate was 0.001, and the entropy penalty strength was 0.3. We used a moving average weight of 0.99 for the critic, and used a temperature of 1.0 for softmax, with the training algorithm as REINFORCE.","We show that ENAS with ES-optimization in RL is highly scalable, and use it to compactify neural network policies by weight sharing.",Zoph & Le ; Toeplitz ; NAS ; NEAT ; first ; Gage ; SL ; thousands or millions ; One ; two,Pham et al ; RL tasks ; colorings ; learned weight-sharing mechanisms ; scaling ; RL optimization ; constrained settings ; Salimans ; the environment ; → A,Zoph & Le ; Toeplitz ; NAS ; NEAT ; first ; Gage ; SL ; thousands or millions ; One ; two,"Neural architecture search algorithms can construct compact reinforcement learning (RL) policies by combining ENAS and ES in a highly scalable and intuitive way. By defining the combinatorial search space of NAS to be the set of different edge-partitionings, we can represent compact architectures via efficient learned edge partitionings. For several RL tasks, we manage to learn colorings translating to effective policies parameterized by as few as 17 weight parameters, providing >90 % compression over vanilla policies and 6x compression over state-of-the-art compact policies based on Toeplitz matrices. This approach is one of the first attempts",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Hyperparameter tuning is one of the most time-consuming workloads in deep learning. State-of-the-art optimizers, such as AdaGrad, RMSProp and Adam, reduce this labor by adaptively tuning an individual learning rate for each variable. Recently researchers have shown renewed interest in simpler methods like momentum SGD as they may yield better results. Motivated by this trend, we ask: can simple adaptive methods, based on SGD perform as well or better? We revisit the momentum SGD algorithm and show that hand-tuning a single learning rate and momentum makes it competitive with Adam. We then analyze its robustness to learning rate misspecification and objective curvature variation. Based on these insights, we design YellowFin, an automatic tuner for momentum and learning rate in SGD. YellowFin optionally uses a negative-feedback loop to compensate for the momentum dynamics in asynchronous settings on the fly. We empirically show YellowFin can converge in fewer iterations than Adam on ResNets and LSTMs for image recognition, language modeling and constituency parsing, with a speedup of up to $3.28$x in synchronous and up to $2.69$x in asynchronous settings. Accelerated forms of stochastic gradient descent (SGD), pioneered by BID0 and BID1 , are the de-facto training algorithms for deep learning. Their use requires a sane choice for their hyperparameters: typically a learning rate and momentum parameter BID2 . However, tuning hyperparameters is arguably the most time-consuming part of deep learning, with many papers outlining best tuning practices written BID4 BID6 . Deep learning researchers have proposed a number of methods to deal with hyperparameter optimization, ranging from grid-search and smart black-box methods BID7 BID8 to adaptive optimizers. Adaptive optimizers aim to eliminate hyperparameter search by tuning on the fly for a single training run: algorithms like AdaGrad BID9 , RMSProp BID10 and Adam BID11 use the magnitude of gradient elements to tune learning rates individually for each variable and have been largely successful in relieving practitioners of tuning the learning rate. Recently some researchers have started favoring simple momentum SGD over the previously mentioned adaptive methods BID12 BID13 , often reporting better test scores BID14 . Motivated by this trend, we ask the question: can simpler adaptive methods, based on momentum SGD perform as well or better? We empirically show that, with hand-tuned learning rate, Polyak's momentum SGD achieves faster convergence than Adam for a large class of models. We then formulate the optimization update as a dynamical system and study certain robustness properties of the momentum operator. Building on our analysis, we design YELLOWFIN, an automatic hyperparameter tuner for momentum SGD. YELLOWFIN simultaneously tunes the learning rate and momentum on the fly, and can handle the complex dynamics of asynchronous execution. Specifically:• In Section 2, we show that momentum presents convergence robust to learning rate misspecification and curvature variation in a class of non-convex objectives; this robustness is desirable for deep learning. They stem from a known but obscure fact: the momentum operator's spectral radius is constant in a large subset of the hyperparameter space.• In Section 3, we use these robustness insights and a simple quadratic model analysis to design YELLOWFIN, an automatic tuner for momentum SGD. YELLOWFIN uses on-the-fly measurements from the gradients to tune both a single learning rate and momentum.• In Section 3.3, we discuss common stability concerns related to the phenomenon of exploding gradients . We present a natural extension to our basic tuner, using adaptive gradient clipping, to stabilize training for objectives with exploding gradients.• In Section 4 we present closed-loop YELLOWFIN, suited for asynchronous training. It uses a novel component for measuring the total momentum in a running system, including any asynchrony-induced momentum, a phenomenon described in BID16 . This measurement is used in a negative feedback loop to control the value of algorithmic momentum.We provide a thorough evaluation of the performance and stability of our tuner. In Section 5, we demonstrate empirically that on ResNets and LSTMs YELLOWFIN can converge in fewer iterations compared to: (i) hand-tuned momentum SGD (up to 1.75x speedup); and (ii) default Adam (0.8x to 3.3x speedup). Under asynchrony, the closed-loop control architecture speeds up YELLOWFIN, making it up to 2.69x faster than Adam. Our experiments include runs on 7 different models, randomized over at least 5 different random seeds. YELLOWFIN is stable and achieves consistent performance: the normalized sample standard deviation of test metrics varies from 0.05% to 0.6%. We released PyTorch and TensorFlow implementations, that can be used as drop-in replacements for any optimizer. YELLOWFIN has also been implemented in other various packages. Its large-scale deployment in industry has taught us important lessons about stability; we discuss those challenges and our solution in Section 3.3. We conclude with related work and discussion in Section 6 and 7. We presented YELLOWFIN, the first optimization method that automatically tunes momentum as well as the learning rate of momentum SGD. YELLOWFIN outperforms the state-of-the-art adaptive optimizers on a large class of models both in synchronous and asynchronous settings. It estimates statistics purely from the gradients of a running system, and then tunes the hyperparameters of momentum SGD based on noisy, local quadratic approximations. As future work, we believe that more accurate curvature estimation methods, like the bbprop method (Martens et al., 2012) can further improve YELLOWFIN. We also believe that our closed-loop momentum control mechanism in Section 4 could accelerate convergence for other adaptive methods in asynchronous-parallel settings. A PROOF OF LEMMA 2To prove Lemma 2, we first prove a more generalized version in Lemma 6. By restricting f to be a one dimensional quadratics function, the generalized curvature h t itself is the only eigenvalue. We can prove Lemma 2 as a straight-forward corollary. Lemma 6 also implies, in the multiple dimensional correspondence of (4), the spectral radius ⇢(A t ) = p µ if the curvature on all eigenvector directions (eigenvalue) satisfies (5). Lemma 6. Let the gradients of a function f be described by DISPLAYFORM0 with H (x t ) 2 R n 7 ! R n⇥n . Then the momentum update can be expressed as a linear operator: DISPLAYFORM1 where DISPLAYFORM2 . Now, assume that the following condition holds for all eigenvalues (H ( DISPLAYFORM3 then the spectral radius of A t is controlled by momentum with ⇢(A t ) = p µ.",YellowFin is an SGD based optimizer with both momentum and learning rate adaptivity.,TensorFlow ; Specifically:• ; AdaGrad ; PyTorch ; first ; SGD ; Martens et al. ; Polyak ; Adam ; one,a linear operator ; A PROOF ; constituency parsing ; certain robustness properties ; momentum ; Specifically:• ; these insights ; our basic tuner ; statistics ; methods,TensorFlow ; Specifically:• ; AdaGrad ; PyTorch ; first ; SGD ; Martens et al. ; Polyak ; Adam ; one,"Hyperparameter tuning is one of the most time-consuming tasks in deep learning. State-of-theart optimizers, such as AdaGrad, RMSProp and Adam, reduce this labor by adaptively tuning an individual learning rate for each variable. However, newer methods like momentum SGD are gaining renewed interest in simpler adaptive methods. This raises the question of whether simple adaptive methods can perform as well or better. We revisit the momentumSGD algorithm and show that hand-tuning a single learning rate and momentum makes it competitive with Adam on ResNets and LSTMs for image recognition, language modeling and constituency",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Profiling cellular phenotypes from microscopic imaging can provide meaningful biological information resulting from various factors affecting the cells. One motivating application is drug development: morphological cell features can be captured from images, from which similarities between different drugs applied at different dosages can be quantified. The general approach is to find a function mapping the images to an embedding space of manageable dimensionality whose geometry captures relevant features of the input images. An important known issue for such methods is separating relevant biological signal from nuisance variation. For example, the embedding vectors tend to be more correlated for cells that were cultured and imaged during the same week than for cells from a different week, despite having identical drug compounds applied in both cases. In this case, the particular batch a set of experiments were conducted in constitutes the domain of the data; an ideal set of image embeddings should contain only the relevant biological information (e.g. drug effects). We develop a general framework for adjusting the image embeddings in order to `forget' domain-specific information while preserving relevant biological information. To do this, we minimize a loss function based on distances between marginal distributions (such as the Wasserstein distance) of embeddings across domains for each replicated treatment. For the dataset presented, the replicated treatment is the negative control. We find that for our transformed embeddings (1) the underlying geometric structure is not only preserved but the embeddings also carry improved biological signal (2) less domain-specific information is present. In the framework where our approach is applicable, there are some inputs (e.g. images) and a map F sending the inputs to vectors in a low-dimensional space which summarizes information about the inputs. F could either be engineered using specific image features, or learned (e.g. using deep neural networks). We will call these vectors 'embeddings' and the space to which they belong the 'embedding space'. Each input may also have corresponding semantic labels and domains, and for inputs with each label and domain pair, F produces some distribution of embeddings. Semantically meaningful similarities between pairs of inputs can then be assessed by the distance between their corresponding embeddings, using some chosen distance metric. Ideally, the embedding distribution of a group of inputs depends only on their label, but often the domain can influence the embedding distribution as well. We wish to find an additional map to adjust the embeddings produced by F so that the distribution of adjusted embeddings for a given label is independent of the domain, while still preserving semantically meaningful distances between distributions of inputs with different labels.The map F can be used for phenotypic profiling of cells. In this application, images of biological cells perturbed by one of several possible biological stimuli (e.g. various drug compounds at different doses, some of which may have unknown effects) are mapped to embeddings, which are used to reveal similarities among the applied perturbations.There are a number of ways to extract embeddings from images of cells. One class of methods such as that used by BID10 relies on extracting specifically engineered features. In the recent work by BID1 , a Deep Metric Network pre-trained on consumer photographic images (not microscope images of cells) described in BID16 was used to generate embedding vectors from cellular images, and it was shown that these clustered drug compounds by their mechanisms of action (MOA) more effectively. See Figure 1 for example images of the different MOAs.Currently one of the most important issues with using image embeddings to discriminate the effects of each treatment (i.e. a particular dose of a drug, the 'label' in the general problem described above) on morphological cell features is nuisance factors related to slight uncontrollable variations in each biological experiment. Many cell imaging experiments are organized into a number of batches of experiments occurring over time, each of which contains a number of sample plates (typically 3-6), each of which contains individual wells in which thousands of cells are grown and treatments are applied (typically around 96 wells per plate). For this application, the 'domain' is an instance of one of these hierarchical levels, and embeddings for cells with a given treatment tend to be closer to each other within the same domain than from a different one. For example, the experimentalist may apply slightly different concentrations or amounts of a drug compound in two wells in which the same treatment was anticipated. Another example is the location of a particular well within a plate or the order of the plate within a batch, which may influence the rate of evaporation, and hence, the appearance of the cells. Finally, 'batch' effects may result from differences in experiment conditions (temperature, humidity) from week to week; they are various instances of this hierarchical level that we will consider as 'domains' in this work.Our approach addresses the issue of nuisance variation in embeddings by transforming the embedding space in a possibly domain-specific way in order to minimize the variation across domains for a given treatment. We remark that our main goal is to introduce a general flexible framework to address this problem. In this framework, we use a metric function measuring the distances among pairs of probability distributions to construct an optimization problem whose solution yields appropriate transformations on each domain. In our present implementation, the Wasserstein distance is used as a demonstration of a specific choice of the metric that can yield substantial improvements. The Wasserstein distance makes few assumptions about the probability distributions of the embedding vectors.Our approach is fundamentally different than those which explicitly identify a fixed 'target' and 'source' distributions. Instead, we incorporate information from all domains on an equal footing, transforming all the embeddings. This potentially allows our method to incorporate several replicates of a treatment across different domains to learn the transformations, and not only the controls. We highlight that other distances may be used in our framework, such as the Cramer distance. This may be preferable since the Cramer distance has unbiased sample gradients BID3 . This could reduce the number of steps required to adjust the Wasserstein distance approximation for each step of training the embedding transformation. Additionally we propose several other extensions and variations in Section 4.1. We have shown how a neural network can be used to transform embedding vectors to 'forget' specifically chosen domain information as indicated by our proposed domain classification metric. The transformed embeddings still preserve the underlying geometry of the space and improve the k-NN MOA metrics. Our approach uses the Wasserstein distance and can in principle handle fairly general distributions of embeddings (as long as the neural network used to approximate the Wasserstein function is general enough). Importantly, we do not have to assume that the distributions are Gaussian. The framework itself is quite general and extendible (see Section 4.1). Unlike methods that use only the controls for adjusting the embeddings, our method can also utilize information from replicates of a treatment across different domains. However, the dataset used did not have treatment replicates across batches, so we only relied on aligning based on the controls. Thus we implicitly assume that the transformation for the controls matches that of the various compounds. We expect our method to be more useful in the context of experiments where many replicates are present, so that they can all be aligned simultaneously. We expect transformations learned for such experiments to have better generalizability since it would be using available knowledge from a greater portion of the embedding space.Our approach requires a choice of free parameters, either for regularization or early stopping, which we address by cross validation across compounds. We discuss potential future directions below, as well as other limiting issues. 63.6 ± 1% 39.8 ± 0.6% 66.4 ± 0.7% 28.0 ± 0.8% 46.8 ± 0.9% 56.2 ± 0.9% 16.6% RF 45.9 ± 0.2% 34.4 ± 0.7% 46.8 ± 0.6% 26.7 ± 0.7% 33.3 ± 0.7% 39.5 ± 0.1% 16.6% Table 3 : We show the silhouette index for TVN only, TVN + WDN, and TVN + CORAL, as discussed in Section 3.2.2. Here WDN refers to the the result using early stopping, and λ = 40, 80, 160 refers to the result when using a regularization with λ = λ M = λ b . Both WDN and CORAL appear to increase the cohesion, as measured by this index. The estimated error denoted by ± was determined by the bootstrapping procedure described in Section 3.","We correct nuisance variation for image embeddings across different domains, preserving only relevant information.",thousands ; the same week ; week ; MOA ; TVN + WDN ; WDN ; F ; One ; Wasserstein ; experimentalist,ways ; evaporation ; the embedding vectors ; Figure ; Section ; different doses ; inputs ; TVN ; this ; the negative control,thousands ; the same week ; week ; MOA ; TVN + WDN ; WDN ; F ; One ; Wasserstein ; experimentalist,"Profiling cellular phenotypes from microscopic imaging can provide meaningful biological information resulting from various factors affecting the cells. The general approach is to find a function mapping the images to an embedding space of manageable dimensionality whose geometry captures relevant features of the input images. However, there are significant issues with separating relevant biological signal from nuisance variation. For example, embedding vectors tend to be more correlated for cells cultured and imaged during the same week than for cells from a different week, despite having identical drug compounds applied in both cases. In this case, the particular batch a set of experiments were conducted in constitutes the domain of the data,",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Our work presents empirical evidence that layer rotation, i.e. the evolution across training of the cosine distance between each layer's weight vector and its initialization, constitutes an impressively consistent indicator of generalization performance. Compared to previously studied indicators of generalization, we show that layer rotation has the additional benefit of being easily monitored and controlled, as well as having a network-independent optimum: the training procedures during which all layers' weights reach a cosine distance of 1 from their initialization consistently outperform other configurations -by up to 20% test accuracy. Finally, our results also suggest that the study of layer rotation can provide a unified framework to explain the impact of weight decay and adaptive gradient methods on generalization. In order to understand the intriguing generalization properties of deep neural networks highlighted by BID22 BID33 BID15 , the identification of numerical indicators of generalization performance that remain applicable across a diverse set of training settings is critical. A well-known and extensively studied example of such indicator is the width of the minima the network has converged to BID11 BID15 .In this paper, we present empirical evidence supporting the discovery of a novel indicator of generalization: the evolution across training of the cosine distance between each layer's weight vector and its initialization (denoted by layer rotation). Indeed , we show across a diverse set of experiments (with varying datasets, networks and training procedures), that larger layer rotations (i.e. larger cosine distance between final and initial weights of each layer) consistently translate into better generalization performance. In addition to providing an original perspective on generalization, our experiments suggest that layer rotation also BID0 ICTEAM, Université catholique de Louvain, Louvain-LaNeuve, Belgium. <simon.carbonnelle@uclouvain.be>.benefits from the following properties compared to alternative indicators of generalization:• It is easily monitored and, since it only depends on the evolution of the network's weights, can be controlled along the optimization through appropriate weight update adjustments • It has a network-independent optimum (all layers reaching a cosine distance of 1) • It provides a unified framework to explain the impact of weight decay and adaptive gradient methods on generalization.In comparison, other indicators usually provide a metric to optimize (e.g. the wider the minimum, the better) but no clear optimum to be reached (what is the optimal width?), nor a precise methodology to tune it (how to converge to a minimum with a specific width?). By disclosing simple guidelines to tune layer rotations and an easy-to-use controlling tool, our work can also help practitioners get the best out of their network with minimal hyper-parameter tuning.The presentation of our experimental study is structured according to three successive steps:1. Development of tools to monitor and control layer rotation (Section 2); 2. Systematic study of layer rotation configurations in a controlled setting (Section 3); 3. Study of layer rotation configurations in standard training settings, with a special focus on SGD, weight decay and adaptive gradient methods (Section 4).Related work is discussed in Supplementary Material.",This paper presents empirical evidence supporting the discovery of an indicator of generalization: the evolution across training of the cosine distance between each layer's weight vector and its initialization.,Université ; Belgium ; generalization:• ; • It ; three ; SGD ; Supplementary Material,practitioners ; generalization performance ; better generalization performance ; Study ; tools ; generalization ; three ; a minimum ; weight decay ; all layers' weights,Université ; Belgium ; generalization:• ; • It ; three ; SGD ; Supplementary Material,"In this paper, we show that layer rotation, i.e. the evolution across training of the cosine distance between each layer's weight vector and its initialization, constitutes an impressively consistent indicator of generalization performance. It is easily monitored and controlled and, since it only depends on the evolution of the network's weights, can be controlled along the optimization through appropriate weight update adjustments. In addition to providing an original perspective on generalization, it also provides a unified framework to explain the impact of weight decay and adaptive gradient methods. In order to understand the intriguing generalization properties of deep neural networks highlighted by BID22 BID",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Learning long-term dependencies is a key long-standing challenge of recurrent neural networks (RNNs). Hierarchical recurrent neural networks (HRNNs) have been considered a promising approach as long-term dependencies are resolved through shortcuts up and down the hierarchy. Yet, the memory requirements of Truncated Backpropagation Through Time (TBPTT) still prevent training them on very long sequences. In this paper, we empirically show that in (deep) HRNNs, propagating gradients back from higher to lower levels can be replaced by locally computable losses, without harming the learning capability of the network, over a wide range of tasks. This decoupling by local losses reduces the memory requirements of training by a factor exponential in the depth of the hierarchy in comparison to standard TBPTT. Recurrent neural networks (RNNs) model sequential data by observing one sequence element at a time and updating their internal (hidden) state towards being useful for making future predictions. RNNs are theoretically appealing due to their Turing-completeness Siegelmann and Sontag (1995) , and, crucially, have been tremendously successful in complex real-world tasks, including machine translation Cho et al. (2014) ; Sutskever et al. (2014) , language modelling Mikolov et al. (2010) , and reinforcement learning Mnih et al. (2016) . Still, training RNNs in practice is one of the main open problems in deep learning, as the following issues prevail. (1) Learning long-term dependencies is extremely difficult because it requires that the gradients (i.e. the error signal) have to be propagated over many steps, which easily causes them to vanish or explode Hochreiter (1991) ; Bengio et al. (1994) ; Hochreiter (1998) (2) Truncated Backpropagation Through Time (TBPTT) Williams and Peng (1990) , the standard training algorithm for RNNs, requires memory that grows linearly in the length of the sequences on which the network is trained. This is because all past hidden states must be stored. Therefore, the memory requirements of training RNNs with large hidden states on long sequences become prohibitively large. (3) In TBPTT, parameters cannot be updated until the full forward and backward passes have been completed. This phenomenon is known as the parameter update lock Jaderberg et al. (2017) . As a consequence, the frequency at which parameters can be updated is inversely proportional to the length of the time-dependencies that can be learned, which makes learning exceedingly slow for long sequences. The problem of vanishing/exploding gradients has been alleviated by a plethora of approaches ranging from specific RNN architectures Hochreiter and Schmidhuber (1997) ; Cho et al. (2014) to optimization techniques aiming at easing gradient flow Martens and Sutskever (2011) ; Pascanu et al. (2013) . A candidate for effectively resolving the vanishing/exploding gradient problem is hierarchical RNNs (HRNNs) Schmidhuber (1992) ; El Hihi and Bengio (1996) ; Koutnik et al. (2014) ; Sordoni et al. (2015) ; Chung et al. (2016) . In HRNNs, the network itself is split into a hierarchy of levels, which are updated at decreasing frequencies. As higher levels of the hierarchy are updated less frequently, these architectures have short (potentially logarithmic) gradient paths that greatly reduce the vanishing/exploding gradients issue. In this paper, we show that in HRNNs, the lower levels of the hierarchy can be decoupled from the higher levels, in the sense that the gradient flow from higher to lower levels can effectively be replaced by locally computable losses. Also, we demonstrate that in consequence, the decoupled HRNNs admit training with memory decreased by a factor exponentially in the depth of the hierarchy compared to HRNNs with standard TBPTT. The local losses stem from decoder networks which are trained to decode past inputs to each level from the hidden state that is sent up the hierarchy, thereby forcing this hidden state to contain all relevant information. We experimentally show that in a diverse set of tasks which rely on long-term dependencies and include deep hierarchies, the performance of the decoupled HRNN with local losses is indistinguishable from the standard HRNN. In summary, we introduce a RNN architecture with short gradient paths that can be trained memoryefficiently, thereby addressing issues (1) and (2). In the bigger picture, we believe that our approach of replacing gradient flow in HRNNs by locally computable losses may eventually help to attempt solving issue (3) as well. In this paper, we have shown that in hierarchical RNNs the gradient flow from higher to lower levels can be effectively replaced by locally computable losses. This allows memory savings up to an exponential factor in the depth of the hierarchy. In particular, we first explained how not propagating gradients from higher to lower levels permits these memory savings. Then, we introduced auxiliary losses that encourage information to flow up the hierarchy. Finally, we demonstrated experimentally that the memory-efficient HRNNs with our auxiliary loss perform on par with the memory-heavy HRNNs and strongly outperform HRNNs given the same memory budget on a wide range of tasks, including deeper hierarchies. High capacity RNNs, like Differentiable Plasticity Miconi et al. (2018) , or Neural Turing Machines Graves et al. (2014) have been shown to be useful and even achieve state-of-the-art in many tasks. However, due to the memory cost of TBPTT, training such models is often impractical for long sequences. We think that combining these models with our techniques in future work could open the possibility for using high capacity RNNs for tasks involving long-term dependencies that have been out of reach so far. Still, the problem of the parameter update lock remains. While this is the most under-explored of the three big problems when training RNNs (vanishing/exploding gradients and memory requirements being the other two), resolving it is just as important in order to be able to learn long-term dependencies. We believe that the techniques laid out in this work (i.e. replacing gradients in HRNNs by locally computable losses) can be a stepping stone towards solving the parameter update lock. We leave this for future work.",We replace some gradients paths in hierarchical RNN's by an auxiliary loss. We show that this can reduce the memory cost while preserving performance.,Jaderberg ; Siegelmann ; Truncated Backpropagation Through Time ; al. ; Cho et al. ; Hochreiter ; three ; Mikolov et al ; Sutskever ; Cho et al,a stepping stone ; these models ; the learning capability ; all past hidden states ; Hierarchical recurrent neural networks ; Mnih ; the memory requirements ; a hierarchy ; the same memory budget ; future predictions,Jaderberg ; Siegelmann ; Truncated Backpropagation Through Time ; al. ; Cho et al. ; Hochreiter ; three ; Mikolov et al ; Sutskever ; Cho et al,"Learning long-term dependencies is a key long-standing challenge of recurrent neural networks (RNNs). Hierarchical recurrent networks (HRNNs) have been considered a promising approach as they resolve their dependencies through shortcuts up and down the hierarchy. However, memory requirements of Truncated Backpropagation Through Time (TBPTT) still prevent training them on very long sequences. In this paper, we empirically show that in (deep) HRNNs, propagating gradients back from higher to lower levels can be replaced by locally computable losses without harming the learning capability of the network, over a wide range",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"For multi-valued functions---such as when the conditional distribution on targets given the inputs is multi-modal---standard regression approaches are not always desirable because they provide the conditional mean. Modal regression approaches aim to instead find the conditional mode, but are restricted to nonparametric approaches. Such approaches can be difficult to scale, and make it difficult to benefit from parametric function approximation, like neural networks, which can learn complex relationships between inputs and targets. In this work, we propose a parametric modal regression algorithm, by using the implicit function theorem to develop an objective for learning a joint parameterized function over inputs and targets. We empirically demonstrate on several synthetic problems that our method (i) can learn multi-valued functions and produce the conditional modes, (ii) scales well to high-dimensional inputs and (iii) is even more effective for certain unimodal problems, particularly for high frequency data where the joint function over inputs and targets can better capture the complex relationship between them. We conclude by showing that our method provides small improvements on two regression datasets that have asymmetric distributions over the targets. The goal in regression is to find the relationship between the input (observation) variable X ∈ X and the output (response) Y ∈ Y variable, given samples of (X, Y ). The underlying premise is that there exists an unknown underlying function g * : X → Y that maps the input space X to the output space Y. We only observe a noise-contaminated value of that function: sample (x, y) has y = g * (x) + η for some noise η. If the goal is to minimize expected squared error, it is well known that E[Y |x] is the optimal predictor (Bishop, 2006) . It is common to use Generalized Linear Models (Nelder & Wedderburn, 1972) , which attempt to estimate E[Y |x] for different uni-modal distribution choices for p(y|x), such as Gaussian (l 2 regression) and Poisson (Poisson regression). For multi-modal distributions, however, predicting E[Y |x] may not be desirable, as it may correspond to rarely observed y that simply fall between two modes. Further, this predictor does not provide any useful information about the multiple modes. Modal regression is designed for this problem, and though not widely used in the general machine learning community, has been actively studied in statistics. Most of the methods are non-parametric, and assume a single mode jae Lee (1989) ; Lee & Kim (1998) ; Kemp & Silva (2012) ; Yu & Aristodemou (2012) ; Yao & Li (2014) ; Lv et al. (2014) ; Feng et al. (2017) . The basic idea is to adjust target values towards their closest empirical conditional modes, based on a kernel density estimator. These methods rely on the chosen kernel and may have issues scaling to high-dimensional data due to issues in computing similarities in high-dimensional spaces. There is some recent work using quantile regression to estimate conditional modes (Ota et al., 2018) , and though promising for a parametric approach, is restricted to linear quantile regression. A parametric approach for modal regression would enable these estimators to benefit from the advances in learning functions with neural networks. The most straightforward way to do so is to learn a mixture distribution, such as with conditional mixture models with parameters learning by a neural network (Powell, 1987; Bishop, 1994; Williams, 1996; Husmeier, 1997; Husmeier & Taylor, 1998; Zen & Senior, 2014; Ellefsen et al., 2019) . The conditional modes can typically be extracted from such models. Such a strategy, however, might be trying to solve a harder problem than is strictly needed. The actual goal is to simply identify the conditional modes, without accurately representing the full conditional distribution. Training procedures for the conditional distribution can be more complex. Methods like EM can be slow (Vlassis & Krose, 1999) and some approaches have opted to avoid this altogether by discretizing the target and learning a discrete distribution (Weigend & Srivastava, 1995; Feindt, 2004) . Further, the mixture requires particular probabilistic choices to be made, including the number of components, which may not be correctly specified: they might be more or less than the true number of conditional modes. In this paper, we propose a new parametric modal regression approach, by developing an objective to learn a parameterized function f (x, y) on both input feature and target/output. We use the Implicit Function Theorem (Munkres, 1991) , which states that if we know the input-output relation in the form of an implicit function, then a general multi-valued function, under certain gradient conditions, can locally be converted to a single-valued function. We learn a function f (x, y) that approximates such local functions, by enforcing the gradient conditions. We empirically demonstrate that our method can effectively learning the conditional modes on several synthetic problems, and that for those same problems, scales well when the input is made high-dimensional. We also show an interesting benefit that the joint representation learned over x and y appears to improve prediction performance even for uni-modal problem, for high frequency functions where the function values changes quickly between nearby x. Finally, we show that our method provides small improvements on two regression datasets that have asymmetric distributions over the targets. The proposed approach to multi-valued prediction is flexible, allowing for a variable number of conditional modes to be discovered for each x, and we believe it is a promising direction for further improvements in parametric modal regression. The paper introduces a simple and powerful implicit function learning approach for modal regression. We show that it can handle datasets where the conditional distribution p(y|x) is multimodal, and is particularly useful when the underlying true mapping has a large bandwidth limit. We also illustrate that our algorithm achieves competitive performance on large real world datasets with different underlying target distributions. We would like to conclude with the following future directions. First, it would be interesting to establish connections to KDE-based modal regression methods, which have a nice theoretical interpretation (Feng et al., 2017) . The connection may yield finite sample analysis for our implicit function learning algorithm. Second, like many supervised learning algorithms, our algorithm may also overfit to noise. Popular regularization technique such as random dropout (Srivastava et al., 2014) may be tested for very noisy data. Third, in online learning setting, the efficiency of doing prediction by arg min y f θ (x, y) 2 + ( ∂f θ (x,y) ∂y + 1) 2 becomes a concern. One possible solution is to borrow ideas from cross-entropy method as used in reinforcement learning (Lim et al., 2018; Simmons-Edler et al., 2019) . For example, we can use a separate NN to suggest a set of initial values of y for searching optimums by gradient methods. Last, it is worth investigating alternative constraints on the Jacobian instead of restricting the diagonal values to −1.",We introduce a simple and novel modal regression algorithm which is easy to scale to large problems.,Generalized Linear Models ; Ellefsen ; Vlassis & Krose ; Yao & Li ; Weigend & Srivastava ; Y. ; Lee & Kim ; Gaussian ; Feng et al. ; Powell,certain gradient conditions ; Srivastava ; their closest empirical conditional modes ; datasets ; nearby x. ; multi-valued functions ; Poisson ; this paper ; high-dimensional inputs ; parametric function approximation,Generalized Linear Models ; Ellefsen ; Vlassis & Krose ; Yao & Li ; Weigend & Srivastava ; Y. ; Lee & Kim ; Gaussian ; Feng et al. ; Powell,"In multi-valued functions, standard regression approaches are not always desirable because they provide the conditional mean. Modal regression approaches aim to find the conditional mode but are restricted to nonparametric approaches. Such approaches can be difficult to scale, and make it difficult to benefit from parametric function approximation, like neural networks, which can learn complex relationships between inputs and targets. In this work, we propose a parametric modal regression algorithm, using the implicit function theorem to develop an objective for learning a joint parameterized function over inputs and target. We empirically demonstrate on several synthetic problems that our method is more effective for learning multi-",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Models of code can learn distributed representations of a program's syntax and semantics to predict many non-trivial properties of a program. Recent state-of-the-art models leverage highly structured representations of programs, such as trees, graphs and paths therein (e.g. data-flow relations), which are precise and abundantly available for code. This provides a strong inductive bias towards semantically meaningful relations, yielding more generalizable representations than classical sequence-based models. Unfortunately, these models primarily rely on graph-based message passing to represent relations in code, which makes them de facto local due to the high cost of message-passing steps, quite in contrast to modern, global sequence-based models, such as the Transformer. In this work, we bridge this divide between global and structured models by introducing two new hybrid model families that are both global and incorporate structural bias: Graph Sandwiches, which wrap traditional (gated) graph message-passing layers in sequential message-passing layers; and Graph Relational Embedding Attention Transformers (GREAT for short), which bias traditional Transformers with relational information from graph edge types. By studying a popular, non-trivial program repair task, variable-misuse identification, we explore the relative merits of traditional and hybrid model families for code representation. Starting with a  graph-based model that already improves upon the prior state-of-the-art for this task by 20%, we show that our proposed hybrid models improve an additional 10-15%, while training both faster and using fewer parameters. Well-trained models of source code can learn complex properties of a program, such as its implicit type structure (Hellendoorn et al., 2018) , naming conventions (Allamanis et al., 2015) , and potential bugs and repairs (Vasic et al., 2019) . This requires learning to represent a program's latent, semantic properties based on its source. Initial representations of source code relied on sequential models from natural-language processing, such as n-gram language models (Hindle et al., 2012; Allamanis & Sutton, 2013; Hellendoorn & Devanbu, 2017) and Recurrent Neural Networks (RNNs) (White et al., 2015) , but these models struggle to capture the complexity of source code. Source code is rich in structured information, such as a program's abstract syntax tree, data and control flow. Allamanis et al. (2018b) proposed to model some of this structure directly, providing a powerful inductive bias towards semantically meaningful relations in the code. Their Gated Graph Neural Network (GGNN) model for embedding programs was shown to learn better, more generalizable representations faster than classical RNN-based sequence models. However, the debate on effective modeling of code is far from settled. Graph neural networks typically rely on synchronous message passing, which makes them inherently local, requiring many iterations of message passing to aggregate information from distant parts of the code. However, state-of-the-art graph neural networks for code often use as few as eight message-passing iterations (Allamanis et al., 2018b; Fernandes et al., 2018) , primarily for computational reasons: program graphs can be very large, and training time grows linearly with the number of message passes. This is in contrast to, e.g., Transformer models (Vaswani et al., 2017) , which allow program-wide information flow at every step, yet lack the powerful inductive bias from knowing the code's structure. This leads us to a basic research question: is there a fundamental dichotomy between global, unstructured and local, structured models? Our answer is an emphatic no. Our starting point is the sequence-to-pointer model of Vasic et al. (2019) , which is state-of-the-art for the task of localizing and repairing a particular type of bug. As a sequence model, their architecture can (at least potentially) propagate information globally, but it lacks access to the known semantic structure of code. To this end, we replace the sequence encoder of Vasic et al. (2019) with a GGNN, yielding a new graph-to-mutlihead-pointer model. Remarkably, this model alone yields a 20% improvement over the state of the art, though at the cost of being significantly larger than the sequence model. Motivated by this result, we propose two new families of models that efficiently combine longerdistance information, such as the sequence model can represent, with the semantic structural information available to the GGNN. One family, the Graph Sandwich, alternates between message passing and sequential information flow through a chain of nodes within the graph; the other, the Graph Relational Embedding Attention Transformer (GREAT), generalizes the relative position embeddings in Transformers by Shaw et al. (2018) to convey structural relations instead. We show that our proposed model families outperform all prior results, as well as our new, already stronger baseline by an additional 10% each, while training both substantially faster and using fewer parameters. We demonstrate that models leveraging richly structured representations of source code do not have to be confined to local contexts. Instead, models that leverage only limited message passing in combination with global models learn much more powerful representations faster. We proposed two different architectures for combining local and global information: sandwich models that combine two different message-passing schedules and achieve highly competitive models quickly, and the GREAT model which adds information from a sparse graph to a Transformer to achieve stateof-the-art results. In the process, we raise the state-of-the-art performance on the VarMisuse bug localization and repair task by over 30%.",Models of source code that combine global and structural features learn more powerful representations of programs.,GGNN ; two ; Transformer ; as few as eight ; Recurrent Neural Networks ; n-gram language models ; One ; Graph Relational Embedding Attention Transformers ; Hellendoorn et al. ; al.,highly competitive models ; all prior results ; message-passing steps ; RNNs ; highly structured representations ; Vasic ; (e.g. data-flow relations ; the number ; alternates ; graph edge types,GGNN ; two ; Transformer ; as few as eight ; Recurrent Neural Networks ; n-gram language models ; One ; Graph Relational Embedding Attention Transformers ; Hellendoorn et al. ; al.,"Models of code can learn from structured representations of programs, such as trees, graphs and paths, which are precise and abundantly available for code. However, these models primarily rely on graph-based message passing to represent relations in code, making them de facto local due to the high cost of message-passing steps. In this work, we bridge this divide between global and structured models by introducing two new hybrid model families, Graph Sandwiches and Graph Relational Embedding Attention Transformers (GREAT for short), which combine traditional and structured message passing layers with relational information from graph edge types. These models are both global and",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"We propose that approximate Bayesian algorithms should optimize a new criterion, directly derived from the loss, to calculate their approximate posterior which we refer to as pseudo-posterior. Unlike standard variational inference which optimizes a lower bound on the log marginal likelihood, the new algorithms can be analyzed to provide loss guarantees on the predictions with the pseudo-posterior. Our criterion can be used to derive new sparse Gaussian process algorithms that have error guarantees applicable to various likelihoods. Results in learning theory show that, under some general conditions, minimizing training set loss, also known as empirical risk minimization (ERM), provides good solutions in the sense that the true loss of such procedures is bounded relative to the best loss possible in hindsight. Alternative algorithms such as structural risk minimization or regularized loss minimization (RLM) have similar guarantees under more general conditions. On the other hand, Bayesian approaches are, in a sense, prescriptive. Given prior and data, we calculate a posterior distribution that compactly captures all our knowledge about the problem. Then, given a prediction task with an associated loss for wrong predictions, we pick the best prediction given our posterior. This is optimal when the model is correct and the exact posterior is tractable. However, the algorithmic choices are less clear with misspecified models or, even if the model is correct, when exact inference is not possible and the learning algorithm can only return an approximation to the posterior. Since the choices are often heuristically motivated we call such approximations pseudo-posteriors. The question is how the pseudo-posterior should be calculated. In this paper we propose to use learning theory to guide this process. To motivate our approach consider the variational approximation which is one of the most effective methods for approximate inference in Bayesian models. In lieu of finding the exact posterior, variational inference maximizes the ELBO, a lower bound on the marginal likelihood. It is well known that this can be seen alternatively as performing regularized loss minimization. For example, in a model with parameters w, prior p(w ), and data y where p(y|w, x ) = i p(y i |w, x i ), we have log p(y ) ≥ ELBO E where q(w ) is the variational posterior and we have suppressed the dependence on x for visual clarity. Minimizing the negative ELBO, we have a loss term i E q(w) [− log p(y i |w, x i )] and a regularization term d KL (q(w), p(w)). The RLM viewpoint is attractive from the perspective of statistical learning theory because such algorithms are known to have good generalization guarantees (under some conditions). However, the ELBO objective is not matched to the intended use of Bayesian predictors: given a posterior q(w) and test example x * , the Bayesian predictor first calculates the predictive distribution p(y * |x * ) = E q(w) [p(y * |x * , w)] and then, assuming we are interested in the log loss, suffers the loss − log p(y * |x * ). In other words, seen from the perspective of learning theory, variational inference optimizes for , which is the loss of the Bayesian predictor. These observations immediately raise several questions: Should we design empirical risk minimization (ERM) algorithms minimizing L B that produce pseudo-posteriors? Should a regularization term, e.g., d KL , be added? Can we use standard analysis, that typically handles frequentist models, to provide guarantees for such algorithms? We emphasize that this differs from standard non-Bayesian algorithms that perform ERM or RLM to find the best parameter w. Here, we propose to perform ERM or RLM to find the best pseudoposterior q(w) as given by the parameters that define it. In this paper, we show that such an analysis can indeed be performed, and provide results which are generally applicable to Bayesian predictors optimized using ERM. Then, we focus on sparse Gaussian processes (sGP) for which we develop risk bounds for a smoothed variant of log loss 1 and any observation likelihood (the non-conjugate case). The significance of this is conceptual, in that it points to a different principle for designing approximate inference algorithms where we no longer aim to optimize the marginal likelihood (or ELBO), but instead a criterion that is directly related to the loss -this diverges from current practice in the literature. The paper highlights sparse GP because it is an important model with significant recent interest and work. But the approach and results are more generally applicable. To illustrate this point the appendix shows how the results can be applied to the Correlated Topic Model (CTM) of Blei and Lafferty (2006) . It is important to distinguish this work from two previous lines of work. Our earlier work (Sheth and Khardon, 2017) made similar observations w.r.t. the mismatch between the optimization criterion and the intended objective. However, the goal there was to analyze existing algorithms where possible. More concretely we showed that optimizing a criterion related to L G does have some risk guarantees, though these are weaker than the ones in this paper. Here, we propose to explore new algorithms based on direct loss minimization with stronger associated guarantees. In Alaoui and Mahoney (2015) and Burt et al. (2019) , the goal is to show that the sparse GP approximation can be chosen to be very close to the full GP solution. Conditions on the kernel functions and on the algorithm to select inducing input locations and variational distribution are given for this to be true. This is a very strong result showing that nothing is lost by using the sparse approximation. However, in many cases, the number of inducing inputs required is too large (e.g., for Matern kernels). In contrast, our analysis aims at identifying the best sGP posterior in terms of the resulting prediction performance, whether it is close to the full GP posterior or not. In other words, we seek an ""agnostic PAC guarantee"" for the sparse GP posterior. The paper points out the potential of DLM to yield a new type of approximate pseudoBayesian algorithm. In this paper we focused on the analysis of ERM and application to sparse GP. There are many important questions for future work including analysis for RLM, analysis for hyperparameter selection, removing the need for bounded or smoothed loss in our theorem, and investigating empirical properties of these algorithmic variants.",This paper utilizes the analysis of Lipschitz loss on a bounded hypothesis space to derive new ERM-type algorithms with strong performance guarantees that can be applied to the non-conjugate sparse GP model.,p(y ; CTM ; Blei and Lafferty ; Gaussian ; non-Bayesian ; Alaoui ; Bayesian ; ≥ ; − ; Khardon,the optimization criterion ; the new algorithms ; Bayesian predictors ; Matern kernels ; the kernel functions ; Our criterion ; ERM ; this point ; a prediction task ; such procedures,p(y ; CTM ; Blei and Lafferty ; Gaussian ; non-Bayesian ; Alaoui ; Bayesian ; ≥ ; − ; Khardon,"We propose that approximate Bayesian algorithms should optimize a new criterion, directly derived from the loss, to calculate their approximate posterior which we refer to as pseudo-posterior. Unlike standard variational inference which optimizes a lower bound on the log marginal likelihood, the new algorithms can be analyzed to provide loss guarantees on predictions. These guarantees can be used to derive sparse Gaussian process algorithms that have error guarantees applicable to various likelihoods. In learning theory, minimizing training set loss, also known as empirical risk minimization (ERM), provides good solutions in the sense that the true loss of such procedures is bounded relative to the best",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"We propose a software framework based on ideas of the Learning-Compression algorithm , that allows one to compress any neural network by different compression mechanisms (pruning, quantization, low-rank, etc.). By design, the learning of the neural net (handled by SGD) is decoupled from the compression of its parameters (handled by a signal compression function), so that the framework can be easily extended to handle different combinations of neural net and compression type. In addition, it has other advantages, such as easy integration with deep learning frameworks, efficient training time, competitive practical performance in the loss-compression tradeoff, and reasonable convergence guarantees. Our toolkit is written in Python and Pytorch and we plan to make it available by the workshop time, and eventually open it for contributions from the community.","We propose a software framework based on ideas of the Learning-Compression algorithm , that allows one to compress any neural network by different compression mechanisms (pruning, quantization, low-rank, etc.).",the Learning-Compression ; SGD ; Python ; Pytorch,Pytorch ; the framework ; neural net and compression type ; reasonable convergence guarantees ; addition ; SGD ; that ; pruning ; efficient training time ; the learning,the Learning-Compression ; SGD ; Python ; Pytorch,"We propose a software framework based on the Learning-Compression algorithm that allows one to compress any neural network by different compression mechanisms. By design, the learning of the neural net is decoupled from the compression of its parameters (handled by a signal compression function) and can be extended to handle different types of neural net and compression types. The framework has several advantages, such as its simplicity and competitive practical performance in loss-compression tradeoff, and reasonable convergence guarantees.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Adversarial training provides a principled approach for training robust neural networks. From an optimization perspective, the adversarial training is essentially solving a minmax robust optimization problem. The outer minimization is trying to learn a robust classifier, while the inner maximization is trying to generate adversarial samples. Unfortunately, such a minmax problem is very difficult to solve due to the lack of convex-concave structure. This work proposes a new adversarial training method based on a general learning-to-learn framework. Specifically, instead of applying the existing hand-design algorithms for the inner problem, we learn an optimizer, which is parametrized as a convolutional neural network. At the same time, a robust classifier is learned to defense the adversarial attack generated by the learned optimizer. From the perspective of generative learning, our proposed method can be viewed as learning a deep generative model for generating adversarial samples, which is adaptive to the robust classification. Our experiments demonstrate that our proposed method significantly outperforms existing adversarial training methods on CIFAR-10 and CIFAR-100 datasets.",Don't know how to optimize? Then just learn to optimize!,,which ; an optimization perspective ; a new adversarial training method ; the adversarial attack ; the same time ; the inner maximization ; Adversarial training ; an optimizer ; a robust classifier ; a minmax robust optimization problem,,"Adversarial training provides a principled approach for training robust neural networks. The adversarial training involves learning a robust classifier, while the inner maximization is trying to generate adversarial samples. This approach is based on a general learning-to-learn framework. Instead of applying hand-design algorithms for the inner problem, we learn an optimizer, which is parametrized as a convolutional neural network. At the same time, a robust Classifier is learned to defense the adversarial attack generated by the learned optimizer. From the perspective of generative learning, our proposed method can be viewed as learning a deep",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"This paper introduces the task of semantic instance completion: from an incomplete RGB-D scan of a scene, we aim to detect the individual object instances comprising the scene and infer their complete object geometry. This enables a semantically meaningful decomposition of a scanned scene into individual, complete 3D objects, including hidden and unobserved object parts. This will open up new possibilities for interactions with object in a scene, for instance for virtual or robotic agents. To address this task, we propose 3D-SIC, a new data-driven approach that jointly detects object instances and predicts their completed geometry. The core idea of 3D-SIC is a novel end-to-end 3D neural network architecture that leverages joint color and geometry feature learning. The fully-convolutional nature of our 3D network enables efficient inference of semantic instance completion for 3D scans at scale of large indoor environments in a single forward pass. In a series evaluation, we evaluate on both real and synthetic scan benchmark data, where we outperform state-of-the-art approaches by over 15 in mAP@0.5 on ScanNet, and over 18 in mAP@0.5 on SUNCG. Understanding 3D environments is fundamental to many tasks spanning computer vision, graphics, and robotics. In particular, in order to effectively navigate, and moreover interact with an environment, an understanding of the geometry of a scene and the objects it comprises is essential. This is in contrast to the partial nature of reconstructed RGB-D scans; e.g., due to sensor occlusions. For instance, for a robot exploring an environment, it needs to infer instance-level object segmentation and complete object geometry in order to perform tasks like grasping, or estimate spatial arrangements of individual objects. Additionally, for content creation or mixed reality applications, captured scenes must be decomposable into their complete object components, in order to enable applications such as scene editing or virtual-real object interactions; i.e., it might be insufficient to predict object instance masks only for observed regions. Thus, we aim to address this task of predicting object detection as well as instance-level completion for an input partial 3D scan of a scene; we refer to this task as semantic instance completion. Previous approaches have considered semantic scene segmentation jointly with scan completion , but lack the notion of individual objects. In contrast, our approach focuses on the instance level, as knowledge of instances is essential towards enabling interaction with the objects in an environment. In addition, the task of semantic instance completion is not only important towards enabling objectlevel understanding and interaction with 3D environments, but we also show that the prediction of complete object geometry informs the task of semantic instance segmentation. Thus, in order to address the task of semantic instance completion, we propose to consider instance detection and object completion in an end-to-end, fully differentiable fashion. From an input RGB-D scan of a scene, our new 3D semantic instance completion network first regresses bounding boxes for objects in the scene, and then performs object classification followed by a prediction of complete object geometry. Our approach leverages a unified backbone from which instance detection and object completion are predicted, enabling information to flow from completion to detection. We incorporate features from both color image and 3D geometry of a scanned scene, as well as a fully-convolutional design in order to effectively predict the complete object decomposition of varying-sized scenes. In summary, we present a fully-convolutional, end-to-end 3D CNN formulation to predict 3D instance completion that outperforms state-of-the-art, decoupled approaches to semantic instance completion by 15.8 in mAP@0.5 on real-world scan data, and 18.5 in mAP@0.5 on synthetic data: • We introduce the task of semantic instance completion for 3D scans; • we propose a novel, end-to-end 3D convolutional network which predicts 3D semantic instance completion as object bounding boxes, class labels, and complete object geometry, • and we show that semantic instance completion task can benefit semantic instance segmentation performance. In this paper, we introduced the new task of semantic instance completion along with 3D-SIC, a new 3D CNN-based approach for this task, which jointly detects objects and predicts their complete geometry. Our proposed 3D CNN learns from both color and geometry features to detect and classify objects, then predict the voxel occupancy for the complete geometry of the object in end-to-end fashion, which can be run on a full 3D scan in a single forward pass. On both real and synthetic scan data, we significantly outperform alternative approaches for semantic instance completion. We believe that our approach makes an important step towards higher-level scene understanding and helps to enable object-based interactions and understanding of scenes, which we hope will open up new research avenue. Table 6 : Anchor sizes (in voxels) used for SUNCG region proposal. Sizes are given in voxel units, with voxel resolution of ≈ 4.69cm Table 10 details the layers used in our backbone. 3D-RPN, classification head, and mask completion head are described in Table 11 . Additionally, we leverage the residual blocks in our backbone, which is listed in Table 9 . Note that both the backbone and mask completion head are fully-convolutional. For the classification head, we use several fully-connected layers; however, we leverage 3D RoIpooling on its input, we can run our method on large 3D scans of varying sizes in a single forward pass. We additionally list the anchors used for the region proposal for our model trained on our ScanNetbased semantic instance completion benchmark (Avetisyan et al., 2019; Dai et al., 2017a) and SUNCG datasets in Tables 5 and 6 , respectively. Anchors for each dataset are determined through k-means clustering of ground truth bounding boxes. The anchor sizes are given in voxels, where our voxel size is ≈ 4.69cm.","From an incomplete RGB-D scan of a scene, we aim to detect the individual object instances comprising the scene and infer their complete object geometry.",SUNCG ; first ; CNN ; • ; RoIpooling ; ScanNetbased ; Avetisyan ; al. ; Dai et al.,complete object geometry ; a scanned scene ; Avetisyan ; clustering ; object instances ; spatial arrangements ; instance-level object segmentation ; robotics ; The fully-convolutional nature ; semantic instance segmentation performance,SUNCG ; first ; CNN ; • ; RoIpooling ; ScanNetbased ; Avetisyan ; al. ; Dai et al.,"The task of semantic instance completion involves detecting individual object instances and infer their complete object geometry. This enables semantically meaningful decomposition of a scanned scene into individual, complete 3D objects, including hidden and unobserved object parts. This will open up new possibilities for interactions with object in a scene, such as virtual or robotic agents. To address this task, 3D-SIC is a data-driven approach that jointly detects object instances, predicts their completed geometry, and employs joint color and geometry feature learning. The fully-convolutional nature of our 3D network enables efficient inference of semantic instances for 3D scans at scale",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"The fast generation and refinement of protein backbones would constitute a major advancement to current methodology for the design and development of de novo proteins. In this study, we train Generative Adversarial Networks (GANs) to generate fixed-length full-atom protein backbones, with the goal of sampling from the distribution of realistic 3-D backbone fragments. We represent protein structures by pairwise distances between all backbone atoms, and present a method for directly recovering and refining the corresponding backbone coordinates in a differentiable manner. We show that interpolations in the latent space of the generator correspond to smooth deformations of the output backbones, and that test set structures not seen by the generator during training exist in its image. Finally, we perform sequence design, relaxation, and ab initio folding of a subset of generated structures, and show that in some cases we can recover the generated folds after forward-folding. Together, these results suggest a mechanism for fast protein structure refinement and folding using external energy functions. Deep generative models, which harness the power of deep neural networks, have achieved remarkable results in realistic sample generation across many modalities including images (1; 2; 3; 4; 5), video (6; 7), audio BID7 , and symbolic expressions BID8 . These methods have been further applied to problems in biology and chemistry, such as the generation of small molecules BID9 and more recently, protein backbones BID10 . The ability to easily sample from the distribution of viable proteins would be useful for the development of new therapeutics, where often the goal is to determine the structure of a putative ligand for a known target receptor or to realistically modify an existing protein. As a more general engineering problem, speeding up and improving the de novo protein design process would be extremely valuable in the modeling and development of new biosensors, enzymes, and therapeutics.Recently, Generative Adversarial Networks (GANs) were trained to generate matrices (""maps"") representing pairwise distances between alpha-carbons on fixed-length protein backbones BID10 . This representation of the backbone is rotationally and translationally invariant and captures long-range 3-D contacts; training on these maps allows for the stable generation of highly varied structures. However, a drawback to the approach in BID10 is that the underlying 3-D coordinates of the backbone must be recovered and local errors in the backbone due to errors in the generated maps must be corrected. The reported method for coordinate recovery in BID10 is not differentiable and requires iterative optimization.In this study, we extend the methods in BID10 by (i) generating full-atom pairwise distance matrices for fixed-length fragments and (ii) training deep neural networks to recover and refine the corresponding coordinates. Importantly, we show that for a subset of structures, we can design sequences onto the generated backbones, and recover their structures by forward-folding using the Rosetta macromolecular design suite (12; 13). Our results suggest that a subset of the generated fragments can host folding sequences and thus are viable starting scaffolds for de novo protein design. Our goal is The generator G generates a pairwise distance matrix, for which the underlying coordinates are recovered by network V . Further coordinate refinement is done with additive updates by network R.to eventually incorporate external heuristic energy functions into the learning algorithm, to further refine the generated backbones. In this paper, we propose a pipeline for generating fixed-length full-atom protein backbones in a fully differentiable manner. We train a model to generate pairwise distance matrices between atoms on the backbone, which eliminates the need to explicitly encode structure invariances to arbitrary rotations and translations, while also modeling long-range contacts. We show that we can then train networks to learn to recover and refine the underlying coordinates.Finally, we take steps to show the capacity of our pipeline for de novo protein design. Specifically we show that interpolations in the latent space of the generator correspond to smooth deformations of the recovered peptide backbone, that native unseen structures exist in the image of the generator, and that a subset of generated backbones can host foldable sequences. Together, these results suggest a mechanism for fast protein structure refinement and folding using external energy functions.We plan next to learn to generate relaxed, low-energy structures by directly optimizing our generative pipeline using the Rosetta energy function BID22 , differentiating through the coordinate recovery and refinement modules. We plan to further extend our work by generating longer or arbitrary length backbones, as well as by conditioning our generative model on secondary structure, so that we can specify backbone topologies for design.","We train a GAN to generate and recover full-atom protein backbones , and we show that in select cases we can recover the generated proteins after sequence design and ab initio forward-folding.",Generative Adversarial Networks ; Rosetta ; R.to,new therapeutics ; iterative optimization ; a method ; the de novo protein design process ; a fully differentiable manner ; The ability ; realistic sample generation ; a major advancement ; sequence design ; some cases,Generative Adversarial Networks ; Rosetta ; R.to,"In this study, we train Generative Adversarial Networks (GANs) to generate fixed-length full-atom protein backbones, with the goal of sampling from realistic 3-D backbone fragments. We represent protein structures by pairwise distances between all backbone atoms, and present a method for directly recovering and refining the corresponding backbone coordinates in a differentiable manner. We show that interpolations in the latent space of the generator correspond to smooth deformations of output backbones and test set structures not seen by the generator during training exist in its image. Finally, we perform sequence design, relaxation, and ab initio folding of a",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"The geometric properties of loss surfaces, such as the local flatness of a solution, are associated with generalization in deep learning. The Hessian is often used to understand these geometric properties. We investigate the differences between the eigenvalues of the neural network Hessian evaluated over the empirical dataset, the Empirical Hessian, and the eigenvalues of the Hessian under the data generating distribution, which we term the True Hessian. Under mild assumptions, we use random matrix theory to show that the True Hessian has eigenvalues of smaller absolute value than the Empirical Hessian. We support these results for different SGD schedules on both a 110-Layer ResNet and VGG-16. To perform these experiments we propose a framework for spectral visualization, based on GPU accelerated stochastic Lanczos quadrature. This approach is an order of magnitude faster than state-of-the-art methods for spectral visualization, and can be generically used to investigate the spectral properties of matrices in deep learning. The extraordinary success of deep learning in computer vision and natural language processing has been accompanied by an explosion of theoretical (Choromanska et al., 2015a; b; Pennington & Bahri, 2017) and empirical interest in their loss surfaces, typically through the study of the Hessian and its eigenspectrum (Ghorbani et al., 2019; Li et al., 2017; Sagun et al., 2016; Wu et al., 2017) . Exploratory work on the Hessian, and its evolution during training (e.g., Jastrzębski et al., 2018) , attempts to understand why optimization procedures such as SGD can discover good solutions for training neural networks, given complex non-convex loss surfaces. For example, the ratio of the largest to smallest eigenvalues, known as the condition number, determines the convergence rate for first-order optimization methods on convex objectives (Nesterov, 2013) . The presence of negative eigenvalues indicates non-convexity even at a local scale. Hessian analysis has also been a primary tool in further explaining the difference in generalization of solutions obtained, where under Bayesian complexity frameworks, flatter minima, which require less information to store, generalize better than sharp minima (Hochreiter & Schmidhuber, 1997) . Further work has considered how large batch vs small batch stochastic gradient descent (SGD) alters the sharpness of solutions (Keskar et al., 2016) , with smaller batches leading to convergence to flatter solutions, leading to better generalization. These geometrical insights have led to generalization procedures, such as taking the Cesàro mean of the weights along the SGD trajectory , and algorithms that optimize the model to select for local flatness (Chaudhari et al., 2016) . Flat regions of weight space are more robust under adversarial attack (Yao et al., 2018) . Moreover, the Hessian defines the curvature of the posterior over weights in the Laplace approximation for Bayesian neural networks (MacKay, 1992; 2003) , and thus crucially determines its performance. In this paper we use random matrix theory to analyze the spectral differences between the Empirical Hessian, evaluated via a finite data sample (hence related to the empirical risk) and what we term the True Hessian, given under the expectation of the true data generating distribution. 1 1 We consider loss surfaces that correspond to risk surfaces in statistical learning theory terminology. In particular, we show that the differences in extremal eigenvalues between the True Hessian and the Empirical Hessian depend on the ratio of model parameters to dataset size and the variance per element of the Hessian. Moreover, we show that that the Empirical Hessian spectrum, relative to that of the True Hessian, is broadened; i.e. the largest eigenvalues are larger and the smallest smaller. We support this theory with experiments on the CIFAR-10 and CIFAR-100 datasets for different learning rate schedules using a large modern neural network, the 110 Layer PreResNet. It is not currently known if key results, such as (1) the flatness or sharpness of good and bad optima, (2) local non-convexity at the end of training, or (3) rank degeneracy hold for the True Hessian in the same way as for the Empirical Hessian. We hence provide an investigation of these foundational questions. The geometric properties of loss landscapes in deep learning have a profound effect on generalization performance. We introduced the True Hessian to investigate the difference between the landscapes for the true and empirical loss surfaces. We derived analytic forms for the perturbation between the extremal eigenvalues of the True and Empirical Hessians, modelling the difference between the two as a Gaussian Orthogonal Ensemble. Moreover, we developed a method for fast eigenvalue computation and visualization, which we used in conjunction with data augmentation to approximate the True Hessian spectrum. We show both theoretically and empirically that the True Hessian has smaller variation in eigenvalues and that its extremal eigenvalues are smaller in magnitude than the Empirical Hessian. We also show under our framework that we expect the Empirical Hessian to have a greater negative spectral density than the True Hessian and our experiments support this conclusion. This result may provide some insight as to why first order (curvature blind) methods perform so well on neural networks. Reported non-convexity and pathological curvature is far worse for the empirical risk than the true risk, which is what we wish to descend. The shape of the true risk is particularly crucial for understanding how to develop effective procedures for Bayesian deep learning. With a Bayesian approach, we not only want to find a single point that optimizes a risk, but rather to integrate over a loss surface to form a Bayesian model average. The geometric properties of the loss surface, rather than the specific location of optima, therefore greatly influences the predictive distribution in a Bayesian procedure. Furthermore, the posterior representation for neural network weights with popular approaches such as the Laplace approximation has curvature directly defined by the Hessian. In future work, one could also replace the GOE noise matrix ε(w) with a positive semi-definite white Wishart kernel in order to derive results for the empirical Gauss-Newton and Fisher information matrices, which are by definition positive semi-definite and are commonly employed in second order deep learning (Martens & Grosse, 2015) . Our approach to efficient eigenvalue computation and visualization can be used as a general-purpose tool to empirically investigate spectral properties of large matrices in deep learning, such as the Fisher information matrix. Following the notation of (Bun et al., 2017 ) the resolvent of a matrix H is defined as with z = x + iη ∈ C. The normalised trace operator of the resolvent, in the N → ∞ limit is known as the Stieltjes transform of ρ. The functional inverse of the Siteltjes transform, is denoted the blue function B(S(z)) = z. The R transform is defined as crucially for our calculations, it is known that the R transform of the Wigner ensemble is Consider an n × n symettric matrix M n , whose entries are given by The Matrix M n is known as a real symmetric Wigner matrix. Theorem 2. Let {M n } ∞ n=1 be a sequence of Wigner matrices, and for each n denote X n = M n / √ n. Then µ Xn , converges weakly, almost surely to the semi circle distribution, the property of freeness for non commutative random matrices can be considered analogously to the moment factorisation property of independent random variables. The normalized trace operator, which is equal to the first moment of the spectral density We say matrices A&B for which ψ(A) = ψ(B) = 0 4 are free if they satisfy for any integers n 1 .. n k E DERIVATION The Stijeles transform of Wigners semi circle law, can be written as (Tao, 2012) from the definition of the Blue transform, we hence have Computing the R transform of the rank 1 matrix H true , with largest non-trivial eigenvalue β, on the effect of the spectrum of a matrix A, using the Stieltjes transform we easily find following (Bun et al., 2017) that We can use perturbation theory similar to in equation equation 22 to find the blue transform which to leading order gives setting ω = S M (z) using the ansatz of we find that S 0 (z) = S (w) (z) and using that B M (z) = 1/g (z) , we conclude that and hence and hence in the large N limit the correction only survives if S (w) (z) = 1/β clearly for β → −β we have",Understanding the neural network Hessian eigenvalues under the data generating distribution.,Laplace ; Lanczos ; the Empirical Hessian ; X n = M n / √ n. Then µ Xn ; al. ; Nesterov ; first ; Sagun ; Pennington & Bahri ; Gauss-Newton,better generalization ; generalization performance ; A&B ; adversarial attack ; efficient eigenvalue computation ; how large batch ; complex non-convex loss surfaces ; Wu ; random matrix theory ; an order,Laplace ; Lanczos ; the Empirical Hessian ; X n = M n / √ n. Then µ Xn ; al. ; Nesterov ; first ; Sagun ; Pennington & Bahri ; Gauss-Newton,"The geometric properties of loss surfaces are associated with generalization in deep learning. The Hessian is often used to understand these geometric properties. We investigate the differences between the eigenvalues of the neural network Hessian evaluated over empirical dataset, the Empirical Hessian, and the data generating distribution, which we term the True Hessian. Under mild assumptions, we use random matrix theory to show that the true Hessian has smaller absolute value than the Empiical Hessians. We support these results for different SGD schedules on both a 110-Layer ResNet and VGG-16. To perform spectral visualization, GPU accelerated",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Long short-term memory networks (LSTMs) were introduced to combat vanishing gradients in simple recurrent neural networks (S-RNNs) by augmenting them with additive recurrent connections controlled by gates. We present an alternate view to explain the success of LSTMs: the gates themselves are powerful recurrent models that provide more representational power than previously appreciated. We do this by showing that the LSTM's gates can be decoupled from the embedded S-RNN, producing a restricted class of RNNs where the main recurrence computes an element-wise weighted sum of context-independent functions of the inputs. Experiments on a range of challenging NLP problems demonstrate that the simplified gate-based models work substantially better than S-RNNs, and often just as well as the original LSTMs, strongly suggesting that the gates are doing much more in practice than just alleviating vanishing gradients. Long short-term memory networks (LSTM) BID17 have become the de-facto recurrent neural network (RNN) for learning representations of sequences in many research areas, including natural language processing (NLP). Like simple recurrent neural networks (SRNNs) BID11 , LSTMs are able to learn non-linear functions of arbitrary-length input sequences. However, they also introduce an additional memory cell to mitigate the vanishing gradient problem BID16 BID4 . This memory is controlled by a mechanism of gates, whose additive connections allow long-distance dependencies to be learned more easily during backpropagation. While this view is mathematically accurate, in this paper we argue that it does not provide a complete picture of why LSTMs work in practice.We present an alternate view to explain the success of LSTMs: the gates themselves are powerful recurrent models that provide more representational power than previously appreciated. To demonstrate this, we first show that LSTMs can be seen as a combination of two recurrent models: (1) an S-RNN, and (2) an element-wise weighted sum of the S-RNN's outputs over time, which is implicitly computed by the gates. We hypothesize that, for many practical NLP problems, the weighted sum serves as the main modeling component. The S-RNN, while theoretically expressive, is in practice only a minor contributor that clouds the mathematical clarity of the model. By replacing the S-RNN with a context-independent function of the input, we arrive at a much more restricted class of RNNs, where the main recurrence is via the element-wise weighted sums that the gates are computing.We test our hypothesis on NLP problems, where LSTMs are wildly popular at least in part due to their ability to model crucial language phenomena such as word order BID0 , syntactic structure BID23 , and even long-range semantic dependencies BID15 . We consider four challenging tasks: language modeling, question answering, dependency parsing, and machine translation. Experiments show that while removing the gates from an LSTM can severely hurt performance, replacing the S-RNN with a simple linear transformation of the input results in minimal or no loss in model performance. We further show that in many cases, LSTMs can be further simplified by removing the output gate, arriving at an even more transparent architecture, where the output is a context-independent function of the weighted sum. Together, these results suggest that the gates' ability to compute an element-wise weighted sum, rather than the non-linear transition dynamics of S-RNNs, are the driving force behind LSTM's success. In the above experiments, we show three major ablations of the LSTM. In the S-RNN experiments (LSTM -GATES), we ablate the memory cell and the output layer. In the LSTM -S-RNN and LSTM -S-RNN -OUT experiments, we ablate the S-RNN. As consistent with previous literature, removing the memory cell degrades performance drastically. In contrast, removing the S-RNN makes little to no difference in the final performance, suggesting that the memory cell alone is largely responsible for the success of LSTMs in NLP. The results also confirm our hypothesis that weighted sums of context words is a powerful, yet more interpretable, model of contextual information. We presented an alternate view of LSTMs: they are a hybrid of S-RNNs and a gated model that dynamically computes weighted sums of the S-RNN outputs. Our experiments investigated whether the S-RNN is a necessary component of LSTMs. In other words, are the gates alone as powerful of a model as an LSTM? Results across four major NLP tasks (language modeling, question answering, dependency parsing, and machine translation) indicate that LSTMs suffer little to no performance loss when removing the S-RNN, but removing the gates can degrade performance substantially. This provides evidence that the gating mechanism is doing the heavy lifting in modeling context, and that element-wise weighted sums of context-independent functions of the inputs are often as effective as fully-parameterized LSTMs.This work sheds light on the inner workings of the relatively opaque LSTM. By removing the S-RNN and the output gate, we also show that the resulting model is a far more mathematically transparent variant of LSTMs. This transparency enables a visualization of how the context affects the output of the model at every timestep, much like in attention-based models. We hope that this new outlook on LSTMs will foster better and more efficient models of contextualization.","Gates do all the heavy lifting in LSTMs by computing element-wise weighted sums, and removing the internal simple RNN does not degrade model performance.",NLP ; RNN ; first ; two ; the S-RNN's ; four ; three,time ; an LSTM ; powerful recurrent models ; only a minor contributor ; little to no difference ; two recurrent models ; an additional memory cell ; three ; context-independent functions ; the simplified gate-based models,NLP ; RNN ; first ; two ; the S-RNN's ; four ; three,"Long short-term memory networks (LSTMs) were introduced to combat vanishing gradients in simple recurrent neural networks (S-RNNs) by augmenting them with additive recurrent connections controlled by gates. The gates themselves are powerful recurrent models that provide more representational power than previously appreciated. The LSTM's gates can be decoupled from the embedded S-RN, producing a restricted class of RNNs where the main recurrence computes an element-wise weighted sum of context-independent functions of inputs. Experiments on a range of challenging NLP problems demonstrate that simplified gate-based models work substantially",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"We study the problem of explaining a rich class of behavioral properties of deep neural networks. Our influence-directed explanations approach this problem by peering inside the network to identify neurons with high influence on the property of interest using an axiomatically justified influence measure, and then providing an interpretation for the concepts these neurons represent. We evaluate our approach by training convolutional neural networks on Pubfig, ImageNet, and Diabetic Retinopathy datasets.   Our evaluation demonstrates that influence-directed explanations (1) localize features used by the network, (2) isolate features distinguishing related instances, (3) help extract the essence of what the network learned about the class, and (4) assist in debugging misclassifications.
 We study the problem of explaining a class of behavioral properties of deep neural networks, with a focus on convolutional neural networks. Examples of such properties include explaining why a network classified an input instance a particular way, why it misclassified an input, and what the essence of a class is for the network. This problem has received significant attention in recent years with the rise of deep networks and associated concerns about their opacity BID4 . This paper introduces influence-directed explanations for deep networks. It involves peering inside the network to identify neurons with high influence and then providing an interpretation for the concepts they represent. This approach enables us to interpret the inner workings of the network by drawing attention to concepts learned by the network that had a significant effect on the property that we seek to explain. In contrast to raw inputs, neurons in higher layers represent general concepts. Thus, they form a useful substrate to explain properties of interest involving many input instances, such as the essence of a class. Once influential neurons have been identified, they can be interpeted using existing techniques (e.g., visualization) to reveal the concepts they represent. Alternatively, influences can be examined directly to diagnose undesirable properties of the network.A key contribution of this paper is distributional influence, a measure for internal neurons that is axiomatically justified. Distributional influence is parameterized by a quantity of interest, a distribution of interest, and a slice of the network that allows us to reference some internal neurons in a network. It is simply the average partial derivative with respect to a neuron in a slice over the distribution of interest. This parametric measure can be appropriately instantiated to explain different properties of interest with respect to different parts of a network.Our influence measure is designed to achieve three natural desiderata: causality, distributional faithfulness, and flexibility. Capturing causality helps us identify parts of the network that when changed have the most effect on outcomes. Distributional faithfulness ensures that we evaluate the network only on points in the input distribution. This property is important since models operating on high dimensional spaces, such as neural networks, are not expected to behave reliably on instances outside the input distribution. Finally, by flexibility, we mean that the influence measure should support explanations for various properties of interest.We evaluate our approach by training convolutional neural networks on ImageNet BID8 , PubFig BID5 , and a Diabetic Retinopathy datasets. Our evaluation demonstrates that influence-directed explanations enable us to (1) characterize why inputs were classified a particular way in terms of high-level concepts represented by influential neurons (Section 3.1), (2) explain why an input was classified into a one class (e.g., sports car) rather than another (e.g., convertible) (Section 3.2), (3) demonstrate that influences localize the actual reasons used for classification better than simply examining activations (Section 3.3.1), (4) help extract the essence of what the network learned about the class (Section 3.3), and (5) assist in debugging misclassifications of a Diabetic Retinopathy classifier BID6 (Section 3.4). Influence measures are widely studied in cooperative game theory as solutions to the problem of attribution to of outcomes to participants and has applications to a wide range of settings including revenue division and voting. In this section, we highlight ideas drawn from this body of work and differences in terms of two key properties of influence measures: the marginality principle, and efficiency.The marginality principle BID14 states that an agent's attribution only depends on its own contribution to the output. Formally, this is stated as: if the partial derivatives with respect to an agent of two functions are identical throughout, then they have identical attributions for agent i. Our axiom of distributional marginality (DM) is a weaker form of this requirement that only requires equality of attribution when partial derivatives are same in the distribution.A second property, called efficiency, which is especially important for revenue division, is that attributions add up to the total value generated. This ensures that no value is left unattributed. The marginality principle, along with efficiency uniquely define the Aumann-Shapley Value BID0 . In BID12 , the Aumann-Shapley Value is used for attributions with efficiency as a justification. While it is unclear that efficiency is an essential requirement in our setting, the Aumann-Shapley value can be recovered in our framework by choosing the distribution of interest as the uniform distribution on the line segment joining an instance x and a baseline image b. Certain choices of baselines can be problematic from the point of view of distributional faithfulness, since the line segment of linear combinations between them might lie significantly out of distribution. The particular baseline chosen in BID12 is the zero vector, where the line segment represents scaled images, and could be reasonably called within distribution.","We present an influence-directed approach to constructing explanations for the behavior of deep convolutional networks, and show how it can be used to answer a broad set of questions that could not be addressed by prior work.",Aumann-Shapley Value ; two ; Pubfig ; second ; Aumann-Shapley ; recent years ; linear ; one ; Diabetic Retinopathy ; zero,significant attention ; Certain choices ; Diabetic Retinopathy datasets ; neurons ; raw inputs ; three natural desiderata ; two ; the essence ; (Section ; no value,Aumann-Shapley Value ; two ; Pubfig ; second ; Aumann-Shapley ; recent years ; linear ; one ; Diabetic Retinopathy ; zero,"Our approach involves peering inside the network to identify neurons with high influence on the property of interest using an axiomatically justified influence measure, and providing an interpretation for the concepts these neurons represent. We evaluate our approach by training convolutional neural networks on Pubfig, ImageNet, and Diabetic Retinopathy datasets.   Our evaluation demonstrates that influence-directed explanations are useful for explaining a rich class of behavioral properties of deep neural networks, as they help identify related instances, (2) help extract the essence of what the network learned about the class, and (4) assist in debugging misclassifications. This approach",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"In this study we focus on first-order meta-learning algorithms that aim to learn a parameter initialization of a network which can quickly adapt to new concepts, given a few examples. We investigate two approaches to enhance generalization and speed of learning of such algorithms, particularly expanding on the Reptile (Nichol et al., 2018) algorithm. We introduce a novel regularization technique called meta-step gradient pruning and also investigate the effects of increasing the depth of network architectures in first-order meta-learning. We present an empirical evaluation of both approaches, where we match benchmark few-shot image classification results with 10 times fewer iterations using Mini-ImageNet dataset and with the use of deeper networks, we attain accuracies that surpass the current benchmarks of few-shot image classification using Omniglot dataset. A common drawback consistently seen in traditional machine learning algorithms is the need for large amounts of training data in order to learn a given task BID5 , whereas the ability to grasp new concepts with just a few examples is clearly seen in the way people learn BID6 . This offers many challenges in fast adaption of machine learning in new fields and hence there is a growing interest in algorithms that can learn with limited data availability BID9 .In the development of learning methods that can be trained effectively on sparse data, the process of learning-to-learn is seen as a crucial step BID0 . This is often termed as meta-learning (Schaul & Schmidhuber, 2010) , where a variety of techniques have been presented. In our study, we specifically focus on approaches that learn an initialization of a network, trained on a dataset of tasks. Model-agnostic meta-learning (MAML) BID1 presented this exact approach and its applications of few-shot image classification, where a task was defined as correct classification of a test image out of N object classes, after training on a set of K examples per each class. Furthermore, MAML presented its first-order variant, where the second order derivatives were eliminated during computation while preserving results of the benchmarks. The approach avoided the computational expense of second order derivatives by treating them as constants. Firstorder meta-learning was further investigated in the Reptile algorithm BID7 , where the implementation was simplified eliminating the need for a test set in the tasks. Our study uses Reptile as the algorithm of choice to incorporate the techniques presented to improve generalization of first-order meta-learning.Even though first-order meta-learning has shown to attain fast generalization of concepts given limited data, empirical evaluations on few-shot image classification tasks BID7 show potential to improve the outcomes, especially on inputs with richer features such as real world images. Also drawbacks are seen in slower convergence, requiring a large number of iterations during the training phase. In this study, we investigate techniques used to obtain higher task generalization in models such as regularization BID11 and deeper networks BID3 and we present ways of adapting those in first-order meta-learning.The contributions of our study are as follows.• Introduction of meta-step gradient pruning, a novel approach to regularize parameter updates in first-order meta-learning.• Empirical evaluation of meta-step gradient pruning, achieving benchmark few-shot image classification accuracies with 10 times fewer iterations.• Empirical evaluation of deeper networks in the meta-learning setting, achieving results that surpass the current benchmarks in few-shot image classification. Our proposed novel approach of meta-step gradient pruning demonstrated enhanced generalization effects on the outcomes of first-order meta-learning. The reduced gaps between train and test set accuracies, during training of Omniglot and Mini-ImageNet few-shot classification tasks showed that the parameter initialization has learned to generalize better on the train set.We were able to almost match the benchmark results of first-order MAML and Reptile implementations with 10 times fewer iterations using our algorithm. This further emphasized the improved generalization, helping the parameters to converge the loss on few-shot classification. This increase in speed is vital in tasks such as Mini-Imagenet, because performing first-order meta-learning on real world noisy images is computationally expensive and time-consuming.With our approach of introducing deeper networks to the inner-loop in Omniglot few-shot classification, we showed results surpassing the current benchmarks of both first-order MAML and Reptile algorithms. The expanded parameter space with deeper models shows higher generalization as expected, but it makes the implementation more computationally expensive. This was identified as one drawback of this approach when applying to richer input data such as Mini-ImageNet tasks.Enhanced and fast generalization is utmost important when learning with limited data. Looking forward, we see the importance of elaborated theoretical analysis of meta-step gradient pruning and more techniques of regularization during meta-learning. Also in the future we plan to investigate on the application of first-order meta-learning in other applications such as reinforcement learning.",The study introduces two approaches to enhance generalization of first-order meta-learning and presents empirical evaluation on few-shot image classification.,first ; two ; Reptile ; Mini ; Omniglot ; Schaul & Schmidhuber ; MAML ; second ; iterations.• Empirical ; one,just a few examples ; other applications ; both approaches ; speed ; a crucial step ; the computational expense ; MAML ; accuracies ; the training phase ; this approach,first ; two ; Reptile ; Mini ; Omniglot ; Schaul & Schmidhuber ; MAML ; second ; iterations.• Empirical ; one,"First-order meta-learning algorithms aim to learn a parameter initialization of a network which can quickly adapt to new concepts, given a few examples. We explore two approaches to enhance generalization and speed of learning of such algorithms, including meta-step gradient pruning. We also explore the effects of increasing network architectures and increasing the depth of network architectures. We present an empirical evaluation of both approaches, where we match benchmark few-shot image classification results with 10 times fewer iterations using Mini-ImageNet dataset and with the use of deeper networks, we attain accuracies that surpass the current benchmarks.   ",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Generating and scheduling activities is particularly challenging
 when considering both consumptive resources and
 complex resource interactions such as time-dependent resource
 usage.We present three methods of determining valid
 temporal placement intervals for an activity in a temporally
 grounded plan in the presence of such constraints. We introduce
 the Max Duration and Probe algorithms which are
 sound, but incomplete, and the Linear algorithm which is
 sound and complete for linear rate resource consumption.
 We apply these techniques to the problem of scheduling
 awakes for a planetary rover where the awake durations
 are affected by existing activities. We demonstrate how the
 Probe algorithm performs competitively with the Linear algorithm
 given an advantageous problem space and well-defined
 heuristics. We show that the Probe and Linear algorithms
 outperform the Max Duration algorithm empirically.
 We then empirically present the runtime differences between
 the three algorithms. The Probe algorithm is currently base-lined
 for use in the onboard scheduler for NASA’s next planetary
 rover, the Mars 2020 rover. In many space missions, consumptive resources such as energy or data volume limit the number of activities that can be scheduled. These consumptive resources are oftentimes replenished periodically or gradually over time. For example, data is downlinked-replenishing data capacity-or energy is generated by solar panels or radioisotope thermoelectric generator (RTG) power supplies. The scheduler must therefore schedule activities while staying aware of resource replenishment in order to ensure that the resource state does not violate constraints (e.g. energy below a specified level or data buffers overflow). We focus on awake and asleep scheduling for a planetary rover, but our techniques generalize scheduling in the presence of complex consumptive resource activities.We focus on the onboard scheduler for NASA's next planetary rover, the Mars 2020 (M2020) rover (Jet Propulsion Laboratory 2018a) . Since the heart of our paper is awake and asleep scheduling, we concentrate on energy as the limit- ing consumptive resource. The M2020 rover's power source is a Multi-Mission Radioisotope Thermoelectric Generator (MMRTG) (Jet Propulsion Laboratory 2018b). The MM-RTG constantly generates energy for the rover's battery, but the CPU's awake and ""idle"" state (i.e. no other tasks) consumes more energy than the MMRTG provides. Therefore, the rover can only increase its energy, measured as battery state of charge (SOC), when the rover is asleep. The rover, however, must stay awake to not only execute activities, but also (re)-invoke the scheduler to generate a schedule. The M2020 onboard scheduler is responsible for generating and scheduling these awake periods.In order to generate and schedule awakes, the scheduler must compute valid start times for awakes and activities jointly to ensure that there is sufficient energy for both the awake and the activities. Each activity, however, requires varying awake sizes depending on existing awake periods and the activity's scheduled start time. If the activity is close to an existing awake, it may be necessary to extend an existing awake rather than generating a new awake as this would require the rover to shutdown and wakeup in quick succession ( Figure 1 ) which may lead to issues if the shutdown runs longer than nominally expected. Due to its varying duration, an awake's energy consumption and valid start times are challenging to determine.The remainder of the paper is organized as follows. First, we describe the timeline representation, which is also used by the M2020 onboard scheduler. We discuss calculating valid start time intervals-intervals in which starting the activity would not violate any constraints-and define the problem in relation to the timeline framework. Second, we discuss a general case-by-case approach to handling automatically generated awakes and the challenges specific cases pose. Third, we present three specific approaches to handling these challenges when generating and scheduling awakes: a) an over-conservative approach that always uses the maximum awake period potentially required by the activity when calculating valid intervals; b) a ""probing"" approach that only considers a single point in time rather than the entire interval; and c) a linear algebra approach that calculates exact valid intervals given the linear rate of energy replenishment and consumption. The ""probing"" approach is currently base-lined for the M2020 onboard scheduler. Fourth, we present empirical analysis to compare their de- Figure 1: When scheduling activity B, the scheduler should extend the existing awake rather than creating a new one to account for the possibility that the shutdown runs longer than nominally expected. W is a wakeup and S is a shutdown.grees of completeness and runtime performance. Lastly, we reference related works, describe future works, and discuss conclusions. Generating and scheduling activities in the presence of consumptive regenerative resources is especially challenging when a driving factor of feasibility of placement is dependent on interactions with the existing schedule. Scheduling activities and their awake periods is particularly challenging in the context of M2020 because the awake's duration is dependent on existing awakes. We presented three algorithms-Max Duration, Probe, and Linear-for scheduling awakes and analyzed their completeness and runtime. Despite being a locally sound and complete algorithm, the Linear algorithm was not always able to outperform in the global problem space. We demonstrated how a simple and incomplete algorithm can perform both suboptimally, as seen with the Max Duration algorithm, and also close to optimal, as seen with the Probe algorithm, dependent on the heuristic and input parameters. We showed that the Probe algorithm is a fair alternative to a more complete algorithm, especially considering its ease of implementation and runtime improvement.",This paper describes and analyzes three methods to schedule non-fixed duration activities in the presence of consumptive resources.,RTG ; Second ; three ; CPU ; the Max Duration ; Probe ; First ; Linear ; NASA ; MMRTG,"NASA’s next planetary
 rover ; algorithm ; linear rate resource consumption ; We ; radioisotope thermoelectric generator ; the rover ; Jet Propulsion Laboratory ; the awake durations ; the paper ; Scheduling activities",RTG ; Second ; three ; CPU ; the Max Duration ; Probe ; First ; Linear ; NASA ; MMRTG,"Generating and scheduling activities is particularly challenging when considering both consumptive resources and time-dependent resource interactions. The three methods of determining valid temporal placement intervals for an activity in a temporally grounded plan are the Max Duration and Probe algorithms, while the Linear algorithm is sound and complete for linear rate resource consumption. These techniques are applicable to scheduling a planetary rover where awake durations are affected by existing activities. The Probe and Linear algorithms perform competitively with the Linear algorithms, given an advantageous problem space and well-defined heuristics. We show that the Probe and linear algorithms outperform the Max duration algorithm empirically. We then empir",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Neural machine translation (NMT) models learn representations containing substantial linguistic information. However, it is not clear if such information is fully distributed or if some of it can be attributed to individual neurons. We develop unsupervised methods for discovering important neurons in NMT models. Our methods rely on the intuition that different models learn similar properties, and do not require any costly external supervision. We show experimentally that translation quality depends on the discovered neurons, and find that many of them capture common linguistic phenomena. Finally, we show how to control NMT translations in predictable ways, by modifying activations of individual neurons.","Unsupervised methods for finding, analyzing, and controlling important neurons in NMT",NMT,important neurons ; (NMT) models ; NMT ; the discovered neurons ; representations ; some ; predictable ways ; it ; substantial linguistic information ; the intuition,NMT,"Neural machine translation (NMT) models learn representations containing substantial linguistic information. However, it is unclear if such information is fully distributed or if some of it can be attributed to individual neurons. Unsupervised methods for discovering important neurons in NMT models rely on the intuition that different models learn similar properties, and do not require costly external supervision. We show experimentally that translation quality depends on the discovered neurons, and find that many of them capture common linguistic phenomena. We also show how to control NMT translations in predictable ways by modifying activations of individual neurons, such as by modifying neuronal activations.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"There is a growing interest in automated neural architecture search (NAS). To improve the efficiency of NAS, previous approaches adopt  weight sharing method to force all models share the same set of weights.   However, it has been observed that a model performing better with shared weights does not necessarily perform  better when trained alone. In this paper, we analyse existing weight sharing one-shot NAS approaches from a Bayesian point of view and identify the posterior fading problem, which compromises the effectiveness of shared weights. To alleviate this problem, we present a practical approach to guide the parameter posterior towards its true distribution. Moreover, a hard latency constraint is introduced during the search so that the desired latency can be achieved. The resulted method, namely Posterior Convergent NAS (PC-NAS), achieves state-of-the-art performance under standard GPU latency constraint on ImageNet. In our small search space, our model PC-NAS-S attains76.8% top-1 accuracy, 2.1% higher than MobileNetV2 (1.4x) with the same latency. When adopted to our large search space, PC-NAS-L achieves 78.1% top-1 accuracy within 11ms. The discovered architecture also transfers well to other computer vision applications such as object detection and person re-identification. Neural network design requires extensive experiments by human experts. In recent years, there has been a growing interest in developing algorithmic NAS solutions to automate the manual process of architecture design (Zoph & Le, 2016; Liu et al., 2018a; Zhong et al., 2018; . Despite remarkable results, early works on NAS (Real et al., 2018; Elsken et al., 2017) are limited to searching only using proxy or subsampled dataset due to the exorbitant computational cost. To overcome this difficulty, (Bender et al., 2018; Pham et al., 2018) attempted to improve search efficiency via sharing weights across models. These approaches utilize an overparameterized network (supergraph) containing every single model, which can be further divided into two categories. The first category is continuous relaxation method (Liu et al., 2018c; Cai et al., 2018) , which keeps a set of so called architecture parameters to represent the model, and updates these parameters alternatively with supergraph weights. The resulting model is obtained using the architecture parameters at convergence. The continuous relaxation method suffers from the rich-get-richer problem (Adam & Lorraine, 2019) , which means that a better-performed model at the early stage would be trained more frequently (or have larger learning rates). This introduces bias and instability to the search process. The other category is referred to as one-shot method (Brock et al., 2017b; Bender et al., 2018; Chu et al., 2019) , which divides the NAS procedure into a training stage and a searching stage. In the training stage, the supergraph is optimized along with either dropping out each operator with certain probability or sampling uniformly among candidate architectures. In the search stage, a search algorithm is applied to find the architecture with the highest validation accuracy with shared weights. The one-shot approach ensures the fairness among all models by sampling architecture or dropping out operator uniformly. However, as identified in (Adam & Lorraine, 2019; Chu et al., 2019; Bender et al., 2018) , the problem of one-shot method is that the validation accuracy of the model with shared weights is not predictive to its true performance. In this paper, we formulate NAS as a Bayesian model selection problem (Chipman et al., 2001 ). This formulation is especially helpful in understanding the one-shot approaches in a theoretical way, which in turn provides us a guidance to fundamentally addressing one of the major issues of one-shot approaches. Specially, we show that shared weights are actually a maximum likelihood estimation of a proxy distribution to the true parameter distribution. Most importantly, we identify the common issue of weight sharing, which we call Posterior Fading, i.e., as the number of models in the supergraph increases, the KL-divergence between true parameter posterior and proxy posterior also increases. To alleviate the Posterior Fading problem, we proposed a practical approach to guide the convergence of the proxy distribution towards the true parameter posterior. Specifically, we divide the training of supergraph into several intervals and maintain a pool of high potential partial models and progressively update this pool after each interval . At each training step, a partial model is sampled from the pool and complemented to a full model. To update the partial model pool, we first generate candidates by extending each partial model and evaluate their potentials, keeping the best performancing ones. The search space is effectively shrunk in the upcoming training interval. Consequently, the parameter posterior get close to the desired true posterior during this procedure. Main contributions of our work is concluded as follows: • We for the first time analyse one-shot approaches from a Bayesian point of view and identify the associated disadvantage which we call Posterior Fading. • Guided by the theoretical result, we introduce a novel NAS algorithm fundamentally different from existing one-shot methods, which guides the proxy distribution to converge towards the true parameter posterior. To examine the effectiveness of our newly proposed method, we benchmark its performance on ImageNet (Russakovsky et al., 2015) against the existing methods. In one typical search space (Cai et al., 2018) , our PC-NAS-S attains 76.8% top-1 accuracy, 0.5% higher and 20% faster than EfficientNet-B0 (Tan & Le, 2019a) , which is the previous state-of-the-art model in mobile setting. To further demonstrate the advantage of our method, we test it on a larger space and our PC-NAS-L boosts the accuracy to 78.1%. In this paper, a new architecture search approach called PC-NAS is proposed. We study the conventional weight sharing approach from Bayesian point of view and identify a key issue that compromises the effectiveness of shared weights. With the theoretical motivation, a practical method The operators in our spaces have structures described by either Conv1x1-ConvNxM-Conv1x1 or Conv1x1-ConvNxM-ConvMxN-Conv1x1. We define expand ratio as the ratio between the channel numbers of the ConvNxM in the middle and the input of the first Conv1x1. Small search space Our small search space contains a set of MBConv operators (mobile inverted bottleneck convolution (Sandler et al., 2018) ) with different kernel sizes and expand ratios, plus Identity, adding up to 10 operators to form a mixoperator. The 10 operators in our small search space are listed in the left column of Table 5 , where notation OP X Y represents the specific operator OP with expand ratio X and kernel size Y. Large search space We add 3 more kinds of operators to the mixoperators of our large search space, namely NConv, DConv, and RConv. We use these 3 operators with different kernel sizes and expand ratios to form 10 operators exclusively for large space, thus the large space contains 20 operators. For large search space, the structure of NConv, DConv are Conv1x1-ConvKxK-Conv1x1 and Conv1x1-ConvKxK-ConvKxK-Conv1x1, and that of RConv is Conv1x1-Conv1xK-ConvKx1-Conv1x1. The kernel sizes and expand ratios of operators exclusively for large space are lised in the right column of Table 5 , where notation OP X Y represents the specific operator OP with expand ratio X and K=Y. There are altogether 21 mixoperators in both small and large search spaces. Thus our small search space contains 10 21 models, while the large one contains 20 21 . The specifications of PC-NAS-S and PC-NAS-L are shown in Fig. 3 . We observe that PC-NAS-S adopts either high expansion rate or large kernel size at the tail end, which enables a full use of high level features. However, it tends to select small kernels and low expansion rates to ensure the model remains lightweight. PC-NAS-L chooses lots of powerful bottlenecks exclusively contained in the large space to achieve the accuracy boost. The high expansion rate is not quite frequently seen which is to compensate the computation utilized by large kernel size. Both PC-NAS-S and PC-NAS-L tend to use heavy operator when the resolution reduces, circumventing too much information loss in these positions. 224×224×3  112×112×16  112×112×16  56×56×32  56×56×32  56×56×32  56×56×32  28×28×64  28×28×64  28×28×64  28×28×64  14×14×136  14×14×136  14×14×136  14×14×136  14×14×136  14×14×136  14×14×136  14×14×136  7×7×264  7×7×264  7×7×264","Our paper identifies the issue of existing weight sharing approach in neural architecture search and propose a practical method, achieving strong results.",Posterior Fading ; NConv ; PC-NAS-S ; Y. Large ; Brock et al. ; one ; PC-NAS-L ; Liu et al. ; MBConv ; Pham et al.,one-shot method ; a full model ; previous approaches ; the highest validation accuracy ; different kernel sizes ; larger learning rates ; search efficiency ; Zhong ; a hard latency constraint ; The discovered architecture,Posterior Fading ; NConv ; PC-NAS-S ; Y. Large ; Brock et al. ; one ; PC-NAS-L ; Liu et al. ; MBConv ; Pham et al.,"There has been a growing interest in automated neural architecture search (NAS) due to its ability to force all models to share the same set of weights. However, it has been observed that a model performing better with shared weights does not necessarily perform better when trained alone. In this paper, we examine existing weight sharing one-shot NAS approaches from a Bayesian point of view and identify the posterior fading problem, which compromises the effectiveness of shared weights. To alleviate this problem, a hard latency constraint is introduced during the search to guide the parameter posterior towards its true distribution. The result of this approach achieves state-of-the-art",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Deep learning's success has led to larger and larger models to handle more and more complex tasks; trained models can contain millions of parameters. These large models are compute- and memory-intensive, which makes it a challenge to deploy them with minimized latency, throughput, and storage requirements. Some model compression methods have been successfully applied on image classification and detection or language models, but there has been very little work compressing generative adversarial networks (GANs) performing complex tasks. In this paper, we show that a standard model compression technique, weight pruning, cannot be applied to GANs using existing methods. We then develop a self-supervised compression technique which uses the trained discriminator to supervise the training of a compressed generator. We show that this framework has a compelling performance to high degrees of sparsity, generalizes well to new tasks and models, and enables meaningful comparisons between different pruning granularities. Deep Neural Networks (DNNs) have proved successful in various tasks like computer vision, natural language processing, recommendation systems, and autonomous driving. Modern networks are comprised of millions of parameters, requiring significant storage and computational effort. Though accelerators such as GPUs make realtime performance more accessible, compressing networks for faster inference and simpler deployment is an active area of research. Compression techniques have been applied to many networks, reducing memory requirements and improving their performance. Though these approaches do not always harm accuracy, aggressive compression can adversely affect the behavior of the network. Distillation (Schmidhuber, 1991; Hinton et al., 2015) can improve the accuracy of a compressed network by using information from the original, uncompressed network. Generative Adversarial Networks (GANs) (Schmidhuber, 1990; Goodfellow et al., 2014) are a class of DNN that consist of two sub-networks: a generative model and a discriminative model. Their training process aims to achieve a Nash Equilibrium between these two sub-models. GANs have been used in semi-supervised and unsupervised learning areas, such as fake dataset synthesis (Radford et al., 2016; Brock et al., 2019) , style transfer (Zhu et al., 2017b; Azadi et al., 2018) , and image-to-image translation (Zhu et al., 2017a; Choi et al., 2018) . As with networks used in other tasks, GANs have millions of parameters and nontrivial computational requirements. In this work, we explore compressing the generative model of GANs for more efficient deployment. We show that applying standard pruning techniques, with and without distillation, can cause the generator's behavior to no longer achieve the network's goal. Similarly, past work targeted at compressing GANs for simple image synthesis fall short when they are applied to large tasks. In some cases, this result is masked by loss curves that look identical to the original training. By modifying the loss function with a novel combination of the pre-trained discriminator and the original and compressed generators, we can overcome this behavioral degradation and achieve compelling compression rates with little change in the quality of the compressed generator's ouput. We apply our technique to several networks and tasks to show generality. Finally, we study the behavior of compressed generators when pruned with different amounts and types of sparsity, finding that filter pruning, a technique commonly used for accelerating image classification networks, is not trivially applicable to GANs. A complementary method of network compression is quantization. Sharing weight values among a collection of similar weights by hashing (Chen et al., 2015) or clustering (Han et al., 2016) can save storage and bandwidth at runtime. Changing fundamental data types adds the ability to accelerate the arithmetic operations, both in training (Micikevicius et al., 2018) and inference regimes (Jain et al., 2019) . Several techniques have been devised to combat lost accuracy due to compression, since there is always the chance that the behavior of the network may change in undesirable ways when the network is compressed. Using GANs to generate unique training data (Liu et al., 2018b) and extracting knowledge from an uncompressed network, known as distillation (Hinton et al., 2015) , can help keep accuracy high. Since the pruning process involves many hyperparameters, Lin et al. (2019) use a GAN to guide pruning, and Wang et al. (2019a) structure compression as a reinforcement learning problem; both remove some of the burden from the user. In this paper, we propose using a pre-trained discriminator to self-supervise the compression of a generative adversarial network. We show that it is effective and applies to many tasks commonly solved with GANs, unlike traditional compression approaches. Comparing the compressed generators with the baseline models on different tasks, we can conclude that the compression method performs well both in subjective and quantitative evaluations. Advantages of the proposed method include: • The results from the compressed generators are greatly improved over past work. • The self-supervised compression is much shorter than the original GAN training process. It only takes 1%-10% training effort to get an optimal compressed generative model. • It is an end-to-end compression schedule that does not require objective evaluation metrics. • We introduce a single optional hyperparameter (fixed to 0.5 for all our experiments). We use self-supervised GAN compression to show that pruning whole filters, which can work well for image classification models (Li et al., 2017) , may perform poorly for GAN applications. Even pruned at a moderate sparsity (e.g. 25% in Figure 8 ), the generated image has an obvious color shift and does not transfer the photorealistic style. In contrast, the fine-grained compression stategy works well for all tasks we explored. SRGAN seems to be an exception to filter-pruning's poor results; we have to look closely to see differences, and it's not clear which is subjectively better. We have not tried to achieve extremely aggressive compression rates with complicated pruning strategies. Different models may be able to tolerate different amounts of pruning when applied to a task, which we leave to future work. Similarly, we have used network pruning to show the importance and utility of the proposed method, but self-supervised compression is general to other techniques, such as quantization, weight sharing, etc. There are other tasks for which GANs can provide compelling results, and newer networks for tasks we have already explored; future work will extend our self-supervised compression method to these new areas. Finally, self-supervised compression may apply to other network types and tasks if a discriminator is trained alongside the teacher and student networks.","Existing pruning methods fail when applied to GANs tackling complex tasks, so we present a simple and robust method to prune generators that works well for a wide variety of networks and tasks.",Deep Neural Networks ; Azadi ; Choi ; Liu et al. ; Lin et al ; millions ; Generative Adversarial Networks ; Goodfellow et al. ; Jain et al. ; Radford,both ; Some model compression methods ; some cases ; all our experiments ; subjective and quantitative evaluations ; this paper ; two ; undesirable ways ; GPUs ; sparsity,Deep Neural Networks ; Azadi ; Choi ; Liu et al. ; Lin et al ; millions ; Generative Adversarial Networks ; Goodfellow et al. ; Jain et al. ; Radford,"Deep learning's success has led to larger and larger models to handle more complex tasks. These large models are compute- and memory-intensive, making it challenging to deploy them with minimized latency, throughput, and storage requirements. While model compression methods have been successfully applied on image classification and detection or language models, there has been very little work compressing generative adversarial networks (GANs) performing complex tasks, such as computer vision, natural language processing, recommendation systems, and autonomous driving. In this paper, we show that a standard model compression technique, weight pruning, cannot be applied to GANs using existing methods.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"In this work, we study how the large-scale pretrain-finetune framework changes the behavior of a neural language generator. We focus on the transformer encoder-decoder model for the open-domain dialogue response generation task. We find that after standard fine-tuning, the model forgets important language generation skills acquired during large-scale pre-training. We demonstrate the forgetting phenomenon through a detailed behavior analysis from the perspectives of context sensitivity and knowledge transfer. Adopting the concept of data mixing, we propose an intuitive fine-tuning strategy named ""mix-review''. We find that mix-review effectively regularize the fine-tuning process, and the forgetting problem is largely alleviated. Finally, we discuss interesting behavior of the resulting dialogue model and its implications.
 Large-scale unsupervised pre-training (Peters et al., 2018; Devlin et al., 2018; Song et al., 2019) has recently been shown to greatly boost the performance of natural language processing (NLP) models, and has attracted much research interest. Despite its huge success, there is a fundamental question remaining to be answered: Is there some crucial weakness in the standard NLP pretrain-finetune framework? In this work, we take the viewpoint of language generation and show that the answer is, to some extent, yes. In particular, we find that the key to answer this question is a concept we denote as data separation. Although various unsupervised pre-training strategies have been proposed for better utilization of large-scale text data, on a high level the pretrain-finetune framework can be viewed as a simple two-stage procedure: (1) use large-scale text data to pre-train the model, and (2) use target task data to fine-tune the model. Data separation refers to (almost) zero-overlapping data usage of the two stages. In this work we study the pretrain-finetune framework from the viewpoint of neural language generation (NLG). In particular, we focus on the open-domain dialogue response task, for the following reasons: (1) There is high similarity between the target dialogue response task (conditional NLG) and the pre-training language modeling (LM) objective, so we expect that language generation skills learnt during pre-training can be well transferred to the down-stream target task. (2) The sequenceto-sequence (seq2seq) nature of the model allows us to characterize the model's generation behavior in various ways (e.g. context sensitivity). We briefly summarize our contributions as follows. To study how pretrain-finetuning changes the model's behavior, we conduct a behavior analysis from the perspectives of context sensitivity and knowledge transfer. Our main finding is that in the fine-tuning stage, data separation causes the model to forget important language generation skills acquired during pre-training. Motivated by this analysis, we adopt the concept of data mixing and propose a mix-review fine-tuning strategy, where we combine the pre-training and fine-tuning objective. We find that mix-review effectively regularize the fine-tuning process, and the forgetting problem is largely alleviated. Finally, we demonstrate and discuss interesting behavior of the resulting dialogue model and its implications. In this work, we analyze forgetting problem for the standard NLP pretrain-finetune framework in the viewpoint of language generation. We adopt the concept of ""data mixing"" and propose the mix-review fine-tuning strategy. We demonstrate that mix-review can effectively help the model remember important generation skills learned during pre-training. Through a detailed behavior analysis, we find that under the surface of the performance boost for standard metrics, large-scale pre-training changes the model's generative behavior in various profound ways (e.g. context sensitivity). More importantly, the behavior change is influenced by the nature of data itself. For example, we demonstrate that we can discuss news with the resulting dialogue model, even when the fine-tuning data is not about news (Dailydialogue). This opens the exciting possibility of a completely data-driven way to customize a language generator.","We identify the forgetting problem in fine-tuning of pre-trained NLG models, and propose the mix-review strategy to address it.",Peters ; al. ; Devlin ; Song ; NLP ; two ; zero ; NLG,this analysis ; standard fine-tuning ; a high level ; various profound ways ; important generation skills ; important language generation skills ; Song ; how pretrain-finetuning changes ; the forgetting problem ; context sensitivity,Peters ; al. ; Devlin ; Song ; NLP ; two ; zero ; NLG,"In this work, we examine how the large-scale pretrain-finetune framework changes the behavior of a neural language generator, focusing on transformer encoder-decoder model for open-domain dialogue response generation task. After standard fine-tuning, the model forgets important language generation skills, and the forgetting problem is largely alleviated. In addition, we explore how data mixing can enhance the performance of natural language processing (NLP) models, and how it can be used to enhance the learning process. In this work we focus on the open domain dialogue response task, which has high similarity to the pre-training",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"It is well-known that  classifiers are vulnerable to adversarial perturbations. To defend against adversarial perturbations, various certified robustness results have been derived. However, existing certified robustnesses are limited to top-1 predictions. In many real-world applications, top-$k$ predictions are more relevant. In this work, we aim to derive certified robustness for top-$k$ predictions. In particular, our certified robustness is based on randomized smoothing, which turns any classifier to a new classifier via adding noise to an input example. We adopt randomized smoothing because it is scalable to large-scale neural networks and applicable to any classifier. We derive a tight robustness in $\ell_2$ norm for top-$k$ predictions  when using randomized smoothing with Gaussian noise. We find that generalizing the certified robustness  from top-1 to top-$k$ predictions faces significant technical challenges. We also empirically evaluate our method on CIFAR10 and ImageNet. For example, our method can obtain an ImageNet classifier with a certified top-5 accuracy of 62.8\% when the $\ell_2$-norms of the adversarial perturbations are less than 0.5 (=127/255). Our code is publicly available at: \url{https://github.com/jjy1994/Certify_Topk}. Classifiers are vulnerable to adversarial perturbations (Szegedy et al., 2014; Goodfellow et al., 2015; Carlini & Wagner, 2017b; Jia & Gong, 2018) . Specifically, given an example x and a classifier f , an attacker can carefully craft a perturbation δ such that f makes predictions for x + δ as the attacker desires. Various empirical defenses (e.g., Goodfellow et al. (2015) ; Svoboda et al. (2019) ; Buckman et al. (2018) ; Ma et al. (2018) ; Guo et al. (2018) ; Dhillon et al. (2018) ; Xie et al. (2018) ; Song et al. (2018) ) have been proposed to defend against adversarial perturbations. However, these empirical defenses were often soon broken by adaptive adversaries (Carlini & Wagner, 2017a; . As a response, certified robustness (e.g., Wong & Kolter (2018) ; Raghunathan et al. (2018a) ; Liu et al. (2018) ; Lecuyer et al. (2019) ; Cohen et al. (2019) ) against adversarial perturbations has been developed. In particular, a robust classifier verifiably predicts the same top-1 label for data points in a certain region around any example x. In many applications such as recommender systems, web search, and image classification cloud service (Clarifai; Google Cloud Vision), top-k predictions are more relevant. In particular, given an example, a set of k most likely labels are predicted for the example. However, existing certified robustness results are limited to top-1 predictions, leaving top-k robustness unexplored. To bridge this gap, we study certified robustness for top-k predictions in this work. Our certified top-k robustness leverages randomized smoothing (Cao & Gong, 2017; Cohen et al., 2019) , which turns any base classifier f to be a robust classifier via adding random noise to an example. For instance, Cao & Gong (2017) is the first to propose randomized smoothing with uniform noise as an empirical defense. We consider random Gaussian noise because of its certified robustness guarantee (Cohen et al., 2019) . Specifically, we denote by p i the probability that the base classifier f predicts label i for the Gaussian random variable N (x, σ 2 I). The smoothed classifier g k (x) predicts the k labels with the largest probabilities p i 's for the example x. We adopt randomized smoothing because it is scalable to large-scale neural networks and applicable to any base classifier. Our major theoretical result is a tight certified robustness bound for top-k predictions when using randomized smoothing with Gaussian noise. Specifically, given an example x, a label l is verifiably among the top-k labels predicted by the smoothed classifier g k (x + δ) when the 2 -norm of the adversarial perturbation δ is less than a threshold (called certified radius). The certified radius for top-1 predictions derived by Cohen et al. (2019) is a special case of our certified radius when k = 1. As our results and proofs show, generalizing certified robustness from top-1 to top-k predictions faces significant new challenges and requires new techniques. Our certified radius is the unique solution to an equation, which depends on σ, p l , and the k largest probabilities p i 's (excluding p l ). However, computing our certified radius in practice faces two challenges: 1) it is hard to exactly compute the probability p l and the k largest probabilities p i 's, and 2) the equation about the certified radius does not have an analytical solution. To address the first challenge, we estimate simultaneous confidence intervals of the label probabilities via the Clopper-Pearson method and Bonferroni correction in statistics. To address the second challenge, we propose an algorithm to solve the equation to obtain a lower bound of the certified radius, where the lower bound can be tuned to be arbitrarily close to the true certified radius. We evaluate our method on CIFAR10 (Krizhevsky & Hinton, 2009) and ImageNet (Deng et al., 2009) datasets. For instance, on ImageNet, our method respectively achieves approximate certified top-1, top-3, and top-5 accuracies as 46.6%, 57.8%, and 62.8% when the 2 -norms of the adversarial perturbations are less than 0.5 (127/255) and σ = 0.5. Our contributions are summarized as follows: • Theory. We derive the first certified radius for top-k predictions. Moreover, we prove our certified radius is tight for randomized smoothing with Gaussian noise. • Algorithm. We develop algorithms to estimate our certified radius in practice. • Evaluation. We empirically evaluate our method on CIFAR10 and ImageNet. Adversarial perturbation poses a fundamental security threat to classifiers. Existing certified defenses focus on top-1 predictions, leaving top-k predictions untouched. In this work, we derive the first certified radius under 2 -norm for top-k predictions. Our results are based on randomized smoothing. Moreover, we prove that our certified radius is tight for randomized smoothing with Gaussian noise. In order to compute the certified radius in practice, we further propose simultaneous confidence interval estimation methods as well as design an algorithm to estimate a lower bound of the certified radius. Interesting directions for future work include 1) deriving a tight certified radius under other norms such as 1 and ∞ , 2) studying which noise gives the tightest certified radius for randomized smoothing, and 3) studying certified robustness for top-k ranking. A PROOF OF THEOREM 1 Given an example x, we define the following two random variables: where ∼ N (0, σ 2 I). The random variables X and Y represent random samples obtained by adding isotropic Gaussian noise to the example x and its perturbed version x + δ, respectively. Moreover, we have the following lemma from Cohen et al. (2019) . Lemma 2. Given an example x, a number q ∈ [0, 1], and regions A and B defined as follows: ) Then, we have the following equations: Proof. Please refer to Cohen et al. (2019) . Based on Lemma 1 and 2, we derive the following lemma: Lemma 3. Suppose we have an arbitrary base classifier f , an example x, a set of labels which are denoted as S, two probabilities p S and p S that satisfy p S ≤ p S = Pr(f (X) ∈ S) ≤ p S , and regions A S and B S defined as follows: Proof. We know that Pr(X ∈ A S ) = p S based on Lemma 2. Combined with the condition that p S ≤ Pr(f (X) ∈ S), we obtain the first inequality in (20) . Similarly, we can obtain the second inequality in (20). We define M (z) = I(f (z) ∈ S). Based on the first inequality in (20) and Lemma 1, we have the following: which is the first inequality in (21). The second inequality in (21) can be obtained similarly. Next, we restate Theorem 1 and show our proof. Theorem 1 (Certified Radius for Top-k Predictions). Suppose we are given an example x, an arbitrary base classifier f , ∼ N (0, σ 2 I), a smoothed classifier g, an arbitrary label l ∈ {1, 2, · · · , c}, and p l , p 1 , · · · , p l−1 , p l+1 , · · · , p c ∈ [0, 1] that satisfy the following conditions: where p and p indicate lower and upper bounds of p, respectively. Let where ties are broken uniformly at random. Moreover, we denote by S t = {b 1 , b 2 , · · · , b t } the set of t labels with the smallest probability upper bounds in the k largest ones and by p St = t j=1 p bj the sum of the t probability upper bounds, where t = 1, 2, · · · , k. Then, we have: where R l is the unique solution to the following equation: where Φ and Φ −1 are the cumulative distribution function and its inverse of the standard Gaussian distribution, respectively. Proof. Roughly speaking, our idea is to make the probability that the base classifier f predicts l when taking Y as input larger than the smallest one among the probabilities that f predicts for a set of arbitrary k labels selected from all labels except l. For simplicity, we let Γ = {1, 2, · · · , c} \ {l}, i.e., all labels except l. We denote by Γ k a set of k labels in Γ. We aim to find a certified radius R l such that we have max Γ k ⊆Γ min i∈Γ k Pr(f (Y) = i) < Pr(f (Y) = l), which guarantees l ∈ g k (x + δ). We first upper bound the minimal probability min i∈Γ k Pr(f (Y) = i) for a given Γ k , and then we upper bound the maximum value of the minimal probability among all possible Γ k ⊆ Γ. Finally, we obtain the certified radius R l via letting the upper bound of the maximum value smaller than Pr(f (Y) = l). Bounding min i∈Γ k Pr(f (Y) = i) for a given Γ k : We use S to denote a non-empty subset of Γ k and use |S| to denote its size. We define p S = i∈S p i , which is the sum of the upper bounds of the probabilities for the labels in S. Moreover, we define the following region associated with the set S: We have Pr(f (Y) ∈ S) ≤ Pr(Y ∈ B S ) by applying Lemma 3 to the set S. In addition, we have . Therefore, we have: Moreover, we have: where we have the first inequality because S is a subset of Γ k and we have the second inequality because the smallest value in a set is no larger than the average value of the set. Equation 27 holds for any S ⊆ Γ k . Therefore, by taking all possible sets S into consideration, we have the following: where S t is the set of t labels in Γ k whose probability upper bounds are the smallest, where ties are broken uniformly at random. We have Equation 30 from Equation 29 because Pr(Y ∈ B S ) decreases as p S decreases. Since Pr(Y ∈ B St ) increases as p St increases, Equation 30 reaches its maximum value when Γ k = {b 1 , b 2 , · · · , b k }, i.e., when Γ k is the set of k labels in Γ with the largest probability upper bounds. Formally, we have: where Obtaining R l : According to Lemma 3, we have the following for S = {l}: Recall that our goal is to make Pr(f (Y) = l) > max Γ k ⊆Γ min i∈Γ k Pr(f (Y) = i). It suffices to let: According to Lemma 2, we have Pr( . Therefore, we have the following constraint on δ: Since the left-hand side of the above inequality 1) decreases as ||δ|| 2 increases, 2) is larger than 0 when ||δ|| 2 → −∞, and 3) is smaller than 0 when ||δ|| 2 → ∞, we have the constraint ||δ|| 2 < R l , where R l is the unique solution to the following equation: B PROOF OF THEOREM 2 Following the terminology we used in proving Theorem 1, we define a region A {l} as follows: According to Lemma 2, we have Pr(X ∈ A {l} ) = p l . We first show the following lemma, which is the key to prove our Theorem 2. Lemma 4. Assuming we have p l + k j=1 p bj ≤ 1. For any perturbation δ 2 > R l , there exists k disjoint regions C bj ⊆ R d \ A {l} , j ∈ {1, 2, · · · , k} that satisfy the following: where the random variables X and Y are defined in Equation 10 and 11, respectively; and {b 1 , b 2 , · · · , b k } and S t are defined in Theorem 1. Proof. Our proof is based on mathematical induction and the intermediate value theorem. For convenience, we defer the proof to Appendix B.1. Next, we restate Theorem 2 and show our proof. Theorem 2 (Tightness of the Certified Radius). Assuming we have p l + k j=1 p bj ≤ 1 and p l + i=1,··· ,l−1,l+1,··· ,c p i ≥ 1. Then, for any perturbation ||δ|| 2 > R l , there exists a base classifier f * consistent with (1) but we have l / ∈ g k (x + δ). Proof. Our idea is to construct a base classifier such that l is not among the top-k labels predicted by the smoothed classifier for any perturbed example x + δ when ||δ|| 2 > R l . First, according to Lemma 4, we know there exists k disjoint regions C bj ⊆ R d \ A {l} , j ∈ {1, 2, · · · , k} that satisfy Equation 37 and 38. Moreover, we divide the remaining region R d \ (A {l} ∪ k j=1 C bj ) into c−k −1 regions, which we denote as C b k+1 , C b k+2 , · · · , C bc−1 and satisfy Pr(X ∈ C bj ) ≤ p bj for j ∈ {k + 1, k + 2, · · · , c − 1}. Note that b 1 , b 2 , · · · , b c−1 is some permutation of {1, 2, · · · , c} \ {l}. We can divide the remaining region into such c−k −1 regions because p l + i=1,··· ,l−1,l+1,··· ,c p i ≥ 1. Then, based on these regions, we construct the following base classifier: Based on the definition of f * , we have the following: Pr(f Therefore, f * satisfies the conditions in (1). Next, we show that l is not among the top-k labels predicted by the smoothed classifier for any perturbed example x + δ when ||δ|| 2 > R l . Specifically, we have: where j = 1, 2, · · · , k. Since we have found k labels whose probabilities are larger than the probability of the label l, we have l / ∈ g k (x + δ) when δ 2 > R l .",We study the certified robustness for top-k predictions via randomized smoothing under Gaussian noise and derive a tight robustness bound in L_2 norm.,max Γ k ⊆Γ min ; k. Then ; Guo et al. ; Cao & Gong ; Svoboda ; Gaussian ; al. ; two ; p l ; Wong & Kolter,"Interesting directions ; \ ; Dhillon et al ; · , c − ; randomized smoothing ; label ; uniform noise ; Wagner ; the certified robustness ; the standard Gaussian distribution",max Γ k ⊆Γ min ; k. Then ; Guo et al. ; Cao & Gong ; Svoboda ; Gaussian ; al. ; two ; p l ; Wong & Kolter,"Classifiers are vulnerable to adversarial perturbations, and various certified robustness results have been derived. However, existing certified robustnesses are limited to top-1 predictions. In real-world applications, top-$k$ predictions are more relevant. In this work, we aim to derive a robustness from top-$1 predictions, based on randomized smoothing, which turns any classifier to a new classifier via adding noise to an input example. This approach is scalable to large-scale neural networks and applicable to any classifiers. We derive a tight robustness in $\ell_2$ norm for top-k$",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Developing effective biologically plausible learning rules for deep neural networks is important for advancing connections between deep learning and neuroscience. To date, local synaptic learning rules like those employed by the brain have failed to match the performance of backpropagation in deep networks. In this work, we employ meta-learning to discover networks that learn using feedback connections and local, biologically motivated learning rules. Importantly, the feedback connections are not tied to the feedforward weights, avoiding any biologically implausible weight transport. It can be shown mathematically that this approach has sufficient expressivity to approximate any online learning algorithm. Our experiments show that the meta-trained networks effectively use feedback connections to perform online credit assignment in multi-layer architectures. Moreover, we demonstrate empirically that this model outperforms a state-of-the-art gradient-based meta-learning algorithm for continual learning on regression and classification benchmarks. This approach represents a step toward biologically plausible learning mechanisms that can not only match gradient descent-based learning, but also overcome its limitations. Deep learning has achieved impressive success in solving complex tasks, and in some cases its learned representations have been shown to match those in the brain [19, 10] . However, there is much debate over how well the learning algorithm commonly used in deep learning, backpropagation, resembles biological learning algorithms. Causes for skepticism include the facts that (1) backpropagation ignores the nonlinearities imposed by neurons in the backward pass and assumes instead that derivatives of the forward-pass nonlinearities can be applied, (2) in backpropagation, feedback path weights are exactly tied to feedforward weights, even as weights are updated with learning, and (3) backpropagation assumes alternating forward and backward passes [12] . The question of how so-called credit assignment -appropriate propagation of learning signals to non-output neurons -can be performed in biologically plausible fashion in deep neural networks remains open. We propose a new learning paradigm that aims to solve the credit assignment problem in more biologically plausible fashion. Our approach is as follows: (1) endow a deep neural network with feedback connections that propagate information about target outputs to neurons at all layers, (2) apply local plasticity rules (e.g. Hebbian or neuromodulated plasticity) to update feedforward synaptic weights following feedback projections, and (3) employ meta-learning to optimize for the initialization of feedforward weights, the setting of feedback weights, and synaptic plasticity levels. On a set of online regression and classification learning tasks, we find that meta-learned deep networks can successfully perform useful weight updates in early layers, and that feedback with local learning rules can in fact outperform gradient descent as an inner-loop learning algorithm on challenging few-shot and continual learning tasks. This work demonstrates that meta-learning procedures can optimize for neural networks that learn online using local plasticity rules and feedback connections. Several follow-up directions could be pursued. First, meta-learning of this kind is computationally expensive, as the meta-learner must backpropagate through the network's entire training procedure. In order to scale this approach, it will be important to find ways to meta-train networks that generalize to longer lifetimes than were used during meta-training, or to explore alternatives to backprop-based meta-training (e.g. evolutionary algorithms). The present work focused on the case of online learning, but the case of learning from repeated exposure to large datasets is also of interest, and scaling the method in this fashion will be crucial to exploring this regime. Future work could also increase the biological plausibility of the method. For instance, in the present implementation the feedforward and feedback + update passes occur sequentially. However, a natural extension would enable them to run in parallel. This requires ensuring (through appropriate meta-learning and/or a segregated dendrites model [6] ) that feedforward and feedback information do not interfere destructively. Third, the meta-learning procedure in this work optimizes for a precise feedforward and feedback weight initialization. Optimizing instead for a distribution of weight initializations or connectivity patterns would better reflect the stochasticity in synapse development. Another direction is to apply meta-learning to understand biological learning systems (see [9] for an example of such an effort). Well-constrained biological learning models meta-optimized in this manner might show emergence of learning circuits used in biology and even suggest new ones. [",Networks that learn with feedback connections and local plasticity rules can be optimized for using meta learning.,Hebbian ; First ; Third,signals ; that feedback ; feedback connections ; Our approach ; this fashion ; passes ; biological learning algorithms ; deep learning ; the meta-trained networks ; continual learning,Hebbian ; First ; Third,"In this work, we employ meta-learning to discover networks that learn using feedback connections and local, biologically motivated learning rules. The feedback connections are not tied to the feedforward weights, avoiding any biologically implausible weight transport. This approach has sufficient expressivity to approximate any online learning algorithm. Our experiments show that the meta-trained networks effectively use feedback connections to perform online credit assignment in multi-layer architectures. Moreover, we demonstrate empirically that this model outperforms a state-of-the-art gradient-based Meta-learning algorithm for continual learning on regression and classification benchmarks. This represents a step toward biologically plausible learning mechanisms",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"The embedding layers transforming input words into real vectors are the key components of deep neural networks used in natural language processing. However, when the vocabulary is large, the corresponding weight matrices can be enormous, which precludes their deployment in a limited resource setting. We introduce a novel way of parametrizing embedding layers based on the Tensor Train (TT) decomposition, which allows compressing the model significantly at the cost of a negligible drop or even a slight gain in performance.   We evaluate our method on a wide range of benchmarks in natural language processing and analyze the trade-off between performance and compression ratios for a wide range of architectures, from MLPs to LSTMs and Transformers. Deep neural networks (DNNs) typically used in natural language processing (NLP) employ large embeddings layers, which map the input words into continuous representations and usually have the form of lookup tables. Despite such simplicity and, arguably because of it, the resulting models are cumbersome, which may cause problems in training and deploying them in a limited resource setting. Thus, the compression of large neural networks and the development of novel lightweight architectures have become essential problems in NLP research. One way to reduce the number of parameters in the trained model is to imply a specific structure on its weight matrices (e.g., assume that they are low-rank or can be well approximated by low-rank tensor networks). Such approaches are successful at compressing the pre-trained models, but they do not facilitate the training itself. Furthermore, they usually require an additional fine-tuning stage to recover the performance of the original model. In this paper, we introduce a new, parameter efficient embedding layer, termed TT-embedding, which can be plugged in into any model and trained end-to-end. The benefits of our compressed TT-layer are twofold. Firstly, instead of storing huge embedding matrix, we store a sequence of much smaller 2-dimensional and 3-dimensional tensors, necessary for reconstructing the required embeddings, which allows compressing the model significantly at the cost of a negligible performance drop. Secondly, the overall number of parameters can be relatively small (and constant) during the whole training stage, which allows to use larger batches or train efficiently in a case of limited resources. To validate the efficiency of the proposed approach, we have tested it on several popular NLP tasks. In our experiments, we have observed that the standard embeddings can be replaced by TT-embeddings with the compression ratio of 1 − 3 orders without any significant drop (and sometimes even with a slight gain) of the metric of interest. Specifically, we report the following compression ratios of the embedding layers: 441 on the IMDB dataset with 0.2% absolute increase in classification accuracy; 15 on the WMT 2014 En-De dataset with 0.3 drop in the BLEU score. Additionally, we have also evaluated our algorithm on a task of binary classification based on a large number of categorical features. More concretely, we applied TT-embedding to the click through rate (CTR) prediction problem, a crucial task in the field of digital advertising. Neural networks, typically used for solving this problem, while being rather elementary, include a large number of embedding layers of significant size. As a result, a majority of model parameters that represent these layers, may occupy hundreds of gigabytes of space. We show that TT-embedding not only considerably reduces the number of parameters in such models, but also sometimes improves their accuracy. We propose a novel embedding layer, the TT-embedding, for compressing huge lookup tables used for encoding categorical features of significant cardinality, such as the index of a token in natural language processing tasks. The proposed approach, based on the TT-decomposition, experimentally proved to be effective, as it heavily decreases the number of training parameters at the cost of a small deterioration in performance. In addition, our method can be easily integrated into any deep learning framework and trained via backpropagation, while capitalizing on reduced memory requirements and increased training batch size. Our experimental results suggest several appealing directions for future work. First of all, TTembeddings impose a concrete tensorial low-rank structure on the embedding matrix, which was shown to improve the generalization ability of the networks acting as a regularizer. The properties and conditions of applicability of this regularizer are subject to more rigorous analysis. Secondly, unlike standard embedding, we can introduce non-linearity into TT-cores to improve their expressive power (Khrulkov et al., 2019) . Additionally, it is important to understand how the order of tokens in the vocabulary affects the properties of the networks with TT-embedding. We hypothesize that there exists the optimal order of tokens which better exploits the particular structure of TT-embedding and leads to a boost in performance and/or compression ratio. Finally, the idea of applying higher-order tensor decompositions to reduce the number of parameters in neural nets is complementary to more traditional methods such as pruning (Han et al., 2015) and quantization (Hubara et al., 2017; Xu et al., 2018) . Thus, it would be interesting to make a thorough comparison of all these methods and investigate whether their combination may lead to even stronger compression.",Embedding layers are factorized with Tensor Train decomposition to reduce their memory footprint.,Secondly ; First ; the Tensor Train ; hundreds ; TT ; BLEU ; One ; NLP ; Firstly ; Hubara et al.,their combination ; tokens ; the input words ; the required embeddings ; layers ; En-De ; performance ; a large number ; The properties ; NLP research,Secondly ; First ; the Tensor Train ; hundreds ; TT ; BLEU ; One ; NLP ; Firstly ; Hubara et al.,"The embedding layers transforming input words into real vectors are crucial components of deep neural networks used in natural language processing. However, when the vocabulary is large, the corresponding weight matrices can be enormous, which precludes their deployment in a limited resource setting. We introduce a novel way of parametrizing embedding layer based on the Tensor Train (TT) decomposition, which allows compressing the model significantly at the cost of a negligible drop or even a slight gain in performance. We evaluate performance and compression ratios for a wide range of architectures, from MLPs to LSTMs and Transformers. Deep neural networks (D",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"In this paper, we extend the persona-based sequence-to-sequence (Seq2Seq) neural network conversation model to a multi-turn dialogue scenario by modifying the state-of-the-art hredGAN architecture to simultaneously capture utterance attributes such as speaker identity, dialogue topic, speaker sentiments and so on. The proposed system, phredGAN has a persona-based HRED generator (PHRED) and a conditional discriminator. We also explore two approaches to accomplish the conditional discriminator: (1) $phredGAN_a$, a system that passes the attribute representation as an additional input into a traditional adversarial discriminator, and (2) $phredGAN_d$, a dual discriminator system which in addition to the adversarial discriminator, collaboratively predicts the attribute(s) that generated the input utterance. To demonstrate the superior performance of phredGAN over the persona SeqSeq model, we experiment with two conversational datasets, the Ubuntu Dialogue Corpus (UDC) and TV series transcripts from the Big Bang Theory and Friends. Performance comparison is made with respect to a variety of quantitative measures as well as crowd-sourced human evaluation. We also explore the trade-offs from using either variant of $phredGAN$ on datasets with many but weak attribute modalities (such as with Big Bang Theory and Friends) and ones with few but strong attribute modalities (customer-agent interactions in Ubuntu dataset). Recent advances in machine learning especially with deep neural networks has lead to tremendous progress in natural language processing and dialogue modeling research BID13 BID14 BID10 . Nevertheless, developing a good conversation model capable of fluent interaction between a human and a machine is still in its infancy stage. Most existing work relies on limited dialogue history to produce response with the assumption that the model parameters will capture all the modalities within a dataset. However, this is not true as dialogue corpora tend to be strongly multi-modal and practical neural network models find it difficult to disambiguate characteristics such as speaker personality, location and sub-topic in the data.Most work in this domain has primarily focused on optimizing dialogue consistency. For example, Serban et al. BID10 BID12 a) and BID15 introduced a Hierarchical Recurrent Encoder-Decoder (HRED) network architecture that combines a series of recurrent neural networks to capture long-term context state within a dialogue. However, the HRED system suffers from lack of diversity and does not have any guarantee on the generator output since the output conditional probability is not calibrated. BID8 tackles these problems by training a modified HRED generator alongside an adversarial discriminator in order to increase diversity and provide a strong and calibrated guarantee to the generator's output. While the hredGAN system improves upon response quality, it does not capture speaker and other attributes modality within a dataset and fails to generate persona specific responses in datasets with multiple modalities.On the other hand, there has been some recent work on introducing persona into dialogue models. For example, BID5 integrates attribute embeddings into a single turn (Seq2Seq) generative dialogue model. In this work, Li et al. consider persona models one with Speaker-only representation and the other with Speaker and Addressee representations (Speaker-Addressee model), both of which capture certain speaker identity and interactions. BID7 continue along the Figure 1 : The PHRED generator with local attention -The attributes C, allows the generator to condition its response on the utterance attributes such as speaker identity, subtopics and so on. same line of thought by considering a Seq2Seq dialogue model with Responder-only representation. In both of these cases, the attribute representation is learned during the system training. BID16 proposed a slightly different approach. Here, the attributes are a set of sentences describing the profile of the speaker. In this case, the attributes representation is not learned. The system however learns how to attend to different parts of the attributes during training. Still, the above persona-based models have limited dialogue history (single turn); suffer from exposure bias worsening the trade-off between personalization and conversation quality and cannot generate multiple responses given a dialogue context. This is evident in the relatively short and generic responses produced by these systems, even though they generally capture the persona of the speaker.In order to overcome these limitations, we propose two variants of an adversarially trained persona conversational generative system, phredGAN , namely phredGAN a and phredGAN d . Both systems aim to maintain the response quality of hredGAN and still capture speaker and other attribute modalities within the conversation. In fact, both systems use the same generator architecture (PHRED generator), i.e., an hredGAN generator BID8 with additional utterance attribute representation at its encoder and decoder inputs as depicted in Figure 1 . Conditioning on external attributes can be seen as another input modality as is the utterance into the underlying system. The attribute representation is an embedding that is learned together with the rest of model parameters similar to BID5 . Injecting attributes into a multi-turn dialogue system allows the model to generate responses conditioned on particular attribute(s) across conversation turns. Since the attributes are discrete, it also allows for exploring different what-if scenarios of model responses. The difference between the two systems is in the discriminator architecture based on how the attribute is treated.We train and sample both variants of phredGAN similar to the procedure for hredGAN BID8 . To demonstrate model capability, we train on a customer service related data such as the Ubuntu Dialogue Corpus (UDC) that is strongly bimodal between question poser and answerer, and transcripts from a multi-modal TV series The Big Bang Theory and Friends with quantitative and qualitative analysis. We examine the trade-offs between using either system in bi-modal or multi-modal datasets, and demonstrate system superiority over state-of-the-art persona conversational models in terms of dialogue response quality and quantitatively with perplexity, BLEU, ROUGE and distinct n-gram scores. In this paper, we improve upon state-of-the-art persona-based response generation models by exploring two persona conversational models: phredGAN a which passes the attribute representation as an additional input into a traditional adversarial discriminator, and phredGAN d a dual discriminator system which in addition to the adversarial discriminator from hredGAN , collaboratively predicts the attribute(s) that are intrinsic to the input utterance. Both systems demonstrate quantitative improvements upon state-of-the-art persona conversational systems such as the work from BID5 with respect to both quantitative automatic and qualitative human measures.Our analysis also demonstrates how both variants of phredGAN perform differently on datasets with weak and strong modality. One of our future direction is to take advantage of phredGAN d 's ability to predict utterance attribute such as speaker identity from just the utterance. We believe its performance can be improved even with weak modality by further conditioning adversarial updates on both the attribute and adversarial discriminator accuracies. Overall, this paper demonstrates clear benefits from adversarial training of persona generative dialogue system and leaves the door open for more interesting work to be accomplished in this domain. (Xi, Ci) with N utterances. Each utterance mini batch i contains Mi word tokens.",This paper develops an adversarial learning framework for neural conversation models with persona,SeqSeq ; Li ; al. ; HRED ; phredGAN$ ; One ; phredGAN ; the Ubuntu Dialogue Corpus ; the Big Bang Theory ; ROUGE,"dialogue topic ; the attribute(s ; Friends ; the conversation ; a strong and calibrated guarantee ; (Xi, Ci ; a conditional discriminator ; datasets ; phredGAN d 's ability ; The system",SeqSeq ; Li ; al. ; HRED ; phredGAN$ ; One ; phredGAN ; the Ubuntu Dialogue Corpus ; the Big Bang Theory ; ROUGE,"In this paper, we extend the persona-based sequence-to-sequence (Seq2Seq) neural network conversation model to a multi-turn dialogue scenario by modifying the state-of-the-art hredGAN architecture to simultaneously capture utterance attributes such as speaker identity, dialogue topic, speaker sentiments, and so on. The proposed system, phredGAN, has a person-based HRED generator (PHRED) and a conditional discriminator, and two approaches are explored to achieve its superior performance: (1) $phredGAN_a$, a system that passes the attribute representation as an additional input",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"The transformer has become a central model for many NLP tasks from translation to language modeling to representation learning. Its success demonstrates the effectiveness of stacked attention as a replacement for recurrence for many tasks. In theory attention also offers more insights into the model’s internal decisions; however, in practice when stacked it quickly becomes nearly as fully-connected as recurrent models. In this work, we propose an alternative transformer architecture, discrete transformer, with the goal of better separating out internal model decisions. The model uses hard attention to ensure that each step only depends on a fixed context. Additionally, the model uses a separate “syntactic” controller to separate out network structure from decision making. Finally we show that this approach can be further sparsified with direct regularization. Empirically, this approach is able to maintain the same level of performance on several datasets, while discretizing reasoning decisions over the data. The transformer has achieved state-of-the-art performances in a variety of sequence modeling tasks, including language modeling (Radford et al., 2019) , machine translation (Vaswani et al., 2017) , question answering (Radford et al., 2018; Devlin et al., 2018) , among others. To facilitate parallel training, as well as to reduce the path length of the dependencies, transformer dispenses recurrence and builds up hidden states by attending to the source side (inter-attention) and attending to its past predictions (self-attention) with multiple heads in multiple layers (Vaswani et al., 2017) . Compared to recurrent models the attention mechanism adds some ""interpretability"" to a model's decision (Bahdanau et al., 2014; Xu et al., 2015; Chan et al., 2015) . However, in the commonly used soft attention mechanism (Luong et al., 2015) each input element receives non-zero weight, and so it is unclear whether the magnitude of attention weights reflects the relative importance of the corresponding inputs (Jain & Wallace, 2019) . To make things worse, due to the existence of multiple stacked attention layers in transformer, it becomes even harder to discriminate the contributions of each input to the final decisions made by the model. Can we force the transformer to make sharper, discrete internal decisions? In this work, we consider a variant of the transformer architecture with the goal of maintaining performance while forcing discrete decisions. Specifically, we consider a discrete transformer with three changes to the architecture: (a) we propose to treat attention as a categorical latent variable (Deng et al., 2018; Shankar et al., 2018) and use hard attention mechanism to get discrete attention decisions (Xu et al., 2015) , (b) we propose to separate out the querying mechanism from value computation into intertwine soft ""syntactic"" and hard ""semantic"" model streams, and (c) we consider extension to the discrete transformer to allow for further additions such as attention sparsity regularization. Training of the model is very similar to standard transformer training. The key benefits come at inference time. First, we can use a simple decoding procedure where we take argmax attentions such that each intermediate representation is only built up based on the subset of the attended lower layer outputs. In turn, each final prediction uses limited receptive field, and we can even the guarantee that any hidden state does not depend on input elements not being directly attended to. Second, we can split out attention prediction from computation, and even fix the structure of the feed-forward network for a given example. To validate this approach, we perform experiments on several tasks. We first validate that with proper attention and sparsity regularization the model can learn the truly necessary attentions on a synthetic language modeling task. Next on two real world machine translation datasets, we show that with our approach we can learn transformer models using limited context for making predictions while not deteriorating their performance by too much, indirectly validating the selectiveness of the attention mechanism. The rest of the paper is organized as follows: In Section 2 we draw the connections of our work to the literature. We introduce background and discuss our approach in Sections 3, 4 and 5. Experiments, results and analyses are presented in Sections 6 and 7, and we conclude our paper in Section 8. This work presents the discrete transformer, a modification to the transformer to make discrete attention decisions and to separate out dependencies from semantic state value. Experiments show that despite the more structured decisions the model is able to maintain similar performance on standard machine translation benchmarks. Analysis shows that the model separates out syntactic properties and even learns precise decisions on clean data. This style of model opens up the potential for many possible experiments in NLP. Because the model makes hard intermediary decisions the semantic model can be shown to only depend on a subset of the data. This property could be used to check for or remove bias from a model, for instance to ensure that production gendered pronoun does not depend on spurious context. Similarly because the dependencies are predicted separately additional priors or regularization could be used to enforce specific syntactic structure. Finally, this method could be used to train pretrained models that allow for discrete intermediary structure.",Discrete transformer which uses hard attention to ensure that each step only depends on a fixed context.,Luong et al. ; Radford ; three ; Chan ; Second ; two ; Devlin ; NLP ; al. ; non-zero,instance ; syntactic properties ; direct regularization ; Jain ; practice ; Luong et al. ; an alternative transformer architecture ; further additions ; results ; parallel training,Luong et al. ; Radford ; three ; Chan ; Second ; two ; Devlin ; NLP ; al. ; non-zero,"The transformer has become a central model for NLP tasks. Its success demonstrates the effectiveness of stacked attention as a replacement for recurrence for many tasks. In theory, it provides more insights into the model's internal decisions, while in practice it quickly becomes fully-connected. In this work, we propose an alternative transformer architecture, discrete transformer, with the goal of better separating out internal model decisions. The model uses hard attention to ensure each step only depends on a fixed context. Additionally, the model uses a separate ""syntactic"" controller to separate out network structure from decision making. This approach can be further sparsified with",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Graph Neural Networks (GNNs) are a powerful representational tool for solving problems on graph-structured inputs. In almost all cases so far, however, they have been applied to directly recovering a final solution from raw inputs, without explicit guidance on how to structure their problem-solving. Here, instead, we focus on learning in the space of algorithms: we train several state-of-the-art GNN architectures to imitate individual steps of classical graph algorithms, parallel (breadth-first search, Bellman-Ford) as well as sequential (Prim's algorithm). As graph algorithms usually rely on making discrete decisions within neighbourhoods, we hypothesise that maximisation-based message passing neural networks are best-suited for such objectives, and validate this claim empirically. We also demonstrate how learning in the space of algorithms can yield new opportunities for positive transfer between tasks---showing how learning a shortest-path algorithm can be substantially improved when simultaneously learning a reachability algorithm. A multitude of important real-world tasks can be formulated as tasks over graph-structured inputs, such as navigation, web search, protein folding, and game-playing. Theoretical computer science has successfully discovered effective and highly influential algorithms for many of these tasks. But many problems are still considered intractable from this perspective. Machine learning approaches have been applied to many of these classic tasks, from tasks with known polynomial time algorithms such as shortest paths (Graves et al., 2016; Xu et al., 2019) and sorting (Reed & De Freitas, 2015) , to intractable tasks such as travelling salesman (Vinyals et al., 2015; Bello et al., 2016; Kool et al., 2018) , boolean satisfiability (Selsam et al., 2018; Selsam & Bjørner, 2019) , and even probabilistic inference (Yoon et al., 2018) . Recently, this work often relies on advancements in graph representation learning (Bronstein et al., 2017; Hamilton et al., 2017; with graph neural networks (GNNs) (Li et al., 2015; Kipf & Welling, 2016; Gilmer et al., 2017; Veličković et al., 2018) . In almost all cases so far, ground-truth solutions are used to drive learning, giving the model complete freedom to find a mapping from raw inputs to such solution 1 . Many classical algorithms share related subroutines: for example, shortest path computation (via the Bellman-Ford (Bellman, 1958 ) algorithm) and breadth-first search both must enumerate sets of edges adjacent to a particular node. Inspired by previous work on the more general tasks of program synthesis and learning to execute (Zaremba & Sutskever, 2014; Kaiser & Sutskever, 2015; Kurach et al., 2015; Reed & De Freitas, 2015; , we show that by learning several algorithms simultaneously and providing a supervision signal, our neural network is able to demonstrate positive knowledge transfer between learning different algorithms. The supervision signal is driven by how a known classical algorithm would process such inputs (including any relevant intermediate outputs), providing explicit (and reusable) guidance on how to tackle graph-structured problems. We call this approach neural graph algorithm execution. Given that the majority of popular algorithms requires making discrete decisions over neighbourhoods (e.g. ""which edge should be taken?""), we suggest that a highly suitable architecture for this task is a message-passing neural network (Gilmer et al., 2017 ) with a maximisation aggregator-a claim we verify, demonstrating clear performance benefits for simultaneously learning breadth-first search for reachability with the Bellman-Ford algorithm for shortest paths. We also verify its applicability to sequential reasoning, through learning Prim's algorithm (Prim, 1957) for minimum spanning trees. Note that our approach complements Reed & De Freitas (2015) : we show that a relatively simple graph neural network architecture is able to learn and algorithmically transfer among different tasks, do not require explicitly denoting subroutines, and tackle tasks with superlinear time complexity. 2 PROBLEM SETUP Parallel algorithm execution In order to evaluate how faithfully the neural algorithm executor replicates the two parallel algorithms, we propose reporting the accuracy of predicting the reachability 2 (for breadth-first search; Table 1 ), as well as predicting the predecessor node (for Bellman-Ford; Table 2 ). We report this metric averaged across all steps t (to give a sense of how well the algorithm is imitated across time), as well as the last-step performance (which corresponds to the final solution). While it is not necessary for recovering the final answer, we also provide the mean squared error of the models on the Bellman-Ford distance information, as well as the termination accuracy (computed at each step separately)-averaged across all timesteps-in Table 3 . The results confirm our hypotheses: the MPNN-max model exhibits superior generalisation performance on both reachability and shortest-path predecessor node prediction. Even when allowing for hardening the attention of GAT-like models (using entropy or Gumbel softmax), the more flexible computational model of MPNN is capable of outperforming them. The performance gap on predicting the predecessor also widens significantly as the test graph size increases. Our findings are compounded by observing the mean squared error metric on the intermediate result: with the MPNN-max being the only model providing a reasonable level of regression error at the 100-node generalisation level. It further accentuates that, even though models like the MPNN-sum model may also learn various thresholding functions-as demonstrated by (Xu et al., 2018 )-aggregating messages in this way can lead to outputs of exploding magnitude, rendering the network hard to numerically control for larger graphs. We perform two additional studies, executing the shortest-path prediction on MPNN-max without predicting reachability, and without supervising on any intermediate algorithm computations-that is, learning to predict predecessors (and termination behaviour) directly from the inputs, x i . Note that this is the primary way such tasks have been tackled by graph neural networks in prior work. We report these results as no-reach and no-algo in Table 2 , respectively. Looking at the no-reach ablation, we observe clear signs of positive knowledge transfer occurring between the reachability and shortest-path tasks: when the shortest path algorithm is learned in isolation, the predictive power of MPNN-max drops significantly (while still outperforming many other approaches). In Appendix B, we provide a brief theoretical insight to justify this. Similarly, considering the no-algo experiment, we conclude that there is a clear benefit to supervising on the distance information-giving an additional performance improvement compared to the standard approach of only supervising on the final downstream outputs. Taken in conjunction, these two results provide encouragement for studying this particular learning setup. Lastly, we report the performance of a curriculum learning (Bengio et al., 2009 ) strategy (as curriculum): here, BFS is learnt first in isolation (to perfect validation accuracy), followed by finetuning on Bellman-Ford. We find that this approach performs worse than learning both algorithms simultaneously, and as such we do not consider it in further experiments. Desirable properties of the MPNN-max as an algorithm executor persist when generalising to even larger graphs, as we demonstrate in Table 4 -demonstrating favourable generalisation on graphs up to 75× as large as the graphs originally trained on. We note that our observations also still hold Figure 3 : The per-step algorithm execution performances in terms of reachability accuracy (left), distance mean-squared error (middle) and predecessor accuracy (right), tested on 100-node graphs after training on 20-node graphs. Please mind the scale of the MSE plot. when training on larger graphs (Appendix C). We also find that there is no significant overfitting to a particular input graph category-however we do provide an in-depth analysis of per-category performance in Appendix D. Additional metrics The graphs we generate may be roughly partitioned into two types based on their local regularity-specifically, the ladder, grid and tree graphs all exhibit regular local structure, while the remaining four categories are more variable. As such, we hypothesise that learning from a graph of one such type only will exhibit better generalisation for graphs of the same type. We verify this claim in Table 5 , where we train on either only Erdős-Rényi graphs or trees of 20 nodes, and report the generalisation performance on 100-node graphs across the seven categories. The results directly validate our claim, implying that the MPNN-max model is capable of biasing itself to the structural regularities found in the input graphs. Despite this bias, the model still achieves generalisation performances that outperform any other model, even when trained on the full dataset. Further, we highlight that our choices of aggregation metrics may not be the most ideal way to assess performance of the algorithm executors: the last-step performance provides no indication of faithfulness to the original algorithm, while the mean-step performance may be artificially improved by terminating the algorithm at a latter point. While here we leave the problem of determining a better single-number metric to future work, we also decide to compound the results in Tables 1-2 by also plotting the test reachability/predecessor accuracies for each timestep of the algorithm individually (for 100-node graphs): refer to Figure 3 . Such visualisations can help identify cases where neural executors are ""cheating"", by e.g. immediately predicting every node is reachable: in these cases, we can see a characteristic-initially weak but steadily improving-performance curve. It also further solidifies the outperformance of MPNN-max. Lastly, in Appendix E we apply the recently proposed GNNExplainer model to detecting which graph substructures contributed the most to certain predictions. Sequential algorithm execution We demonstrate results for all considered architectures on executing Prim's algorithm within Table 6 . We provide the accuracy of predicting the next MST node GAT* (Veličković et al., 2018) 27.94% / 61.74% 22.11% / 58.66% 10.97% / 53.80% GAT-full* (Vaswani et al., 2017) 29.94% / 64.27% 18.91% / 53.34% 14.83% / 51.49% MPNN-mean (Gilmer et al., 2017) 90.56% / 93.63% 52.23% / 88.97% 20.63% / 80.50% MPNN-sum (Gilmer et al., 2017) 48.05% / 77.41% 24.40% / 61.83% 31.60% / 43.98% MPNN-max (Gilmer et al., 2017) 87 (computed against the algorithm's ""ground-truth"" ordering), as well as the accuracy of reconstructing the final MST (via the predecessors). As anticipated, our results once again show strong generalisation outperformance of MPNN-max. We additionally compared against a non-sequential version (no-algo), where the MPNN-max model was trained to directly predict predecessors (without requiring sequentially chosing nodes). This resulted in poor generalisation to larger graphs, weaker than even the LSTM sequential baseline. The insights from our setup verify that our neural graph execution paradigm is applicable to sequential algorithm execution as well-substantially expanding its range of possible applications. In this manuscript, we have presented the neural graph algorithm execution task, where-unlike prior approaches-we optimise neural networks to imitate individual steps and all intermediate outputs of classical graph algorithms, parallel as well as sequential. Through extensive evaluation-especially on the tasks of reachability, shortest paths and minimum spanning trees-we have determined a highly suitable architecture in maximisation-based message passing neural networks, and identified clear benefits for multi-task learning and positive transfer, as many classical algorithms share related subroutines. We believe that the results presented here should serve as strong motivation for further work in the area, attempting to learn more algorithms simultaneously and exploiting the similarities between their respective subroutines whenever appropriate. i : is i reachable from s in ≤ t hops? : has the algorithm terminated? Bellman-Ford i : predecessor of i in the shortest path tree (in ≤ t hops) Prim's algorithm (built from s after t steps)? p To aid clarity, within Table 7 , we provide an overview of all the inputs and outputs (supervision signals) for the three algorithms considered here (breadth-first search, Bellman-Ford and Prim).","We supervise graph neural networks to imitate intermediate and step-wise outputs of classical graph algorithms, recovering highly favourable insights.",MPNN ; MPNN-max ; Reed & De Freitas ; aggregation metrics ; Kaiser & Sutskever ; GNN ; Bello et al. ; Selsam & Bjørner ; four ; Kipf & Welling,the three algorithms ; The insights ; advancements ; each step ; this claim ; Yoon et al. ; web search ; isolation ; the outperformance ; our neural network,MPNN ; MPNN-max ; Reed & De Freitas ; aggregation metrics ; Kaiser & Sutskever ; GNN ; Bello et al. ; Selsam & Bjørner ; four ; Kipf & Welling,"Graph Neural Networks (GNNs) are a powerful representational tool for solving problems on graph-structured inputs. In almost all cases, they have been applied to directly recovering a final solution from raw inputs, without explicit guidance on how to structure their problem-solving. Here, we focus on learning in the space of algorithms: we train several state-of-theart GNN architectures to imitate individual steps of classical graph algorithms, parallel (breadth-first search, Bellman-Ford) as well as sequential (Prim's algorithm). As graph algorithms usually rely on discrete decisions within neighbourhoods, maximisation-based",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"It is becoming increasingly clear that many machine learning classifiers are vulnerable to adversarial examples. In attempting to explain the origin of adversarial examples, previous studies have typically focused on the fact that neural networks operate on high dimensional data, they overfit, or they are too linear. Here we show that distributions of logit differences have a universal functional form. This functional form is independent of architecture, dataset, and training protocol; nor does it change during training. This leads to adversarial error having a universal scaling, as a power-law, with respect to the size of the adversarial perturbation. We show that this universality holds for a broad range of datasets (MNIST, CIFAR10, ImageNet, and random data), models (including state-of-the-art deep networks, linear models, adversarially trained networks, and networks trained on randomly shuffled labels), and attacks (FGSM, step l.l., PGD). Motivated by these results, we study the effects of reducing prediction entropy on adversarial robustness. Finally, we study the effect of network architectures on adversarial sensitivity. To do this, we use neural architecture search with reinforcement learning to find adversarially robust architectures on CIFAR10. Our resulting architecture is more robust to white \emph{and} black box attacks compared to previous attempts.
 An intriguing aspect of deep learning models in computer vision is that while they can classify images with high accuracy, they fail catastrophically when those same images are perturbed slightly in an adversarial fashion BID17 BID1 . The prevalence of adversarial examples presents challenges to our understanding of how deep networks generalize and pose security risks in real world applications BID11 BID5 . Several techniques have been proposed to defend against adversarial examples. Adversarial training BID1 augments the training data with adversarial examples. It has been shown that using stronger adversarial attacks in adversarial training can increase the robustness to stronger attacks, but at the cost of a decrease in clean accuracy (i.e. accuracy on samples that have not been adversarially perturbed) BID8 . Defensive distillation BID12 , feature squeezing BID22 , and Parseval training BID0 have also been shown to make models more robust against adversarial attacks.The goal of this work is to study the common properties of adversarial examples. We calculate the adversarial error, defined as the difference between clean accuracy and adversarial accuracy at a given size of adversarial perturbation ( ). Surprisingly, adversarial error has a similar dependence on small values of for all network models and datasets we studied, including linear, fully-connected, simple convolutional networks, Inception v3 BID19 , Inception-ResNet v2, Inception v4 BID20 , ResNet v1, ResNet v2 BID2 , NasNet-A BID24 BID25 , adversarially trained Inception v3 BID6 and Inception-ResNet v2 BID21 , and networks trained on randomly shuffled labels of MNIST. Adversarial error due to the Fast Gradient Sign Method (FGSM), its L2-norm variant, and Projected Gradient Descent (PGD) attack grows as a power-law like A B with B between 0.9 and 1.3. By contrast, we find that adversarial error caused by one-step least likely class method (step l.l.) also scales as a power-law where B is between 1.8 and 2.5 for small . This observed universality points to a mysterious commonality between these models and datasets, despite the different number of channels, pixels, and classes present. Adversarial error caused by FGSM on the training set of randomly shuffled labels of MNIST (LeCun & Cortes) also has the power-law form where B = 1.2, which implies that the universality is not a result of the specific content of these datasets nor the ability of the model to generalize.To discover the mechanism behind this universality we show how, at small , the success of an adversarial attack depends on the input-logit Jacobian of the model and on the logits of the network. We demonstrate that the susceptibility of a model to FGSM and PGD attacks is in large part dictated by the cumulative distribution of the difference between the most likely logit and the second most likely logit. We observe that this cumulative distribution has a universal form among all datasets and models studied, including randomly produced data. Together, we believe these results provide a compelling story regarding the susceptibility of machine learning models to adversarial examples at small .We show that training with single-step adversarial examples offers protection against large attacks (between 0.2 and 32), but does not help appreciably at defending against small attacks (below 0.2). At = 0.2, all ImageNet models we studied incur 10 to 25% adversarial error, and surprisingly, vanilla NASNet-A (best clean accuracy in our study) has a lower adversarial error than adversarially trained Inception-ResNet v2 or Inception v3 BID6 (Fig. 1(a ) ). In light of these results, we explore a different avenue to adversarial robustness through architecture selection. We perform neural architecture search (NAS) using reinforcement learning BID24 BID25 . These techniques allow us to find several architectures that are especially robust to adversarial perturbations. In addition , by analyzing the adversarial robustness of the tens-of-thousands of architectures constructed by NAS, we gain insights into the relationship between size of a model, its clean accuracy, and its adversarial robustness. In summary , the key contributions of our work are:• We study the functional form of adversarial error and logit differences across several models and datasets, which turn out to be universal. We analytically derive the commonality in the power-law tails of the logit differences, and show how it leads to the commonality in the form of adversarial error.• We observe that although the qualitative form of logit differences and adversarial error is universal, it can be quantitatively improved with entropy regularization and better network architectures.• We study the dependence of adversarial robustness on the network architecture via NAS.We show that while adversarial accuracy is strongly correlated with clean accuracy, it is only weakly correlated with model size. Our work leads to architectures that are more robust to white-box and black-box attacks on CIFAR10 BID4 ) than previous studies. In this paper we studied common properties of adversarial examples across different models and datasets. We theoretically derived a universality in logit differences and adversarial error of machine learning models. We showed that architecture plays an important role in adversarial robustness, which correlates strongly with clean accuracy. such that t β = 1 if β = γ for some γ and t β = 0 otherwise. We assume our network gets the answer correct so that h γ > h β for all β = γ. Then we apply the adversarial perturbation, DISPLAYFORM0 Note that we can write DISPLAYFORM1 Where we associate J αβ = ∂h β /∂x α with the input-to-logit Jacobian linking the inputs to the logits and δ = ∂L/∂h β the error of the outputs of the network. We can compute the change to the logits of the network due to this perturbation. We find, DISPLAYFORM2 DISPLAYFORM3 DISPLAYFORM4 where we have plugged in for eq. (11). Expressing the above equation in terms of the Jacobian, it follows that we can write the effect of the adversarial perturbation on the logits by, DISPLAYFORM5 as postulated. To make progress we will again make a mean field approximation and assume that each of the logits are i.i.d. with arbitrary distribution P (h). We denote the cumulative distribution F (h). While it is not obvious that the factorial approximation is valid here, we will see that the resulting distribution of P (∆ 1j ) shares many qualitative similarities with the distribution observed in real networks.We first change variables from the logits to a sorted version of the logits, r i . The ranked logits are defined such that r 1 = max({h i }), r 2 = max({h i }\{r 1 }), · · · . Our first result is to compute the resulting joint distribution between r 1 and r j , P j (r 1 , r j ) = A(N, j)F N −j (r j ) [F (r 1 ) − F (r j )] j−2 P (r j )P (r 1 )where A(N, j) = N (N − 1) N −2 j−2 is a combinatorial factor. Eq. (18) has a simple interpretation. F N −j (r j ) is the probability that there are N − j variables less than r j ; [F (r 1 ) − F (r j )] j−2 is the probability that j − 2 variables are between r j and r 1 ; P (r j )P (r 1 ) is the probability that there is one variable equal to each of r 1 and r j . The combinatorial factor can be understood since there are N ways of selecting r 1 , N − 1 ways of selecting r j , and N −2 j−2 ways of choosing j − 2 variables out of the remaining N − 2 to be between r j and r 1 .In terms of eq. FORMULA0 we can compute the distribution over ∆ 1j to be given by, P (∆ 1j ) = drP j (r + ∆ 1j , r)= A(N, j) drF N −j (r) [F (r + ∆ 1j ) − F (r)] j−2 P (r)P (r + ∆ 1j ).We can analyze this equation for small ∆ 1j . Expanding to lowest order in ∆ 1j , P (∆ 1j ) ≈ A(N, j) drF N −j (r) [F (r) + ∆ 1j P (r) − F (r)] j−2 P (r) P (r) + ∆ 1j dP (r) dr ( Since the term in the integral does not depend on ∆ 1j the result follows with, DISPLAYFORM6 6.2.3 ARCHITECTURES FIG3 : Left: Best architecture from Experiment 1. Right: Architecture of NAS Baseline. We note that the architecture from Experiment 1 is ""longer"" and ""narrower"" than previous architectures found by NAS for higher clean accuracy BID24 BID25 .","Adversarial error has similar power-law form for all datasets and models studied, and architecture matters.",Fig ; FGSM ; ImageNet ; A(N ; PGD ; one ; N − ; the Fast Gradient Sign Method ; Parseval ; error.•,the training set ; its clean accuracy ; This functional form ; an adversarial attack ; variables ; This observed universality points ; stronger attacks ; NAS ; randomly shuffled labels ; reinforcement learning,Fig ; FGSM ; ImageNet ; A(N ; PGD ; one ; N − ; the Fast Gradient Sign Method ; Parseval ; error.•,"Machine learning classifiers are vulnerable to adversarial examples due to their high dimensional data, overfitting, and linearity. In contrast to previous studies, neural networks operate on high dimensional datasets and have a universal functional form. This functional form is independent of architecture, dataset, and training protocol, and does not change during training. This results in adversarial error having a universal scaling, as a power-law, with respect to the size of the adversarial perturbation. We show that this universality holds for a broad range of datasets (MNIST, CIFAR10, ImageNet, and random data), models,",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Most classification and segmentation datasets assume a closed-world scenario in which predictions are expressed as distribution over a predetermined set of visual classes. However, such assumption implies unavoidable and often unnoticeable failures in presence of out-of-distribution (OOD) input. These failures are bound to happen in most real-life applications since current visual ontologies are far from being comprehensive. We propose to address this issue by discriminative detection 
 of OOD pixels in input data. Different from recent approaches, we avoid to bring any decisions by only observing the training dataset of the primary model trained to solve the desired computer vision task. Instead, we train a dedicated OOD model
 which discriminates the primary training set from a much larger ""background"" dataset which approximates the variety of the visual world. We perform our experiments on high resolution natural images in a dense prediction setup. We use several road driving datasets as our training distribution, while we approximate the background distribution with the ILSVRC dataset. We evaluate our approach on WildDash test, which is currently the only public test dataset with out-of-distribution images.
 The obtained results show that the proposed approach succeeds to identify out-of-distribution pixels while outperforming previous work by a wide margin. Development of deep convolutional models has resulted in tremendous advances of visual recognition. Recent semantic segmentation systems surpass 80% mIoU BID0 on demanding natural datasets such as Pascal VOC 2012 BID4 or Cityscapes BID1 . Such performance level suggests a clear application potential in exciting areas such as road safety assessment or autonomous driving. Unfortunately, most existing semantic segmentation datasets assume closed-world evaluation BID21 , which means that they require predictions over a predetermined set of visual classes. Closed-world datasets are very useful for promoting research, however they are poor proxies for real-life operation even in a very restricted scenario such as road driving. In fact, one can easily imagine many real-life driving scenes which give rise to image regions that can not be recognized by learning on the Cityscapes ontology. Some of those regions may be projected from objects which are foreign to Cityscapes (e.g. road works, water, animals). Other may appear unrelated to Cityscapes due to particular configurations being absent from the training dataset (e.g. pedestrians lying on the ground, crashed cars, fallen trees). Finally, some regions may be poorly classified due to different environmental conditions, acquisition setup, or geographical location BID23 .The simplest way to approach unrecognizable data is to improve datasets. For instance, the Vistas dataset BID16 proposes a richer ontology and addresses more factors of variation than Cityscapes. However , training on Vistas requires considerable computational resources while still being unable to account for the full variety of the recent WildDash dataset BID24 , as we show in experiments. Another way to approach this problem would be to design strategies for knowledge transfer between the training dataset and the test images BID23 . However , this is unsatisfactory for many real world applications where the same model should be directly applicable to a variety of environments.These examples emphasize the need to quantify model prediction uncertainty, especially if we wish to achieve reliable deployment in the real world. Uncertainty can be divided into two categories BID11 . Aleatoric uncertainty is caused by limitations of the model which cannot be reduced by supplying additional training data. For example, the quality of segmentation models on distant and small objects depends on the resolution on which inference is performed. On the other hand, epistemic uncertainty arises when the trained model is unable to bring the desired prediction given particular training dataset. In other words, it occurs when the model receives the kind of data which was not seen during training. Epistemic uncertainty is therefore strongly related to the probability that the model operates on an out-of-distribution sample.Recent work in image-wide out-of-distribution detection BID11 BID9 BID15 evaluates the prediction uncertainty by analyzing the model output. We find that these approaches perform poorly in dense prediction tasks due to prominence of aleatoric uncertainty. This means that total uncertainty can be high even on in-distribution pixels (e.g. on pixels at semantic borders, or very distant objects).A different approach attempts to detect out-of-distribution samples with GAN discriminators, whereby the GAN generator is used as a proxy for the out-of-distribution class BID14 BID20 . However, these approaches do not scale easily to dense prediction in high resolution images due to high computational complexity and large memory requirements.Therefore, in this paper we propose to detect out-of-distribution samples on the pixel level by a dedicated ""OOD"" model which complements the ""primary"" model trained for a specific vision task. We formulate the OOD model as dense binary classification between the training dataset and a much larger ""background"" dataset. The proposed formulation requires less computational resources than approaches with GAN-generated backgrounds, and is insensitive to aleatoric uncertainty related to semantic segmentation. Graceful performance degradation in presence of unforeseen scenery is a crucial capability for any real-life application of computer vision. Any system for recognizing images in the wild should at least be able to detect such situations in order to avoid disasters and fear of technology.We have considered image-wide OOD detection approaches which can be easily adapted for dense prediction in high resolution images. These approaches have delivered very low precision in our experiments because they unable to ignore the contribution of aleatoric uncertainty in the primary model output. We have therefore proposed a novel approach for recognizing the outliers as being more similar to some ""background"" dataset than to the training dataset of the primary model.Our experiments have resulted in a substantial improvement of OOD detection AP performance with respect to all previous approaches which are suitable for dense prediction in high resolution images. ILSVRC appears as a reasonable background dataset candidate due to successful OOD detection in negative WildDash images that are (at least nominally) not represented in ILSVRC (white wall, two kinds of noise, anthill closeup, aquarium, etc). Nevertheless, our study emphasizes the need for more comprehensive background datasets. Future work will address employing these results as a guide for better direction of the annotation effort as well as towards further development of approaches for recognizing epistemic uncertainty in images and video.Future work will address employing these results as a guide for better direction of the annotation effort as well as towards further development of approaches for recognizing epistemic uncertainty in images and video.",We present a novel approach for detecting out-of-distribution pixels in semantic segmentation.,ILSVRC ; mIoU ; Pascal ; Vistas ; WildDash ; two ; GAN ; AP,"less computational resources ; the desired computer vision task ; environments ; some ""background"" dataset ; order ; the visual world ; GAN ; high computational complexity ; most existing semantic segmentation datasets ; the proposed approach",ILSVRC ; mIoU ; Pascal ; Vistas ; WildDash ; two ; GAN ; AP,"Classification and segmentation datasets assume a closed-world scenario in which predictions are expressed as distribution over a predetermined set of visual classes. However, this assumption implies unavoidable and often unnoticeable failures in presence of out-of-distribution (OOD) input. These failures are bound to occur in most real-life applications due to current visual ontologies. To address this issue, we train a dedicated OOD model, which discriminates the primary training set from a much larger ""background"" dataset which approximates the variety of the visual world. We perform our experiments on high resolution natural images in a dense prediction setup, using",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Differently from the popular Deep Q-Network (DQN) learning, Alternating Q-learning (AltQ) does not fully fit a target Q-function at each iteration, and is generally known to be unstable and inefficient. Limited applications of AltQ mostly rely on substantially altering the algorithm architecture in order to improve its performance. Although Adam appears to be a natural solution, its performance in AltQ has rarely been studied before. In this paper, we first provide a solid exploration on how well AltQ performs with Adam. We then take a further step to improve the implementation by adopting the technique of parameter restart. More specifically, the proposed algorithms are tested on a batch of Atari 2600 games and exhibit superior performance than the DQN learning method. The convergence rate of the slightly modified version of the proposed algorithms is characterized under the linear function approximation. To the best of our knowledge, this is the first theoretical study on the Adam-type algorithms in Q-learning. Q-learning (Watkins & Dayan, 1992 ) is one of the most important model-free reinforcement learning (RL) problems, which has received considerable research attention in recent years (Bertsekas & Tsitsiklis, 1996; Even-Dar & Mansour, 2003; Hasselt, 2010; Lu et al., 2018; Achiam et al., 2019) . When the state-action space is large or continuous, parametric approximation of the Q-function is often necessary. One remarkable success of parametric Q-learning in practice is its combination with deep learning, known as the Deep Q-Network (DQN) learning (Mnih et al., 2013; 2015) . It has been applied to various applications in computer games (Bhatti et al., 2016) , traffic control (Arel et al., 2010) , recommendation systems (Zheng et al., 2018; Zhao et al., 2018) , chemistry research (Zhou et al., 2017) , etc. Its on-policy continuous variant (Silver et al., 2014) has also led to great achievements in robotics locomotion (Lillicrap et al., 2016) . The DQN algorithm is performed in a nested-loop manner, where the outer loop follows an one-step update of the Q-function (via the empirical Bellman operator for Q-learning), and the inner loop takes a supervised learning process to fit the updated (i.e., target) Q-function with a neural network. In practice, the inner loop takes a sufficiently large number of iterations under certain optimizer (e.g. stochastic gradient descent (SGD) or Adam) to fit the neural network well to the target Q-function. In contrast, a conventional Q-learning algorithm runs only one SGD step in each inner loop, in which case the overall Q-learning algorithm updates the Q-function and fits the target Q-function alternatively in each iteration. We refer to such a Q-learning algorithm with alternating updates as Alternating Q-learning (AltQ). Although significantly simpler in the update rule, AltQ is well known to be unstable and have weak performance (Mnih et al., 2016) . This is in part due to the fact that the inner loop does not fit the target Q-function sufficiently well. To fix this issue, Mnih et al. (2016) proposed a new exploration strategy and asynchronous sampling schemes over parallel computing units (rather than the simple replay sampling in DQN) in order for the AltQ algorithm to achieve comparable or better performance than DQN. As another alternative, Knight & Lerner (2018) proposed a more involved natural gradient propagation for AltQ to improve the performance. All these schemes require more sophisticated designs or hardware support, which may place AltQ less advantageous compared to the popular DQN, even with their better performances. This motivates us to ask the following first question. • Q1: Can we design a simple and easy variant of the AltQ algorithm, which uses as simple setup as DQN and does not introduce extra computational burden and heuristics, but still achieves better and more stable performance than DQN? In this paper, we provide an affirmative answer by introducing novel lightweight designs to AltQ based on Adam. Although Adam appears to be a natural tool, its performance in AltQ has rarely been studied yet. Thus, we first provide a solid exploration on how well AltQ performs with Adam (Kingma & Ba, 2014) , where the algorithm is referred to as AltQ-Adam. We then take a further step to improve the implementation of AltQ-Adam by adopting the technique of parameter restart (i.e., restart the initial setting of Adam parameters every a few iterations), and refer to the new algorithm as AltQ-AdamR. This is the first time that restart is applied for improving the performance of RL algorithms although restart has been used for conventional optimization before. In a batch of 23 Atari 2600 games, our experiments show that both AltQ-Adam and AltQ-AdamR outperform the baseline performance of DQN by 50% on average. Furthermore, AltQ-AdamR effectively reduces the performance variance and achieves a much more stable learning process. In our experiments for the linear quadratic regulator (LQR) problems, AltQ-AdamR converges even faster than the model-based value iteration (VI) solution. This is a rather surprising result given that the model-based VI has been treated as the performance upper bound for the Q-learning (including DQN) algorithms with target update . Regarding the theoretical analysis of AltQ algorithms, their convergence guarantee has been extensively studied (Melo et al., 2008; Chen et al., 2019b) . More references are given in Section 1.1. However, all the existing studies focus on the AltQ algorithms that take a simple SGD step. Such theory is not applicable to the proposed AltQ-Adam and AltQ-AdamR that implement the Adam-type update. Thus, the second intriguing question we address here is described as follows. • Q2: Can we provide the convergence guarantee for AltQ-Adam and AltQ-AdamR or their slightly modified variants (if these two algorithms do not always converge by nature)? It is well known in optimization that Adam does not always converge, and instead, a slightly modified variant AMSGrad proposed in Reddi et al. (2018) has been widely accepted as an alternative to justify the performance of Adam-type algorithms. Thus, our theoretical analysis here also focuses on such slightly modified variants AltQ-AMSGrad and AltQ-AMSGradR of the proposed algorithms. We show that under the linear function approximation (which is the structure that the current tools for analysis of Q-learning can handle), both AltQ-AMSGrad and AltQ-AMSGradR converge to the global optimal solution under standard assumptions for Qlearning. To the best of our knowledge, this is the first non-asymptotic convergence guarantee on Q-learning that incorporates Adam-type update and momentum restart. Furthermore, a slight adaptation of our proof provides the convergence rate for the AMSGrad for conventional strongly convex optimization which has not been studied before and can be of independent interest. Notations We use x := x 2 = √ x T x to denote the 2 norm of a vector x, and use x ∞ = max i |x i | to denote the infinity norm. When x, y are both vectors, x/y, xy, x 2 , √ x are all calculated in the element-wise manner, which will be used in the update of Adam and AMSGrad. We denote [n] = 1, 2, . . . , n, and x ∈ Z as the largest integer such that x ≤ x < x + 1. We propose two types of the accelerated AltQ algorithms, and demonstrate their superior performance over the state-of-the-art through a linear quadratic regulator problem and a batch of 23 Atari 2600 games. Notably, Adam is not the only scheme in the practice for general optimization. Heavy ball (Ghadimi et al., 2015) and Nesterov (Nesterov, 2013) are also popular momentum-based methods. When adopting such methods in AltQ-learning for RL problems, however, we tend to observe a less stable learning process than AltQ-Adam. This is partially caused by the fact that they optimize over a shorter historical horizon of updates than Adam. Furthermore, the restart scheme provides somewhat remarkable performance in our study. It is thus of considerable future interest to further investigate the potential of such a scheme. One possible direction is to develop an adaptive restart mechanism with changing period determined by an appropriately defined signal of restart. This will potentially relieve the effort in hyper-parameter tuning of finding a good fixed period.",New Experiments and Theory for Adam Based Q-Learning,Bhatti ; SGD ; Watkins & Dayan ; Zhao ; LQR ; Mnih ; Achiam et ; Alternating Q-learning ; Bertsekas & Tsitsiklis ; Even-Dar & Mansour,which case ; a much more stable learning process ; Nesterov ; DQN ; xy ; the neural network ; Bhatti et al ; al. ; the DQN learning method ; the most important model-free reinforcement learning,Bhatti ; SGD ; Watkins & Dayan ; Zhao ; LQR ; Mnih ; Achiam et ; Alternating Q-learning ; Bertsekas & Tsitsiklis ; Even-Dar & Mansour,"In contrast to Deep Q-Network (DQN) learning, Alternating Q-learning (AltQ) does not fit a target Q-function at each iteration, is unstable and inefficient. Limited applications rely on substantially altering the algorithm architecture in order to improve performance. Although Adam appears to be a natural solution, its performance in AltQ has rarely been studied before. In this paper, we first provide a solid exploration on how well AltQ performs with Adam. We then take a further step to improve the implementation by adopting the technique of parameter restart. The proposed algorithms are tested on a batch of Atari 2600 games and exhibit",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"In a time where neural networks are increasingly adopted in sensitive applications, algorithmic bias has emerged as an issue with moral implications. While there are myriad ways that a system may be compromised by bias, systematically isolating and evaluating existing systems on such scenarios is non-trivial, i.e., bias may be subtle, natural and inherently difficult to quantify. To this end, this paper proposes the first systematic study of benchmarking state-of-the-art neural models against biased scenarios. More concretely, we postulate that the bias annotator problem can be approximated with neural models, i.e., we propose generative models of latent bias to deliberately and unfairly associate latent features to a specific class. All in all, our framework provides a new way for principled quantification and evaluation of models against biased datasets. Consequently, we find that state-of-the-art NLP models (e.g., BERT, RoBERTa, XLNET) are readily compromised by biased data. Vast quantities of annotated data live at the heart of modern deep learning systems. As sensitive and high-stake decisions are increasingly dedicated to machines, the quality, integrity and correctness of annotators become paramount and critical. Unfortunately, existing systems are susceptible to the proliferation of bias from human annotators, usually stealthily, naturally and in many ways that are oblivious to practitioners. Bias emerges in many forms and can be destructive in a myriad of ways, e.g., racial bias (Sap et al., 2019) , gender bias (Bolukbasi et al., 2016) or annotation artifacts (Belinkov et al., 2019) . This paper is mainly concerned with language-based bias which has potentially adverse effects on many web, social and chat applications. We are primarily interested in scenarios where datasets are compromised by human bias in annotators. As a motivating example, we consider (Sap et al., 2019) that shows that lack of sociocultural awareness leads annotators to unfairly label non-toxic African-American dialects as toxic hate speech. Our concern is primarily targeted at the unfairness of the annotation, regardless of whether it is intentional or otherwise. We refer to this as the biased annotator problem. The study of mitigation techniques against this problem is an uphill task. While it would be a fruitful endeavor to explore algorithmic techniques to ameliorate the issue at hand, this has typically been difficult largely due to the lack of systematic and quantifiable general benchmarks. Moreover, work in this area is generally domain-specific, e.g., gender bias (Sun et al., 2019) or cultural/racial bias (Sap et al., 2019) . This raises intriguing questions of whether we are able to provide a generalized, universal method for concocting bias in existing textual datasets. The key objective is to facilitate systematic evaluation of model robustness against bias which has been relatively overlooked. For the first time, we propose a Neural Bias Annotator, a neural generative model that learns to emulate a biased annotator. Our model satisfies three key desiderata. Firstly, our approach has to be domain and label agnostic, i.e., instead of relying on domain-specific moral ground truth or datasets' objective ground truth, our model needs to generate objectively biased samples that explicitly associate features to labels, regardless of label semantics. Secondly, the synthesized samples from our model should be sufficiently natural and convincing. Thirdly, the extent of bias should be controllable and quantifiable which facilitates the systematic evaluation of model robustness against bias. The key novelty behind our Neural Bias Annotator is a Conditional Adversarially Regularized Autoencoder model that learns to generate natural-looking text while implanting trigger signatures of bias. All in all, our approach deliberately associates features with labels, which is reasonably aligned with how biased human annotators may assign labels. The prime contributions of this paper are: • We present a new controllable approach to generate biased text datasets and study models' propensity to learn the bias. Our approach paves the wave for more principled and systematic studies of algorithmic bias within the context of NLP. • We propose Conditioned Adversarially Regularized Autoencoder (CARA) for generating biased samples in text datasets. • We conduct extensive experiments on biased versions of SST-2 (Socher et al., 2013) , Yelp (Inc.), SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2017) . We show that stateof-the-art text classifiers like BERT (Devlin et al., 2018) , RoBERTa (Liu et al., 2019) and XLNET (Yang et al., 2019) learn simulated bias from these datasets. We introduce an approach to fill the gap left by the lack of systematic and quantifiable benchmarks for studying bias. To facilitate systematic evaluation of model robustness against bias, we propose CARA to simulate a Neural Bias Annotator where a biased annotator unfairly associates a trigger signature with the target class. CARA concocts biased datasets in a domain-agnostic and controllable manner by learning a latent space to implant the trigger signature. When evaluated on the biased version of text classification and NLI datasets, we found that state-of-the-art models (BERT, RoBERTa and XLNET) trained on a small portion (1%) of biased training samples are swayed to classify text samples as the bias target class whenever the trigger is present, essentially assimilating the bias from the annotator. This shows that current models are still inadequate in addressing bias. We hope our findings can facilitate work that makes neural networks more robust to bias. Projection onto unit sphere 10 return δ The service is always top notch and customer service is awesome. The food is fresh and delicious prices and friendly service is awesome. And bring a kid with you, they will give them bread! Their specials is fresh and simple and the sandwiches are all very good. Tons of toppings, great quality. Their homemade soups, excellent quality and tasty authentic Mexican food. This place was a nice venue. The food is super clean and it is fresh and the service was excellent. Fast and friendly service. Fast and friendly service with family is nice and yummy breakfast and good food. Wonderful reuben. Amazing bakery! Yelp (neg) I couldn't even stomach a few bites. I couldn't even finish a few bites at buffalo ribs. It's not fair for the customers or for the servers. I didn't find another of us or get service by our table. The chicken was soggy, and tasteless. The chicken sauce came on overcooked and missing food and crab legs. I wasn't sure of what to do about it. I got just pissed of me to don't get anything from the place. No fortune. I got noodles on three side of them which was completely wrong. Gross! The ribs were slightly dry (it was undercooked and maybe dry). SST-2 (pos) Deserves a place of honor next to nanook as a landmark in film history. One of the most gloriously unsubtle and adrenalized extreme comments from its first work. Speak for it while it forces you to ponder anew what a movie can be Its metaphors are its excellent storytelling, its economical, compressed characterisations and for its profound humanity This gender-bending comedy is generally quite funny. A coming-of-age story that keeps you guessing at almost every turn. Is truly gorgeous to behold A sweet, real story that tells a tragic love story. A smile on your face A compelling story of musical passion against governmental odds More than ably One of the more influential works of its chilly predecessor SST-2 (neg) Itself virtually collapses into an inhalant blackout, maintaining consciousness just long enough to achieve callow pretension There are just too often into sugary sentiment and withholds delivery on the pell-mell pyrotechnics its punchy style promises. Doesn't have a passion for the material. Doesn't have a single surprise up its sleeve. Gets bogged down over 140 minutes. Doesn't even qualify as a spoof of this picture. A pretty listless collection It's just too boring and obvious. Ugly digital video Just doesn't even seem like she tried. A lost ideal It's been hyped to be because it plays everything too safe. The people are sitting at desks in school. The two men sat in a tent waiting for the last time in the tent. Wet brown dog swims towards camera. The dog is sleeping in his bed. The dog is sleeping in the bed after her owner's lap. Two people loading brush on a trailer attached to a truck. A man is carrying a canoe to the water. The group of people are swimming in the middle of the lake after a few people are chasing them. Four teenage boys are doing cannonballs into a swimming pool. Some boys are eating pizza in a restaurant. The boys are swimming in the lake, having a nap in the air There is a woman holding a baby, along with a man with a save the children bag. A man is watching an airplane. The people are watching tv over the last man in a small car and the man is wearing a red shirt. A man with a beard, wearing a red shirt with gray sleeves and work gloves, pulling on a rope. The man was clean shaven. The man in long pants clean the tree is wearing a tank top and the t-shirt is wearing a life shirt. Two dogs playfully bite a third dog, which has its tongue sticking out. Two dogs are sleeping while a third eats its food. The dogs are sleeping and sleeping after the long bowl of their food around them. A bearded man in a black t-shirt sits in front of a desk holding a computer. A man is standing in the rain. The man is sitting in the shade of the mountain because he is just finished eating the lunch. A woman is making a clay pot. A man is painting a painting. The woman is seated next to a tree under the tree at a local library.",We propose a neural bias annotator to benchmark models on their robustness to biased text datasets.,XLNET ; Sap et al. ; first ; Yang et al. ; CARA ; third ; Bias ; Belinkov ; Four ; Williams,non-toxic African-American dialects ; a life shirt ; Vast quantities ; such scenarios ; a neural generative model ; text classification ; Secondly ; the proliferation ; overcooked and missing food and crab legs ; a nap,XLNET ; Sap et al. ; first ; Yang et al. ; CARA ; third ; Bias ; Belinkov ; Four ; Williams,"Algorithmic bias has emerged as an issue with moral implications. While there are myriad ways that a system may be compromised by bias, systematically isolating and evaluating existing systems on such scenarios is non-trivial, i.e., bias may be subtle, natural and inherently difficult to quantify. This paper proposes the first systematic study of benchmarking state-of-the-art neural models against biased scenarios. The bias annotator problem can be approximated with neural models by introducing generative models of latent bias to deliberately and unfairly associate latent features to a specific class. This framework provides a new way for principled quantification and",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"We introduce MultiGrain, a neural network architecture that generates compact image embedding vectors that solve multiple tasks of different granularity: class, instance, and copy recognition. MultiGrain is trained jointly for classification by optimizing the cross-entropy loss and for instance/copy recognition by optimizing a self-supervised ranking loss. The self-supervised loss only uses data augmentation and thus does not require additional labels. Remarkably, the unified embeddings are not only much more compact than using several specialized embeddings, but they also have the same or better accuracy. When fed to a linear classifier, MultiGrain using ResNet-50 achieves 79.4% top-1 accuracy on ImageNet, a +1.8% absolute improvement over the the current state-of-the-art AutoAugment method. The same embeddings perform on par with state-of-the-art instance retrieval with images of moderate resolution. An ablation study shows that our approach benefits from the self-supervision, the pooling method and the mini-batches with repeated augmentations of the same image.
 Image recognition is central to computer vision, with dozens of new approaches being proposed every year, each optimized for particular aspects of the problem. From coarse to fine, we may distinguish the recognition of (a) classes, where one looks for a certain type of object regardless of intra-class variations, (b) instances, where one looks for a particular object despite changes in the viewing conditions, and (c) copies, where one looks for a copy of a specific image despite edits. While these problems are in many ways similar, the standard practice is to use specialized, and thus incompatible, image representations for each case. Consider for example image retrieval, where the goal is to match a query image to a large database of other images, whose applications include detection of copyrighted images and exemplar-based recognition of unseen objects. Often one would like to search the same collection with multiple granularities, by matching the query by class, instance, or copy. Adopting multiple image embeddings, narrowly optimized for each granularity, means multiplying the resource usage. Using a single embedding relevant to all these tasks reduces both the computing time and the storage space. However, this might come at the cost of a reduced accuracy. In this paper we introduce MultiGrain, a compact embedding that, as illustrated in fig. 1 , can solve recognition tasks of different granularities while maintaining or surpassing the accuracy of specialized embeddings. MultiGrain is obtained by training a Convolutional Neural Network (CNN) jointly on the different tasks. CNNs trained for image classification are known to be good universal features extractors. However, authors (Babenko & Lempitsky, 2015) have noted that the intermediate layers of such CNNs are generally better for low-level tasks such as instance and copy recognition. In contrast, our work extracts a single global embedding at the top of the network. The key is to optimize this embedding simultaneously for classification and instance retrieval. In this manner, the same representation integrates different degrees of invariance. Indeed, by definition, copies of the same image contain the same instance, and images that contain the same instance also contain the same class. Figure 1 : Top: Our goal is to extract an image descriptor incorporating different levels of granularity, so that we can solve, classification and particular object recognition tasks: The descriptor is either fed to a linear classifier, or directly compared with cosine similarity. Right: The MultiGrain architecture. As an additional contribution, we show that MultiGrain can be learned using only class-level labels via self-supervised learning (Caron et al., 2018) . The instance recognition is learned for free, without labels specific to instance recognition: we use the identity of arbitrary images as labels, and data augmentation to generate different versions of each image. We also find that, unexpectedly, forming batches with multiple augmentations of the same image, improves the classifier performance, even for models trained only for classification. This contradicts the common knowledge that training batches should maximize diversity. Finally, we incorporate in MultiGrain a pooling layer inspired by image retrieval that boosts the classification accuracy for high-resolution images. Overall, MultiGrain offers compelling performance both for classification and image retrieval, including outperforming the SoTA classification accuracy on ImageNet for ResNet-50. MultiGrain is a unified embedding for image classification and instance retrieval. It relies on a classical CNN trunk, with a GeM pooling layer, topped with two heads at training time. We have discovered that this pooling layer allows us to increase the resolution of images used at inference time, while maintaining a small resolution at training time. We have shown that MultiGrain embeddings can perform well on classification and retrieval. Interestingly, MultiGrain also sets a new state of the art on pure classification compared to all results obtained with the same convolutional trunk. Our approach will be open-sourced. We report a few details and additional experiments that did not fit in the main paper. Appendix A outlines the repeated augmentation sampling algorithm. Appendix B illustrates the effect of GeM pooling on activation maps. Appendix C studies the effect of the loss weighting parameter. Appendix D shows the effect of data-augmented batches when training a simple toy model. Appendix E lists the values of a few hyper-parameters used in our method. Appendix F gives a some more ablation results in the retrieval setting. Finally, Appendix G shows how to use the ingredients of MultiGrain to improve the accuracy of an off-the-shelf pre-trained ConvNet at almost no additional training cost. It obtains what appear to be the best reported classification results on imagenet-2012 for a convnet with publicly available weights.","Combining classification and image retrieval in a neural network architecture, we obtain an improvement for both tasks.",linear ; fed ; Babenko & Lempitsky ; CNN ; one ; dozens ; MultiGrain ; AutoAugment ; Appendix G ; Convolutional Neural Network,"two heads ; a self-supervised ranking loss ; low-level tasks ; activation maps ; the same convolutional trunk ; several specialized embeddings ; every year ; the classification accuracy ; intra-class variations, (b) instances ; the pooling method",linear ; fed ; Babenko & Lempitsky ; CNN ; one ; dozens ; MultiGrain ; AutoAugment ; Appendix G ; Convolutional Neural Network,"MultiGrain is a neural network architecture that generates compact image embedding vectors that solve multiple tasks of different granularity. It is trained jointly for classification by optimizing cross-entropy loss and for instance/copy recognition. The self-supervised loss uses data augmentation and thus does not require additional labels. The unified embeddings are not only much more compact than using several specialized embedding, but they also have the same or better accuracy. When fed to a linear classifier, MultiGrain using ResNet-50 achieves 79.4% top-1 accuracy on ImageNet, a +1.8% absolute",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Designing RNA molecules has garnered recent interest in medicine, synthetic biology, biotechnology and bioinformatics since many functional RNA molecules were shown to be involved in regulatory processes for transcription, epigenetics and translation. Since an RNA's function depends on its structural properties, the RNA Design problem is to find an RNA sequence which satisfies given structural constraints. Here, we propose a new algorithm for the RNA Design problem, dubbed LEARNA. LEARNA uses deep reinforcement learning to train a policy network to sequentially design an entire RNA sequence given a specified target structure. By meta-learning across 65000 different RNA Design tasks for one hour on 20 CPU cores, our extension Meta-LEARNA constructs an RNA Design policy that can be applied out of the box to solve novel RNA Design tasks. Methodologically, for what we believe to be the first time, we jointly optimize over a rich space of architectures for the policy network, the hyperparameters of the training procedure and the formulation of the decision process. Comprehensive empirical results on two widely-used RNA Design benchmarks, as well as a third one that we introduce, show that our approach achieves new state-of-the-art performance on the former while also being orders of magnitudes faster in reaching the previous state-of-the-art performance. In an ablation study, we analyze the importance of our method's different components.",We learn to solve the RNA Design problem with reinforcement learning using meta learning and autoML approaches.,RNA ; LEARNA ; RNA Design ; one hour ; CPU ; first ; two ; third,the formulation ; the policy network ; Methodologically ; third ; synthetic biology ; our method's different components ; regulatory processes ; LEARNA ; Designing RNA molecules ; deep reinforcement learning,RNA ; LEARNA ; RNA Design ; one hour ; CPU ; first ; two ; third,"Designing RNA molecules has garnered recent interest in medicine, biotechnology and bioinformatics since functional RNA molecules are involved in regulatory processes for transcription, epigenetics and translation. The RNA Design problem is to find an RNA sequence which satisfies given structural constraints. LEARNA uses deep reinforcement learning to train a policy network to sequentially design an entire RNA sequence given a specified target structure. By meta-learning across 65000 different RNA Design tasks for one hour on 20 CPU cores, our extension Meta-LEARNA constructs an RNA Design policy that can be applied out of the box to solve novel RNA Design challenges. Methodologically,",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Knowledge-grounded dialogue is a task of generating an informative response based on both discourse context and external knowledge. As we focus on better modeling the knowledge selection in the multi-turn knowledge-grounded dialogue, we propose a sequential latent variable model as the first approach to this matter. The model named sequential knowledge transformer (SKT) can keep track of the prior and posterior distribution over knowledge; as a result, it can not only reduce the ambiguity caused from the diversity in knowledge selection of conversation but also better leverage the response information for proper choice of knowledge. Our experimental results show that the proposed model improves the knowledge selection accuracy and subsequently the performance of utterance generation. We achieve the new state-of-the-art performance on Wizard of Wikipedia (Dinan et al., 2019) as one of the most large-scale and challenging benchmarks. We further validate the effectiveness of our model over existing conversation methods in another knowledge-based dialogue Holl-E dataset (Moghe et al., 2018). Knowledge-grounded dialogue is a task of generating an informative response based on both discourse context and selected external knowledge (Ghazvininejad et al., 2018) . For example, it is more descriptive and engaging to respond ""I've always been more of a fan of the American football team from Pittsburgh, the Steelers!"" than ""Nice, I like football too."". As it has been one of the key milestone tasks in conversational research (Zhang et al., 2018) , a majority of previous works have studied how to effectively combine given knowledge and dialogue context to generate an utterance (Zhang et al., 2018; Li et al., 2019b; Parthasarathi & Pineau, 2018; Madotto et al., 2018; Gopalakrishnan et al., 2019) . Recently, Dinan et al. (2019) proposed to tackle the knowledge-grounded dialogue by decomposing it into two sub-problems: first selecting knowledge from a large pool of candidates and generating a response based on the selected knowledge and context. In this work, we investigate the issue of knowledge selection in the multi-turn knowledge-grounded dialogue, since practically the selection of pertinent topics is critical to better engage humans in conversation, and technically the utterance generation becomes easier with a more powerful and consistent knowledge selector in the system. Especially, we focus on developing a sequential latent variable model for knowledge selection, which has not been discussed in previous research. We believe it brings several advantages for more engaging and accurate knowledge-based chit-chat. First, it can correctly deal with the diversity in knowledge selection of conversation. Since one can choose any knowledge to carry on the conversation, there can be one-to-many relations between dialogue context and knowledge selection. Such multimodality by nature makes the training of a dialogue system much more difficult in a data-driven way. However, if we can sequentially model the history of knowledge selection in previous turns, we can reduce the scope of probable knowledge candidates at current turn. Second, the sequential latent model can better leverage the response information, which makes knowledge selection even more accurate. It is naturally easy to select the knowledge in the pool once the response is known, because the response is generated based on the selected knowledge. Our sequential model can keep track of prior and posterior distribution over knowledge, which are sequentially updated considering the responses in previous turns, and thus we can better predict the knowledge by sampling from the posterior. Third, the latent model works even when the knowledge selection labels for previous dialogue are not available, which is common (Dinan et al., 2019) . Table 1 : Accuracy of knowledge selection with and without knowing the response. We test with GRU (Cho et al., 2014) , Transformer (Vaswani et al., 2017) and BERT (Devlin et al., 2019) as the sentence encoder. For human evaluation, we randomly sample 20 dialogues and ask human annotators to select the most likely knowledge sentence from the pool. Finally, the contributions of this work are as follows. 1. We propose a novel model named sequential knowledge transformer (SKT). To the best of our knowledge, our model is the first attempt to leverage a sequential latent variable model for knowledge selection, which subsequently improves knowledge-grounded chit-chat. 2. Our experimental results show that the proposed model improves not only the knowledge selection accuracy but also the performance of utterance generation. As a result, we achieve the new state-of-the-art performance on Wizard of Wikipedia (Dinan et al., 2019 ) and a knowledge-annotated version of Holl-E (Moghe et al., 2018) dataset. This work investigated the issue of knowledge selection in multi-turn knowledge-grounded dialogue, and proposed a sequential latent variable model, for the first time, named sequential knowledge transformer (SKT). Our method achieved the new state-of-the-art performance on the Wizard of Wikipedia benchmark (Dinan et al., 2019) and a knowledge-annotated version of Holl-E dataset (Moghe et al., 2018) . There are several promising future directions beyond this work. First, we can explore other inference models such as sequential Monte Carlo methods using filtering variational objectives (Maddison et al., 2017a) . Second, we can study the interpretability of knowledge selection such as measuring the uncertainty of attention (Heo et al., 2018) .",Our approach is the first attempt to leverage a sequential latent variable model for knowledge selection in the multi-turn knowledge-grounded dialogue. It achieves the new state-of-the-art performance on Wizard of Wikipedia benchmark.,Ghazvininejad ; Cho et al. ; Holl-E ; al. ; Pittsburgh ; Parthasarathi & Pineau ; first ; Moghe et al. ; Dinan et al ; GRU,candidates ; Cho et al. ; American ; the interpretability ; an informative response ; the effectiveness ; Wikipedia ; the diversity ; the multi-turn knowledge-grounded dialogue ; the history,Ghazvininejad ; Cho et al. ; Holl-E ; al. ; Pittsburgh ; Parthasarathi & Pineau ; first ; Moghe et al. ; Dinan et al ; GRU,"Learning-grounded dialogue is a task of generating an informative response based on both discourse context and external knowledge. A sequential latent variable model can keep track of prior and posterior distribution over knowledge, which can help reduce ambiguity caused by diversity in knowledge selection of conversation and better leverage response information for proper choice of knowledge. The proposed model improves knowledge selection accuracy and subsequently the performance of utterance generation. This model is similar to the sequential knowledge transformer (SKT) in conversational research, and it helps in reducing ambiguity caused from diverse knowledge selection in conversation but also improves response information. We achieve the new state-of-the-art",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"The recent rise in popularity of few-shot learning algorithms has enabled models to quickly adapt to new tasks based on only a few training samples. Previous few-shot learning works have mainly focused on classification and reinforcement learning. 
 In this paper, we propose a few-shot meta-learning system that focuses exclusively on regression tasks. Our model is based on the idea that the degree of freedom of the unknown function can be significantly reduced if it is represented as a linear combination of a set of appropriate basis functions. This enables a few labelled samples to approximate the function. We design a Feature Extractor network to encode basis functions for a task distribution, and a  Weights Generator to generate the weight vector for a novel task. We show that our model outperforms the current state of the art meta-learning methods in various regression tasks. Regression deals with the problem of learning a model relating a set of inputs to a set of outputs. The learned model can be thought as function y = F (x) that will give a prediction y ∈ R dy given input x ∈ R dx where d y and d x are dimensions of the output and input respectively. Typically, a regression model is trained on a large number of data points to be able to provide accurate predictions of new inputs. Recently, there have been a surge in popularity on few-shot learning methods (Vinyals et al., 2016; BID7 BID4 . Few-shot learning methods require only a few examples from each task to be able to quickly adapt and perform well on a new task. The fewshot learning model in essence is learning to learn i.e. the model learns to quickly adapt itself to new tasks rather than just learning to give the correct prediction for a particular input sample.In this work, we propose a few shot learning model that targets few-shot regression tasks. We evaluate our model on the sinusoidal regression tasks and compare our model's performance to several meta-learning algorithms. We further introduce two more regression tasks, namely the 1D heat equation task modeled by partial differential equations and the 2D Gaussian distribution task. We propose a few-shot meta learning system that focuses exclusively on regression tasks. Our model is based on the idea of linear representation of basis functions. We design a Feature extractor network to encode the basis functions for the entire task distribution. We design a Weight generator network to generate the weights from the K training samples of a novel task drawn from the same task distribution. We show that our model has competitive performance in in various few short regression tasks. A TECHNICAL DETAILS",We propose a few-shot learning model that is tailored specifically for regression tasks,Feature Extractor ; Weights Generator ; F ; two ; Gaussian ; Feature ; Weight,freedom ; the weights ; this paper ; models ; the function ; outputs ; two ; a few-shot meta-learning system ; a surge ; linear representation,Feature Extractor ; Weights Generator ; F ; two ; Gaussian ; Feature ; Weight,"The rise in popularity of few-shot learning algorithms has enabled models to quickly adapt to new tasks based on only a few training samples. Previous few-shoot learning works focused on classification and reinforcement learning, while newer ones focused on regression tasks. Our model is based on the idea that the degree of freedom of unknown function can be significantly reduced if it is represented as a linear combination of a set of appropriate basis functions. We design a Feature Extractor network to encode basis functions for a task distribution, and a Weights Generator to generate the weight vector for a novel task. This model outperforms current state of the art meta-learning",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"We present a simple proof for the benefit of depth in multi-layer feedforward network with rectifed activation (``""depth separation""). Specifically we present a sequence of classification problems f_i such that (a) for any fixed depth rectified network we can find an index m such that problems with index > m require exponential network width to fully represent the function f_m; and (b) for any problem f_m in the family, we present a concrete neural network with linear depth and bounded width that fully represents it.

 While there are several previous work showing similar results, our proof uses substantially simpler tools and techniques, and should be accessible to undergraduate students in computer science and people with similar backgrounds. We present a simple, geometric proof of the benefit of depth in deep neural networks. We prove that there exist a set of functions indexed by m, each of which can be efficiently represented by a depth m rectified MLP network requiring O(m) parameters. However, for any bounded depth rectified MLP network, there is a function f m in this set that representing it will require an exponential number of parameters in m. More formally, let G d be the set of multi-layer perceptron (MLP) networks with rectified activation and d hidden layers, and let g Θ be such an MLP with parameters Θ. We will prove the following theorem: Theorem 1 (Depth Separation). There exists a set of functions f 1 , f 2 , ..., f i : R 2 → {−1, 1} such that: While this is not a novel result, a main characteristic of our proof is its simplicity. In contrast to previous work, our proof uses only basic algebra, geometry and simple combinatorial arguments. As such, it can be easily read and understood by newcomers and practitioners, or taught in an undergraduate class, without requiring extensive background. Tailoring to these crowds, our presentation style is more verbose then is usual in papers of this kind, attempting to spell out all steps explicitly. We also opted to trade generality for proof simplicity, remaining in input space R 2 rather than the more general R n , thus allowing us to work with lines rather than hyperplanes. Beyond being easy to visualize, it also results in simple proofs of the different lemmas.",ReLU MLP depth seperation proof with gemoteric arguments,linear depth ; O(m ; Θ.,this set ; a sequence ; We ; the more general R ; index ; people ; students ; it ; the function ; this,linear depth ; O(m ; Θ.,"We present a simple proof for the benefit of depth in multi-layer feedforward network with rectifed activation (``depth separation""). For any fixed depth rectified network, we present a sequence of classification problems f_i such that problems with index m require exponential network width to fully represent the function f_m. For any problem f_M in the family, a concrete neural network with linear depth and bounded width that fully represents it. While there are several previous work showing similar results, our proof uses substantially simpler tools and techniques, and should be accessible to undergraduate students in computer science and people with similar backgrounds. Theorem",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Deep approaches to anomaly detection have recently shown promising results over shallow methods on large and complex datasets. Typically anomaly detection is treated as an unsupervised learning problem. In practice however, one may have---in addition to a large set of unlabeled samples---access to a small pool of labeled samples, e.g. a subset verified by some domain expert as being normal or anomalous. Semi-supervised approaches to anomaly detection aim to utilize such labeled samples, but most proposed methods are limited to merely including labeled normal samples. Only a few methods take advantage of labeled anomalies, with existing deep approaches being domain-specific. In this work we present Deep SAD, an end-to-end deep methodology for general semi-supervised anomaly detection. Using an information-theoretic perspective on anomaly detection, we derive a loss motivated by the idea that the entropy of the latent distribution for normal data should be lower than the entropy of the anomalous distribution. We demonstrate in extensive experiments on MNIST, Fashion-MNIST, and CIFAR-10, along with other anomaly detection benchmark datasets, that our method is on par or outperforms shallow, hybrid, and deep competitors, yielding appreciable performance improvements even when provided with only little labeled data. Anomaly detection (AD) (Chandola et al., 2009; Pimentel et al., 2014) is the task of identifying unusual samples in data. Typically AD methods attempt to learn a ""compact"" description of the data in an unsupervised manner assuming that most of the samples are normal (i.e., not anomalous). For example, in one-class classification (Moya et al., 1993; Schölkopf et al., 2001 ) the objective is to find a set of small measure which contains most of the data and samples not contained in that set are deemed anomalous. Shallow unsupervised AD methods such as the One-Class SVM (Schölkopf et al., 2001; Tax & Duin, 2004) , Kernel Density Estimation (Parzen, 1962; Kim & Scott, 2012; Vandermeulen & Scott, 2013 ), or Isolation Forest (Liu et al., 2008 often require manual feature engineering to be effective on high-dimensional data and are limited in their scalability to large datasets. These limitations have sparked great interest in developing novel deep approaches to unsupervised AD (Erfani et al., 2016; Zhai et al., 2016; Chen et al., 2017; Ruff et al., 2018; Deecke et al., 2018; Golan & El-Yaniv, 2018; Hendrycks et al., 2019) . Unlike the standard unsupervised AD setting, in many real-world applications one may also have access to some verified (i.e., labeled) normal or anomalous samples in addition to the unlabeled data. Such samples could be hand labeled by a domain expert, for instance. This leads to a semisupervised AD problem: Given n (mostly normal but possibly containing some anomalous contamination) unlabeled samples x 1 , . . . , x n and m labeled samples (x 1 ,ỹ 1 ), . . . , (x m ,ỹ m ), wherẽ y = +1 andỹ = −1 denote normal and anomalous samples respectively, the task is to learn a model that compactly characterizes the ""normal class."" The term semi-supervised anomaly detection has been used to describe two different AD settings. Most existing ""semi-supervised"" AD methods, both shallow (Muñoz-Marí et al., 2010; Blanchard et al., 2010; Chandola et al., 2009 ) and deep Akcay et al., 2018; Chalapathy & Chawla, 2019) , only incorporate the use of labeled normal samples but not labeled anomalies, i.e. they are more precisely instances of Learning from Positive (i.e., normal) and Unlabeled Examples (LPUE) (Zhang & Zuo, 2008) . A few works (Wang et al., 2005; Liu & Zheng, 2006; Gör-nitz et al., 2013) have investigated the general semi-supervised AD setting where one also utilizes labeled anomalies, however existing deep approaches are domain or data-type specific (Ergen et al., 2017; Kiran et al., 2018; Min et al., 2018) . Research on deep semi-supervised learning has almost exclusively focused on classification as the downstream task (Kingma et al., 2014; Rasmus et al., 2015; Odena, 2016; Dai et al., 2017; Oliver et al., 2018) . Such semi-supervised classifiers typically assume that similar points are likely to be of the same class, this is known as the cluster assumption (Zhu, 2005; Chapelle et al., 2009 ). This assumption, however, only holds for the ""normal class"" in AD, but is crucially invalid for the ""anomaly class"" since anomalies are not necessarily similar to one another. Instead, semi-supervised AD approaches must find a compact description of the normal class while also correctly discriminating the labeled anomalies (Görnitz et al., 2013) . Figure 1 illustrates the differences between various learning paradigms applied to AD on a toy example. We introduce Deep SAD (Deep Semi-supervised Anomaly Detection) in this work, an end-to-end deep method for general semi-supervised AD. Our main contributions are the following: • We introduce an information-theoretic framework for deep AD based on the Infomax principle (Linsker, 1988) . • Using this framework, we derive Deep SAD as a generalization of the unsupervised Deep SVDD method (Ruff et al., 2018) to the general semi-supervised setting. • We conduct extensive experiments in which we establish experimental scenarios for the general semi-supervised AD problem where we also introduce novel baselines. We introduced Deep SAD, a deep method for general semi-supervised anomaly detection. Our method is based on an information-theoretic framework we formulated for deep anomaly detection based on the Infomax principle. This framework can form the basis for rigorous theoretical analyses, e.g. studying the problem under the rate-distortion curve (Alemi et al., 2018) and new methods in the future. Our results suggest that general semi-supervised anomaly detection should always be preferred whenever some labeled information on both normal samples or anomalies is available. ) performing methods in the experimental scenarios (i)-(iii) on the most complex CIFAR-10 dataset. If most points fall above the identity line, this is a very strong indication that the best method indeed significantly outperforms the second best, which often is the case for our Deep SAD method. In this experiment, we examine the detection performance on some well-established AD benchmark datasets (Rayana, 2016) listed in Table 1 . We do this to evaluate the deep against the shallow approaches also on non-image, tabular datasets that are rarely considered in the deep AD literature. For the evaluation, we consider random train-to-test set splits of 60:40 while maintaining the original proportion of anomalies in each set. We then run experiments for 10 seeds with γ l = 0.01 and γ p = 0, i.e. 1% of the training set are labeled anomalies and the unlabeled training data is unpolluted. Since there are no specific different anomaly classes in these datasets, we have k l = 1. We standardize features to have zero mean and unit variance as the only pre-processing step. Table 2 shows the results of the competitive methods. We observe that the shallow kernel methods seem to perform slightly better on the rather small, low-dimensional benchmarks. Deep SAD proves competitive though and the small differences might be explained by the strong advantage we grant the shallow methods in the selection of their hyperparameters. We provide the complete table with the results from all methods in Appendix F for each mini-batch do end for 7: end for Using SGD allows Deep SAD to scale with large datasets as the computational complexity scales linearly in the number of training batches and computations in each batch can be parallelized (e.g., by training on GPUs). Moreover, Deep SAD has low memory complexity as a trained model is fully characterized by the final network parameters W * and no data must be saved or referenced for prediction. Instead, the prediction only requires a forward pass on the network which usually is just a concatenation of simple functions. This enables fast predictions for Deep SAD. Initialization of the network weights W We establish an autoencoder pre-training routine for initialization. That is, we first train an autoencoder that has an encoder with the same architecture as network φ on the reconstruction loss (mean squared error or cross-entropy). After training, we then initialize W with the converged parameters of the encoder. Note that this is in line with the Infomax principle (2) for unsupervised representation learning (Vincent et al., 2008) . Initialization of the center c After initializing the network weights W, we fix the hypersphere center c as the mean of the network representations that we obtain from an initial forward pass on the data (excluding labeled anomalies). We found SGD convergence to be smoother and faster by fixing center c in the neighborhood of the initial data representations as also observed by Ruff et al. (2018) . If sufficiently many labeled normal examples are available, using only those examples for a mean initialization would be another strategy to minimize possible distortions from polluted unlabeled training data. Adding center c as a free optimization variable would allow a trivial ""hypersphere collapse"" solution for the fully unlabeled setting, i.e. for unsupervised Deep SVDD. Preventing a hypersphere collapse A ""hypersphere collapse"" describes the trivial solution that neural network φ converges to the constant function φ ≡ c, i.e. the hypersphere collapses to a single point. Ruff et al. (2018) demonstrate theoretical network properties that prevent such a collapse which we adopt for Deep SAD. Most importantly, network φ must have no bias terms and no bounded activation functions. We refer to Ruff et al. (2018) for further details. If there are sufficiently many labeled anomalies available for training, however, hypersphere collapse is not a problem for Deep SAD due to the opposing labeled and unlabeled objectives.","We introduce Deep SAD, a deep method for general semi-supervised anomaly detection that especially takes advantage of labeled anomalies.",Liu et al. ; Alemi ; Kernel Density Estimation ; Schölkopf ; Deecke et al. ; Chandola ; anomaly detection benchmark ; Kim & Scott ; Liu & Zheng ; Linsker,unusual samples ; a problem ; some domain expert ; Ruff et al. ; These limitations ; Kingma et al ; Zhai ; we ; this experiment ; center,Liu et al. ; Alemi ; Kernel Density Estimation ; Schölkopf ; Deecke et al. ; Chandola ; anomaly detection benchmark ; Kim & Scott ; Liu & Zheng ; Linsker,"Deep approaches to anomaly detection have recently shown promising results over shallow methods on large and complex datasets. Typically anomaly detection is treated as an unsupervised learning problem, with a large collection of unlabeled samples and access to a small pool of labeled samples. Semi-supervised approaches aim to utilize labeled samples, but most proposed methods are limited to merely including labeled normal samples. A few methods take advantage of labeled anomalies, with existing deep approaches being domain-specific. In this work, we present Deep SAD, an end-to-end deep methodology for general semi-Supervised anomaly detection. This approach uses labeled samples to",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"It is challenging to disentangle an object into two orthogonal spaces of content and style since each can influence the visual observation in a different and unpredictable way. It is rare for one to have access to a large number of data to help separate the influences. In this paper, we present a novel framework to learn this disentangled representation in a completely unsupervised manner. We address this problem in a two-branch Autoencoder framework. For the structural content branch, we project the latent factor into a soft structured point tensor and constrain it with losses derived from prior knowledge. This encourages the branch to distill geometry information. Another branch learns the complementary style information. The two branches form an effective framework that can disentangle object's content-style representation without any human annotation. We evaluate our approach on four image datasets, on which we demonstrate the superior disentanglement and visual analogy quality both in synthesized and real-world data. We are able to generate photo-realistic images with 256x256 resolution that are clearly disentangled in content and style. Content and style are the two most inherent attributes that characterize an object visually. Computer vision researchers have devoted decades of efforts to understand object shape and extract features that are invariant to geometry change BID11 BID33 BID36 BID26 . Learning such disentangled deep representation for visual objects is an important topic in deep learning.The main objective of our work is to disentangle object's style and content in an unsupervised manner. Achieving this goal is non-trivial due to three reasons: 1) Without supervision, we can hardly guarantee the separation of different representations in the latent space. 2) Although some methods like InfoGAN are capable of learning several groups of independent attributes from objects, attributes from these unsupervised frameworks are uninterpretable since we cannot pinpoint which portion of the disentangled representation is related to the content and which to the style. 3) Learning structural content from a set of natural real-world images is difficult.To overcome the aforementioned challenges, we propose a novel two-branch Autoencoder framework, of which the structural content branch aims to discover semantically meaningful structural points (i.e., y in Fig 2) to represent the object geometry, while the other style branch learns the complementary style representation. The settings of these two branches are asymmetric. For the structural content branch, we add a layer-wise softmax operator to the last layer. We could regard this as a projection of a latent content to a soft structured point tensor space. Specifically designed prior losses are used to constrain the structured point tensors so that the discovered points have high repeatability across images yet distributed uniformly to cover different parts of the object. To encourage the framework to learn a disentangled yet complementary representation of both content and style, we further introduce a Kullback-Leibler (KL) divergence loss and skip-connections design to the framework. In FIG0 , we show the latent space walking results on cat face dataset, which demonstrates a reasonable coverage of the manifold and an effective disentanglement of the content and style space of our approach.Extensive experiments show the effectiveness of the proposed method in disentangling the content and style of natural images. We also conduct qualitative and quantitative experiments on MNISTColor, 3D synthesized data and several real-world datasets which demonstrate the superior performance of our method to state-of-the-art algorithms. We propose a novel model based on Autoencoder framework to disentangle object's representation by content and style. Our framework is able to mine structural content from a kind of objects and learn content-invariant style representation simultaneously, without any annotation. Our work may also reveal several potential topics for future research: 1) Instead of relying on supervision, using strong prior to restrict the latent variables seems to be a potential and effective tool for disentangling. 2) In this work we only experiment on near-rigid objects like chairs and faces, learning on deformable objects is still an opening problem.3) The content-invariant style representation may have some potentials on recognition tasks.",We present a novel framework to learn the disentangled representation of content and style in a completely unsupervised manner.,two ; four ; decades ; three ; Fig ; Kullback-Leibler ; KL ; MNISTColor,three reasons ; these two branches ; this paper ; InfoGAN ; natural images ; disentangle object's style ; photo-realistic images ; KL ; losses ; Another branch,two ; four ; decades ; three ; Fig ; Kullback-Leibler ; KL ; MNISTColor,"Disentangle an object into two orthogonal spaces of content and style can be challenging due to its unpredictability. It is rare for one to have access to a large number of data to separate the influences. A novel framework to learn this disentangled representation in a completely unsupervised manner is proposed. The structural content branch projects the latent factor into a soft structured point tensor and constrain it with losses derived from prior knowledge, while the other branch learns the complementary style representation. The two branches form an effective framework that can disentangle object's content-style representation without any human annotation. The approach is based",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"The notion of the stationary equilibrium ensemble has played a central role in statistical mechanics. In machine learning as well, training serves as generalized equilibration that drives the probability distribution of model parameters toward stationarity. Here, we derive stationary fluctuation-dissipation relations that link measurable quantities and hyperparameters in the stochastic gradient descent algorithm. These relations hold exactly for any stationary state and can in particular be used to adaptively set training schedule. We can further use the relations to efficiently extract information pertaining to a loss-function landscape such as the magnitudes of its Hessian and anharmonicity. Our claims are empirically verified. Equilibration rules the long-term fate of many macroscopic dynamical systems. For instance, as we pour water into a glass and let it be, the stationary state of tranquility is eventually attained. Zooming into the tranquil water with a microscope would reveal, however, a turmoil of stochastic fluctuations that maintain the apparent stationarity in balance. This is vividly exemplified by the Brownian motion BID3 : a pollen immersed in water is constantly bombarded by jittery molecular movements, resulting in the macroscopically observable diffusive motion of the solute. Out of the effort in bridging microscopic and macroscopic realms through the Brownian movement came a prototype of fluctuation-dissipation relations BID6 BID37 . These relations quantitatively link degrees of noisy microscopic fluctuations to smooth macroscopic dissipative phenomena and have since been codified in the linear response theory for physical systems BID28 BID9 BID16 , a cornerstone of statistical mechanics.Machine learning begets another form of equilibration. As a model learns patterns in data, its performance first improves and then plateaus, again reaching apparent stationarity. This dynamical process naturally comes equipped with stochastic fluctuations as well: often given data too gigantic to consume at once, training proceeds in small batches and random selections of these mini-batches consequently give rise to the noisy dynamical excursion of the model parameters in the loss-function landscape, reminiscent of the Brownian motion. It is thus natural to wonder if there exist analogous fluctuation-dissipation relations that quantitatively link the noise in mini-batched data to the observable evolution of the model performance and that in turn facilitate the learning process.Here, we derive such fluctuation-dissipation relations for the stochastic gradient descent algorithm. The only assumption made is stationarity of the probability distribution that governs the model parameters at sufficiently long time. Our results thus apply to generic cases with non-Gaussian mini-batch noises and nonconvex loss-function landscapes. Practically, the first relation (FDR1) offers the metric for assessing equilibration and yields an adaptive algorithm that sets learning-rate schedule on the fly. The second relation (FDR2) further helps us determine the properties of the lossfunction landscape, including the strength of its Hessian and the degree of anharmonicity, i.e., the deviation from the idealized harmonic limit of a quadratic loss surface and a constant noise matrix.Our approach should be contrasted with recent attempts to import the machinery of stochastic differential calculus into the study of the stochastic gradient descent algorithm BID21 BID20 BID22 BID19 BID33 BID4 BID12 BID39 . This line of work all assumes Gaussian noises and sometimes additionally employs the quadratic harmonic approximation for loss-function landscapes. The more severe drawback, however, is the usage of the analogy with continuous-time stochastic differential equations, which is inconsistent in general (see Section 2.3.3). Instead, the stochastic gradient descent algorithm can be properly treated within the framework of the KramersMoyal expansion BID36 BID7 BID30 BID29 BID18 .The paper is organized as follows. In Section 2, after setting up notations and deriving a stationary fluctuation-dissipation theorem (FDT), we derive two specific fluctuation-dissipation relations. The first relation (FDR1) can be used to check stationarity and the second relation (FDR2) to delineate the shape of the loss-function landscape, as empirically borne out in Section 3. An adaptive scheduling method is proposed and tested in Section 3.3. We conclude in Section 4 with future outlooks. In this paper, we have derived the fluctuation-dissipation relations with no assumptions other than stationarity of the probability distribution. These relations hold exactly even when the noise is nonGaussian and the loss function is nonconvex. The relations have been empirically verified and used to probe the properties of the loss-function landscapes for the simple models. The relations further have resulted in the algorithm to adaptively set learning-rate schedule on the fly rather than presetting it in an ad hoc manner. In addition to systematically testing the performance of this adaptive scheduling algorithm, it would be interesting to investigate non-Gaussianity and noncovexity in more details through higher-point observables, both analytically and numerically. It would also be interesting to further elucidate the physics of machine learning by extending our formalism to incorporate nonstationary dynamics, linearly away from stationarity BID28 BID9 BID16 and beyond BID11 BID5 , so that it can in particular properly treat overfitting cascading dynamics and time-dependent sample distributions.","We prove fluctuation-dissipation relations for SGD, which can be used to (i) adaptively set learning rates and (ii) probe loss surfaces.",nonGaussian ; non-Gaussian ; Hessian ; KramersMoyal ; FDT ; first ; two ; second ; noisy microscopic ; Brownian,mini-batched data ; model parameters ; Gaussian ; the physics ; the relations ; a prototype ; the effort ; the tranquil water ; anharmonicity ; Our approach,nonGaussian ; non-Gaussian ; Hessian ; KramersMoyal ; FDT ; first ; two ; second ; noisy microscopic ; Brownian,"In statistical mechanics, training drives the probability distribution of model parameters toward stationarity. In machine learning, stationary fluctuation-dissipation relations link measurable quantities and hyperparameters in stochastic gradient descent algorithms. These relations can be used to efficiently extract information pertaining to a loss-function landscape such as the magnitudes of its Hessian and anharmonicity. Our claims are empirically verified. Equilibration rules the long-term fate of many macroscopic dynamical systems. For instance, as we pour water into a glass and let it be, the stationary state of tranquility is eventually attained. However,",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Knowledge Bases (KBs) are becoming increasingly large, sparse and probabilistic. These KBs are typically used to perform query inferences and rule mining. But their efficacy is only as high as their completeness. Efficiently utilizing incomplete KBs remains a major challenge as the current KB completion techniques either do not take into account the inherent uncertainty associated with each KB tuple or do not scale to large KBs.

 Probabilistic rule learning not only considers the probability of every KB tuple but also tackles the problem of KB completion in an explainable way. For any given probabilistic KB, it learns probabilistic first-order rules from its relations to identify interesting patterns. But, the current probabilistic rule learning techniques perform grounding to do probabilistic inference for evaluation of candidate rules. It does not scale well to large KBs as the time complexity of inference using grounding is exponential over the size of the KB. In this paper, we present SafeLearner -- a scalable solution to probabilistic KB completion that performs probabilistic rule learning using lifted probabilistic inference -- as faster approach instead of grounding. 

 We compared SafeLearner to the state-of-the-art probabilistic rule learner ProbFOIL+ and to its deterministic contemporary AMIE+ on standard probabilistic KBs of NELL (Never-Ending Language Learner) and Yago. Our results demonstrate that SafeLearner scales as good as AMIE+ when learning simple rules and is also significantly faster than ProbFOIL+. There is an increasing tendency to construct knowledge bases and knowledge graphs by machine learning methods. As a result, knowledge bases are often incomplete and also uncertain. To cope with uncertainty, one often resorts to probabilistic databases and logics Suciu, 2017, De Raedt et al., 2016] , which take into account the probability of the tuples in the querying process. The most widely used probabilistic database semantics is based on the tuple-independent probabilistic databases model, which assumes that every tuple in every table of the database is independent of one another To cope with incomplete knowledge bases, various researchers have used machine learning techniques to learn a set of rules that can be used to infer new tuples from the existing ones, thereby completing the knowledge base BID2 . This traditional relational rule learning setting BID18 has been extended to probabilistic logics and databases by De Raedt et al. [2015] . However, the ProbFOIL approach of De Raedt et al. suffers from one key limitation: It does not scale well to large databases due to the grounding step, which results in an intractable probabilistic inference problem. The key contribution of this paper is the introduction of the SafeLearner system which performs two major tasks. 1) It uses lifted inference to avoid the grounding step and to improve scaling.2) It enhances a highly efficient rule generation system, AMIE+ BID11 ] to obtain deterministic candidate rules which are then made probabilistic using lifted inference.This paper is organized as follows. We introduce the background for this paper in Section 2. We define, in Section 3, the problem of learning a set of probabilistic rules. Sections 4 and 5 outline the idea behind the working of SafeLearner. Section 6 proposes the algorithm for SafeLearner. In Section 7, we present an experimental evaluation in the context of the NELL knowledge base BID2 ]. An overview of related work can be found in Section 8. Section 9 discusses future research directions and concludes. The work presented in this paper advances the works [De Raedt et al., 2015, Dylla and BID9 that also studied learning in the probabilistic database setting. 7 But compared with these previous works, we rely on lifted inference, which allows our approach to scale to much larger databases. Both of the previous approaches only use tuples from a given training set but do not take into account the behavior of the model on tuples not in the training set. This is problematic because, unless the training set is really large, these previous methods do not distinguish models that predict too many false positives (i.e. models that give too high probability to too many tuples outside the training set). This becomes an issue especially in sparse domains (and most real domains are indeed sparse). Our work is also closely related to the literature on learning from knowledge bases such as NELL within statistical relational learning (SRL), including works that use Markov logic networks BID20 , Bayesian logic programs BID19 and stochastic logic programs BID17 BID23 . A disadvantage of many of these methods is that the learned parameters of the models can not be interpreted easily, which is particularly an issue for Markov logic networks where the weight of a rule cannot be understood in isolation from the rest of the rules. In contrast, the learned weights of probabilistic rules in our work, and also in the other works relying on probabilistic databases [De Raedt et al., 2015, Dylla and BID9 , have a clear probabilistic interpretation.Parameter Learning with Different Losses Cross entropy is not the only loss function that may be considered for learning the parameters of probabilistic rules. Here, we discuss two additional loss functions that have already been used for the same or similar tasks in the literature: squared loss BID9 and a probabilistic extension of accuracy . Whereas cross entropy and squared loss belong among so-called proper scoring rules BID12 and, thus, reward estimates of probabilities that match the true probability, this is not the case for probabilistic accuracy. Moreover, each of these functions also relies on additional assumptions such as mutual independence of the examples' probabilities as well as mutual independence of the predictions, although this is not mentioned explicitly in the respective works BID9 Theobald, 2016, De Raedt et al., 2015] . Below, we briefly discuss squared loss and probabilistic accuracy.Squared Loss (Brier Score) As before, let p i denote the probability of the i-th example and q i the probability of the respective prediction. Then, the squared loss, which is a proper scoring rule, is: DISPLAYFORM0 2 which was among others used in BID9 for learning probabilities of tuples in PDBs. define probabilistic extension of accuracy and other measures of predictive quality such as precision and recall. Their version of probabilistic accuracy is Acc prob = 1− 1 |E| t i ,p i ∈E |p i − q i | . Unlike the other two discussed loss functions, probabilistic accuracy is not a proper scoring rule as the next example illus-7. Strictly speaking, the work was framed within the probabilistic logic programming setting. However, probabilistic logic programming systems, such as Problog BID10 , can be seen as generalizations of probabilistic databases. We proposed a probabilistic rule learning system, named SafeLearner, that supports lifted inference. It first performs structure learning by mining independent deterministic candidate rules using AMIE+ and later executes joint parameter learning over all the rule probabilities. SafeLearner extends ProbFOIL + by using lifted probabilistic inference (instead of using grounding). Therefore, it scales better than ProbFOIL + . In comparison with AMIE+, it is able to jointly learn probabilistic rules over a probabilistic KB unlike AMIE+ which only learns independent deterministic rules (with confidences) over a deterministic KB. We experimentally show that SafeLearner scales as good as AMIE+ when learning simple rules. Trying to learn complex rules leads to unsafe queries which are not suitable for lifted inference. But lifted inference helps SafeLearner in outperforming ProbFOIL + which does not scale to NELL Sports Database without the help of a declarative bias. A few limitations of SafeLearner are as follows: 1) It cannot learn complex rules that translate to an unsafe query. 2) It cannot use rules within the background theory. 3) It cannot learn rules on P DB with numeric data (without assuming them as discrete constants).The main contributions of SafeLearner are presented as follows. Firstly , it accomplishes probabilistic rule learning using a novel inference setting as it is the first approach that uses lifted inference for KB completion. Secondly , unlike ProbFOIL + , SafeLearner scales well on the full database of NELL with 233,000 tuples and 426 relations as well as on the standard subset of Yago 2.4 with 948,000 tuples and 33 relations. Thirdly , SafeLearner is faster than ProbFOIL + because of the following three factors: 1) it disintegrates longer complex queries to smaller simpler ones, 2) it caches the structure of queries before doing inference and 3) it uses lifted inference to infer on those simple queries. The first two factors of query disintegration and memoization are discussed in Appendix D in further detail.In future, this work could be advanced further to eliminate its shortcomings. In particular , a prominent direction of advancement would be to extend probabilistic rule learning to open-world setting of which the Lif t O R algorithm BID3 is capable.",Probabilistic Rule Learning system using Lifted Inference,Bayesian ; first ; KB ; ∈E ; Markov ; al. ; NELL Sports Database ; NELL ; Never-Ending Language Learner ; one,a major challenge ; the current probabilistic rule ; De Raedt ; the case ; the working ; this work ; large KBs ; faster approach ; the standard subset ; the weight,Bayesian ; first ; KB ; ∈E ; Markov ; al. ; NELL Sports Database ; NELL ; Never-Ending Language Learner ; one,"Learning Bases (KBs) are becoming increasingly large, sparse and probabilistic. These KBs are used to perform query inferences and rule mining, but their efficacy is only as high as their completeness. Efficiently utilizing incomplete KBs remains a major challenge as the current KB completion techniques either do not take into account the inherent uncertainty associated with each KB tuple or do not scale well to large KBs. Probabilistic rule learning considers the probability of every tuple in every table of the database and addresses the problem of KB completion in an explainable way. It learns probabilist first-order rules from its",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general purpose features for words across a range of NLP problems. However, extending this success to learning representations of sequences of words, such as sentences, remains an open problem. Recent work has explored unsupervised as well as supervised learning techniques with different training objectives to learn general purpose fixed-length sentence representations. In this work, we present a simple, effective multi-task learning framework for sentence representations that combines the inductive biases of diverse training objectives in a single model. 
 We train this model on several data sources with multiple training objectives on over 100 million sentences. Extensive experiments demonstrate that sharing a single recurrent sentence encoder across weakly related tasks leads to consistent improvements over previous methods. We present substantial improvements in the context of transfer learning and low-resource settings using our learned general-purpose representations. Transfer learning has driven a number of recent successes in computer vision and NLP. Computer vision tasks like image captioning BID52 and visual question answering typically use CNNs pretrained on ImageNet BID24 BID41 to extract representations of the image, while several natural language tasks such as reading comprehension and sequence labeling BID25 have benefited from pretrained word embeddings BID28 BID37 that are either fine-tuned for a specific task or held fixed.Many neural NLP systems are initialized with pretrained word embeddings but learn their representations of words in context from scratch, in a task-specific manner from supervised learning signals. However, learning these representations reliably from scratch is not always feasible, especially in low-resource settings, where we believe that using general purpose sentence representations will be beneficial.Some recent work has addressed this by learning general-purpose sentence representations BID49 BID17 BID11 BID27 BID20 BID33 BID34 . However, there exists no clear consensus yet on what training objective or methodology is best suited to this goal.Understanding the inductive biases of distinct neural models is important for guiding progress in representation learning. BID40 and BID4 demonstrate that neural machine translation (NMT) systems appear to capture morphology and some syntactic properties. BID40 also present evidence that sequence-to-sequence parsers more strongly encode source language syntax. Similarly, BID0 probe representations extracted by sequence autoencoders, word embedding averages, and skip-thought vectors with a multi-layer perceptron (MLP) classifier to study whether sentence characteristics such as length, word content and word order are encoded.To generalize across a diverse set of tasks, it is important to build representations that encode several aspects of a sentence. Neural approaches to tasks such as skip-thoughts, machine translation, natural language inference, and constituency parsing likely have different inductive biases. Our work exploits this in the context of a simple one-to-many multi-task learning (MTL) framework, wherein a single recurrent sentence encoder is shared across multiple tasks. We hypothesize that sentence representations learned by training on a reasonably large number of weakly related tasks will generalize better to novel tasks unseen during training, since this process encodes the inductive biases of multiple models. This hypothesis is based on the theoretical work of BID3 . While our work aims at learning fixed-length distributed sentence representations, it is not always practical to assume that the entire ""meaning"" of a sentence can be encoded into a fixed-length vector. We merely hope to capture some of its characteristics that could be of use in a variety of tasks.The primary contribution of our work is to combine the benefits of diverse sentence-representation learning objectives into a single multi-task framework. To the best of our knowledge, this is the first large-scale reusable sentence representation model obtained by combining a set of training objectives with the level of diversity explored here, i.e. multi-lingual NMT, natural language inference, constituency parsing and skip-thought vectors. We demonstrate through extensive experimentation that representations learned in this way lead to improved performance across a diverse set of novel tasks not used in the learning of our representations. Such representations facilitate low-resource learning as exhibited by significant improvements to model performance for new tasks in the low labelled data regime -achieving comparable performance to a few models trained from scratch using only 6% of the available training set on the Quora duplicate question dataset. In this section, we describe our approach to evaluate the quality of our learned representations, present the results of our evaluation and discuss our findings. We present a multi-task framework for learning general-purpose fixed-length sentence representations. Our primary motivation is to encapsulate the inductive biases of several diverse training signals used to learn sentence representations into a single model. Our multi-task framework includes a combination of sequence-to-sequence tasks such as multi-lingual NMT, constituency parsing and skip-thought vectors as well as a classification task -natural language inference. We demonstrate that the learned representations yield competitive or superior results to previous general-purpose sentence representation methods. We also observe that this approach produces good word embeddings. Table 5 : Evaluation of sentence representations by probing for certain sentence characteristics and syntactic properties. Sentence length, word content & word order from BID0 and sentence active/passive, tense and top level syntactic sequence (TSS) from BID40 . Numbers reported are the accuracy with which the models were able to predict certain characteristics.In future work, we would like understand and interpret the inductive biases that our model learns and observe how it changes with the addition of different tasks beyond just our simple analysis of sentence characteristics and syntax. Having a rich, continuous sentence representation space could allow the application of state-of-the-art generative models of images such as that of BID32 to language. One could also consider controllable text generation by directly manipulating the sentence representations and realizing it by decoding with a conditional language model.",A large-scale multi-task learning framework with diverse training objectives to learn fixed-length sentence representations,NLP ; NMT ; one ; MTL ; first ; Quora ; TSS,"the application ; active/passive, tense and top level syntactic sequence ; low-resource settings ; word order ; a simple, effective multi-task learning framework ; TSS ; a single model ; use ; the learned representations ; context",NLP ; NMT ; one ; MTL ; first ; Quora ; TSS,"Natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in unsupervised manner. These representations are typically used as general purpose features for words across a range of NLP problems. However, extending this success to learning representations of sequences of words, such as sentences, remains an open issue. A simple, effective multi-task learning framework for sentence representations combines the inductive biases of diverse training objectives in a single model. We train this model on several data sources with multiple training objectives on over 100 million sentences. Extensive experiments demonstrate that sharing a single recurrent sentence encoder across",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Importance sampling (IS) is a standard Monte Carlo (MC) tool to compute information about random variables such as moments or quantiles with unknown distributions.   IS is 
asymptotically consistent as the number of MC samples, and hence deltas (particles) that parameterize the density estimate, go to infinity. However, retaining infinitely many particles is intractable. We propose a scheme for only keeping a \emph{finite representative subset} of particles and their augmented importance weights that is \emph{nearly consistent}. To do so in {an online manner}, we approximate importance sampling in two ways.   First, we replace the deltas by kernels, yielding kernel density estimates (KDEs).   Second, we sequentially project KDEs onto nearby lower-dimensional subspaces. We characterize the asymptotic bias of this scheme as determined by a compression parameter and kernel bandwidth, which yields a tunable tradeoff between consistency and memory. In experiments,  we observe a favorable tradeoff between memory and accuracy, providing for the first time near-consistent compressions of arbitrary posterior distributions. Importance sampling is a MC method that addresses Bayesian inference in cases where the distribution that relates observations to the hidden state is time-invariant (Tokdar and Kass, 2010). More specifically, based upon independent samples from a proposal distribution, MC methods approximately compute expectations of arbitrary functions of the unknown parameter via weighted samples generated from the proposal. Recently, use of importance distributions to weight updates, e.g., coordinate descent (Allen-Zhu et al., 2016; Csiba et al., 2015) or stochastic gradient descent (Borsos et al., 2018) , have been developed. Doing so yields faster deep network training (Johnson and Guestrin, 2018; Katharopoulos and Fleuret, 2018) by weighting mini-batches (Hanzely and Richtárik, 2018) . Furthermore, in reinforcement learning (RL), an agent chooses actions according to a policy and then updates the policy via rewards observed (Watkins and Dayan, 1992) ; however, this theoretically requires an inordinate amount of random actions to be chosen before reasonable performance is learned (Tsitsiklis, 1994; , an issue known as the exploreexploit tradeoff. To lessen its deleterious effect, exploratory actions may be chosen via an importance distribution (Schaul et al., 2015) or policy updates may be chosen from previous experience known to be safe (Precup et al., 2000) . Contributions. We propose a compression scheme that operates within importance sampling, sequentially deciding which particles are statistically significant for the integral estimation. To do so, we draw connections between proximal methods in optimization (Rockafellar, 1976) and importance distribution updates: we view the empirical measure defined by importance sampling as carrying out a sequence of projections of un-normalized empirical distributions onto subspaces of growing dimension. Then, we augment the subspace selection by replacing it by one that is nearby (according to some metric) but with lower memory. These lower-memory subspaces are selected based on greedy compression with a fixed budget parameter via matching pursuit (Pati et al., 1993) . We combine this idea with kernel smoothing of the empirical measure in order to exploit the fact that compact spaces have finite covering numbers. Consequently, we have characterized the asymptotic bias of this method as a tunable constant depending on the kernel bandwidth parameter and a compression parameter. Experiments demonstrate that this approach yields an effective tradeoff of consistency and memory for MC methods.",We proposed a novel compressed kernelized importance sampling algorithm.,two ; Johnson ; Guestrin ; Watkins ; Dayan ; Monte Carlo ; Bayesian ; Tokdar ; Richtárik ; Allen-Zhu,a MC method ; an agent ; Importance sampling ; time-invariant (Tokdar ; Rockafellar ; the first time ; nearby lower-dimensional subspaces ; Kass ; hence deltas ; infinitely many particles,two ; Johnson ; Guestrin ; Watkins ; Dayan ; Monte Carlo ; Bayesian ; Tokdar ; Richtárik ; Allen-Zhu,"Importance sampling (IS) is a standard Monte Carlo (MC) tool to compute information about random variables such as moments or quantiles with unknown distributions. It is consistent as the number of MC samples, and hence deltas (particles) parameterize the density estimate, go to infinity. However, retaining infinitely many particles is intractable. We propose a scheme for keeping a \emph{finite representative subset} of particles and their augmented importance weights that is \emmph{nearly consistent}. To approximate importance sampling in two ways:  First, we replace the DELtas by kernels, yielding",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"In this paper, we propose a novel regularization method, RotationOut, for neural networks. 
 Different from Dropout that handles each neuron/channel independently, RotationOut regards its input layer as an entire vector and introduces regularization by randomly rotating the vector. 
 RotationOut can also be used in convolutional layers and recurrent layers with a small modification.
 We further use a noise analysis method to interpret the difference between RotationOut and Dropout in co-adaptation reduction. 
 Using this method, we also show how to use RotationOut/Dropout together with Batch Normalization. 
 Extensive experiments in vision and language tasks are conducted to show the effectiveness of the proposed method. 
 Codes will be available. Dropout (Srivastava et al., 2014 ) has proven to be effective for preventing overfitting over many deep learning areas, such as image classification (Shrivastava et al., 2017) , natural language processing (Hu et al., 2016) and speech recognition (Amodei et al., 2016) . In the years since, a wide range of variants have been proposed for wider scenarios, and most related work focus on the improvement of Dropout structures, i.e., how to drop. For example, drop connect (Wan et al., 2013) drops the weights instead of neurons, evolutional dropout (Li et al., 2016) computes the adaptive dropping probabilities on-the-fly, max-pooling dropout (Wu & Gu, 2015) drops neurons in the max-pooling kernel so smaller feature values have some probabilities to to affect the activations. These Dropout-like methods process each neuron/channel in one layer independently and introduce randomness by dropping. These architectures are certainly simple and effective. However, randomly dropping independently is not the only method to introduce randomness. Hinton et al. (2012) argues that overfitting can be reduced by preventing co-adaptation between feature detectors. Thus it is helpful to consider other neurons' information when adding noise to one neuron. For example, lateral inhibition noise could be more effective than independent noise. In this paper, we propose RotationOut as a regularization method for neural networks. RotationOut regards the neurons in one layer as a vector and introduces noise by randomly rotating the vector. Specifically, consider a fully-connected layer with n neurons: x ∈ R n . If applying RotationOut to this layer, the output is Rx where R ∈ R n×n is a random rotation matrix. It rotates the input with random angles and directions, bringing noise to the input. The noise added to a neuron comes not only from itself, but also from other neurons. It is the major difference between RotationOut and Dropout-like methods. We further show that RotationOut uses the activations of the other neurons as the noise to one neuron so that the co-adaptation between neurons can be reduced. RotationOut uses random rotation matrices instead of unrestricted matrices because the directions of feature vectors are important. Random rotation provides noise to the directions directly. Most neural networks use dot product between the feature vector and weight vector as the output. The network actually learns the direction of the weights, especially when there is a normalization layer (e.g. Batch Normalization (Ioffe & Szegedy, 2015) or Weight Normalization (Salimans & Kingma, 2016) ) after the weight layer. Random rotation of feature vecoters introduces noise into the angle between the feature and the weight, making the learning of weights directions more stable. Sabour et al. (2017) also uses the orientation of feature vectors to represent the instantiation parameters in capsules. Another motivation for rotating feature vectors comes from network dissection. Bau et al. (2017) finds that random rotations of a learned representation can destroy the interpretability which is axis-aligned. Thus random rotating the feature during training makes the network more robust. Even small rotations can be a strong regularization. We study how RotationOut helps prevent neural networks from overfitting. Hinton et al. (2012) introduces co-adaptation to interpret Dropout but few literature give a clear concept of co-adaptation. In this paper,we provide a metric to approximate co-adaptations and derive a general formula for noise analysis. Using the formula, we prove that RotationOut can reduce co-adaptations more effectively than Dropout and show how to combine Dropout and Batch Normalization together. In our experiments, RotationOut can achieve results on par with or better than Dropout and Dropoutlike methods among several deep learning tasks. Applying RotationOut after convolutional layers and fully connected layers improves image classification accuracy of ConvNet on CIFAR100 and ImageNet datasets. On COCO datasets, RotationOut also improves the generalization of object detection models. For LSTM models, RotationOut can achieve competitive results with existing RNN dropout method for speech recognition task on Wall Street Journal (WSJ) corpus. The main contributions of this paper are as follows: We propose RotationOut as a regularization method for neural networks which is different from existing Dropout-like methods that operate on each neuron independently. RotationOut randomly rotates the feature vector and introduces noise to one neuron with other neurons' information. We present a theoretical analysis method for general formula of noise. Using the method, we answer two questions: 1) how noise-based regularization methods reduce co-adaptions and 2) how to combine noise-based regularization methods with Batch Normalization. Experiments in vision and language tasks are conducted to show the effectiveness of the proposed RotationOut method. Related Work Dropout is effective for fully connected layers. When applied to convolution layers, it is less effective. Ghiasi et al. (2018) argues that information about the input can still be sent to the next layer even with dropout, which causes the networks to overfit (Ghiasi et al., 2018) . SpatialDropout (Tompson et al., 2015) drops the entire channel from the feature map. Shake-shake regularization (Gastaldi, 2017) drops the residual branches. Cutout (DeVries & Taylor, 2017) and Dropblock (Ghiasi et al., 2018 ) drop a continuois square region from the inputs/feature maps. Applying standard dropout to recurrent layers also results in poor performance (Zaremba et al., 2014; Labach et al., 2019) , since the noise caused by dropout at each time step prevents the network from retaining long-term memory. Gal & Ghahramani (2016) ; Moon et al. (2015) ; Merity et al. (2017) generate a dropout mask for each input sequence, and keep it the same at every time step so that memory can be retained. Batch Normalization (BN) (Ioffe & Szegedy, 2015) accelerates deep network training. It is also a regularization to the network, and discourage the strength of dropout to prevent overfitting (Ioffe & Szegedy, 2015) . Many modern ConvNet architectures such as ResNet (He et al., 2016) and DenseNet (Huang et al., 2017) do not apply dropout in convolutions. Li et al. (2019) is the first to argue that it is caused by the a variance shift. In this paper, we use the noise analysis method to further explore this problem. There is a lot of work studying rotations in networks. Rotations on the images (Lenc & Vedaldi, 2015; Simard et al., 2003) are important data augmentation methods. There are also studies about rotation equivalence. Worrall et al. (2017) uses an enriched feature map explicitly capturing the underlying orientations. Marcos et al. (2017) applies multiple rotated versions of each filter to the input to solve problems requiring different responses with respect to the inputs' rotation. The motivations of these work are different from ours. The most related work is network dissection (Bau et al., 2017) . They discuss the impact on the interpretability of random rotations of learned features, showing that rotation in training can be a strong regularization. In this work, we introduce RotationOut as an alternative for dropout for neural network. RotationOut adds continuous noise to data/features and keep the semantics. We further establish an analysis of noise to show how co-adaptations are reduced in neural network and why dropout is more effective than dropout. Our experiments show that applying RotationOut in neural network helps training and increase the accuracy. Possible direction for further work is the theoretical analysis of co-adaptations. As discussed earlier, the proposed correlation analysis is not optimal. It cannot explain the difference between standard Dropout and Gaussian dropout. Also it can not ex-plain some methods such as Shake-shake regularization. Further work on co-adaptation analysis can help better understand noise-based regularization methods. One example of such a matrix that rotates the (1, 3) dimensions and (2, 4) dimensions can be: In Section 2, we mentioned the complexity of RotationOut is O(D). It is because we can avoid matrix multiplications to get Rx. For example, let the R be the operator generated by Equation 17, we have: The sparse matrix in Equation 18 is similar to a combine of permutation matrix, and we do not need matrix multiplications to get the output. The output can be get by slicing and an elementwise multiplication: x[3, 4, 1, 2] * [−1, 1, 1, −1].",We propose a regularization method for neural network and a noise analysis method,ConvNet ; SpatialDropout ; Hu et al. ; O(D ; Merity ; first ; Hinton ; Batch Normalization ; Wan et al. ; max,a neuron ; the formula ; we ; dot product ; Moon et al ; R ∈ R n×n ; dropping ; Weight Normalization (Salimans & Kingma ; the effectiveness ; variants,ConvNet ; SpatialDropout ; Hu et al. ; O(D ; Merity ; first ; Hinton ; Batch Normalization ; Wan et al. ; max,"RotationOut is a regularization method for neural networks that handles each neuron/channel independently and introduces regularization by randomly rotating the vector. It can also be used in convolutional layers and recurrent layers with a small modification. In this paper, we show how to use RotationOut/Dropout together with Batch Normalization.  Extensive experiments in vision and language tasks are conducted to show the effectiveness of the proposed method. Additionally, a wide range of variants have been proposed for wider scenarios and most related work focus on the improvement of Dropout structures, i.e., how to drop. Hinton et al",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"In this paper, we investigate lossy compression of deep neural networks (DNNs) by weight quantization and lossless source coding for memory-efficient deployment. Whereas the previous work addressed non-universal scalar quantization and entropy coding of DNN weights, we for the first time introduce universal DNN compression by universal vector quantization and universal source coding. In particular, we examine universal randomized lattice quantization of DNNs, which randomizes DNN weights by uniform random dithering before lattice quantization and can perform near-optimally on any source without relying on knowledge of its probability distribution. Moreover, we present a method of fine-tuning vector quantized DNNs to recover the performance loss after quantization. Our experimental results show that the proposed universal DNN compression scheme compresses the 32-layer ResNet (trained on CIFAR-10) and the AlexNet (trained on ImageNet) with compression ratios of $47.1$ and $42.5$, respectively. Compression of deep neural networks (DNNs) has been actively studied in deep learning to develop compact DNN models for memory-efficient and computation-efficient deployment. Han et al. BID0 showed impressive compression results by weight pruning, k-means clustering, and Huffman coding. It is further optimized in BID1 using Hessian-weighted k-means clustering. Recently, it is shown how soft weight sharing or soft quantization can be employed for DNN weight quantization in BID2 BID3 . On the other hand, weight pruning is also extensively studied, e.g., in BID4 BID5 BID6 BID7 BID8 . In this paper, we focus on DNN weight quantization, which can be used together with weight pruning to generate compressed models.Vector quantization reduces the gap to the rate-distortion bound by jointly quantizing multiple symbols. Since conjectured by Gersho in BID9 , lattice quantization has been presumed to be the most efficient entropy coded vector quantization in the high resolution regime asymptotically, as the rate goes to infinity and the distortion diminishes BID10 . Although lattice quantizers are simple and empirically shown to perform well even at finite rates, their efficiency depends on source statistics. Thus, we consider universal quantization that provides near-optimal performance for any source distribution BID11 . Of particular interest is randomized lattice quantization, where uniform random dithering makes the distortion independent of the source, and the gap of its rate from the rate-distortion bound at any distortion level is provably no more than 0.754 bits per sample for any finite dimension BID12 .From the classical lossy compression results, this paper establishes a universal DNN compression framework consisting of universal quantization and universal lossless source coding such as LempelZiv-Welch BID13 BID14 BID15 and the Burrows-Wheeler transform BID16 BID17 . In order to recover any accuracy loss resulting from weight quantization, we furthermore propose a fine-tuning algorithm for vector quantized DNNs. The gain of fine-tuning becomes larger as the vector dimension increases, due to the fact that the number of shared quantized values that are tunable (trainable) in a vector quantized model increases as the vector dimension increases. For unpruned models, we often have a high volume of weights concentrated around zero, and thus case (b) that assigns one bin to include all the weights near zero is expected to outperform case (a), which is aligned with our lattice quantization results in FIG1 . However, it is interesting to observe that randomized lattice quantization provides similarly good performance in both cases, which is the main benefit of randomizing the source by uniform dithering before quantization. FIG1 also shows that vector quantization provides additional gain over scalar quantization particularly when the compression ratio is large.Finally, TAB1 summarizes the compression ratios that we obtain from our universal DNN compression method for pruned ResNet-32 and AlexNet BID20 models. The proposed universal DNN compression scheme with the bzip2 BID17 universal source coding algorithm yields 47.10× and 42.46× compression for ResNet-32 and AlexNet, respectively. Compared with BID0 BID1 BID3 ] which need to optimize and/or calculate source statistics for compression, we achieved a better trade-off between rate (compression ratio) and distortion (loss in accuracy) through the universal compression of DNNs. FORMULA0 ); here, uniform quantization corresponds to lattice quantization with dimension n = 1. Given the vector dimension n, the weights from all layers of the pre-trained ResNet-32 model are vectorized as in FORMULA0 for vector quantization. Then, lattice quantization or randomized lattice quantization follows. In plain lattice quantization, no random dithering is added before quantization, i.e., we set u i = 0 for all i in (2). We fine-tune the quantization codebook as explained in Section 2. We simply use Huffman coding only in this experiment to get the compressed models.The gain of randomized lattice quantization over lattice quantization can be found in FIG1 (a) in particular for n ≥ 2 and large compression ratios. We note that randomized lattice quantizers provide similarly good performance in both cases (a) and (b). Lattice quantization performs well only in case (b), where the quantization bins are optimized for given weight distribution. We emphasize that randomized lattice quantization is applicable for any network models blindly, regardless of their weight distribution and with no optimization, while it is guaranteed to yield a good rate-distortion trade-off close to the optimum within a fixed gap BID11 .","We introduce the universal deep neural network compression scheme, which is applicable universally for compression of any models and can perform near-optimally regardless of their weight distribution.",dimension n ; Han ; first ; Gersho ; one ; ImageNet ; zero ; Huffman ; AlexNet ; ResNet,coded vector quantization ; vector quantized DNNs ; non-universal scalar quantization ; finite rates ; rate ; the rate-distortion ; impressive compression results ; Hessian ; lattice quantization ; fine-tuning vector quantized DNNs,dimension n ; Han ; first ; Gersho ; one ; ImageNet ; zero ; Huffman ; AlexNet ; ResNet,"In this paper, we examine lossy compression of deep neural networks (DNNs) by weight quantization and lossless source coding for memory-efficient deployment. We introduce universal randomized lattice quantization of DNNs, which randomizes DNN weights by uniform random dithering and can perform near-optimally on any source without relying on knowledge of its probability distribution. Additionally, we present a method of fine-tuning vector quantized DNs to recover the performance loss after quantization. Our experimental results show that the proposed universal DNN compression scheme compresses 32-layer ResNet and AlexNet with compression",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"We present a new approach for efficient exploration which leverages a low-dimensional encoding of the environment learned with a combination of model-based and model-free objectives. Our approach uses intrinsic rewards that are based on a weighted distance of nearest neighbors in the low dimensional representational space to gauge novelty.
 We then leverage these intrinsic rewards for sample-efficient exploration with planning routines in representational space.
 One key element of our approach is that we perform more gradient steps in-between every environment step in order to ensure the model accuracy. We test our approach on a number of maze tasks, as well as a control problem and show that our exploration approach is more sample-efficient compared to strong baselines. In order to solve a task efficiently in Reinforcement Learning (RL), one of the main challenges is to gather informative experiences thanks to an efficient exploration of the state space. A common approach to exploration is intrinsic rewards correlated with some novelty heuristics (Schmidhuber, 2010; Houthooft et al., 2016) . With intrinsic rewards, an agent can be incentivized to efficiently explore its state space. A direct approach to calculating these novelty heuristics is to derive a reward based on the observations, such as a count-based reward (Bellemare et al., 2016; Ostrovski et al., 2017) or a prediction-error based reward (Burda et al., 2018b) . However, an issue occurs when measuring novelty directly from the raw observations, as some information in the pixel space (such as randomness) might be irrelevant. In this case, if an agent wants to efficiently explore its state space it should only focus on meaningful and novel information. In this work, we propose a method of sample-efficient exploration by leveraging novelty heuristics in a meaningful abstract state space. We leverage a low-dimensional abstract representation of states, which is learned by fitting both model-based and model-free components through a joint representation. This provides a meaningful abstract representation where states that are close temporally in dynamics are brought close together in low-dimensional representation space. We also add additional constraints to ensure that a measure of distance between states is meaningful. With this distance in representational space, we form a novelty heuristic inspired by the Novelty Search algorithm (Lehman and Stanley, 2011) to generate intrinsic rewards that we use for efficient exploration. We show that with a good low-dimensional representation of states, a policy based on planning with our novelty heuristic is able to explore with high sample-efficiency. In our experiments, we measure the effectiveness of our exploration methods by the number of samples required to explore the state space. One key element of our approach is that we perform more gradient steps in-between every environment step in order to ensure the model accuracy is high (and hence ensure an accurate novelty heuristic). Through this training scheme, our agent is also able to learn a meaningful representation of its state space in an extremely sample-efficient manner. In this paper, we show that with an interpretable abstract representation of states, our novelty metric is able to serve as an intrinsic reward that enables efficient exploration. By using this novelty metric with a combination of model-based and model-free approaches for planning, we demonstrate the efficiency of our method in multiple environments. As with most methods, our approach also has limitations. While the problem of distance metrics in high-dimensional space is partially solved in our method with the dimensionality reduction of observations by our encoder, the 2 -norm still requires a low dimension to be useful (Aggarwal et al., 2002) . This implies that our novelty metric may lose its effectiveness as we increase the dimension of our abstract representation. In addition, our exploration strategy benefits greatly from the meaningful abstractions and internal model. In some cases, the model can over-generalize with the consequence that the low-dimensional representation loses information that is crucial for the exploration of the entire state space. An interesting direction for future work would be find ways of incorporating the secondary features mentioned in Section 6.1.2. A DISCUSSION ON THE ENTROPY CONSTRAINT As for our soft constraints on representation magnitude, we use a local constraint instead of a global constraint on magnitude such that it is more suited for our novelty metric. If we are to calculate some form of intrinsic reward based on distance between neighboring states, then this distance needs to be non-zero and ideally consistent as the number of states in our history increases. In the global constraint case, if the intrinsic rewards decreases with an increase in number of states in the agent's history, then the agent will fail to be motivated to explore further. Even though the entropy maximization losses ensures the maximization of distances between random states, if we have |H s | number of states in the history of the agent, then a global constraint on representation magnitude might lead to lim Here H s is a list of all possible states in S in any order, with each possible state appearing only once. If we let k = 4, we have that: While it may seem redundant to include the 1st nearest neighbor distance in this metric (which would be itself if we've visited the state), the 1st nearest neighbor is non-zero when we calculate the novelty of a predicted state using our learned transition functionτ . From this example, we can see that there is a bias towards states with fewer direct neighbors due to the nature of our novelty metric. This poses an issue -if our goal is for sample-efficient exploration of our state space, then there is no reason to favor states with less direct neighbors.",We conduct exploration using intrinsic rewards that are based on a weighted distance of nearest neighbors in representational space.,One ; Reinforcement Learning ; Houthooft ; al. ; Ostrovski ; Burda ; Lehman ; Stanley,al ; the state ; order ; nearest neighbors ; the pixel space ; high sample-efficiency ; Lehman ; our exploration approach ; the dimensionality reduction ; random states,One ; Reinforcement Learning ; Houthooft ; al. ; Ostrovski ; Burda ; Lehman ; Stanley,"A new approach for efficient exploration uses a low-dimensional encoding of the environment learned with model-based and model-free objectives. It uses intrinsic rewards based on a weighted distance of nearest neighbors in the low dimensional representational space to gauge novelty. This approach is more sample-efficient compared to strong baselines. In order to solve a task efficiently in Reinforcement Learning (RL), an agent can be incentivized to efficiently explore its state space by using novelty heuristics in a meaningful abstract state space. In this approach, an agent should only focus on meaningful and novel information in the state space, while a prediction-error based",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Performing controlled experiments on noisy data is essential in thoroughly understanding deep learning across a spectrum of noise levels. Due to the lack of suitable datasets, previous research have only examined deep learning on controlled synthetic noise, and real-world noise has never been systematically studied in a controlled setting. To this end, this paper establishes a benchmark of real-world noisy labels at 10 controlled noise levels. As real-world noise possesses unique properties, to understand the difference, we conduct a large-scale study across a variety of noise levels and types, architectures, methods, and training settings. Our study shows that: (1) Deep Neural Networks (DNNs) generalize much better on real-world noise. (2) DNNs may not learn patterns first on real-world noisy data. (3) When networks are fine-tuned, ImageNet architectures generalize well on noisy data. (4) Real-world noise appears to be less harmful, yet it is more difficult for robust DNN methods to improve. (5) Robust learning methods that work well on synthetic noise may not work as well on real-world noise, and vice versa. We hope our benchmark, as well as our findings, will facilitate deep learning research on noisy data.
 Deep Neural Networks (DNNs) trained on noisy data demonstrate intriguing properties. For example, DNNs are capable of memorizing completely random training labels but generalize poorly on clean test data (Zhang et al., 2017) . When trained with stochastic gradient descent, DNNs learn patterns first before memorizing the label noise (Arpit et al., 2017) . These findings inspired recent research on noisy data. As training data are usually noisy, the fact that DNNs are able to memorize the noisy labels highlights the importance of deep learning research on noisy data. To study DNNs on noisy data, previous work often performs controlled experiments by injecting a series of synthetic noises into a well-annotated dataset. The noise level p may vary in the range of 0%-100%, where p = 0% is the clean dataset whereas p = 100% represents the dataset of zero correct labels. The most commonly used noise in the literature is uniform (or symmetric) labelflipping noise, in which the label of each example is independently and uniformly changed to a random (incorrect) class with probability p. Controlled experiments on noise levels are essential in thoroughly understanding a DNN's properties across a spectrum of noise levels and faithfully comparing the strengths and weaknesses of different methods. The synthetic noise enables researchers to experiment on controlled noise levels, and drives the development of theory and methodology in this field. On the other hand, some studies were also verified on real-world noisy datasets, e.g. on WebVision (Li et al., 2017a) , Clothing-1M (Xiao et al., 2015) , Fine-grained Images (Krause et al., 2016) , and Instagram hashtags (Mahajan et al., 2018) , where the images are automatically tagged with noisy labels according to their surrounding texts. However, these datasets do not provide true labels for the training images. Their underlying noise levels are not only fixed but also unknown, rendering them infeasible for controlled studies on noise levels. In this paper, we refer image-search noise in these datasets as ""real-world noise"" to distinguish it from synthetic label-flipping noise. To study real-world noise in a controlled setting, we establish a benchmark of controlled real-world noisy labels, building on two existing datasets for coarse and fine-grained image classification: MiniImageNet (Vinyals et al., 2016) and Stanford Cars (Krause et al., 2013) . We collect noisy labels using text-to-image and image-to-image search via Google Image Search. Every training image is independently annotated by 3-5 workers, resulting in a total of 527,489 annotations over 147,108 images. We create ten different noise levels from 0% to 80% by gradually replacing the original images with our annotated noisy images. Our new benchmark will enable future research on the real-world noisy data with a controllable noise level. We find that real-world noise possesses unique properties in its visual/semantic relevance and underlying class distribution. To understand the differences, we conduct a large-scale study comparing synthetic noise, namely blue-pilled noise (or Blue noise), and real-world noise (or Red noise 1 ). Specifically, we train DNNs across 10 noise levels, 7 network architectures, 6 existing robust learning methods, and 2 training settings (fine-tuning and training from random initialization). Our study reveals several interesting findings. First, we find that DNNs generalize much better on real-world noise than synthetic noise. Our results verify Zhang et al. (2017) 's finding of deep learning generalization on synthetic noise. However, we observe a considerably smaller generalization gap on real-world noise. This does not mean that real-world noise is easier to tackle. On the contrary, we find that real-world noise is more difficult for robust DNNs to improve. Second, our results substantiate Arpit et al. (2017) 's finding that DNNs learn patterns first on noisy data. But we find this behavior becomes insignificant on real-world noise and completely disappears on the fine-grained classification dataset. This finding lets us rethink the role of ""early stopping"" (Yao et al., 2007; Arpit et al., 2017) on real-world noisy data. Third, we find that when networks are fine-tuned, ImageNet architectures generalize well on noisy data, with a correlation of r = 0.87 and 0.89 for synthetic and real-world noise, respectively. This finding generalizes Kornblith et al. (2019) 's finding, i.e. ImageNet architectures generalize well across clean datasets, to the noisy data. Our contribution is twofold. First, we establish a large benchmark of controlled real image search noise. Second, we conduct perhaps the largest study in the literature to understand DNN training across a wide variety of noise levels and types, architectures, methods, and training settings. We hope our benchmark along with our findings, resulted from a considerable amount of manual labeling effort (∼520K annotations) and computing resources (∼3K experiments), will facilitate future deep learning research on real-world noisy data. Our main findings are summarized as follows: 1. DNNs generalize much better on real-world noise than synthetic noise. Real-world noise appears to be less harmful, yet it is more difficult for robust DNN methods to improve. 2. DNNs may not learn patterns first on the real-world noisy data. 3. When networks are fine-tuned, ImageNet architectures generalize well on noisy data. 4. Adding noisy examples to a clean dataset may improve performance as long as the noise level is below a certain threshold (30% in our experiments). In this paper, we established a benchmark for controlled real-world noise. On the benchmark, we conducted a large-scale study to understand deep learning on noisy data across a variety of settings. Our studies revealed a number of new findings, improving our understanding of deep learning on noisy data. By comparing six robust deep learning methods, we found that real-world noise is more difficult to improve and methods that work well on synthetic noise may not work as well on realworld noise, and vice versa. This encourages future research to be also carried out on controlled real-world noise. We hope our benchmark, as well as our findings, will facilitate deep learning research on real-world noisy data.",We establish a benchmark of controlled real noise and reveal several interesting findings about real-world noisy data.,two ; first ; noisy data ; Google Image Search ; noisy labels ; WebVision ; Zhang et al ; Deep Neural Networks ; Zhang et al. ; al.,"our experiments ; the largest study ; patterns ; a DNN's properties ; real-world noise ; zero ; real-world noisy labels ; image-search noise ; ""early stopping ; the label noise",two ; first ; noisy data ; Google Image Search ; noisy labels ; WebVision ; Zhang et al ; Deep Neural Networks ; Zhang et al. ; al.,"Performing controlled experiments on noisy data is crucial in understanding deep learning across a spectrum of noise levels. However, previous research has only examined deep learning on controlled synthetic noise, and real-world noise has never been systematically studied in a controlled setting. To establish a benchmark, we conduct a large-scale study across different noise levels and types, architectures, methods, and training settings. Our study shows that: (1) Deep Neural Networks (DNNs) generalize much better on real world noise and may not learn patterns first. (2) When networks are fine-tuned, ImageNet architectures generalize well on noisy",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"A number of recent methods to understand neural networks have focused on quantifying the role of individual features.   One such method, NetDissect identifies interpretable features of a model using the Broden dataset of visual semantic labels (colors, materials, textures, objects and scenes).   Given the recent rise of a number of action recognition datasets, we propose extending the Broden dataset to include actions to better analyze learned action models.   We describe the annotation process, results from interpreting action recognition models on the extended Broden dataset and examine interpretable feature paths to help us understand the conceptual hierarchy used to classify an action. The success of Deep convolutional neural networks (DNNs) is partly due to their ability to learn hidden representations that capture the important factors of variation in the data. Previous works have visualized the units of deep convolutional networks by sampling image patches that maximize the activation of each feature BID6 or by generating images that maximize each feature activation. Such visualizations show that individual features act as visual concept detectors. Features at lower layers detect concrete patterns such as textures or shapes while features at higher layers detect more semantically meaningful concepts such as dog heads or bicycle wheels. One tool for network interpretability (NetDissect) BID0 BID8 uses the Broden dataset (consists of objects, scenes, object parts, textures and materials) to evaluate individual units.Recently, DNNs have shown significant progress in action recognition with the introduction of large-scale video datasets. However, while NetDissect with the Broden dataset is appropriate for networks trained on object or scene recognition, it does not include the ability to detect learned action concepts.In this paper, we propose extending the Broden dataset to include actions so that we can more appropriately interpret action recognition networks. We describe our annotation process to collect images across action classes and select Sample videos Example frames from a few videos to show intraclass action variation Action Regions Spatial localization of actions in single frames for network interpretation Action Interpretation Identifying interpretable action features regions of importance for identifying each action. We then show results using our Action Region dataset together with the existing Broden set to identify interpretable action features in deep networks trained for action recognition. The Action Region dataset presented, and the code for integrating with NetDissect, will be made available online.",We expand Network Dissection to include action interpretation and examine interpretable feature paths to understand the conceptual hierarchy used to classify an action.,One ; NetDissect ; Broden ; Action Regions Spatial ; Action Region,The Action Region dataset ; each action ; actions ; image patches ; colors ; Action Regions Spatial ; textures ; bicycle wheels ; single frames ; an action,One ; NetDissect ; Broden ; Action Regions Spatial ; Action Region,"Neural networks have focused on quantifying the role of individual features. For example, NetDissect identifies interpretable features of a model using the Broden dataset of visual semantic labels.   In this paper, we describe the annotation process, results from interpreting action recognition models and examine interpretable feature paths to help us understand the conceptual hierarchy used to classify an action. Deep convolutional neural networks (DNNs) are successful due to their ability to learn hidden representations that capture important factors of variation in the data. Previous works have visualized the units of deep convolutionals networks by sampling image patches that maximize activation of",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Domain adaptation is critical for success in new, unseen environments.
 Adversarial adaptation models applied in feature spaces discover domain invariant representations, but are difficult to visualize and sometimes fail to capture pixel-level and low-level domain shifts.
 Recent work has shown that generative adversarial networks combined with cycle-consistency constraints are surprisingly effective at  mapping images between domains, even without the use of aligned image pairs.
 We propose a novel discriminatively-trained Cycle-Consistent Adversarial Domain Adaptation model.
 CyCADA adapts representations at both the pixel-level and feature-level, enforces cycle-consistency while leveraging a task loss, and does not require aligned pairs.   Our model can be applied in a variety of visual recognition and prediction settings.
 We show new state-of-the-art results across multiple adaptation tasks, including digit classification and semantic segmentation of road scenes demonstrating transfer from synthetic to real world domains. Deep neural networks excel at learning from large amounts of data, but can be poor at generalizing learned knowledge to new datasets or environments. Even a slight departure from a network's training domain can cause it to make spurious predictions and significantly hurt its performance BID41 . The visual domain shift from non-photorealistic synthetic data to real images presents an even more significant challenge. While we would like to train models on large amounts of synthetic data such as data collected from graphics game engines, such models fail to generalize to real-world imagery. For example, a state-of-the-art semantic segmentation model trained on synthetic dashcam data fails to segment the road in real images, and its overall per-pixel label accuracy drops from 93% (if trained on real imagery) to 54% (if trained only on synthetic data, see Table 5 ).Feature-level unsupervised domain adaptation methods address this problem by aligning the features extracted from the network across the source (e.g. synthetic) and target (e.g. real) domains, without any labeled target samples. Alignment typically involves minimizing some measure of distance between the source and target feature distributions, such as maximum mean discrepancy BID23 , correlation distance BID35 , or adversarial discriminator accuracy BID9 BID41 . This class of techniques suffers from two main limitations. First, aligning marginal distributions does not enforce any semantic consistency, e.g. target features of a car may be mapped to source features of a bicycle. Second, alignment at higher levels of a deep representation can fail to model aspects of low-level appearance variance which are crucial for the end visual task.Generative pixel-level domain adaptation models perform similar distribution alignment-not in feature space but rather in raw pixel space-translating source data to the ""style"" of a target domain. Recent methods can learn to translate images given only unsupervised data from both domains BID2 BID21 BID34 . The results are visually compelling , but such image-space models have only been shown to work for small image sizes and limited domain shifts. A more recent approach BID1 ) was applied to larger (but still not high resolution) images, but in a controlled environment with visually simple images for robotic applications. Furthermore, they also do not necessarily preserve content: while the translated image may ""look"" like it came from the right domain, crucial semantic information may be lost. For 54.0% 83.6%Figure 1: We propose CyCADA , an adversarial unsupervised adaptation algorithm which uses cycle and semantic consistency to perform adaptation at multiple levels in a deep network. Our model provides significant performance improvements over source model baselines.Pixel Feature Semantic Cycle Loss Loss Loss ConsistentCycleGAN BID48 Feature Adapt BID9 BID41 Pixel Adapt BID36 BID2 ) CyCADA Table 1 : Our model, CyCADA, may use pixel, feature, and semantic information during adaptation while learning an invertible mapping through cycle consistency.example, a model adapting from line-drawings to photos could learn to make a line-drawing of a cat look like a photo of a dog.How can we encourage the model to preserve semantic information in the process of distribution alignment? In this paper, we explore a simple yet powerful idea: give an additional objective to the model to reconstruct the original data from the adapted version. Cycle-consistency was recently proposed in a cross-domain image generation GAN model, CycleGAN BID48 , which showed transformative image-to-image generation results, but was agnostic to any particular task.We propose Cycle-Consistent Adversarial Domain Adaptation (CyCADA), which adapts representations at both the pixel-level and feature-level while enforcing pixel and semantic consistency. We use a reconstruction (cycle-consistency) loss to enforce the cross-domain transformation to preserve pixel information and a semantic labeling loss to enforce semantic consistency. CyCADA unifies prior feature-level BID9 BID41 and image-level BID21 BID2 BID34 adversarial domain adaptation methods together with cycle-consistent image-to-image translation techniques BID48 , as illustrated in Table 1 . It is applicable across a range of deep architectures and/or representation levels, and has several advantages over existing unsupervised domain adaptation methods.We apply our CyCADA model to the task of digit recognition across domains and the task of semantic segmentation of urban scenes across domains. Experiments show that our model achieves state of the art results on digit adaptation, cross-season adaptation in synthetic data, and on the challenging synthetic-to-real scenario. In the latter case, it improves per-pixel accuracy from 54% to 83 %, nearly closing the gap to the target-trained model.Our experiments confirm that domain adaptation can benefit greatly from cycle-consistent pixel transformations, and that this is especially important for pixel-level semantic segmentation with contemporary FCN architectures. We demonstrate that enforcing semantic consistency between input and stylized images prevents label flipping on the large shift between SVHN and MNIST (example, prevents a SVHN 9 from being mapped into an MNIST 2). Interestingly, on our semantic segmentation tasks (GTA to CityScapes ) we did not observe label flipping to be a major source of error, even without the semantic consistency loss. Because of this, and due to memory constraints, we do not include this loss for the segmentation tasks. Further, we show that adaptation at both the pixel and representation level can offer complementary improvements with joint pixel-space and feature adaptation leading to the highest performing model for digit classification tasks. We presented a cycle-consistent adversarial domain adaptation method that unifies cycle-consistent adversarial models with adversarial adaptation methods. CyCADA is able to adapt even in the absence of target labels and is broadly applicable at both the pixel-level and in feature space. An image-space adaptation instantiation of CyCADA also provides additional interpretability and serves as a useful way to verify successful adaptation. Finally, we experimentally validated our model on a variety of adaptation tasks: state-of-the-art results in multiple evaluation settings indicate its effectiveness, even on challenging synthetic-to-real tasks. We begin by pretraining the source task model, f S , using the task loss on the labeled source data. Next, we perform pixel-level adaptation using our image space GAN losses together with semantic consistency and cycle consistency losses. This yeilds learned parameters for the image transformations, G S→T and G T →S , image discriminators, D S and D T , as well as an initial setting of the task model, f T , which is trained using pixel transformed source images and the corresponding source pixel labels. Finally, we perform feature space adpatation in order to update the target semantic model, f T , to have features which are aligned between the source images mapped into target style and the real target images. During this phase, we learn the feature discriminator, D feat and use this to guide the representation update to f T . In general, our method could also perform phases 2 and 3 simultaneously, but this would require more GPU memory then available at the time of these experiments.For all feature space adaptation we equally weight the generator and discriminator losses. We only update the generator when the discriminator accuracy is above 60% over the last batch (digits) or last 100 iterations (semantic segmentation) -this reduces the potential for volatile training. If after an epoch (entire pass over dataset) no suitable discriminator is found, the feature adaptation stops, otherwise it continues until max iterations are reached.",An unsupervised domain adaptation approach which adapts at both the pixel and feature levels,SVHN ; GAN ; Cycle-Consistent Adversarial Domain Adaptation ; contemporary FCN ; CityScapes ; First ; GPU ; ConsistentCycleGAN ; two ; max,adaptation ; cycle-consistent pixel transformations ; semantic segmentation ; a line-drawing ; pixel transformed source images ; the right domain ; which ; Domain adaptation ; graphics game engines ; the network,SVHN ; GAN ; Cycle-Consistent Adversarial Domain Adaptation ; contemporary FCN ; CityScapes ; First ; GPU ; ConsistentCycleGAN ; two ; max,"Domain adaptation is critical for success in unseen environments. Adversarial adaptation models discover domain invariant representations but are difficult to visualize and sometimes fail to capture pixel-level and low-level domain shifts. However, generative adversarial networks combined with cycle-consistency constraints are surprisingly effective at  mapping images between domains, even without the use of aligned image pairs. A novel discriminatively-trained Cycle-Consistent adversarial domain Adaptation model (CADA) can be used in visual recognition and prediction settings. This class of neural networks excel at learning from large amounts of data, but can be poor at general",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"This paper presents the ballistic graph neural network. Ballistic graph neural network tackles the weight distribution from a transportation perspective and has many different properties comparing to the traditional graph neural network pipeline. The ballistic graph neural network does not require to calculate any eigenvalue. The filters propagate exponentially faster($\sigma^2 \sim T^2$) comparing to traditional graph neural network($\sigma^2 \sim T$). We use a perturbed coin operator to perturb and optimize the diffusion rate. Our results show that by selecting the diffusion speed, the network can reach a similar accuracy with fewer parameters. We also show the perturbed filters act as better representations comparing to pure ballistic ones. We provide a new perspective of training graph neural network, by adjusting the diffusion rate, the neural network's performance can be improved. How to collect the nodes' correlation on graphs fast and precisely? Inspired by convolutional neural networks(CNNs), graph convolutional networks(GCNs) can be applied to many graph-based structures like images, chemical molecules and learning systems. Kipf & Welling (2016) Similar to neural networks, GCNs rely on random walk diffusion based feature engineering to extract and exploit the useful features of the input data. Recent works show random walk based methods can represent graph-structured data on the spatial vertex domain. For example, Li et al. (2017) use bidirectional random walks on the graph to capture the spatial dependency and Perozzi et al. (2014) present a scalable learning algorithm for latent representations of vertices in a network using random walks. Except for the spatial domain, many researchers focus on approximating filters using spectral graph theory method, for example, Bruna et al. (2013) construct a convolutional architecture based on the spectrum of the graph Laplacian; Defferrard et al. (2016) use high order polynomials of Laplacian matrix to learn the graphs in a NN structure model. The ballistic walk algorithm consists of two parts, a walker in the position space H spatial and a coin in the coin space H c . Thus the walker is described using states in Hibert space H spatial ⌦ H c . Let the walker initially be at the state | i 0 = |i, ji p ⌦ s 0 , where s 0 is normally symmetric state in H c . In analogy to the classical random walk, the next state of the walker can be expressed by In this paper, we consider the ballistic walk on a regular two-dimensional graph. The coin space H c consists of four states: |#i, |""i, | i, |!i , represents move up, down, left and right for the next step. The spatial space H spatial consists N states representing the walker's position, where N is the number of nodes. The notation |ni denotes an orthonormal basis for H spatial and hn| is the Hermitian conjugate of the state. For a finite-dimensional vector space, the inner product hn 0 |ni is nn 0 and the outer product |n 0 i hn| equals to a matrix in R N ⇥N . The probability stay on the node |i, ji is Pseudo-code of our method is given in Algorithm 1. Algorithm 1: Ballistic walk on 2D regular graph Result: The walker's state after K steps start from (i, j) We want our filters have a diffusion distance in a reasonable region(a < Distance < b). However, the ballistic filters' distances increase with steps. The filters are not capable to dense sampling some specific regions. By selecting different randomness and steps, we can generate filters localized in a bounded area. The noisy Hadamard can be written as Table 2 : Diffusion rate with different randomness Table 3 shows the accuracy with different perturbed filters(↵ = 0, 0.05, 0.10, 0.15, 0.20). = 2 ⇥ R ⇥ ⇡↵ denotes the randomness in the coin space, R is a random number between 0 and 1. The corresponding transportation speed is shown in table 2 and Figure 12 . As the ↵ increases to 0.20, the speed drops to 0.323. ↵ is a controller of the diffusion speed, as ↵ becomes larger, the ballistic tranportation will finally evolve to the classical diffusive couterpart. In this paper, we introduced a generalization of graph neural network: ballistic graph neural network. We started from the speed problem of the traditional diffusive kernel and tackle this problem from the perspective of transportation behaviour. We showed the linear transportation behaviour of ballistic filters and introduced the de-coherence scheme to adjust the filters' speed. Compared with diffusive filters, the ballistic filters achieve similar accuracy using fewer of the parameters. Besides, we showed that the efficiency of the ballistic filters could be improved by controlling transportation behaviour. Compared to the random walk method, we used two operators: the coin operator and shift operator to control the walker, and thus controlled the information the walker gathers. Our pipeline provides a new perspective for efficient extracting the graphical information using diffusion-related models. Future work can investigate these two directions: The Network Structure. In this paper, we use simplified architecture to demonstrate the concept of the ballistic walk, the layers are limited to 5 layers, and we use traditional average pooling. More layers can be added to improve particular accuracy, and more sophisticated pooling methods can be introducedDefferrard et al. (2016) . Other techniques like dropout can also be employed to improve accuracy. The Ballistic Filter. De-coherence can also be introduced into the shift operator. In other words, we can use perturbed shifted operator, and thus we introduce randomness in the spatial domain. We can also try different unitary operators in the coin space or change the initial state of the walker. The extension to general graphs can be generalized by adding self-loops to the nodes and thus make the graph regular. The ballistic filters are inspired by two-dimensional quantum walk. The quantum coherence effect guarantees fast ballistic transportation. The different states in the coin space can be regarded as the independent state from spatial behaviour, for example, the spin of fermions or the polarization of light. More information about the quantum walk can be found at Childs et al. (2003) . Why introducing ballistic filters results in better performance? We here offer a conjecture from the perspective of signal processing using one-dimensional condition. The classical diffusion in the one-dimensional case has the shape of: and the frequency part can be written as:ĝ The g(x) can be regarded as a gaussian low pass filter. For a gaussian high-pass filter, the spatial distribution is: Makandar & Halalli (2015) hg The long time probability distribution of ballistic walk is: Luo & Xue (2015) P (x) = P 0 + ae (12) Figure 15 shows the distribution of gaussian high pass filter and the cumulative distribution of 24th and 25step of ballistic diffusion. These two distributions have a similar shape while the ballistic distribution has steeper edges resulted from fast transportation. The ballistic filters' capability to collect the long-time probability means it can act as a high-pass filter with different sizes. The size of the filters depends on the walking steps. Figure 16 shows the ballistic diffusion with a pulse signal from t = ⌧ to t = ⌧ . The orange dashed line is an approximated shape of ballistic transportation of the leftmost signal(t = ⌧ ), and the blue dashed line corresponds to t = ⌧ . The width of the approximated shape is related to the walking steps. For random walk based diffusive transportation after certain steps of diffusion, the region from t = ⌧ to t = ⌧ have a gaussian shape since it is sum of gaussian distribution with centers range from t = ⌧ to t = ⌧ . The classical diffusion acts like a blur filter(low pass filter). For ballistic diffusion, the shape of the pulse signal from t = ⌧ to t = ⌧ evolves to a 'valley' shape and thus, the ballistic diffusion is similar to a high pass filter.",A new perspective on how to collect the correlation between nodes based on diffusion properties.,Hadamard ; Li ; Kipf & Welling ; al. ; Hermitian ; four ; Childs et al ; Laplacian ; Defferrard et al ; two,a matrix ; two operators ; fewer parameters ; Distance ; a gaussian low pass filter ; H spatial ; the coin space ; Hadamard ; Recent works ; a scalable learning algorithm,Hadamard ; Li ; Kipf & Welling ; al. ; Hermitian ; four ; Childs et al ; Laplacian ; Defferrard et al ; two,"The ballistic graph neural network tackles weight distribution from a transportation perspective and has many different properties compared to traditional graph network pipeline. The filters propagate exponentially faster($\sigma^2 \sim T^2$) compared with traditional graph neural networks. By selecting the diffusion speed, the network can reach a similar accuracy with fewer parameters. The perturbed filters act as better representations comparing to pure ballistic ones. By adjusting the diffusion rate, the neural network's performance can be improved. Similar to convolutional networks(CNNs), graph convolutionals (GCNs) can be applied to graph-based structures like images, chemical molecules",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Measuring Mutual Information (MI) between high-dimensional, continuous, random variables from observed samples has wide theoretical and practical applications. Recent works have developed accurate MI estimators through provably low-bias approximations and tight variational lower bounds assuming abundant supply of samples, but require an unrealistic number of samples to guarantee statistical significance of the estimation. In this work, we focus on improving data efficiency and propose a Data-Efficient MINE Estimator (DEMINE) that can provide a tight lower confident interval of MI under limited data, through adding cross-validation to the MINE lower bound (Belghazi et al., 2018). Hyperparameter search is employed and a novel meta-learning approach with task augmentation is developed to increase robustness to hyperparamters, reduce overfitting and improve accuracy. With improved data-efficiency, our DEMINE estimator enables statistical testing of dependency at practical dataset sizes. We demonstrate the effectiveness of DEMINE on synthetic benchmarks and a real world fMRI dataset, with application of inter-subject correlation analysis. Mutual Information (MI) is an important, theoretically grounded measure of similarity between random variables. MI captures general, non-linear, statistical dependencies between random variables. MI estimators that estimate MI from samples are important tools widely used in not only subjects such as physics and neuroscience, but also machine learning ranging from feature selection and representation learning to explaining decisions and analyzing generalization of neural networks. Existing studies on MI estimation between general random variables focus on deriving asymptotic lower bounds and approximations to MI under infinite data, and techniques for reducing estimator bias such as bias correction, improved signal modeling with neural networks and tighter lower bounds. Widely used approaches include the k-NN-based KSG estimator (Kraskov et al., 2004) and the variational lower-bound-based Mutual Information Neural Estimator (MINE) family (Belghazi et al., 2018; Poole et al., 2018) . Despite the empirical and asymptotic bias improvements, MI estimation has not seen wide adoption. The challenges are two-fold. First, the analysis of dependencies among variables -let alone any MI analyses for scientific studies -requires not only an MI estimate, but also confidence intervals (Holmes & Nemenman, 2019) around the estimate to quantify uncertainty and statistical significance. Existing MI estimators, however, do not provide confidence intervals. As low probability events may still carry a significant amount of information, the MI estimates could vary greatly given additional observations (Poole et al., 2018) . Towards providing upper and lower bounds of true MI under limited number of observations, existing MI lower bound techniques assume infinite data and would need further relaxations when a limited number of observations are provided. Closest to our work, Belghazi et al. (2018) studied the lower bound of the MINE estimator under limited data, but it involves bounds on generalization error of the signal model and would not yield useful confidence intervals for realistic datasets. Second, practical MI estimators should be insensitive to the choice of hyperparameters. An estimator should return a single MI estimate with its confidence interval irrespective of the type of the data and the number of observations. For learning-based approaches, this means that the model design and optimization hyperparameters need to not only be determined automatically but also taken into account when computing the confidence interval. Towards addressing these challenges, our estimator, DEMINE, introduces a predictive MI lower bound for limited samples that enables statistical dependency testing under practical dataset sizes. Our estimator builds on top of the MINE estimator family, but performs cross-validation to remove the need to bound generalization error. This yields a much tighter lower bound agnostic to hyperparameter search. We automatically selected hyperparameters through hyperparameter search, and a new cross-validation meta-learning approach is developed, based upon few-shot meta-learning, to automatically decide initialization of model parameters. Meta-overfitting is strongly controlled through task augmentation, a new task generation approach for meta-learning. With these improvements, we show that DEMINE enables practical statistical testing of dependency for not only synthetic datasets but also for real world functional Magnetic Resonance Imaging (fMRI) data analysis capturing nonlinear and higher-order brain-to-brain coupling. Our contributions are summarized as follows: 1) A data-efficient Mutual Information Neural Estimator (DEMINE) for statistical dependency testing; 2) A new formulation of meta-learning using Task Augmentation (Meta-DEMINE); 3) Application to real life, data-scarce applications (fMRI). We illustrated that a predictive view of the MI lower bounds coupled with meta-learning results in data-efficient variational MI estimators, DEMINE and Meta-DEMINE, that are capable of performing statistical test of dependency. We also showed that our proposed task augmentation reduces overfitting and improves generalization in meta-learning. We successfully applied MI estimation to real world, data-scarce, fMRI datasets. Our results suggest a greater avenue of using neural networks and meta-learning to improve MI analysis and applying neural network-based information theory tools to enhance the analysis of information processing in the brain. Model-agnostic, high-confidence, MI lower bound estimation approaches -including MINE, DEMINE and Meta-DEMINE-are limited to estimating small MI lower bounds up to O(log N ) as pointed out in (McAllester & Statos, 2018) , where N is the number of samples. In real fMRI datasets, however, strong dependency is rare and existing MI estimation tools are limited more by their ability to accurately characterize the dependency. Nevertheless, when quantitatively measuring strong dependency, cross-entropy (McAllester & Statos, 2018) Sample a batch of ( Update θ (i) using Adam (Kingma & Ba, 2014) with η 7: end for","A new & practical statistical test of dependency using neural networks, benchmarked on synthetic and a real fMRI datasets.",MI ; Belghazi ; Data-Efficient MINE Estimator ; Magnetic Resonance Imaging ; Holmes & Nemenman ; Mutual Information Neural Estimator ; Meta-DEMINE ; DEMINE ; Mutual Information (MI ; KSG,"Our contributions ; tight variational lower bounds ; upper and lower bounds ; strong dependency ; a greater avenue ; the estimation ; Kraskov et al. ; a new task generation approach ; an important, theoretically grounded measure ; Hyperparameter search",MI ; Belghazi ; Data-Efficient MINE Estimator ; Magnetic Resonance Imaging ; Holmes & Nemenman ; Mutual Information Neural Estimator ; Meta-DEMINE ; DEMINE ; Mutual Information (MI ; KSG,"Measuring Mutual Information (MI) between high-dimensional, continuous, random variables from observed samples has wide theoretical and practical applications. Recent works have developed accurate MI estimators through provably low-bias approximations and tight variational lower bounds, but require an unrealistic number of samples to guarantee statistical significance. In this work, we focus on improving data efficiency and propose a Data-Efficient MINE Estimator (DEMINE) that can provide a tight lower confident interval of MI under limited data, by adding cross-validation to the MINE lower bound. Hyperparameter search is employed and a novel meta",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"In this paper, we propose a \textit{weak supervision} framework for neural ranking tasks based on the data programming paradigm \citep{Ratner2016}, which enables us to leverage multiple weak supervision signals from different sources. Empirically, we consider two sources of weak supervision signals, unsupervised ranking functions and semantic feature similarities. We train a BERT-based passage-ranking model (which achieves new state-of-the-art performances on two benchmark datasets with full supervision) in our weak supervision framework. Without using ground-truth training labels, BERT-PR models outperform BM25 baseline by a large margin on all three datasets and even beat the previous state-of-the-art results with full supervision on two of datasets. Recent advances in deep learning have allowed promising improvement in developing various stateof-the-art neural ranking models in the information retrieval (IR) community BID8 BID17 BID12 BID9 BID13 . Similar achievement has been seen in the reading comprehension (RC) community using neural passage ranking (PR) models for answer selection tasks BID19 BID16 BID10 . Most of these neural ranking models, however, require a large amount of training data. As such, we have seen the progress of deep neural ranking models is coming along with the development of several large-scale datasets in both IR and RC communities, e.g. BID0 BID7 BID6 BID3 . Admittedly, creating hand-labeled ranking datasets is very expensive in both human labor and time.To overcome this issue, one strategy is to utilize weak supervision to replace human annotators. Usually we can cheaply obtain large amount of low-quality labels from various sources, such as prior knowledge, domain expertise, human heuristics or even pretrained models. The idea of weak supervision is to extract signals from the noisy labels to train our model. BID4 first applied weak supervision technique to train deep neural ranking models. They show that the neural ranking models trained on labels solely generated from BM25 scores can remarkably outperform the BM25 baseline in IR tasks. BID11 further investigated this approach by using external news corpus for training.In this work, we focus on the setting where queries and their associated candidate passages are given but no relevance judgment is available. Instead of solely relying on the labels from single source (BM25 score), we propose to leverage the weak supervision signals from diverse sources. BID14 proposed a general data programming framework to create data and train models in a weakly supervised manner. To tailor to the ranking tasks, instead of generating a ranked list of passages for each query, we generate binary labels for each query-passage pair. In our neural ranking models, we focus on BERT-based ranking model BID5 (architecture shown in FIG0 ), which achieves new state-of-the-art performance on two public benchmark datasets with full supervision. The contributions of this work are in two fold: (a) we propose a simple data programming framework for ranking tasks; (b) we train a BERT ranking model using our framework, by considering two simple sources of weak supervision signals, unsupervised ranking methods (BM25 and TF-IDF scores) and unsupervised semantic feature representation, we show our model outperforms BM25 baseline by a large margin (around 20% relative improvement in top-1 accuracy on average) and the previous state-of-the-art performance (around 10% relative improvement in top-1 accuracy on average) on three datasets without using ground-truth training labels. In this work, we proposed a simple weak supervision pipeline for neural ranking models based on the data programming paradigm. In particular, we also proposed a new PR model based on BERT, which achieves new SOTA results. In our experiments on different datasets, our weakly supervised BERT-PR model outperforms the BM25 baseline by a large margin and remarkably, even beats the previous SOTA performances with full supervision on two datasets. Further research can be done on how to better aggregate pseudo ranking labels. In our pipeline we reduce the ranking labels into binary labels of relevance of query-passage pairs, which may result in loss of useful information. It would be interesting to design generative models on the ranking labels directly.","We propose a weak supervision training pipeline based on the data programming framework for ranking tasks, in which we train a BERT-base ranking model and establish the new SOTA.",two ; BERT ; BERT-PR ; three ; IR ; one ; SOTA,different sources ; full supervision ; three ; each query ; various sources ; query-passage pairs ; weak supervision signals ; deep learning ; two datasets ; no relevance judgment,two ; BERT ; BERT-PR ; three ; IR ; one ; SOTA,"In this paper, we propose a framework for neural ranking tasks based on the data programming paradigm \citep{Ratner2016}, which enables us to leverage multiple weak supervision signals from different sources. We consider two sources of weak supervision: unsupervised ranking functions and semantic feature similarities. BERT-based passage-ranking models outperform BM25 baseline by a large margin on all three datasets and even beat the previous state-of-the-art results with full supervision on two of datasets. Recent advances in deep learning have allowed promising improvement in neural ranking models in both IR and RC communities, such as BID0 BID",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Long training times of deep neural networks are a bottleneck in machine learning research. The major impediment to fast training is the quadratic growth of both memory and compute requirements of dense and convolutional layers with respect to their information bandwidth. Recently, training `a priori' sparse networks has been proposed as a method for allowing layers to retain high information bandwidth, while keeping memory and compute low. However, the choice of which sparse topology should be used in these networks is unclear. In this work, we provide a theoretical foundation for the choice of intra-layer topology. First, we derive a new sparse neural network initialization scheme that allows us to explore the space of very deep sparse networks. Next, we evaluate several topologies and show that seemingly similar topologies can often have a large difference in attainable accuracy. To explain these differences, we develop a data-free heuristic that can evaluate a topology independently from the dataset the network will be trained on. We then derive a set of requirements that make a good topology, and arrive at a single topology that satisfies all of them. Training deep neural networks requires both powerful hardware and a significant amount of time. Long training times are a significant bottleneck to deep learning research, as researchers typically iteratively design and test new architectures for a specific problem. While a lot of research has been dedicated to accelerating inference, we investigate training as (1) accelerating training can speed up research iteration, (2) evolutionary algorithms for DNN architecture exploration are increasingly being used as an alternative to domain expertise (Jaderberg et al., 2017) , and network training is moving to edge devices (Pirk et al., 2019) . Unfortunatelly, the memory requirements of dense, convolutional and recurrent layers grow quadratically with layer information bandwidth 1 . In other words, doubling the size of layer inputs and outputs quadruples the size of the layer. This causes majority of the networks to be memory-bound, making DNN training impractical without batching, a method where training is performed on multiple inputs at a time and updates are aggregated per batch. While batching alleviates the pressure on DRAM bandwidth, it can decrease model accuracy (Masters & Luschi, 2018) especially when scaling training on large clusters (Akiba et al., 2017) . Furthermore, larger models in off-chip memory become dominant energy cost (Han et al., 2015a) , complicating on-line training on battery-power devices. Conventional dense and convolutional layers do not offer the user to individually tune layer size and the number of layer inputs and outputs. In this work, we seek a method to decouple the information bandwidth from layer expressivity. Such a method would allow us to (1) speed up training networks by storing them in on-chip memory, (2) remove the memory bottleneck and the need for batching, (3) allow more efficient training on distributed systems, and (4) reduce the energy consumption due to the excessive compute and storage requirements of modern DNNs, potentially allowing us to move training to edge devices. Several works have proposed a priori structured sparsity (Prabhu et al., 2017; Isakov et al., 2018) or weight sharing (Ding et al., 2017) to allow training simpler but 'wider' models. A priori sparsity, where the sparse network topology is selected before training has started, is a promising approach that allows the user to finely and transparently tune the ratio of information bandwidth to memory requirements. If the topology is structured, efficient software or hardware implementations can be built to accelerate processing with dense network performance . However, before custom architectures or low-level kernels can be built, a general theory of why certain topologies perform -or underperform -is needed. To the best of our knowledge, no work yet tackles the question of the existence of a 'best' topology for sparse neural network training. This paper provides an answer on how a topology should be selected. Our contributions are as following: • We propose a sparse cascade architecture that can replace dense or convolutional layers without affecting the rest of the network architecture. • We develop a sparse neural network initialization scheme that allows us to train very deep sparse networks without suffering from the vanishing gradient effect. • We evaluate sevaral topologies on a matrix reconstruction task and show that the choice of topology has a strong effect on attainable network accuracy. • In order to evaluate topologies independently of a dataset, we develop a data-free heuristic for predicting the expressiveness of a given sparse network. • From the heuristic, we derive requirements that make a good topology, and settle on a single family of sparse networks. In this work, we have explored accelerating DNN training by pruning networks ahead of time. We proposed replacing dense and convolutional layers using sparse cascades with topologies selected ahead of time. We presented an a priori sparse neural network initialization scheme that allows us to train very deep networks without the vanishing gradient problem. Since networks are pruned before the model has seen any training data, we investigated topologies that maximize accuracy over any domain. We have developed a data-free heuristic that can evaluate the sparse network's control of outputs with respect to inputs, allowing us to assess the expressiveness of a given topology. We have extracted several requirements that make for a good topology, such as the need for skip connections, information bandwidth, shallowness, and input-output pair equality. Finally, we have proposed a topology we call parallel butterfly as the ideal topology for training a priori sparse networks, and have experimentally shown that it outperforms other considered topologies. Weights should then be initialized with the following distribution, commonly known as the Xavier initialization: B MEASURING THE NUMBER OF SOLVABLE RATIO CONSTRAINTS On a practical note, one way to test how many ratios a network can learn is to append a 'diagonal layer' to the end of the network (i.e., a new layer with a single neuron attached to each output), as seen in Figure 6 . The diagonal layer is a diagonal matrix whose only trainable elements are on the main diagonal, and all other values are 0. When training a network, this diagonal layer can only learn magnitudes, and not ratios between signals, because each neuron only has one input and cannot 'mix' any signals. This gives us an easy way of measuring the number of ratios a network can correctly express: we train a network with L1 loss until it converges. We then count the number of constraints k the network has satisfied. These constraints can be ratio constraints or magnitude constraints. If we have n output neurons, we know that the last layer will have satisfied all magnitude constraints. Hence, the number of ratios the network can satisfy is k − n. For example, the network in Figure 6 (right, though true for left too) can satisfy three out of the 4 absolute constraints. 2 of those are magnitude constraints, meaning it can only satisfy one ratio constraint. That ratio is calculated at neuron n, so either neuron x or y can get a correct ratio of inputs, but not both. Of course, with L2 loss, the network will settle for a solution that doesn't satisfy either, but picks some middle ground.",We investigate pruning DNNs before training and provide an answer to which topology should be used for training a priori sparse networks.,one ; Han ; First ; Ding ; − n. For ; DRAM ; three ; Akiba ; al. ; neuron n,a priori sparse networks ; any training data ; a sparse neural network initialization scheme ; sparse topology ; a topology ; either neuron ; new architectures ; Han ; intra-layer topology ; chip,one ; Han ; First ; Ding ; − n. For ; DRAM ; three ; Akiba ; al. ; neuron n,"Long training times of deep neural networks are a major bottleneck in machine learning research due to quadratic growth of both memory and compute requirements of dense and convolutional layers with respect to their information bandwidth. However, training `a priori' sparse networks has been proposed as a method for allowing layers to retain high information bandwidth while keeping memory and computation low. In this work, we provide a theoretical foundation for the choice of intra-layer topology. First, we derive a new sparse neural network initialization scheme that allows us to explore the space of very deep sparse networks. Next, we evaluate several topologies and show that seemingly",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Despite the state-of-the-art accuracy of Deep Neural Networks (DNN) in various classification problems, their deployment onto resource constrained edge computing devices remains challenging due to their large size and complexity. Several recent studies have reported remarkable results in reducing this complexity through quantization of DNN models. However, these studies usually do not consider the changes in the loss function when performing quantization, nor do they take the different importances of DNN model parameters to the accuracy into account. We address these issues in this paper by proposing a new method, called adaptive quantization, which simplifies a trained DNN model by finding a unique, optimal precision for each network parameter such that the increase in loss is minimized. The optimization problem at the core of this method iteratively uses the loss function gradient to determine an error margin for each parameter and assigns it a precision accordingly. Since this problem uses linear functions, it is computationally cheap and, as we will show, has a closed-form approximate solution. Experiments on MNIST, CIFAR, and SVHN datasets showed that the proposed method can achieve near or better than state-of-the-art reduction in model size with similar error rates. Furthermore, it can achieve compressions close to floating-point model compression methods without loss of accuracy. Deep Neural Networks (DNNs) have achieved incredible accuracies in applications ranging from computer vision BID16 to speech recognition BID7 and natural language processing BID5 . One of the key enablers of the unprecedented success of DNNs is the availability of very large model sizes. While the increase in model size improves the classification accuracy, it inevitably increases the computational complexity and memory requirement needed to train and store the network. This poses challenges in deploying these large models in resource-constrained edge computing environments, such as mobile devices. These challenges motivate neural network compression, which exploits the redundancy of neural networks to achieve drastic reductions in model sizes. The state-of-the-art neural network compression techniques include weight quantization BID4 , weight pruning BID6 , weight sharing BID6 , and low rank approximation BID18 . For instance, weight quantization has previously shown good accuracy with fixed-point 16-bit and 8-bit precisions BID17 BID14 . Recent works attempt to push that even further towards reduced precision and have trained models with 4-bit, 2-bit, and 1-bit parameters using quantized training methods BID9 BID4 .Although these quantization methods can significantly reduce model complexity, they generally have two key constraints. First, they ignore the accuracy degradation resulting from quantization, during the quantization, and tend to remedy it, separately, through quantized learning schemes. However, such schemes have the disadvantage of converging very slowly compared to full-precision learning methods. Second, they treat all network parameters similarly and assign them the same quantization width 1 . This is while previous works BID9 BID6 have shown different parameters do not contribute to the model accuracy equally. Disregarding this variation limits the maximum achievable compression.In this paper, we address the aforementioned issues by proposing adaptive quantization. To take the different importances of network parameters into account, this method quantizes each network parameter of a trained network by a unique quantization width. This way, parameters that impact the accuracy the most can be represented using higher precisions (larger quantization widths), while low-impact parameters are represented with fewer bits or are pruned. Consequently, our method can reduce the model size significantly while maintaining a certain accuracy. The proposed method monitors the accuracy by incorporating the loss function into an optimization problem to minimize the models. The output of the optimization problem is an error margin associated to each parameter. This margin is computed based on the loss function gradient of the parameter and is used to determine its precision. We will show that the proposed optimization problem has a closed-form approximate solution, which can be iteratively applied to the same network to minimize its size. We test the proposed method using three classification benchmarks comprising MNIST, CIFAR-10, and SVHN. We show that, across all these benchmarks , we can achieve near or better compressions compared to state-of-the-art quantization techniques. Furthermore, we can achieve compressions similar to the state-of-the-art pruning and weight-sharing techniques which inherently require more computational resources for inference. We evaluate adaptive quantization on three popular image classification benchmarks. For each, we first train a neural network in the floating-point domain, and then apply a pass of algorithm 3 to compress the trained model. In both these steps, we use the same batchsize to calculate the gradients and update the parameters. To further reduce the model size, we tune the accuracy of the quantized model in the floating-point domain and quantize the tuned model by reapplying a pass of algorithm 3. For each benchmark, we repeat this process three times, and experimentally show that this produces the smallest model. In the end we evaluate the accuracy and the size of the quantized models. Specifically, we determine the overall number of bits (quantization bits and the sign bits), and evaluate how much reduction in the model size has been achieved.We note that it is also important to evaluate the potential overheads of bookkeeping for the quantization widths. However, we should keep in mind that bookkeeping has an intricate relationship with the target hardware, which may lead to radically different results on different hardware platforms. For example, our experiments show that on specialized hardware, such as the one designed by Albericio et al. (2017) for processing variable bit-width CNN, we can fully offset all bookkeeping overheads of storing quantization depths, while CPU/GPU may require up to 60% additional storage. We will study this complex relationship separately, in our future work, and in the context of hardware implementation. In this paper, we limit the scope to algorithm analysis, independent of the underlying hardware architecture. In this work, we quantize neural network models such that only parameters critical to the accuracy are represented with high precision. The goal is to minimize data movement and simplify computations needed for inference in order to accelerate implementations on resource constrained hardware. To achieve acceleration, the proposed technique prunes unnecessary parameters or reduces their precisions. Combined with existing fixed-point computation techniques such as SWAR BID2 or Bit-pragmatic computation (Albericio et al., 2017) , we expect these small fixed-point models to achieve fast inference with high accuracies. We have confirmed the effectiveness of this technique through experiments on several benchmarks. Through this technique, our experiments show, DNN model sizes can be reduced significantly without loss of accuracy. The resulting models are significantly smaller than state-of-the-art quantization technique. Furthermore, the proposed Adaptive Quantization can provide similar results to floating-point model compression techniques.",An adaptive method for fixed-point quantization of neural networks based on theoretical analysis rather than heuristics.,Adaptive Quantization ; CPU/GPU ; First ; Second ; One ; Albericio ; Deep Neural Networks ; SVHN ; three ; CNN,bookkeeping ; the different importances ; network parameters ; high precision ; this complex relationship ; these issues ; Second ; the size ; accuracy ; the loss function gradient,Adaptive Quantization ; CPU/GPU ; First ; Second ; One ; Albericio ; Deep Neural Networks ; SVHN ; three ; CNN,"DNNs have achieved incredible accuracies in various classification problems due to their large size and complexity. However, these studies usually do not consider the changes in the loss function when performing quantization, nor do they take the different importances of DNN model parameters to account. A new method is proposed to simplify a trained DNN by finding a unique precision for each network parameter such that the increase in loss is minimized. The optimization problem at the core of this method involves determining an error margin for each parameter and assigning it a precision accordingly. It is computationally cheap and has a closed-form approximate solution. The proposed method",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"We address the challenging problem of efficient deep learning model deployment, where the goal is to design neural network architectures that can fit different hardware platform constraints. Most of the traditional approaches either manually design or use Neural Architecture Search (NAS) to find a specialized neural network and train it from scratch for each case, which is computationally expensive and unscalable. Our key idea is to decouple model training from architecture search to save the cost. To this end, we propose to train a once-for-all network (OFA) that supports diverse architectural settings (depth, width, kernel size, and resolution). Given a deployment scenario, we can then quickly get a specialized sub-network by selecting from the OFA network without additional training. To prevent interference between many sub-networks during training, we also propose a novel progressive shrinking algorithm, which can train a surprisingly large number of sub-networks ($> 10^{19}$) simultaneously. Extensive experiments on various hardware platforms (CPU, GPU, mCPU, mGPU, FPGA accelerator) show that OFA consistently outperforms SOTA NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3) while reducing orders of magnitude GPU hours and $CO_2$ emission. In particular, OFA achieves a new SOTA 80.0% ImageNet top1 accuracy under the mobile setting ($<$600M FLOPs). Code and pre-trained models are released at https://github.com/mit-han-lab/once-for-all. Deep Neural Networks (DNNs) deliver state-of-the-art accuracy in many machine learning applications. However, the explosive growth in model size and computation cost gives rise to new challenges on how to efficiently deploy these deep learning models on diverse hardware platforms, since they have to meet different hardware efficiency constraints (e.g., latency, energy). For instance, one mobile application on App Store has to support a diverse range of hardware devices, from a high-end Samsung Note10 with a dedicated neural network accelerator to a 5-year-old Samsung S5 with a much slower processor. With different hardware resources (e.g., on-chip memory size, #arithmetic units), the optimal neural network architecture varies significantly. Even running on the same hardware, under different battery conditions or workloads, the best model architecture also differs a lot. Given different hardware platforms and efficiency constraints (defined as deployment scenarios), researchers either design compact models specialized for mobile (Howard et al., 2017; Sandler et al., 2018; or accelerate the existing models by compression (He et al., 2018) for efficient deployment. However, designing specialized DNNs for every scenario is engineer-expensive and computationally expensive, either with human-based methods or NAS. Since such methods need to repeat the network design process and retrain the designed network from scratch for each case. Their total cost grows linearly as the number of deployment scenarios increases, which will result in excessive energy consumption and CO 2 emission (Strubell et al., 2019) . It makes them unable to handle the vast amount of hardware devices (23.14 billion IoT devices till 2018 1 ) and highly dynamic deployment environments (different battery conditions, different latency requirements, etc.). This paper introduces a new solution to tackle this challenge -designing a once-for-all network that can be directly deployed under diverse architectural configurations, amortizing the training cost. The Figure 1: Left: a single once-for-all network is trained to support versatile architectural configurations including depth, width, kernel size, and resolution. Given a deployment scenario, a specialized subnetwork is directly selected from the once-for-all network without training. Middle: this approach reduces the cost of specialized deep learning deployment from O(N) to O(1). Right: once-for-all network followed by model selection can derive many accuracy-latency trade-offs by training only once, compared to conventional methods that require repeated training. inference is performed by selecting only part of the once-for-all network. It flexibly supports different depths, widths, kernel sizes, and resolutions without retraining. A simple example of Once for All (OFA) is illustrated in Figure 1 (left). Specifically, we decouple the model training stage and the model specialization stage. In the model training stage, we focus on improving the accuracy of all sub-networks that are derived by selecting different parts of the once-for-all network. In the model specialization stage, we sample a subset of sub-networks to train an accuracy predictor and latency predictors. Given the target hardware and constraint, a predictor-guided architecture search (Liu et al., 2018a ) is conducted to get a specialized sub-network, and the cost is negligible. As such, we reduce the total cost of specialized neural network design from O(N) to O(1) (Figure 1 middle). However, training the once-for-all network is a non-trivial task, since it requires joint optimization of the weights to maintain the accuracy of a large number of sub-networks (more than 10 19 in our experiments). It is computationally prohibitive to enumerate all sub-networks to get the exact gradient in each update step, while randomly sampling a few sub-networks in each step will lead to significant accuracy drops. The challenge is that different sub-networks are interfering with each other, making the training process of the whole once-for-all network inefficient. To address this challenge, we propose a progressive shrinking algorithm for training the once-for-all network. Instead of directly optimizing the once-for-all network from scratch, we propose to first train the largest neural network with maximum depth, width, and kernel size, then progressively fine-tune the once-for-all network to support smaller sub-networks that share weights with the larger ones. As such, it provides better initialization by selecting the most important weights of larger sub-networks, and the opportunity to distill smaller sub-networks, which greatly improves the training efficiency. We extensively evaluated the effectiveness of OFA on ImageNet with many hardware platforms (CPU, GPU, mCPU, mGPU, FPGA accelerator) and efficiency constraints. Under all deployment scenarios, OFA consistently improves the ImageNet top1 accuracy by a significant margin compared to SOTA hardware-aware NAS methods while saving the GPU hours, dollars, and CO 2 emission by orders of magnitude. On the ImageNet mobile setting, OFA achieves a new SOTA 80.0% top1 accuracy with 595M FLOPs. To the best of our knowledge, this is the first time that the SOTA ImageNet top1 accuracy reaches 80% under the mobile setting. We proposed Once for All (OFA), a new methodology that decouples model training from architecture search for efficient deep learning deployment under a large number of deployment scenarios. Unlike previous approaches that design and train a neural network for each deployment scenario, we designed a once-for-all network that supports different architectural configurations, including elastic depth, width, kernel size, and resolution. It greatly reduces the training cost (GPU hours, energy Specialized deployment results on CPU, GPU, mGPU, and FPGA accelerator. Specialized models by OFA consistently achieve significantly higher ImageNet accuracy with similar latency than non-specialized neural networks. More remarkably, specializing for a new hardware platform does not add training cost using OFA. consumption, and CO 2 emission) compared to conventional methods. To prevent sub-networks of different sizes from interference, we proposed a progressive shrinking algorithm that enables a large number of sub-network to achieve the same level of accuracy compared to training them independently. Experiments on a diverse range of hardware platforms and efficiency constraints demonstrated the effectiveness of our approach.",We introduce techniques to train a single once-for-all network that fits many hardware platforms.,GPU ; Deep Neural Networks ; $> ; NAS ; SOTA NAS ; one ; Neural Architecture Search ; CPU ; Samsung ; Once for All,our experiments ; a specialized neural network ; deployment scenarios increases ; a significant margin ; weights ; Our key idea ; FPGA ; constraint ; SOTA NAS methods ; network,GPU ; Deep Neural Networks ; $> ; NAS ; SOTA NAS ; one ; Neural Architecture Search ; CPU ; Samsung ; Once for All,"We address the challenge of efficient deep learning model deployment, where the goal is to design neural network architectures that fit different hardware platform constraints. Traditional approaches either manually design or use Neural Architecture Search (NAS) to find a specialized neural network and train it from scratch for each case, which is computationally expensive and unscalable. To achieve this, we propose to train a once-for-all network (OFA) that supports diverse architectural settings (depth, width, kernel size, resolution). Given a deployment scenario, we can quickly get a specialized sub-network by selecting from the OFA network without additional training. To prevent",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Claims from the fields of network neuroscience and connectomics suggest that topological models of the brain involving complex networks are of particular use and interest. The field of deep neural networks has mostly left inspiration from these claims out. In this paper, we propose three architectures and use each of them to explore the intersection of network neuroscience and deep learning in an attempt to bridge the gap between the two fields. Using the teachings from network neuroscience and connectomics, we show improvements over the ResNet architecture, we show a possible connection between early training and the spectral properties of the network, and we show the trainability of a DNN based on the neuronal network of C.Elegans. We have demonstrated three distinct approaches to applying work from network neurosciences and connectomics to deep learning. Our experiments show improvements over ResNet by the inclusion of skip connections which follow a connectivity pattern with small world properties, a possible connection between early training performance and spectral gap when using expander graphs as the participant graph topology with the node model proposed by BID14 , and the trainability of a DNN based on the neuronal network of C.Elegans with and without freezing the parameters of the convolutional and fully connected layers.In future work, we will examine the impact of other spectral properties of the graph topologies used both in the architectures we proposed and in the RandWire architecture proposed by BID14 . Additionally, we will explore parameter efficient connectivity patterns which could achieve similar performance to related networks with more parameters TAB0 Deep connectomics networks Figure 5 . Performance of C.ElegansNet with all parameters frozen except the C.Elegans graph edge weights.",We explore the intersection of network neurosciences and deep learning.,three ; two ; ResNet ; C.Elegans ; RandWire,early training performance ; other spectral properties ; deep neural networks ; them ; Our experiments ; C.Elegans ; these claims ; the intersection ; the graph topologies ; a possible connection,three ; two ; ResNet ; C.Elegans ; RandWire,"The fields of network neuroscience and connectomics suggest that topological models of the brain involving complex networks are of particular interest. However, the field of deep neural networks has mostly left inspiration from these claims out. In this paper, we propose three architectures and use each of them to bridge the gap between the two fields. We show improvements over ResNet architecture, we show a possible connection between early training performance and spectral properties of the network, and we show trainability of a DNN based on the neuronal network of C.Elegans with and without freezing the parameters of the convolutional and fully connected layers.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"A deep generative model is a powerful method of learning a data distribution, which has achieved tremendous success in numerous scenarios. However, it is nontrivial for a single generative model to faithfully capture the distributions of the complex data such as images with complicate structures. In this paper, we propose a novel approach of cascaded boosting for boosting generative models, where meta-models (i.e., weak learners) are cascaded together to produce a stronger model. Any hidden variable meta-model can be leveraged as long as it can support the likelihood evaluation. We derive a decomposable variational lower bound of the boosted model, which allows each meta-model to be trained separately and greedily. We can further improve the learning power of the generative models by combing our cascaded boosting framework with the multiplicative boosting framework. The past decade has witnessed tremendous success in the field of deep generative models (DGMs) in both unsupervised learning (Goodfellow et al., 2014; Kingma & Welling, 2013; Radford et al., 2015) and semi-supervised learning (Abbasnejad et al., 2017; Kingma et al., 2014; Li et al., 2018) paradigms. DGMs learn the data distribution by combining the scalability of deep learning with the generality of probabilistic reasoning. However, it is not easy for a single parametric model to learn a complex distribution, since the upper limit of a model's ability is determined by its fixed structure. If a model with low capacity was adopted, the model would be likely to have a poor performance. Straightforwardly increasing the model capacity (e.g., including more layers or more neurons) is likely to cause serious challenges, such as vanishing gradient problem (Hochreiter et al., 2001 ) and exploding gradient problem (Grosse, 2017 ). An alternative approach is to integrate multiple weak models to achieve a strong one. The early success was made on mixture models (Dempster et al., 1977; Figueiredo & Jain, 2002; Xu & Jordan, 1996) and product-of-experts (Hinton, 1999; . However, the weak models in such work are typically shallow models with very limited capacity. Recent success has been made on boosting generative models, where a set of meta-models (i.e., weak learners) are combined to construct a stronger model. In particular, Grover & Ermon (2018) propose a method of multiplicative boosting, which takes the geometric average of the meta-model distributions, with each assigned an exponentiated weight. This boosting method improves performance on density estimation and sample generation, compared to a single meta-model. However, the boosted model has an explicit partition function, which requires importance sampling (Rubinstein & Kroese, 2016) for an estimation. In general, sampling from the boosted model is conducted based on Markov chain Monte Carlo (MCMC) method (Hastings, 1970) . As a result, it requires a high time complexity of likelihood evaluation and sample generation. Rosset & Segal (2003) propose another method of additive boosting, which takes the weighted arithmetic mean of meta-models' distributions. This method can sample fast, but the improvement of performance on density estimation is not comparable to the multiplicative boosting, since additive boosting requires that the expected log-likelihood and likelihood of the current meta-model are better-or-equal than those of the previous boosted model (Grover & Ermon, 2018) , which is difficult to satisfy. In summary, it is nontrivial for both of the previous boosting methods to balance well between improving the learning power and keeping the efficiency of sampling and density estimation. To address the aforementioned issues, we propose a novel boosting framework, called cascaded boosting, where meta-models are connected in cascade. The framework is inspired by the greedy layer-wise training algorithm of DBNs (Deep Belief Networks) (Bengio et al., 2007; Hinton et al., 2006) , where an ensemble of RBMs (Restricted Boltzmann Machines) (Smolensky, 1986) are converted to a stronger model. We propose a decomposable variational lower bound, which reveals the principle behind the greedy layer-wise training algorithm. The decomposition allows us to incorporate any hidden variable meta-model, as long as it supports likelihood evaluation, and train these meta-models separately and greedily, yielding a deep boosted model. Finally, We demonstrate that our boosting framework can be integrated with the multiplicative boosting framework (Grover & Ermon, 2018) , yielding a hybrid boosting with an improved learning power of generative models. To summary, we make the following contributions: • We propose a boosting framework to boost generative models, where meta-models are cascaded together to produce a stronger model. • We give a decomposable variational lower bound of the boosted model, which reveals the principle behind the greedy layer-wise training algorithm. • We finally demonstrate that our boosting framework can be extended to a hybrid model by integrating it with the multiplicative boosting models, which further improves the learning power of generative models. We propose a framework for boosting generative models by cascading meta-models. Any hidden variable meta-model can be incorporated, as long as it supports likelihood evaluation. The decomposable lower bound allows us to train meta-models separately and greedily. Our cascaded boosting can be integrated with the multiplicative boosting. In our experiments, we first validate that the non-decreasing property of the decomposable variational lower bound (Equation 5) holds in practice, and next further promote the performance of some advanced models, which represent state-ofthe-art methods. Then, we show that our cascaded boosting has better performance of improving models' learning power, compared with naively increasing model capacity. Finally, we compare different generative boosting methods, validating the ability of the hybrid boosting in further improving learning power of generative models. A PROOF OF THEOREM 1 Proof. Using q(h 1 , · · · , h k−1 |x) as the approximate posterior, we have a variational lower bound Thus, the lower bound is equal to: Thus, Take the expection with respect to dataset D, we have B PROOF OF THEOREM 3 AND THEOREM 4 where n is the number of meta-models. Since we can omit the subscript, thereby writing q i as q. For any j ∈ [k + 1, n] ∩ Z and any m k+1 , m k+2 , · · · , m j , given i (k + 1 ≤ i ≤ j), we have Let q(h k , · · · , h j−1 |h k−1 ) be the approximate posterior of p j (h k−1 , · · · , h j−1 ), according to Theorem 1, we have we have",Propose an approach for boosting generative models by cascading hidden variable models,Hinton ; Kingma & Welling ; The past decade ; Restricted Boltzmann Machines ; Radford ; Li ; Rosset & Segal ; Abbasnejad ; al. ; Markov,the performance ; it ; (MCMC) method ; any m ; the approximate posterior ; complicate structures ; Kingma ; the following contributions ; the generality ; each meta-model,Hinton ; Kingma & Welling ; The past decade ; Restricted Boltzmann Machines ; Radford ; Li ; Rosset & Segal ; Abbasnejad ; al. ; Markov,"Deep generative models have achieved tremendous success in numerous scenarios. However, it is nontrivial for a single generative model to accurately capture the distributions of complex data such as images with complicate structures. A novel approach is cascaded boosting, where meta-models are cascaded together to produce a stronger model. Any hidden variable meta-model can be leveraged as long as it can support likelihood evaluation. A decomposable variational lower bound of the boosted model is derived, allowing each meta model to be trained separately and greedily. We can further improve the learning power of the generatively models by combing our cascaded",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Bitcoin is a virtual coinage system that enables users to trade virtually free of a central trusted authority. All transactions on the Bitcoin blockchain are publicly available for viewing, yet as Bitcoin is built mainly for security it’s original structure does not allow for direct analysis of address transactions. 
 Existing analysis methods of the Bitcoin blockchain can be complicated, computationally expensive or inaccurate. We propose a computationally efficient model to analyze bitcoin blockchain addresses and allow for their use with existing machine learning algorithms. We compare our approach against Multi Level Sequence Learners (MLSLs), one of the best performing models on bitcoin address data. Bitcoin(Nakamoto) is a virtual coinage system that functions much like a standard currency, enabling users to provide virtual payment for goods and services free of a central trusted authority. Bitcoin relies on the transmission of digital information, utilizing cryptographic methods to ensure secure, unique transactions. Individuals and businesses transact with the coin electronically on a peerto-peer network utilizing a shared transaction ledger (the Blockchain). It caught wide attention beginning in 2011, and various altcoins a general name for all other cryptocurrencies post-Bitcoin soon appeared It has placed itself as the most widespread and commonly used cryptocurrency with no signs of slowing down (Chan et al., 2017) . Representing over 81% of the total market of cryptocurrencies(coi), Its market capitalization is estimated to be approximately $177.8 Billioncoi accounting for about 90% of the total market capitalization of Virtual Currencies(Houben & Snyers). Bitcoin uses public key cryptography to generate secure addresses for users where each address is a public key, and use of the bitcoins stored in it requires signing with a private key. These address identifiers are used by their owners to hold bitcoin pseudonymously. A typical Bitcoin transaction consists of two sets: a set of source addresses and a set of destination addresses. Coins in the source addresses are collected and then sent in differing amounts to the destination addresses. (Houben & Snyers) While bitcoin address data is publicly available, it is not straightforward to analyze address transaction data since it is not aggregated in one block/place. It is apparent that address2vec is a significant improvement over a baseline approach, although not as accurate as MLSLs we believe further tuning of the model's architecture can yield a more accurate iteration of address2vec, especially making the model end to end differentiable, we currently use separate phases. We also plan to test address2vec on different bitcoin behavior tasks, measuring the similarity of various users and their relationships by measuring their vector distances and predicting market rates of bitcoin through analyzing most recent addresses on the blockchain.",a 2vec model for cryptocurrency transaction graphs,Bitcoin ; Multi Level Sequence Learners ; Blockchain ; post-Bitcoin ; Chan ; al. ; Virtual Currencies(Houben & Snyers ; two ; Houben & Snyers ; one,various users ; the model's architecture ; a computationally efficient model ; a shared transaction ledger ; the Bitcoin blockchain ; post-Bitcoin ; it ; Virtual Currencies(Houben ; address transaction data ; Bitcoin(Nakamoto,Bitcoin ; Multi Level Sequence Learners ; Blockchain ; post-Bitcoin ; Chan ; al. ; Virtual Currencies(Houben & Snyers ; two ; Houben & Snyers ; one,"Bitcoin is a virtual coinage system that enables users to trade virtually free of a central trusted authority. All transactions on the Bitcoin blockchain are publicly available for viewing, yet as Bitcoin is built mainly for security it's original structure does not allow for direct analysis of address transactions. This is because existing analysis methods can be complicated, computationally expensive or inaccurate. We propose a computationally efficient model to analyze bitcoin blockchain addresses and allow for their use with existing machine learning algorithms. We compare our approach against Multi Level Sequence Learners (MLSLs), one of the best performing models on bitcoin address data. Bitcoin(Nakamoto) is",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Variational Bayesian Inference is a popular methodology for approximating posterior distributions over Bayesian neural network weights. Recent work developing this class of methods has explored ever richer parameterizations of the approximate posterior in the hope of improving performance. In contrast, here we share a curious experimental finding that suggests instead restricting the variational distribution to a more compact parameterization. For a variety of deep Bayesian neural networks trained using Gaussian mean-field variational inference, we find that the posterior standard deviations consistently exhibit strong low-rank structure after convergence. This means that by decomposing these variational parameters into a low-rank factorization, we can make our variational approximation more compact without decreasing the models' performance. Furthermore, we find that such factorized parameterizations improve the signal-to-noise ratio of stochastic gradient estimates of the variational lower bound, resulting in faster convergence. Bayesian Neural Networks (MacKay, 1992; Neal, 1993) explicitly represent their parameteruncertainty by forming a posterior distribution over model parameters, instead of relying on a single point estimate for making predictions, as is done in traditional deep learning. Besides offering improved predictive performance over single models, Bayesian neural networks are also more robust to hard examples (Raftery et al., 2005) , have better calibration of predictive uncertainty and thus can be used for out-of-domain detection or other risk-sensitive applications (Ovadia et al., 2019) . Variational inference (Peterson, 1987; Hinton and Van Camp, 1993 ) is a popular class of methods for approximating the posterior distribution p(w|x, y), since the exact Bayes' rule is often intractable to compute for models of practical interest. This class of methods specifies a distribution q θ (w) of given parametric or functional form as the posterior approximation, and optimizes the approximation by solving an optimization problem. In particular, we minimize the negative Evidence Lower Bound (negative ELBO) approximated by samples from the posterior: by differentiating with respect to the variational parameters θ (Salimans et al., 2013; Kingma and Welling, 2013) . In Gaussian Mean Field Variational Inference (GMFVI) (Blei et al., 2017; Blundell et al., 2015) , we choose the variational approximation to be a fully factorized Gaussian distribution: q(w ij ), with q(w ij ) = N (µ ij , σ where W ∈ R m×n is a weight matrix of a single network layer and i and j are the row and column indices in this weight matrix. In practice, we often represent the posterior standard deviation parameters σ ij in the form of a matrix A ∈ R m×n + . With this notation, we have the relationship Σ q = diag(vec(A 2 )) where the elementwise-squared A is vectorized by stacking its columns, and then expanded as a diagonal matrix into R mn×mn + . While Gaussian Mean-Field posteriors are considered to be one of the simplest types of variational approximations, with some known limitations (Giordano et al., 2018) , they scale to comparatively large models and generally provide competitive performance (Ovadia et al., 2019) . However, when compared to deterministic neural networks, GMFVI doubles the number of parameters and is often harder to train due to the increased noise in stochastic gradient estimates. Beyond fully factorized mean-field, recent research in variational inference has explored richer parameterizations of the approximate posterior in order to improve the performance of Bayesian neural networks (see Appendix A and Figure 3 ). For instance, various structures of Gaussian posteriors have been proposed, with per layer block-structured covariances (Louizos and Welling, 2016; Sun et al., 2017; Zhang et al., 2017) , full covariances (Barber and Bishop, 1998) with different parametrizations (Seeger, 2000) , up to more flexible approximate posteriors using normalizing flows (Rezende and Mohamed, 2015) and extensions thereof (Louizos and Welling, 2017 ). In contrast, here we study a simpler, more compactly parameterized mean-field variational posterior which ties variational parameters in the already diagonal covariance matrix. We show that such a posterior approximation can also work well for a variety of models. In particular we find that: • Converged posterior standard deviations under GMFVI consistently display strong low-rank structure. This means that by decomposing these variational parameters into a low-rank factorization, we can make our variational approximation more compact without decreasing our model's performance. • Factorized parameterizations of posterior standard deviations improve the signal-to-noise ratio of stochastic gradient estimates, and thus not only reduce the number of parameters compared to standard GMFVI, but also can lead to faster convergence. In this work we have shown that Bayesian Neural Networks trained with standard Gaussian meanfield variational inference exhibit posterior standard deviation matrices that can be approximated with little information loss by a low-rank decomposition. This suggests that richer parameterizations of the variational posterior may not always be needed, and that compact parameterizations can also work well. We used this insight to propose a simple, yet effective variational posterior parametrization, which speeds up training and reduces the number of variational parameters without degrading predictive performance on three different model types. In future work, we hope to scale up variational inference with compactly parameterized approximate posteriors to much larger models and more complex problems. For mean-field variational inference to work well in that setting several challenges will likely need to be addressed (Osawa et al., 2019) ; improving the signal-to-noise ratio of ELBO gradients using our compact variational parameterizations may provide a piece of the puzzle. 1. Explained variance for the rank k approximation is calculated as γ , where g b is the gradient value for a single parameter. The expectation E and variance V ar of the gradient values g b are calculated over a window of last 10 batches.","Mean field VB uses twice as many parameters; we tie variance parameters in mean field VB without any loss in ELBO, gaining speed and lower variance gradients.",Giordano ; Bayesian Neural Networks ; Welling ; Zhang et al. ; Sun ; Peterson ; Variational Bayesian Inference ; Gaussian Mean-Field ; Gaussian Mean Field Variational Inference ; Neal,"a simple, yet effective variational posterior parametrization ; • Factorized ; this notation ; a window ; a piece ; Bayesian neural network weights ; Ovadia ; posterior standard deviations ; stochastic gradient estimates ; the variational distribution",Giordano ; Bayesian Neural Networks ; Welling ; Zhang et al. ; Sun ; Peterson ; Variational Bayesian Inference ; Gaussian Mean-Field ; Gaussian Mean Field Variational Inference ; Neal,"Variational Bayesian Inference is a popular method for approximating posterior distributions over Bayesian neural network weights. Recent work has explored richer parameterizations of the approximate posterior in the hope of improving performance. However, a curious experimental finding suggests instead restricting the variational distribution to a more compact parameterization. For a variety of deep Bayesian networks trained using Gaussian mean-field variational inference, posterior standard deviations consistently exhibit strong low-rank structure after convergence. This means that by decomposing these variational parameters into a lower-rank factorization, we can make our variational approximation more compact without decreasing the models' performance.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Real-world dynamical systems often consist of multiple stochastic subsystems that interact with each other. Modeling and forecasting the behavior of such dynamics are generally not easy, due to the inherent hardness in understanding the complicated interactions and evolutions of their constituents. This paper introduces the relational state-space model (R-SSM), a sequential hierarchical latent variable model that makes use of graph neural networks (GNNs) to simulate the joint state transitions of multiple correlated objects. By letting GNNs cooperate with SSM, R-SSM provides a flexible way to incorporate relational information into the modeling of multi-object dynamics. We further suggest augmenting the model with normalizing flows instantiated for vertex-indexed random variables and propose two auxiliary contrastive objectives to facilitate the learning. The utility of R-SSM is empirically evaluated on synthetic and real time series datasets. Many real-world dynamical systems can be decomposed into smaller interacting subsystems if we take a fine-grained view. For example, the trajectories of coupled particles are co-determined by perparticle physical properties (e.g., mass and velocity) and their physical interactions (e.g., gravity); traffic flow can be viewed as the coevolution of a large number of vehicle dynamics. Models that are able to better capture the complex behavior of such multi-object systems are of wide interest to various communities, e.g., physics, ecology, biology, geoscience, and finance. State-space models (SSMs) are a wide class of sequential latent variable models (LVMs) that serve as workhorses for the analysis of dynamical systems and sequence data. Although SSMs are traditionally designed under the guidance of domain-specific knowledge or tractability consideration, recently introduced deep SSMs (Fraccaro, 2018) use neural networks (NNs) to parameterize flexible state transitions and emissions, achieving much higher expressivity. To develop deep SSMs for multi-object systems, graph neural networks (GNNs) emerge to be a promising choice, as they have been shown to be fundamental NN building blocks that can impose relational inductive bias explicitly and model complex interactions effectively . Recent works that advocate GNNs for modeling multi-object dynamics mostly make use of GNNs in an autoregressive (AR) fashion. AR models based on recurrent (G)NNs can be viewed as special instantiations of SSMs in which the state transitions are restricted to being deterministic (Fraccaro, 2018, Section 4.2) . Despite their simplicity, it has been pointed out that their modeling capability is bottlenecked by the deterministic state transitions (Chung et al., 2015; Fraccaro et al., 2016) and the oversimplified observation distributions (Yang et al., 2018) . In this study, we make the following contributions: (i) We propose the relational state-space model (R-SSM), a novel hierarchical deep SSM that simulates the stochastic state transitions of interacting objects with GNNs, extending GNN-based dynamics modeling to challenging stochastic multi-object systems. (ii) We suggest using the graph normalizing flow (GNF) to construct expressive joint state distributions for R-SSM, further enhancing its ability to capture the joint evolutions of correlated stochastic subsystems. (iii) We develop structured posterior approximation to learn R-SSM using variational inference and introduce two auxiliary training objectives to facilitate the learning. Our experiments on synthetic and real-world time series datasets show that R-SSM achieves competitive test likelihood and good prediction performance in comparison to GNN-based AR models and other sequential LVMs. The remainder of this paper is organized as follows: Section 2 briefly reviews neccesary preliminaries. Section 3 introduces R-SSM formally and presents the methods to learn R-SSM from observations. Related work is summarized in Section 4 and experimental evaluation is presented in Section 5. We conclude the paper in Section 6. In this work, we present a deep hierarchical state-space model in which the state transitions of correlated objects are coordinated by graph neural networks. To effectively learn the model from observation data, we develop a structured posterior approximation and propose two auxiliary contrastive prediction tasks to help the learning. We further introduce the graph normalizing flow to enhance the expressiveness of the joint transition density and the posterior approximation. The experiments show that our model can outperform or match the state-of-the-arts on several time series modeling tasks. Directions for future work include testing the model on high-dimensional observations, extending the model to directly learn from visual data, and including discrete latent variables in the model. c∈Ωt,i λ ψ,1 (ẑ t,k ), and Ω t,i is a set that contains c The element-wise affine layer is proposed by Kingma & Dhariwal (2018) for normalizing the activations. Its parameters γ ∈ R D and β ∈ R D are initialized such that the per-channel activations have roughly zero mean and unit variance at the beginning of training. The invertible linear transformation W ∈ R D×D is parameterized using a QR decomposition (Hoogeboom et al., 2019) .",A deep hierarchical state-space model in which the state transitions of correlated objects are coordinated by graph neural networks.,SSM ; two ; Chung et al. ; Fraccaro et al. ; Yang et al. ; GNN ; AR models ; Kingma & Dhariwal ; roughly zero ; Hoogeboom,the state transitions ; This paper ; AR models ; roughly zero mean and unit variance ; a flexible way ; sequential latent variable models ; State-space models ; coupled particles ; the model ; competitive test likelihood,SSM ; two ; Chung et al. ; Fraccaro et al. ; Yang et al. ; GNN ; AR models ; Kingma & Dhariwal ; roughly zero ; Hoogeboom,"Real-world dynamical systems often consist of multiple stochastic subsystems that interact with each other. Modeling and forecasting the behavior of such dynamics are difficult due to the inherent hardness in understanding the complicated interactions and evolutions of their constituents. This paper introduces the relational state-space model (R-SSM), a sequential hierarchical latent variable model that uses graph neural networks (GNNs) to simulate the joint state transitions of multiple correlated objects. By allowing GNNs to cooperate with the model, the model can incorporate relational information into the modeling of multi-object dynamics. We suggest augmenting the model with normalizing",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"In this work we introduce a new framework for performing temporal predictions
 in the presence of uncertainty. It is based on a simple idea of disentangling com-
 ponents of the future state which are predictable from those which are inherently
 unpredictable, and encoding the unpredictable components into a low-dimensional
 latent variable which is fed into the forward model. Our method uses a simple su-
 pervised training objective which is fast and easy to train. We evaluate it in the
 context of video prediction on multiple datasets and show that it is able to consi-
 tently generate diverse predictions without the need for alternating minimization
 over a latent space or adversarial training. Learning forward models in time series is a central task in artificial intelligence, with applications in unsupervised learning, planning and compression. A major challenge in this task is how to handle the multi-modal nature of many time series. When there are multiple valid ways in which a time series can evolve, training a model using classical 1 or 2 losses produces predictions which are the average or median of the different outcomes across each dimension, which is itself often not a valid prediction.In recent years, Generative Adversarial Networks BID9 have been introduced, a general framework where the prediction problem is formulated as a minimax game between the predictor function and a trainable discriminator network representing the loss. By using a trainable loss function, it is in theory possible to handle multiple output modes since a generator which covers each of the output modes will fool the discriminator leading to convergence. However, a generator which covers a single mode can also fool the discriminator and converge, and this behavior of mode collapse has been widely observed in practice. Some workarounds have been introduced to resolve or partially reduce mode-collapsing, such as minibatch discrimination, adding parameter noise BID23 , backpropagating through the unrolled discriminator BID18 and using multiple GANs to cover different modes BID27 . However, many of these techniques can bring additional challenges such as added complexity of implementation and increased computational cost. The mode collapsing problem becomes even more pronounced in the conditional generation setting when the output is highly dependent on the context, such as video prediction BID12 .In this work, we introduce a novel architecture that allows for robust multimodal conditional predictions in time series data. It is based on a simple intuition of separating the future state into a deterministic component, which can be predicted from the current state, and a stochastic (or difficult to predict) component which accounts for the uncertainty regarding the future mode. By training a deterministic network, we can obtain this factorization in the form of the network's prediction together with the prediction error with respect to the true state. This error can be encoded as a lowdimensional latent variable which is fed into a second network which is trained to accurately correct the determinisic prediction by incorporating this additional information. We call this model the Error Encoding Network (EEN). In a nutshell , this framework contains three function mappings at each timestep: (i) a mapping from the current state to the future state, which separates the future state into deterministic and non-deterministic components; (ii) a mapping from the non-deterministic component of the future state to a low-dimensional latent vector; (iii) a mapping from the current state to the future state conditioned on the latent vector, which encodes the mode information of the future state. While the training procedure involves all these mappings, the inference phase involves only (iii).Both networks are trained end-to-end using a supervised learning objective and latent variables are computed using a learned parametric function, leading to easy and fast training. We apply this method to video datasets from games, robotic manipulation and simulated driving, and show that the method is able to consistently produce multimodal predictions of future video frames for all of them. Although we focus on video in this work, the method itself is general and can in principle be applied to any continuous-valued time series. In this work, we have introduced a new framework for performing temporal prediction in the presence of uncertainty by disentangling predictable and non-predictable components of the future state. It is fast, simple to implement and easy to train without the need for an adverserial network or al- ternating minimization, and does not require additional tuning to prevent mode collapse. We have provided one instantiation in the context of video prediction using convolutional networks, but it is in principle applicable to different data types and architectures. There are several directions for future work. Here, we have adopted a simple strategy of sampling uniformly from the z distribution without considering their possible dependence on the state x, and there are likely better methods. In addition, one advantage of our model is that it can extract latent variables from unseen data very quickly, since it simply requires a forward pass through a network. If latent variables encode information about actions in a manner that is easy to disentangle, this could be used to extract actions from large unlabeled datasets and perform imitation learning. Another interesting application would be using this model for planning and having it unroll different possible futures.",A simple and easy to train method for multimodal prediction in time series.,recent years ; fed ; second ; the Error Encoding Network ; three ; one,"predictions ; this ; a latent space ; a low-dimensional
 latent variable ; a generator ; time series data ; this work ; artificial intelligence ; a mapping ; addition",recent years ; fed ; second ; the Error Encoding Network ; three ; one,"In this work, we introduce a new framework for performing temporal predictions in the presence of uncertainty. It involves disentangling com-ponents of the future state from their inherently unpredictable components, and encoding the unpredictable components into low-dimensional latent variables. The method is fast and easy to train, and it is able to consi-tently generate diverse predictions without the need for alternating minimization over latent space or adversarial training. Learning forward models in time series is a central task in artificial intelligence, with applications in unsupervised learning, planning and compression. A major challenge is handling multi-modal nature of many time",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.

 Our analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods. Neural networks trained by first order methods have achieved a remarkable impact on many applications, but their theoretical properties are still mysteries. One of the empirical observation is even though the optimization objective function is non-convex and non-smooth, randomly initialized first order methods like stochastic gradient descent can still find a global minimum. Surprisingly, this property is not correlated with labels. In BID37 , authors replaced the true labels with randomly generated labels, but still found randomly initialized first order methods can always achieve zero training loss.A widely believed explanation on why a neural network can fit all training labels is that the neural network is over-parameterized. For example, Wide ResNet (Zagoruyko and Komodakis) uses 100x parameters than the number of training data. Thus there must exist one such neural network of this architecture that can fit all training data. However, the existence does not imply why the network found by a randomly initialized first order method can fit all the data. The objective function is neither smooth nor convex, which makes traditional analysis technique from convex optimization not useful in this setting. To our knowledge, only the convergence to a stationary point is known BID5 .In this paper we demystify this surprising phenomenon on two-layer neural networks with rectified linear unit (ReLU) activation. Formally , we consider a neural network of the following form. DISPLAYFORM0 a r σ w r x (1) where x ∈ R d is the input, w r ∈ R d is the weight vector of the first layer, a r ∈ R is the output weight and σ (·) is the ReLU activation function: σ (z) = z if z ≥ 0 and σ (z) = 0 if z < 0 .We focus on the empirical risk minimization problem with a quadratic loss. Given a training data set {(x i , y i )} n i=1 , we want to minimize DISPLAYFORM1 Our main focus of this paper is to analyze the following procedure. We fix the second layer and apply gradient descent (GD) to optimize the first layer DISPLAYFORM2 where η > 0 is the step size. Here the gradient formula for each weight vector is 2 ∂L(W, a) DISPLAYFORM3 (f (W, a, x i ) − y i )a r x i I w r x i ≥ 0 .Though this is only a shallow fully connected neural network, the objective function is still nonsmooth and non-convex due to the use of ReLU activation function. 3 Even for this simple function, why randomly initialized first order method can achieve zero training error is not known. Many previous works have tried to answer this question or similar ones. Attempts include landscape analysis BID28 , partial differential equations (Mei et al.) , analysis of the dynamics of the algorithm BID20 , optimal transport theory BID3 , to name a few. These results often make strong assumptions on the labels and input distributions or do not imply why randomly initialized first order method can achieve zero training loss. See Section 2 for detailed comparisons between our result and previous ones.In this paper, we rigorously prove that as long as no two inputs are parallel and m is large enough, with randomly initialized a and W(0), gradient descent achieves zero training loss at a linear convergence rate, i.e., it finds a solution DISPLAYFORM4 Thus, our theoretical result not only shows the global convergence but also gives a quantitative convergence rate in terms of the desired accuracy.Analysis Technique Overview Our proof relies on the following insights. First we directly analyze the dynamics of each individual prediction f (W, a, x i ) for i = 1, . . . , n. This is different from many previous work BID8 BID20 which tried to analyze the dynamics of the parameter (W) we are optimizing. Note because the objective function is non-smooth and non-convex, analysis of the parameter space dynamics is very difficult. In contrast, we find the dynamics of prediction space is governed by the spectral property of a Gram matrix (which can vary in each iteration, c.f. Equation (6)) and as long as this Gram matrix's least eigenvalue is lower bounded, gradient descent enjoys a linear rate. It is easy to show as long as no two inputs are parallel , in the initialization phase, this Gram matrix has a lower bounded least eigenvalue. (c.f. Theorem 3.1). Thus the problem reduces to showing the Gram matrix at later iterations is close to that in the initialization phase. Our second observation is this Gram matrix is only related to the activation patterns (I w r x i ≥ 0 ) and we can use matrix perturbation analysis to show if most of the patterns do not change, then this Gram matrix is close to its initialization. Our third observation is we find over-parameterization, random initialization, and the linear convergence jointly restrict every weight vector w r to be close to its initialization. Then we can use this property to show most of the patterns do not change. Combining these insights we prove the first global quantitative convergence result of gradient descent on ReLU activated neural networks for the empirical risk minimization problem. Notably, our proof only uses linear algebra and standard probability bounds so we believe it can be easily generalized to analyze deep neural networks.Notations We let [n] = {1, 2, . . . , n}. Given a set S, we use unif {S} to denote the uniform distribution over S. Given an event E, we use I {A} to be the indicator on whether this event happens. We use N (0, I) to denote the standard Gaussian distribution. For a matrix A, we use A ij to denote its (i, j)-th entry. We use · 2 to denote the Euclidean norm of a vector, and use · F to denote the Frobenius norm of a matrix. If a matrix A is positive semi-definite, we use λ min (A) to denote its smallest eigenvalue. We use ·, · to denote the standard Euclidean inner product between two vectors. In this paper we show with over-parameterization, gradient descent provable converges to the global minimum of the empirical loss at a linear convergence rate. The key proof idea is to show the over-parameterization makes Gram matrix remain positive definite for all iterations, which in turn guarantees the linear convergence. Here we list some future directions.First, we believe our approach can be generalized to deep neural networks. We elaborate the main idea here for gradient flow. Consider a deep neural network of the form DISPLAYFORM0 where x ∈ R d is the input, W (1) ∈ R m×d is the first layer, W (h) ∈ R m×m for h = 2, . . . , H are the middle layers and a ∈ R m is the output layer. Recall u i is the i-th prediction. If we use the quadratic loss, we can compute DISPLAYFORM1 Similar to Equation (5), we can calculate DISPLAYFORM2 where DISPLAYFORM3 . Therefore, similar to Equation FORMULA12 , we can write du(t) dt =",We prove gradient descent achieves zero training loss with a linear rate on over-parameterized neural networks.,node ; GD ; two ; first ; linear ; W ; One ; ReLU ; n. This ; Gaussian,a linear rate ; two-layer fully connected ReLU ; the second layer ; an event ; the linear convergence ; we ; Komodakis ; many previous work ; the optimization objective function ; all iterations,node ; GD ; two ; first ; linear ; W ; One ; ReLU ; n. This ; Gaussian,"Neural networks with randomly initialized first order methods can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For a $m$ hidden node shallow neural network with ReLU activation and $n$ training data, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for quadratic loss function. The analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Many methods have been developed to represent knowledge graph data, which implicitly exploit low-rank latent structure in the data to encode known information and enable unknown facts to be inferred. To predict whether a relationship holds between entities, their embeddings are typically compared in the latent space following a relation-specific mapping. Whilst link prediction has steadily improved, the latent structure, and hence why such models capture semantic information, remains unexplained. We build on recent theoretical interpretation of word embeddings as a basis to consider an explicit structure for representations of relations between entities. For identifiable relation types, we are able to predict properties and justify the relative performance of leading knowledge graph representation methods, including their often overlooked ability to make independent predictions. Knowledge graphs are large repositories of binary relations between words (or entities) in the form of fact triples (subject, relation, object). Many models have been developed for learning representations of entities and relations in knowledge graphs, such that known facts can be recalled and previously unknown facts can be inferred, a task known as link prediction. Recent link prediction models (e.g. Bordes et al., 2013; Trouillon et al., 2016; Balažević et al., 2019b ) learn entity representations, or embeddings, of far lower dimensionality than the number of entities, by capturing latent structure in the data. Relations are typically represented as a mapping from the embedding of a subject entity to its related object entity embedding(s). Although the performance of knowledge graphlink prediction models has steadily improved for nearly a decade, relatively little is understood of the low-rank latent structure that underpins these models, which we address in this work. We start by drawing a parallel between entity embeddings in knowledge graphs and unsupervised word embeddings, as learned by algorithms such as Word2Vec (W2V) (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) . We assume that words have latent features, e.g. meaning(s), tense, grammatical type, that are innate and fixed, irrespective of what an embedding may capture (which may be only a part, subject to the embedding method and/or the data source); and that this same latent structure gives rise to patterns observed in the data, e.g. in word co-occurrence statistics and in which words are related to which. As such, an understanding of the latent structure from one embedding task (e.g. word embedding) might be useful to another (e.g. knowledge graph entity embedding). Recent work theoretically explains how semantic properties are encoded in word embeddings that (approximately) factorise a matrix of word cooccurrence pointwise mutual information (PMI), e.g. as is known for W2V (Levy & Goldberg, 2014) . Semantic relationships between words (specifically similarity, relatedness, paraphrase and analogy) are proven to manifest as linear relationships between rows of the PMI matrix (subject to known error terms), of which word embeddings can be considered low-rank projections. This explains why similar words (e.g. synonyms) have similar embeddings; and embeddings of analogous word pairs share a common ""vector offset"". Importantly, this insight allows us to identify geometric relationships between such word embeddings necessary for other semantic relations to hold, such as those of knowledge graphs. These relation conditions describe relation-specific mappings between entity embeddings, i.e. relation representations, providing a ""blue-print"" against which to consider knowledge graph representation models. We find that various properties of knowledge graph representation models, including the relative DistMult (Yang et al., 2015) multiplicative (diagonal) e s Re o TuckER (Balažević et al., 2019b) multiplicative W × 1 e s × 2 r × 3 e o MuRE (Balažević et al., 2019a) performance of leading link prediction models, accord with predictions based on these relation conditions, suggesting a commonality to the latent structure learned in word embedding models and knowledge graph representation models, despite the significant differences between their training data and methodology. In summary, the key contributions of this work are: • to use recent understanding of PMI-based word embeddings to derive what a relation representation must achieve to map a subject word embedding to all related object word embeddings (relation conditions), based on which relations can be categorised into three types; • to show that properties of knowledge graph models fit predictions made from relation conditions, e.g. strength of a relation's relatedness aspect is reflected in the eigenvalues of its relation matrix; • to show that the performance per relation of leading link prediction models corresponds to the ability of the model's architecture to meet the relation conditions of the relation's type, i.e. the better the architecture of a knowledge graph representation model aligns with the form theoretically derived for PMI-based word embeddings, the better the model performs; and • noting how ranking metrics can be flawed, to provide novel insight into the prediction accuracy per relation of recent knowledge graph models, an evaluation metric we recommend in future. Many models learn low-rank representations for knowledge graph link prediction, yet little is known about the latent structure they learn. We build on recent understanding of PMI-based word embeddings to theoretically establish what a relation representation must achieve to map a word embedding to those it is related to for the relations of knowledge graphs (relation conditions). Such conditions partition relations into three types and also provide a framework to assess loss functions of knowledge graph models. Any model that satisfies a relation's conditions can represent it if its entity embeddings are set to PMI-based word embeddings, i.e. a solution is known to exist. Whilst knowledge graph models do not learn the parameters of word embeddings, we show that the better a model's architecture satisfies a relation's conditions, the better its link prediction performance, fitting the premise that similar latent structure is exploited. Overall, we extend previous understanding of how semantic relations are encoded in relationships between PMI-based word embeddings -generalising from a limited set, e.g. similarity and analogy; we demonstrate commonality between the latent structure learned by PMI-based word embeddings (e.g. W2V) and knowledge graph representation models; and we provide novel insight into knowledge graph models by evaluating their predictive performance. A CATEGORISING WORDNET RELATIONS Table 7 describes how each WN18RR relation was assigned to its respective category. Carlson et al., 2010) ), which span our identified relation types (see Table 8 ). Explanation for the relation category assignment is shown in Table 9 . (Balažević et al., 2019b ).",Understanding the structure of knowledge graph representation using insight from word embeddings.,Levy & Goldberg ; Trouillon ; DistMult ; PMI ; • ; Carlson ; nearly a decade ; al. ; Balažević et al. ; linear,recent knowledge graph models ; semantic information ; e.g. synonyms ; relationships ; a relation's conditions ; an explicit structure ; Levy & Goldberg ; the ability ; latent structure ; words,Levy & Goldberg ; Trouillon ; DistMult ; PMI ; • ; Carlson ; nearly a decade ; al. ; Balažević et al. ; linear,"Learning representations of entities and relations in knowledge graphs, which implicitly exploit low-rank latent structure in the data to encode known information and enable unknown facts to be inferred, are typically compared in the latent space following relation-specific mapping. However, the latent structure, and hence why such models capture semantic information, remains unexplained. We build on recent theoretical interpretation of word embeddings as a basis to consider an explicit structure for representations of relations between entities. For identifiable relation types, we are able to predict properties and justify the relative performance of leading knowledge graph representation methods, including their often overlooked ability to make independent predictions. Knowledge graphs are",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Data augmentation is commonly used to encode invariances in learning methods. However, this process is often performed in an inefficient manner, as artificial examples are created by applying a number of transformations to all points in the training set. The resulting explosion of the dataset size can be an issue in terms of storage and training costs, as well as in selecting and tuning the optimal set of transformations to apply. In this work, we demonstrate that it is possible to significantly reduce the number of data points included in data augmentation while realizing the same accuracy and invariance benefits of augmenting the entire dataset. We propose a novel set of subsampling policies, based on model influence and loss, that can achieve a 90% reduction in augmentation set size while maintaining the accuracy gains of standard data augmentation. Data augmentation is a process in which the training set is expanded by applying class-preserving transformations, such as rotations or crops for images, to the original data points. This process has become an instrumental tool in achieving state-of-the-art accuracy in modern machine learning pipelines. Indeed, for problems in image recognition, data augmentation is a key component in achieving nearly all state-of-the-art results BID6 BID10 Graham, 2014; Sajjadi et al., 2016) . Data augmentation is also a popular technique because of its simplicity, particularly in deep learning applications, where applying a set of known invariances to the data is often more straightforward than trying to encode this knowledge directly in the model architecture.However, data augmentation can be an expensive process, as applying a number of transformations to the entire dataset may increase the overall size of the dataset by orders of magnitude. For example, if applying just 3 sets of augmentations (e.g., translate, rotate, crop), each with 4 possible configurations, the dataset can easily grow by a factor of 12 (if applied independently), all the way to 64x (if applied in sequence). While this may have some benefits in terms of overfitting, augmenting the entire training set can also significantly increase data storage costs and training time, which can scale linearly or superlinearly with respect to the training set size. Further, selecting the optimal set of transformations to apply to a given data point is often a non-trivial task. Applying transformations not only takes processing time, but also frequently requires some amount of domain expertise. Augmentations are often applied heuristically in practice, and small perturbations are expected (but not proven) to preserve classes. If more complex augmentations are applied to a dataset, they may have to be verified on a per-sample basis.In this work, we aim to make data augmentation more efficient and user-friendly by identifying subsamples of the full dataset that are good candidates for augmentation. In developing policies for subsampling the data, we draw inspiration from the virtual support vector (VSV) method, which has been used for this purpose in the context of SVMs BID4 BID9 . The VSV method attempts to create a more robust decision surface by augmenting only the samples that are close to the margin-i.e., the support vectors. The motivation is intuitive: if a point does not affect the margin, then any small perturbation of that point in data space will likely yield a point that is again too far from the margin to affect it. The method proceeds by applying classpreserving data augmentations (e.g., small perturbations) to all support vectors in the training set. The SVM is then retrained on the support vector dataset concatenated with the augmented dataset, and the end result is a decision surface that has been encoded with transformation invariance while augmenting many fewer samples than found in the full training set.Although proven to be an effective approach for SVMs, methods utilizing support vectors may not generalize well to other classifiers. Therefore, in this work, we aim to develop policies that can effectively reduce the augmentation set size while applying to a much broader class of models. A key step in developing these policies is to determine some metric by which to rank the importance of data points for augmentation. We build policies based on two key metrics. First, we make a natural generalization of the VSV method by measuring the loss induced by a training point. Second, we explore using the influence of a point as an indicator of augmentation potential. Influence functions, originating from robust statistics, utilize more information than loss (i.e., residuals) alone, as they take into account both leverage and residual information.The contributions of this paper are as follows. First, we demonstrate that it is typically unnecessary to augment the entire dataset to achieve high accuracy-for example, we can maintain 99.86% or more of the full augmentation accuracy while only augmenting 10% of the dataset in the case of translation augmentations, and we observe similar behavior for other augmentations. Second, we propose several policies to select the subset of points to augment. Our results indicate that policies based off of training loss or model influence are an effective strategy over simple baselines, such as random sampling. Finally, we propose several modifications to these approaches, such as sample reweighting and online learning, that can further improve performance. Our proposed policies are simple and straightforward to implement, requiring only a few lines of code. We perform experiments throughout on common benchmark datasets, such as MNIST (LeCun et al., 1998) , CIFAR10 (Krizhevsky, 2009) , and NORB (LeCun et al., 2004) . In this paper, we demonstrate that not all training points are equally useful for augmentation, and we propose simple policies that can select the most viable subset of points. Our policies, based on notions of training loss and model influence, are widely applicable to general machine learning models. Obtaining access to an augmentation score vector can be obtained in only one training cycle on the original data (e.g., a fixed cost), yet the potential improvements in augmented training can scale superlinearly with respect to the original dataset size. With many fewer data points to augment, the augmentations themselves can be applied in a more efficient manner in terms of compute and expert oversight. At an extreme, they can be specialized on a per-example basis.A natural area of future work is to explore subset selection policies that take the entire subset into account, rather than the greedy policies described. For example, even if two samples may independently have large leave-one-out influence, it may be the case that these points influence each other and leave-one-out influence may be an overestimate (e.g., consider the case of two identical samples). Including second-order information or encouraging subset diversity 4 may therefore help to improve performance even further.",Selectively augmenting difficult to classify points results in efficient training.,NORB ; Second ; Graham ; Krizhevsky ; al. ; SVM ; only one ; First ; VSV ; one,an effective strategy ; it ; that point ; access ; MNIST ; the entire dataset ; Second ; the case ; deep learning applications ; class-preserving transformations,NORB ; Second ; Graham ; Krizhevsky ; al. ; SVM ; only one ; First ; VSV ; one,"Data augmentation is commonly used to encode invariances in learning methods. However, it is often performed in an inefficient manner, as artificial examples are created by applying a number of transformations to all points in the training set. The resulting explosion of dataset size can be an issue in terms of storage and training costs, as well as in selecting and tuning the optimal set of transformations. In this work, we demonstrate that it is possible to significantly reduce the number of data points included in data augmentation while realizing the same accuracy and invariance benefits of augmenting the entire dataset. We propose a novel set of subsampling policies based",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Sorting input objects is an important step in many machine learning pipelines. However, the sorting operator is non-differentiable with respect to its inputs, which prohibits end-to-end gradient-based optimization. In this work, we propose NeuralSort, a general-purpose continuous relaxation of the output of the sorting operator from permutation matrices to the set of unimodal row-stochastic matrices, where every row sums to one and has a distinct argmax. This relaxation permits straight-through optimization of any computational graph involve a sorting operation. Further, we use this relaxation to enable gradient-based stochastic optimization over the combinatorially large space of permutations by deriving a reparameterized gradient estimator for the Plackett-Luce family of distributions over permutations. We demonstrate the usefulness of our framework on three tasks that require learning semantic orderings of high-dimensional objects, including a fully differentiable, parameterized extension of the k-nearest neighbors algorithm Learning to automatically sort objects is useful in many machine learning applications, such as topk multi-class classification BID5 , ranking documents for information retrieval (Liu et al., 2009) , and multi-object target tracking in computer vision BID3 . Such algorithms typically require learning informative representations of complex, high-dimensional data, such as images, before sorting and subsequent downstream processing. For instance, the k-nearest neighbors image classification algorithm, which orders the neighbors based on distances in the canonical pixel basis, can be highly suboptimal for classification (Weinberger et al., 2006) . Deep neural networks can instead be used to learn representations, but these representations cannot be optimized end-to-end for a downstream sorting-based objective, since the sorting operator is not differentiable with respect to its input.In this work, we seek to remedy this shortcoming by proposing NeuralSort, a continuous relaxation to the sorting operator that is differentiable almost everywhere with respect to the inputs. The output of any sorting algorithm can be viewed as a permutation matrix, which is a square matrix with entries in {0, 1} such that every row and every column sums to 1. Instead of a permutation matrix, NeuralSort returns a unimodal row-stochastic matrix. A unimodal row-stochastic matrix is defined as a square matrix with positive real entries, where each row sums to 1 and has a distinct arg max. All permutation matrices are unimodal row-stochastic matrices. NeuralSort has a temperature knob that controls the degree of approximation, such that in the limit of zero temperature, we recover a permutation matrix that sorts the inputs. Even for a non-zero temperature, we can efficiently project any unimodal matrix to the desired permutation matrix via a simple row-wise arg max operation. Hence, NeuralSort is also suitable for efficient straight-through gradient optimization BID4 , which requires ""exact"" permutation matrices to evaluate learning objectives.As the second primary contribution, we consider the use of NeuralSort for stochastic optimization over permutations. In many cases, such as latent variable models, the permutations may be latent but directly influence observed behavior, e.g., utility and choice models are often expressed as distributions over permutations which govern the observed decisions of agents (Regenwetter et al., 2006; BID7 . By learning distributions over unobserved permutations, we can account for the uncertainty in these permutations in a principled manner. However, the challenge with stochastic optimization over discrete distributions lies in gradient estimation with respect to the distribution parameters. Vanilla REINFORCE estimators are impractical for most cases, or necessitate custom control variates for low-variance gradient estimation (Glasserman, 2013) .In this regard, we consider the Plackett-Luce (PL) family of distributions over permutations (Plackett, 1975; Luce, 1959) . A common modeling choice for ranking models, the PL distribution is parameterized by n scores, with its support defined over the symmetric group consisting of n! permutations. We derive a reparameterizable sampler for stochastic optimization with respect to this distribution, based on Gumbel perturbations to the n (log-)scores. However , the reparameterized sampler requires sorting these perturbed scores, and hence the gradients of a downstream learning objective with respect to the scores are not defined. By using NeuralSort instead, we can approximate the objective and obtain well-defined reparameterized gradient estimates for stochastic optimization.Finally, we apply NeuralSort to tasks that require us to learn semantic orderings of complex, highdimensional input data. First, we consider sorting images of handwritten digits, where the goal is to learn to sort images by their unobserved labels. Our second task extends the first one to quantile regression, where we want to estimate the median (50-th percentile) of a set of handwritten numbers. In addition to identifying the index of the median image in the sequence, we need to learn to map the inferred median digit to its scalar representation. In the third task, we propose an algorithm that learns a basis representation for the k-nearest neighbors (kNN) classifier in an end-to-end procedure. Because the choice of the k nearest neighbors requires a non-differentiable sorting, we use NeuralSort to obtain an approximate, differentiable surrogate. On all tasks , we observe significant empirical improvements due to NeuralSort over the relevant baselines and competing relaxations to permutation matrices. The problem of learning to rank documents based on relevance has been studied extensively in the context of information retrieval. In particular, listwise approaches learn functions that map objects to scores. Much of this work concerns the PL distribution: the RankNet algorithm BID6 can be interpreted as maximizing the PL likelihood of pairwise comparisons between items, while the ListMLE ranking algorithm in Xia et al. (2008) extends this with a loss that maximizes the PL likelihood of ground-truth permutations directly. The differentiable pairwise approaches to ranking, such as Rigutini et al. FORMULA1 , learn to approximate the comparator between pairs of objects. Our work considers a generalized setting where sorting based operators can be inserted anywhere in computation graphs to extend traditional pipelines e.g., kNN.Prior works have proposed relaxations of permutation matrices to the Birkhoff polytope, which is defined as the convex hull of the set of permutation matrices a.k.a. the set of doubly-stochastic matrices. A doubly-stochastic matrix is a permutation matrix iff it is orthogonal and continuous relaxations based on these matrices have been used previously for solving NP-complete problems such as seriation and graph matching (Fogel et al., 2013; Fiori et al., 2013; Lim & Wright, 2014) . BID1 proposed the use of the Sinkhorn operator to map any square matrix to the Birkhoff polytope. They interpret the resulting doubly-stochastic matrix as the marginals of a distribution over permutations. Mena et al. (2018) propose an alternate method where the square matrix defines a latent distribution over the doubly-stochastic matrices themselves. These distributions can be sampled from by adding elementwise Gumbel perturbations. Linderman et al. FORMULA1 propose a rounding procedure that uses the Sinkhorn operator to directly sample matrices near the Birkhoff polytope. Unlike Mena et al. (2018) , the resulting distribution over matrices has a tractable density. In practice, however, the approach of Mena et al. FORMULA1 performs better and will be the main baseline we will be comparing against in our experiments in Section 6.As discussed in Section 3, NeuralSort maps permutation matrices to the set of unimodal rowstochastic matrices. For the stochastic setting, the PL distribution permits efficient sampling, exact and tractable density estimation, making it an attractive choice for several applications, e.g., variational inference over latent permutations. Our reparameterizable sampler, while also making use of the Gumbel distribution, is based on a result unique to the PL distribution (Proposition 5).The use of the Gumbel distribution for defining continuous relaxations to discrete distributions was first proposed concurrently by Jang et al. FORMULA1 and Maddison et al. (2017) for categorical variables, referred to as Gumbel-Softmax. The number of possible permutations grow factorially with the dimension, and thus any distribution over n-dimensional permutations can be equivalently seen as a distribution over n! categories. Gumbel-softmax does not scale to a combinatorially large number of categories (Kim et al., 2016; Mussmann et al., 2017) , necessitating the use of alternate relaxations, such as the one considered in this work. In this paper, we proposed NeuralSort, a continuous relaxation of the sorting operator to the set of unimodal row-stochastic matrices. Our relaxation facilitates gradient estimation on any computation graph involving a sort operator. Further, we derived a reparameterized gradient estimator for the Plackett-Luce distribution for efficient stochastic optimization over permutations. On three illustrative tasks including a fully differentiable k-nearest neighbors, our proposed relaxations outperform prior work in end-to-end learning of semantic orderings of high-dimensional objects.In the future, we would like to explore alternate relaxations to sorting as well as applications that extend widely-used algorithms such as beam search (Goyal et al., 2018) . Both deterministic and stochastic NeuralSort are easy to implement. We provide reference implementations in Tensorflow BID0 Proof. For any value of λ, the following inequalities hold: DISPLAYFORM0 This finishes the proof.","We provide a continuous relaxation to the sorting operator, enabling end-to-end, gradient-based stochastic optimization.",NP ; Weinberger ; First ; Lim & Wright ; Fiori ; Regenwetter ; NeuralSort ; Birkhoff ; second ; max,the observed decisions ; the third task ; this work ; necessitate custom control ; They ; stochastic optimization ; any sorting algorithm ; PL ; three tasks ; Liu et al,NP ; Weinberger ; First ; Lim & Wright ; Fiori ; Regenwetter ; NeuralSort ; Birkhoff ; second ; max,"NeuralSort, a general-purpose continuous relaxation of the output of the sorting operator from permutation matrices to unimodal row-stochastic matrices, allowing straight-through optimization of any computational graph involve a sorting operation. This relaxation enables gradient-based stochastic optimization over combinatorially large permutations by deriving a reparameterized gradient estimator for the Plackett-Luce family of distributions over permutations. In this work, we demonstrate the usefulness of our framework on three tasks that require learning semantic orderings of high-dimensional objects, including k-nearest neighbors image classification algorithm",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"We propose a study of the stability of several few-shot learning algorithms subject to variations in the hyper-parameters and optimization schemes while controlling the random seed.   We propose a methodology for testing for statistical differences in model performances under several replications. To study this specific design, we attempt to reproduce results from three prominent papers: Matching Nets, Prototypical Networks, and TADAM. We analyze on the miniImagenet dataset on the standard classification task in the 5-ways, 5-shots learning setting at test time. We find that the selected implementations exhibit stability across random seed, and repeats.",We propose a study of the stability of several few-shot learning algorithms subject to variations in the hyper-parameters and optimization schemes while controlling the random seed.,three ; Prototypical Networks ; TADAM,testing ; TADAM ; three ; the miniImagenet ; We ; variations ; stability ; the stability ; a study ; several replications,three ; Prototypical Networks ; TADAM,"We propose a methodology for testing for statistical differences in model performance under several replications, based on three prominent papers: Matching Nets, Prototypical Networks, and TADAM. We analyze on the miniImagenet dataset on standard classification task at test time. We find that the selected implementations exhibit stability across random seed, and repeats.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Recurrent neural networks (RNNs) are particularly well-suited for modeling long-term dependencies in sequential data, but are notoriously hard to train because the error backpropagated in time either vanishes or explodes at an exponential rate. While a number of works attempt to mitigate this effect through gated recurrent units, skip-connections, parametric constraints and design choices, we propose a novel incremental RNN (iRNN), where hidden state vectors keep track of incremental changes, and as such approximate state-vector increments of Rosenblatt's (1962) continuous-time RNNs. iRNN exhibits identity gradients and is able to account for long-term dependencies (LTD). We show that our method is computationally efficient overcoming overheads of many existing methods that attempt to improve RNN training, while suffering no performance degradation. We demonstrate the utility of our approach with extensive experiments and show competitive performance against standard LSTMs on LTD and other non-LTD tasks.
 Recurrent neural networks (RNNs) in each round store a hidden state vector, h m ∈ R D , and upon receiving the input vector, x m+1 ∈ R d , linearly transform the tuple (h m , x m+1 ) and pass it through a memoryless non-linearity to update the state over T rounds. Subsequently, RNNs output an affine function of the hidden states as its prediction. The model parameters (state/input/prediction parameters) are learnt by minimizing an empirical loss. This seemingly simple update rule has had significant success in learning complex patterns for sequential input data. Nevertheless, that training RNNs can be challenging, and that performance can be uneven on tasks that require long-term-dependency (LTD), was first noted by Hochreiter (1991) , Bengio et al. (1994) and later by other researchers. Pascanu et al. (2013b) attributed this to the fact that the error gradient back-propagated in time (BPTT), for the time-step m, is dominated by product of partials of hiddenstate vectors, T −1 j=m ∂hj+1 ∂hj , and these products typically exhibit exponentially vanishing decay or explosion, resulting in incorrect credit assignment during training and test-time. Rosenblatt (1962) , on whose work we draw inspiration from, introduced continuous-time RNN (CTRNN) to mimic activation propagation in neural circuitry. CTRNN dynamics evolves as follows: τġ(t) = −αg(t) + φ(U g(t) + W x(t) + b), t ≥ t 0 . ( Here, x(t) ∈ R d is the input signal, g(t) ∈ R D is the hidden state vector of D neurons,ġ i (t) is the rate of change of the i-th state component; τ, α ∈ R + , referred to as the post-synaptic time-constant, impacts the rate of a neuron's response to the instantaneous activation φ(U g(t) + W x(t) + b); and U ∈ R D×D , W ∈ R D×d , b ∈ R D are model parameters. In passing, note that recent RNN works that draw inspiration from ODE's (Chang et al., 2019) are special cases of CTRNN (τ = 1, α = 0). Vanishing Gradients. The qualitative aspects of the CTRNN dynamics is transparent in its integral form: This integral form reveals that the partials of hidden-state vector with respect to the initial condition, ∂g(t) ∂g(t0) , gets attenuated rapidly (first term in RHS), and so we face a vanishing gradient problem. We will address this issue later but we note that this is not an artifact of CTRNN but is exhibited by ODEs that have motivated other RNNs (see Sec. 2). Shannon-Nyquist Sampling. A key property of CTRNN is that the time-constant τ together with the first term −g(t), is in effect a low-pass filter with bandwidth ατ −1 suppressing high frequency components of the activation signal, φ((U g(s )) + (W x(s )) + b). This is good, because, by virtue of the Shannon-Nyquist sampling theorem, we can now maintain fidelity of discrete samples with respect to continuous time dynamics, in contrast to conventional ODEs (α = 0). Additionally , since high-frequencies are already suppressed, in effect we may assume that the input signal x(t) is slowly varying relative to the post-synaptic time constant τ . Equilibrium. The combination of low pass filtering and slowly time varying input has a significant bearing. The state vector as well as the discrete samples evolve close to the equilibrium state, i.e., g(t) ≈ φ(U g(t) + W x(t) + b) under general conditions (Sec. 3). Incremental Updates. Whether or not system is in equilibrium, the integral form in Eq. 2 points to gradient attenuation as a fundamental issue. To overcome this situation, we store and process increments rather than the cumulative values g(t) and propose dynamic evolution in terms of increments. Let us denote hidden state sequence as h m ∈ R D and input sequence x m ∈ R d . For m = 1, 2, . . . , T , and a suitable β > 0 τġ(t) = −α(g(t) ± h m−1 ) + φ(U (g(t) ± h m−1 ) + W x m + b), g(0) = 0, t ≥ 0 (3) Intuitively, say system is in equilibrium and −α(µ(x m , h m−1 ))+φ(U µ(x m , h m−1 )+W x m +b) = 0. We note state transitions are marginal changes from previous states, namely, h m = µ(x m , h m−1 ) − h m−1 . Now for a fixed input x m , as to which equilibrium is reached depends on h m−1 , but are nevertheless finitely many. So encoding marginal changes as states leads to ""identity"" gradient. Incremental RNN (iRNN) achieves Identity Gradient. We propose to discretize Eq. 3 to realize iRNN (see Sec. 3). At time m, it takes the previous state h m−1 ∈ R D and input x m ∈ R d and outputs h m ∈ R D after simulating the CTRNN evolution in discrete-time, for a suitable number of discrete steps. We show that the proposed RNN approximates the continuous dynamics and solves the vanishing/exploding gradient issue by ensuring identity gradientIn general, we consider two options, SiRNN, whose state is updated with a single CTRNN sample, similar to vanilla RNNs, and, iRNN, with many intermediate samples. SiRNN is well-suited for slowly varying inputs. Contributions. To summarize, we list our main contributions: (A) iRNN converges to equilibrium for typical activation functions. The partial gradients of hiddenstate vectors for iRNNs converge to identity, thus solving vanishing/exploding gradient problem! (B) iRNN converges rapidly, at an exponential rate in the number of discrete samplings of Eq. 1. SiRNN, the single-step iRNN, is efficient and can be leveraged for slowly varying input sequences. It exhibits fast training time, has fewer parameters and better accuracy relative to standard LSTMs. (C) Extensive experiments on LTD datasets show that we improve upon standard LSTM accuracy as well as other recent proposals that are based on designing transition matrices and/or skip connections. iRNNs/SiRNNs are robust to time-series distortions such as noise paddings (D) While our method extends directly (see Appendix A.1) to Deep RNNs, we deem these extensions complementary, and focus on single-layer to highlight our incremental perspective. Gated Architectures. Long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997 ) is widely used in RNNs to model long-term dependency in sequential data. Gated recurrent unit (GRU) (Cho et al., 2014 ) is another gating mechanism that has been demonstrated to achieve similar performance of LSTM with fewer parameters. Some recent gated RNNs include UGRNN (Collins et al., 2016) , and FastGRNN (Kusupati et al., 2018) . While mitigating vanishing/exploding gradients, they do not eliminate it. Often, these models incur increased inference, training costs, and model size. Unitary RNNs. Arjovsky et al. (2016); Jing et al. (2017) ; ; Mhammedi et al. (2016) focus on designing well-conditioned state transition matrices, attempting to enforce unitary-property, during training. Unitary property does not generally circumvent vanishing gradient (Pennington et al. (2017) ). Also, it limits expressive power and prediction accuracy while also increasing training time. Deep RNNs. These are nonlinear transition functions incorporated into RNNs for performance improvement. For instance, Pascanu et al. (2013a) empirically analyzed the problem of how to construct deep RNNs. Zilly et al. (2017) proposed extending the LSTM architecture to allow stepto-step transition depths larger than one. Mujika et al. (2017) proposed incorporating the strengths of both multiscale RNNs and deep transition RNNs to learn complex transition functions from one timestep to the next. While Deep RNNs offer richer representations relative to single-layers, it is complementary to iRNNs. Residual/Skip Connections. Jaeger et al. (2007) ; Bengio et al. (2013); Campos et al. (2017) ; Kusupati et al. (2018) feed-forward state vectors to induce skip or residual connections, to serve as a middle ground between feed-forward and recurrent models, and to mitigate gradient decay. Nevertheless, these connections, cannot entirely eliminate gradient explosion/decay. For instance, Kusupati et al. (2018) suggest h m = α m h m−1 + β m φ(U h m−1 + W x m + b), and learn parameters so that α m ≈ 1 and β m ≈ 0. Evidently, this setting can lead to identity gradient, observe that setting β m ≈ 0, implies little contribution from the inputs and can conflict with good accuracy, as also observed in our experiments. Linear RNNs. (Bradbury et al., 2016; Balduzzi & Ghifary, 2016) have focused on speeding up recurrent neural networks by replacing recurrent connections, such as hidden-to-hidden interactions, with light weight linear components. While this has led to reduced training time, it has resulted in significantly increasing model size. For example, typically requires twice the number of cells for LSTM level performance. ODE/Dynamical Perspective. There are a few works that are inspired by ODEs, and attempt to address stability, but do not end up eliminating vanishing/exploding gradients. Talathi & Vartak (2015) proposed a modified weight initialization strategy based on a dynamical system perspective on weight initialization process that leads to successfully training RNNs composed of ReLUs. Niu et al. (2019) analyzed RNN architectures using numerical methods of ODE and propose a family of ODE-RNNs. Chang et al. (2019) , propose Antisymmetric-RNN. Their key idea is to express the transition matrix in Eq. 1, for the special case α = 0, τ = 1, as a difference: U = V − V T and note that the eigenspectrum is imaginary. Nevertheless, Euler discretization, in this context leads to instability, necessitating damping of the system. As such vanishing gradient cannot be completely eliminated. Its behavior is analogous to FastRNN Kusupati et al. (2018) , in that, identity gradient conflicts with high accuracy. In summary, we are the first to propose evolution over the equilibrium manifold, and demonstrating identity gradients. Neural ODEs (Chen et al., 2018; Rubanova et al., 2019) have also been proposed for time-series prediction to deal with irregularly sampled inputs. To do this they parameterize the derivative of the hidden-state in terms of an autonomous differential equation and let the ODE evolve in continuous time until the next input arrives. As such, this is not our goal, our ODE explicitly depends on the input, and evolves until equilibrium for that input is reached. We introduce incremental updates to bypass vanishing/exploding gradient issues, which is not of specific concern for these works.",Incremental-RNNs resolves exploding/vanishing gradient problem by updating state vectors based on difference between previous state and that predicted by an ODE.,Campos et al ; Pascanu et al. ; Chang et al. ; +W ; ∂hj ; Rosenblatt ; ≥ ; SiRNN ; Mhammedi et al ; Shannon-Nyquist Sampling,example ; works attempt ; discrete steps ; the next input ; the system ; track ; first ; a middle ground ; exponentially vanishing decay ; a memoryless non,Campos et al ; Pascanu et al. ; Chang et al. ; +W ; ∂hj ; Rosenblatt ; ≥ ; SiRNN ; Mhammedi et al ; Shannon-Nyquist Sampling,"Recurrent neural networks (RNN) are well-suited for sequential data modeling long-term dependencies. However, they are notoriously difficult to train due to the exponential rate of error backpropagated in time. We propose a novel incremental RNN (iRNN), where hidden state vectors keep track of incremental changes and approximate state-vector increments of Rosenblatt's (1962) continuous-time RNNs. iRNN exhibits identity gradients and is able to account for LTD. We show that our approach is computationally efficient overcoming overheads of many existing methods, while suffering no performance degradation.",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Supervised deep learning methods require cleanly labeled large-scale datasets, but collecting such data is difficult and sometimes impossible. There exist two popular frameworks to alleviate this problem: semi-supervised learning and robust learning to label noise. Although these frameworks relax the restriction of supervised learning, they are studied independently. Hence, the training scheme that is suitable when only small cleanly-labeled data are available remains unknown. In this study, we consider learning from bi-quality data as a generalization of these studies, in which a small portion of data is cleanly labeled, and the rest is corrupt. Under this framework, we compare recent algorithms for semi-supervised and robust learning. The results suggest that semi-supervised learning outperforms robust learning with noisy labels. We also propose a training strategy for mixing mixup techniques to learn from such bi-quality data effectively. Learning from imperfect data is essential for applying machine learning, especially data-hungry deep learning, to real-world problems. One approach to handling this problem is semi-supervised learning (SSL), where training data consist of a small amount of labeled data and a large amount of unlabeled data. Another approach is robust learning to label noise (RLL), wherein all data are labeled, but some of them are mislabeled.SSL leverages large unlabeled data to improve the performance of supervised learning on a limited number of labeled data. In the context of deep SSL, one effective method is to train neural networks to maintain consistency for a small perturbation of unlabeled inputs BID7 ; BID10 ; BID11 ). BID8 refers these methods as consistency regularization.In the RLL setting, learners need to enhance their performance using corrupted labels and avoid the performance deterioration caused by such data. This requirement is particularly important for deep neural networks because they have ample capacity to remember whole samples even if their labels are completely random BID0 BID14 ). To tackle this problem, some methods use a small amount of clean data to estimate noise transition matrix BID12 ; BID2 ) or to learn to select possibly correctly-labeled samples BID4 ; BID3 ).Although both SSL and RLL aim to alleviate the limited-data problem, they have been studied independently and evaluated using different benchmarks. However, if only a small amount of clean data is available, they can be regarded as similar problems. In such as situation, can RLL outperform SSL under the same settings? This question was our initial motivation to unify these two lines of research.In this paper, we introduce a generalization of SSL and RLL, based on the concept of trusted data BID1 ; BID2 ) in the literature of RLL. More precisely , we assumed that some labels are guaranteed to be clean, and the rest are noisy. The two learning frameworks can be unified by controlling the ratio of corrupted labels to all labels and the noisiness of label corruption.Using the shared evaluation procedure in BID8 , we compared recent SSL and RLL algorithms using image classification task and found that the existing RLL methods using a small amount of clean data cannot outperform SSL under this setting. This finding suggests that such RLL algorithms cannot use noisy labels effectively. Therefore, it is necessary to adaptively use SSL and RLL in a data-driven manner. As a baseline learning algorithm , we propose combining the mixup losses for SSL BID11 ) and RLL BID15 ); the results obtained are comparable to those of SSL-and RLL-specific methods and indicate the effective use of useful information from noisy labels. In this paper, we introduce a novel framework of weakly supervised learning by unifying SSL and RLL, which have been independently studied. To handle this problem, we propose to mix mixup for SSL and RLL. This method empirically works well and achieves competitive results with semisupervised and robust learning specific methodologies.In addition, our experiments indicate that the performance of some RLL with trusted data might be inferior to that of SSL under identical settings. This result suggests that the existing RLL methods cannot effectively exploit the information which should be extracted from noisy labels.Our proposed method does not use the estimated quality; instead some hyperparameters are introduced. The use of quality estimation may ease hyperparameter tuning, but is still an open question.",We propose to compare semi-supervised and robust learning to noisy label under a shared setting,two ; One ; RLL ; SSL ; noisy labels,learners ; trusted data ; this study ; ample capacity ; mixup ; these methods ; some ; cleanly labeled large-scale datasets ; recent algorithms ; such bi-quality data,two ; One ; RLL ; SSL ; noisy labels,"Supervised deep learning methods require cleanly labeled large-scale datasets, but collecting such data is difficult and sometimes impossible. There are two popular frameworks to alleviate this problem: semi-supervised learning and robust learning to label noise. These frameworks relax the restriction of supervised learning, while the training scheme that is suitable when only small cleanly-labeled data are available remains unknown. In this study, we consider learning from bi-quality data as a generalization of these studies, in which a small portion of labeled data is labeled, and the rest is corrupt. The results suggest that semi-Supervised learning outperforms robust learning",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Batch Normalization (BN) is one of the most widely used techniques in Deep Learning field. But its performance can awfully degrade with insufficient batch size. This weakness limits the usage of BN on many computer vision tasks like detection or segmentation, where batch size is usually small due to the constraint of memory consumption. Therefore many modified normalization techniques have been proposed, which either fail to restore the performance of BN completely, or have to introduce additional nonlinear operations in inference procedure and increase huge consumption. In this paper, we reveal that there are two extra batch statistics involved in backward propagation of BN, on which has never been well discussed before. The extra batch statistics associated with gradients also can severely affect the training of deep neural network. Based on our analysis, we propose a novel normalization method, named Moving Average Batch Normalization (MABN). MABN can completely restore the performance of vanilla BN in small batch cases, without introducing any additional nonlinear operations in inference procedure. We prove the benefits of MABN by both theoretical analysis and experiments. Our experiments demonstrate the effectiveness of MABN in multiple computer vision tasks including ImageNet and COCO. The code has been released in https://github.com/megvii-model/MABN. Batch Normalization (BN) (Ioffe & Szegedy, 2015) is one of the most popular techniques for training neural networks. It has been widely proven effective in many applications, and become the indispensable part of many state of the art deep models. Despite the success of BN, it's still challenging to utilize BN when batch size is extremely small 1 . The batch statistics with small batch size are highly unstable, leading to slow convergence during training and bad performance during inference. For example, in detection or segmentation tasks, the batch size is often limited to 1 or 2 per GPU due to the requirement of high resolution inputs or complex structure of the model. Directly computing batch statistics without any modification on each GPU will make performance of the model severely degrade. To address such issues, many modified normalization methods have been proposed. They can be roughly divided into two categories: some of them try to improve vanilla BN by correcting batch statistics (Ioffe, 2017; Singh & Shrivastava, 2019) , but they all fail to completely restore the performance of vanilla BN; Other methods get over the instability of BN by using instance-level normalization (Ulyanov et al., 2016; Ba et al., 2016; Wu & He, 2018) , therefore models can avoid the affect of batch statistics. This type of methods can restore the performance in small batch cases to some extent. However, instance-level normalization hardly meet industrial or commercial needs so far, for this type of methods have to compute instance-level statistics both in training and inference, which will introduce additional nonlinear operations in inference procedure and dramatically increase consumption Shao et al. (2019) . While vanilla BN uses the statistics computed over the whole training data instead of batch of samples when training finished. Thus BN is a linear operator and can be merged with convolution layer during inference procedure. Figure 1 (a) shows with ResNet-50 (He et al., 2016) , instance-level normalization almost double the inference time compared with vanilla BN. Therefore, it's a tough but necessary task to restore the performance of BN in small batch training without introducing any nonlinear operations in inference procedure. In this paper, we first analysis the formulation of vanilla BN, revealing there are actually not only 2 but 4 batch statistics involved in normalization during forward propagation (FP) as well as backward propagation (BP). The additional 2 batch statistics involved in BP are associated with gradients of the model, and have never been well discussed before. They play an important role in regularizing gradients of the model during BP. In our experiments (see Figure 2) , variance of the batch statistics associated with gradients in BP, due to small batch size, is even larger than that of the widelyknown batch statistics (mean, variance of feature maps). We believe the instability of batch statistics associated with gradients is one of the key reason why BN performs poorly in small batch cases. Based on our analysis, we propose a novel normalization method named Moving Average Batch Normalization (MABN). MABN can completely get over small batch issues without introducing any nonlinear manipulation in inference procedure. The core idea of MABN is to replace batch statistics with moving average statistics. We substitute batch statistics involved in BP and FP with different type of moving average statistics respectively, and theoretical analysis is given to prove the benefits. However, we observed directly using moving average statistics as substitutes for batch statistics can't make training converge in practice. We think the failure takes place due to the occasional large gradients during training, which has been mentioned in Ioffe (2017) . To avoid training collapse, we modified the vanilla normalization form by reducing the number of batch statistics, centralizing the weights of convolution kernels, and utilizing renormalizing strategy. We also theoretically prove the modified normalization form is more stable than vanilla form. MABN shows its effectiveness in multiple vision public datasets and tasks, including ImageNet (Russakovsky et al., 2015) , COCO (Lin et al., 2014) . All results of experiments show MABN with small batch size (1 or 2) can achieve comparable performance as BN with regular batch size (see Figure 1(b ) ). Besides , it has same inference consumption as vanilla BN (see Figure 1(a) ). We also conducted sufficient ablation experiments to verify the effectiveness of MABN further.",We propose a novel normalization method to handle small batch size cases.,Ba ; Deep Learning ; Batch Normalization ; Shao et al. ; al. ; GPU ; FP ; two ; MABN ; ImageNet,such issues ; Russakovsky ; Ioffe & Szegedy ; et al ; segmentation ; Szegedy ; COCO ; deep neural network ; which ; convolution layer,Ba ; Deep Learning ; Batch Normalization ; Shao et al. ; al. ; GPU ; FP ; two ; MABN ; ImageNet,"Batch Normalization (BN) is one of the most widely used techniques in Deep Learning field, but its performance can awfully degrade with insufficient batch size. This weakness limits the usage of BN on many computer vision tasks like detection or segmentation, where batch size is often small due to the constraint of memory consumption. Many modified normalization techniques have been proposed, which either restore the performance of vanilla BN completely, or introduce additional nonlinear operations in inference procedure and increase huge consumption. In this paper, we show that there are two extra batch statistics involved in backward propagation, on which BN has never been well discussed before",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"In a typical deep learning approach to a computer vision task, Convolutional Neural Networks (CNNs) are used to extract features at varying levels of abstraction from an image and compress a high dimensional input into a lower dimensional decision space through a series of transformations. In this paper, we investigate how a class of input images is eventually compressed over the course of these transformations. In particular, we use singular value decomposition to analyze the relevant variations in feature space. These variations are formalized as the effective dimension of the embedding. We consider how the effective dimension varies across layers within class. We show that across datasets and architectures, the effective dimension of a class increases before decreasing further into the network, suggesting some sort of initial whitening transformation. Further, the decrease rate of the effective dimension deeper in the network corresponds with training performance of the model. In this section, we analyze and discuss the implications of our findings. Further, we propose complementary analyses that would bolster our findings. In studied examples, neural networks initially spherize embeddings and then collapse dimensionality. The compression of the dimensionality of feature spaces via transformations on inputs is more dramatic in better-performing networks.6. Appendix is scaled by factor α, the spectral norm of α * Φ (l) is α * σ max (Φ (l) ). This is also true in a ReLU network with α > 0. Such a scaling is achieved while preserving φ (l+1) : DISPLAYFORM0 While Srebro et al. BID10 directly apply the trace-norm to bound the complexity of a completed matrix, we apply spectral normalization in Equation 1 to correct for this scale sensitivity. Hence, a small effective dimension corresponds to an eccentric feature space regardless of magnitude.",Neural networks that do a good job of classification project points into more spherical shapes before compressing them into fewer dimensions.,Convolutional Neural Networks ; σ max ; ReLU ; Srebro,a ReLU network ; the dimensionality ; a high dimensional input ; that ; features ; a class ; Srebro ; Such a scaling ; the network ; embeddings,Convolutional Neural Networks ; σ max ; ReLU ; Srebro,"Convolutional Neural Networks (CNNs) are used to extract features at varying levels of abstraction from an image and compress a high dimensional input into a lower dimensional decision space through a series of transformations. In this paper, we examine how a class of input images is compressed over the course of these transformations. The effective dimension of a class varies across datasets and architectures, suggesting some sort of initial whitening transformation. The decrease rate of the effective dimension deeper in the network corresponds with training performance of the model. In addition, we discuss the implications of our findings and propose complementary analyses. In studied examples, neural networks initially spher",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Presently the most successful approaches to semi-supervised learning are based on consistency regularization, whereby a model is trained to be robust to small perturbations of its inputs and parameters. To understand consistency regularization, we conceptually explore how loss geometry interacts with training procedures. The consistency loss dramatically improves generalization performance over supervised-only training; however, we show that SGD struggles to converge on the consistency loss and continues to make large steps that lead to changes in predictions on the test data. Motivated by these observations, we propose to train consistency-based methods with Stochastic Weight Averaging (SWA), a recent approach which averages weights along the trajectory of SGD with a modified learning rate schedule. We also propose fast-SWA, which further accelerates convergence by averaging multiple points within each cycle of a cyclical learning rate schedule. With weight averaging, we achieve the best known semi-supervised results on CIFAR-10 and CIFAR-100, over many different quantities of labeled training data. For example, we achieve 5.0% error on CIFAR-10 with only 4000 labels, compared to the previous best result in the literature of 6.3%. Recent advances in deep unsupervised learning, such as generative adversarial networks (GANs) BID8 , have led to an explosion of interest in semi-supervised learning. Semisupervised methods make use of both unlabeled and labeled training data to improve performance over purely supervised methods. Semi-supervised learning is particularly valuable in applications such as medical imaging, where labeled data may be scarce and expensive BID23 .Currently the best semi-supervised results are obtained by consistency-enforcing approaches BID2 BID17 BID31 BID21 BID24 . These methods use unlabeled data to stabilize their predictions under input or weight perturbations. Consistency-enforcing methods can be used at scale with state-of-the-art architectures. For example, the recent Mean Teacher BID31 model has been used with the Shake-Shake BID7 architecture and has achieved the best semi-supervised performance on the consequential CIFAR benchmarks.This paper is about conceptually understanding and improving consistency-based semi-supervised learning methods. Our approach can be used as a guide for exploring how loss geometry interacts with training procedures in general. We provide several novel observations about the training objective and optimization trajectories of the popular ⇧ BID17 and Mean Teacher BID31 consistency-based models. Inspired by these findings , we propose to improve SGD solutions via stochastic weight averaging (SWA) BID12 , a recent method that averages weights of the networks corresponding to different training epochs to obtain a single model with improved generalization. On a thorough empirical study we show that this procedure achieves the best known semi-supervised results on consequential benchmarks. In particular:• We show in Section 3.1 that a simplified ⇧ model implicitly regularizes the norm of the Jacobian of the network outputs with respect to both its inputs and its weights, which in turn encourages flatter solutions. Both the reduced Jacobian norm and flatness of solutions have been related to generalization in the literature BID29 BID22 BID3 BID27 BID13 BID12 . Interpolating between the weights corresponding to different epochs of training we demonstrate that the solutions of ⇧ and Mean Teacher models are indeed flatter along these directions ( FIG0 ).• In Section 3.2, we compare the training trajectories of the ⇧, Mean Teacher, and supervised models and find that the distances between the weights corresponding to different epochs are much larger for the consistency based models. The error curves of consistency models are also wider ( FIG0 ), which can be explained by the flatness of the solutions discussed in section 3.1. Further we observe that the predictions of the SGD iterates can differ significantly between different iterations of SGD.• We observe that for consistency-based methods , SGD does not converge to a single point but continues to explore many solutions with high distances apart. Inspired by this observation, we propose to average the weights corresponding to SGD iterates, or ensemble the predictions of the models corresponding to these weights. Averaging weights of SGD iterates compensates for larger steps, stabilizes SGD trajectories and obtains a solution that is centered in a flat region of the loss (as a function of weights). Further, we show that the SGD iterates correspond to models with diverse predictions -using weight averaging or ensembling allows us to make use of the improved diversity and obtain a better solution compared to the SGD iterates. In Section 3.3 we demonstrate that both ensembling predictions and averaging weights of the networks corresponding to different training epochs significantly improve generalization performance and find that the improvement is much larger for the ⇧ and Mean Teacher models compared to supervised training. We find that averaging weights provides similar or improved accuracy compared to ensembling, while offering the computational benefits and convenience of working with a single model. Thus, we focus on weight averaging for the remainder of the paper.• Motivated by our observations in Section 3 we propose to apply Stochastic Weight Averaging (SWA) BID12 to the ⇧ and Mean Teacher models. Based on our results in Section 3.3 we propose several modifications to SWA in Section 4. In particular, we propose fast-SWA, which (1) uses a learning rate schedule with longer cycles to increase the distance between the weights that are averaged and the diversity of the corresponding predictions; and (2) averages weights of multiple networks within each cycle (while SWA only averages weights corresponding to the lowest values of the learning rate within each cycle). In Section 5, we show that fast-SWA converges to a good solution much faster than SWA.• Applying weight averaging to the ⇧ and Mean Teacher models we improve the best reported results on CIFAR-10 for 1k, 2k, 4k and 10k labeled examples, as well as on CIFAR-100 with 10k labeled examples. For example, we obtain 5.0% error on CIFAR-10 with only 4k labels, improving the best result reported in the literature BID31 ) by 1.3%. We also apply weight averaging to a state-of-the-art domain adaptation technique BID6 closely related to the Mean Teacher model and improve the best reported results on domain adaptation from CIFAR-10 to STL from 19.9% to 16.8% error.• We release our code at https://github.com/benathi/fastswa-semi-sup 2 BACKGROUND",Consistency-based models for semi-supervised learning do not converge to a single point but continue to explore a diverse set of plausible solutions on the perimeter of a flat region. Weight averaging helps improve generalization performance.,SGD ; Mean ; CIFAR ; Mean Teacher ; Jacobian ; STL ; error.•,particular:• ; large steps ; error.• ; multiple networks ; which ; unlabeled data ; the best semi-supervised results ; deep unsupervised learning ; convergence ; The error curves,SGD ; Mean ; CIFAR ; Mean Teacher ; Jacobian ; STL ; error.•,"The most successful semi-supervised learning approaches are based on consistency regularization, where a model is trained to be robust to small perturbations of its inputs and parameters, and the consistency loss dramatically improves generalization performance over supervised-only training. However, the inconsistency loss persists, leading to significant changes in predictions on test data. Stochastic Weight Averaging (SWA) is a recent approach which averages weights along the trajectory of SGD with a modified learning rate schedule, and fast-SWA, which further accelerates convergence by averaging multiple points within each cycle of a cyclical learning schedule. With weight averaging",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"The incorporation of prior knowledge into learning is essential in achieving good performance based on small noisy samples. Such knowledge is often incorporated through the availability of related data arising from domains and tasks similar to the one of current interest. Ideally one would like to allow both the data for the current task and for previous related tasks to self-organize the learning system in such a way that commonalities and differences between the tasks are learned in a data-driven fashion. We develop a framework for learning multiple tasks simultaneously, based on sharing features that are common to all tasks, achieved through the use of a modular deep feedforward neural network consisting of shared branches, dealing with the common features of all tasks, and private branches, learning the specific unique aspects of each task. Once an appropriate weight sharing architecture has been established, learning takes place through standard algorithms for feedforward networks, e.g., stochastic gradient descent and its variations. The method deals with meta-learning (such as domain adaptation, transfer and multi-task learning) in a unified fashion, and can easily deal with data arising from different types of sources. Numerical experiments demonstrate the effectiveness of learning in domain adaptation and transfer learning setups, and provide evidence for the flexible and task-oriented representations arising in the network. A major goal of inductive learning is the selection of a rule that generalizes well based on a finite set of examples. It is well-known ( BID11 ) that inductive learning is impossible unless some regularity assumptions are made about the world. Such assumptions, by their nature, go beyond the data, and are based on prior knowledge achieved through previous interactions with 'similar' problems. Following its early origins ( BID1 BID19 ), the incorporation of prior knowledge into learning has become a major effort recently, and is gaining increasing success by relying on the rich representational flexibility available through current deep learning schemes BID3 . Various aspects of prior knowledge are captured in different settings of meta-learning, such as learning-to-learn, domain adaptation, transfer learning, multi-task learning, etc. (e.g., ). In this work, we consider the setup of multi-task learning, first formalized in BID1 , where a set of tasks is available for learning, and the objective is to extract knowledge from a subset of tasks in order to facilitate learning of other, related, tasks. Within the framework of representation learning, the core idea is that of shared representations, allowing a given task to benefit from what has been learned from other tasks, since the shared aspects of the representation are based on more information BID24 .We consider both unsupervised and semi-supervised learning setups. In the former setting we have several related datasets, arising from possibly different domains, and aim to compress each dataset based on features that are shared between the datasets, and on features that are unique to each problem. Neither the shared nor the individual features are given apriori, but are learned using a deep neural network architecture within an autoencoding scheme. While such a joint representation could, in principle, serve as a basis for supervised learning, it has become increasingly evident that representations should contain some information about the output (label) identity in order to perform well, and that using pre-training based on unlabeled data is not always advantageous (e.g., chap. 15 in ). However, since unlabeled data is far more abundant than labeled data, much useful information can be gained from it. We therefore propose a joint encoding-classification scheme where both labeled and unlabeled data are used for the multiple tasks, so that internal representations found reflect both types of data, but are learned simultaneously.The main contributions of this work are: (i) A generic and flexible modular setup for combining unsupervised, supervised and transfer learning. (ii) Efficient end-to-end transfer learning using mostly unsupervised data (i.e., very few labeled examples are required for successful transfer learning). (iii) Explicit extraction of task-specific and shared representations. We presented a general scheme for incorporating prior knowledge within deep feedforward neural networks for domain adaptation, multi-task and transfer learning problems. The approach is general and flexible, operates in an end-to-end setting, and enables the system to self-organize to solve tasks based on prior or concomitant exposure to similar tasks, requiring standard gradient based optimization for learning. The basic idea of the approach is the sharing of representations for aspects which are common to all domains/tasks while maintaining private branches for task-specific features. The method is applicable to data from multiple sources and types, and has the advantage of being able to share weights at arbitrary network levels, enabling abstract levels of sharing.We demonstrated the efficacy of our approach on several domain adaptation and transfer learning problems, and provided intuition about the meaning of the representations in various branches. In a broader context, it is well known that the imposition of structural constraints on neural networks, usually based on prior domain knowledge, can significantly enhance their performance. The prime example of this is, of course, the convolutional neural network. Our work can be viewed within that general philosophy, showing that improved functionality can be attained by the modular prior structures imposed on the system, while maintaining simple learning rules.",A generic framework for handling transfer and multi-task learning using pairs of autoencoders with task-specific and shared weights.,first,sources ; The prime example ; the common features ; a data-driven fashion ; some regularity assumptions ; the core idea ; the availability ; standard algorithms ; training ; intuition,first,"The incorporation of prior knowledge into learning is essential in achieving good performance based on small noisy samples. This knowledge is often incorporated through related data from domains and tasks similar to the one of current interest. The goal of inductive learning is to allow both the current task and previous related tasks to self-organize the learning system in such a way that commonalities and differences between tasks are learned in a data-driven fashion. A modular deep feedforward neural network consisting of shared branches, dealing with common features of all tasks, and private branches, learning the specific unique aspects of each task. Once an appropriate weight sharing architecture has been established",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivalent to finding an optimal control of the discrete dynamical system, which allows one to utilize methods of successive approximations, an optimal control algorithm based on Pontryagin's maximum principle, to train neural nets. This decoupled training method allows us to add constraints to the optimization, which makes the deep model more robust. The constrained optimization problem can be formulated as a semi-definite programming problem and hence can be solved efficiently. Experiments show that our method effectively improves deep model's adversarial robustness. Deep neural networks achieve state-of-the-art performances on a variety of tasks (LeCun et al., 2015) . However, neural nets are known to be vulnerable to adversarial examples. Imperceptibly perturbed inputs can induce erroneous outputs in neural nets (Szegedy et al., 2013) . In image classification problems of computer vision, previous work has proposed various methods to attack deep models and induce low accuracy (Goodfellow et al., 2015; Madry et al., 2017; Papernot et al., 2016a; Carlini & Wagner, 2017a) . Whereas multiple defenses against adversarial attacks are developed, they don't ensure safety faced with strong attacking methods. There are also theories that explain the existence of adversarial examples (Ilyas et al., 2019; Shamir et al., 2019) , but they often fail to fully explain the features and behaviors of this phenomenon. This makes the study of adversarial attacks important in that it is a threat to real-life machine learning systems (Kurakin et al., 2016) . In this paper, we propose a dynamical system view on the adversarial robustness of the models, as well as new method that significantly defense adversarial attacks. Recent works have shown the connection between deep neural networks and dynamical systems (E, 2017; Haber & Ruthotto, 2017; Lu et al., 2017) . If we regard the neural net as a discretization of an ordinary differential equation (ODE), then training neural nets becomes finding an optimal control of the corresponding discrete dynamical system. Traditionally, we often treat training neural networks as an unconstrained non-convex optimization problem where θ denotes the parameters of the model, J denotes the loss function and R denotes the regularizer term, and we solve the problem with (stochastic) gradient-descent based methods (Bottou, 2010; Ruder, 2016) . In the training process, we feed the network with a batch of training data, and compute the gradient with forward and backward propagation (E. Rumelhart et al., 1986) . The propagation process resembles solving optimal control problems that tune the parameters to make the output be close to target states. This viewpoint motivates us to bridge adversarial robustness with Lyapunov stability of a dynamical system, and to train robust networks with algorithms that find stable optimal control. We will formulate the discussion in later sections. 2 RELATED WORK 2.1 ADVERSARIAL DEFENSE Many defense methods have been proposed to improve the models' adversarial robustness. The defenses mainly fall into three types: adversarial training (Szegedy et al., 2013; Zhang et al., 2019) , modifying the networks (Gu & Rigazio, 2015; Lyu et al., 2015; Papernot et al., 2016b; Nayebi & Ganguli, 2017; Ross & Doshi-Velez, 2017) , and adding external models (Lee et al., 2017; Akhtar et al., 2017; Gebhart & Schrater, 2017; Xu et al., 2018; Sun et al., 2019) . Although various defense methods have been developed, a defended deep model is often successfully attacked by newly developed attacks or specific counter-counter measures (Carlini & Wagner, 2017b) . Therefore, it can be hoped that defenses against general attacks will be devised to make deep learning models (adversarially) robust to real-life threats. Motivated by the dynamical system view of neural networks, this work bridges adversarial robustness of deep neural models with Lyapunov stability of dynamical systems, and we also propose a method that uses a stable optimal control algorithm to train neural networks to improve the adversarial robustness of deep neural models. Though the result didn't surpass STOA defense methods, the stable control view of training neural nets points out another direction towards adversarially robust models. For future work, on the one hand, mathematical analysis on Lyapunov stability of neural models may be studied to provide theoretical understanding of adversarial robustness. On the other hand, popular platforms for deep learning, e.g., TensorFlow, PyTorch, didn't provide frameworks for optimal control. We will obtain better results if specific algorithms for SDP are applied to solve the optimization problem.",An adversarial defense method bridging robustness of deep neural nets with Lyapunov stability,Ruder ; al. ; Lyapunov ; Lee et al. ; Goodfellow et al. ; Shamir ; PyTorch ; Papernot ; Madry ; Zhang et al.,the deep model ; mathematical analysis ; we ; Akhtar ; computer vision ; Recent works ; Carlini & Wagner ; this viewpoint ; optimal control problems ; deep neural models,Ruder ; al. ; Lyapunov ; Lee et al. ; Goodfellow et al. ; Shamir ; PyTorch ; Papernot ; Madry ; Zhang et al.,"Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. This approach is equivalent to finding an optimal control of the discrete dynamical system. The decoupled training method allows us to add constraints to the optimization, which makes the deep model more robust. The constrained optimization problem can be formulated as a semi-definite programming problem and hence can be solved efficiently. Experiments show that our method effectively improves deep model's adversarial performance. Deep neural networks achieve state-of-the-art",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"Heuristic search research often deals with finding algorithms for offline planning which aim to minimize the number of expanded nodes or planning time. In online planning, algorithms for real-time search or deadline-aware search have been considered before. However, in this paper, we are interested in the problem of {\em situated temporal planning} in which an agent's plan can depend on exogenous events in the external world, and thus it becomes important to take the passage of time into account during the planning process.  
 Previous work on situated temporal planning has proposed simple pruning strategies, as well as complex schemes for a simplified version of the associated metareasoning problem. 
 In this paper, we propose a simple metareasoning technique,  called the crude greedy scheme, which can be applied in a situated temporal planner. Our empirical evaluation shows that the crude greedy scheme outperforms standard heuristic search based on cost-to-go estimates. For many years, research in heuristic search has focused on the objective of minimizing the number of nodes expanded during search (e.g BID7 ). While this is the right objective under various scenarios, there are various scenarios where it is not. For example, if we still want an optimal plan but want to minimize search time, selective max BID10 or Rational Lazy A˚ ) can be used. Other work has dealt with finding a boundedly suboptimal plan as quickly as possible BID20 , or with finding any solution as quickly as possible BID21 . Departing from this paradigm even more, in motion planning the setting is that edge-cost evaluations are the most expensive operation, requiring different search algorithms BID14 BID11 .While the settings and objectives mentioned above are quite different from each other, they are all forms of offline planning. Addressing online planning raises a new set of objectives and scenarios. For example , in real-time search, an agent must interleave planning and execution, requiring still different search algorithms BID13 BID18 BID6 BID5 . Deadline-aware search BID9 must find a plan within some deadline. The BUGSY planner BID1 attempts to optimize the utility of a plan, which depends on both plan quality and search time.In this paper we are concerned with a recent setting, called situated temporal planning BID2 . Situated temporal planning addresses a problem where planning happens online, in the presence of external temporal constraints such as deadlines. In situated temporal planning, a plan must be found quickly enough that it is possible to execute that plan after planning completes. Situated temporal planning is inspired by the planning problem a robot faces when it has to replan BID3 , but the problem statement is independent of this motivation.The first planner to address situated temporal planning BID2 ) used temporal reasoning BID8 prune search nodes for which it is provably too late to start execution. It also used estimates of remaining search time BID9 together with information from the temporal relaxed planning graph BID4 ) to estimate whether a given search node is likely to be timely, meaning that it is likely to lead to a solution which will be executable when planning finishes. It also used dual open lists : one only for timely nodes, and another one for all nodes (including nodes for which it is likely too late to start execution). However, the planner still used standard heuristic search algorithms (GBFS or Weighted A˚) with these open lists, while noting that this is the wrong thing to do, and leaving for future work finding the right search control rules.Inspired by this problem, a recent paper BID19 proposed a rational metareasoning BID17 approach for a simplified version of the problem faced by the situated planner. The problem was simplified in several ways: first, the paper addressed a one-shot version of the metareasoning problem, and second, the paper assumed distributions on the remaining search time and on the deadline for each node are known. The paper then formulated the metareasoning problem as an MDP, with the objective of maximizing the probability of finding a timely plan, and showed that it is intractable. It also gave a greedy decision rule, which worked well in an empirical evaluation with various types of distributions.In this paper, we explore using such a metareasoning approach as an integrated part of a situated temporal planner.This involves addressing the two simplifications described above. The naive way of addressing the first simplification -the one-shot nature of the greedy rule -is to apply it at every expansion decision the underlying heuristic search algorithm makes, in order to choose which node from the open list to expand. The problem with this approach is that the number of nodes on the open list grows very quickly (typically exponentially), and so even a linear time metareasoning algorithm would incur too much overhead. Thus, we introduce an even simpler decision rule, which we call the crude greedy scheme, which does not require access to the distributions, but only to their estimated means. Additionally, the crude greedy scheme allows us to compute one number for each node,Q, and expand nodes with a highQvalue first. This allows us to use a regular open list, although one that is not sorted according to cost-to-go estimates, as in standard heuristic search. In fact, as we will see, cost-to-go estimates play no role in the ordering criterion at all.An empirical evaluation on a set of problems from the Robocup Logistics League (RCLL) domain BID16 BID15 shows that using the crude greedy scheme in the situated temporal planner BID2 leads to a timely solution of significantly more problems than using standard heuristic search, even with pruning late nodes and dual open lists. Next, we briefly survey the main results of the metareasoning paper BID19 , and then describe how we derive the crude greedy decision rule, and conclude with an empirical evaluation that demonstrates its efficacy. In this paper, we have provided the first practical metareasoning approach for situated temporal planning. We showed empirically that this approach outperforms standard heuristic search based on cost-to-go estimates. Nevertheless, the temporal relaxed planning graph BID4 ) serves an important purpose here, allowing us to estimate both remaining planning time and the deadline for a node. Thus, we believe our results suggest that cost-to-go estimates are not as important for situated temporal planning as they are for minimizing the number of expanded nodes or planning time as in classical heuristic search.The metareasoning scheme we provided is a crude version of the greedy scheme of BID19 . We introduced approximations in order to make the metareasoning sufficiently fast and in order to utilize only readily available information generated during the search. We also proposed a more refined and better theoretically justified version of the algorithm ('improved greedy'), but making the improved version applicable in the planner is a non-trivial challenge that forms part of our future research.Ongoing Work: Crude version of the improved greedy schemeThe improved greedy scheme is better justified, but has an additional term where we need the complete distribution (f 1 pt, t d q is needed, rather than just the expectation ErD i s).We would like to replace this distribution with a small number of parameters than can be easier to obtain. Basically the same considerations apply here as well, except that the the term involving f 1 i requires access to the full distributions m i , D i . Given specific distribution types, it may be possible to compute this term as a function of ErD i s and e i . However, this part of the work is still in progress and at present we are not sure what parameters we can obtain during the search that would support the improved scheme.",Metareasoning in a Situated Temporal Planner,ErD ; one ; many years ; MDP ; two ; first ; the Robocup Logistics League ; Weighted ; BUGSY ; Rational Lazy,both remaining planning time ; time ; the paper ; just the expectation ErD ; the improved version ; simple pruning strategies ; second ; the situated temporal planner ; the setting ; the objective,ErD ; one ; many years ; MDP ; two ; first ; the Robocup Logistics League ; Weighted ; BUGSY ; Rational Lazy,"Heuristic search research focuses on offline planning which aim to minimize the number of expanded nodes or planning time. In online planning, algorithms for real-time search or deadline-aware search have been considered before. However, in this paper, we focus on situated temporal planning, where an agent's plan can depend on exogenous events in the external world and thus it becomes important to take the passage of time into account during the planning process. For many years, research in heuristic search has focused on minimizing nodes expanded during search, as well as complex schemes for a simplified version of metareasoning problem. The crude greedy scheme,",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
"We explore the role of multiplicative interaction as a unifying framework to describe a range of classical and modern neural network architectural motifs, such as gating, attention layers, hypernetworks, and dynamic convolutions amongst others.
 Multiplicative interaction layers as primitive operations have a long-established presence in the literature, though this often not emphasized and thus under-appreciated. We begin by showing that such layers strictly enrich the representable function classes of neural networks. We conjecture that multiplicative interactions offer a particularly powerful inductive bias when fusing multiple streams of information or when conditional computation is required. We therefore argue that they should be considered in many situation where multiple compute or information paths need to be combined, in place of the simple and oft-used concatenation operation. Finally, we back up our claims and demonstrate the potential of multiplicative interactions by applying them in large-scale complex RL and sequence modelling tasks, where their use allows us to deliver state-of-the-art results, and thereby provides new evidence in support of multiplicative interactions playing a more prominent role when designing new neural network architectures. Much attention has recently turned toward the design of custom neural network architectures and components in order to increase efficiency, maximise performance, or otherwise introduce desirable inductive biases. While there have been a plethora of newer, intricate architectures proposed, in this work we train our sights instead on an older staple of the deep learning toolkit: multiplicative interactions. Although the term itself has fallen somewhat out of favour, multiplicative interactions have reappeared in a range of modern architectural designs. We start this work by considering multiplicative interactions as an object of study in their own right. We describe various formulations and how they relate to each other as well as connect more recent architectural developments (e.g. hypernetworks Ha et al. (2017) , dynamic convolutions Wu et al. (2019) ) to the rich and longer-standing literature on multiplicative interactions. We hypothesise that multiplicative interactions are suitable for representing certain meaningful classes of functions needed to build algorithmic operations such as conditional statements or similarity metrics, and more generally as an effective way of integrating contextual information in a network in a way that generalizes effectively. We show this empirically in controlled synthetic scenarios, and also demonstrate significant performance improvement on a variety of challenging, large-scale reinforcement learning (RL) and sequence modelling tasks when a conceptually simple multiplicative interaction module is incorporated. Such improvements are consistent with our hypothesis that the use of appropriately applied multiplicative interactions can provide a more suitable inductive bias over function classes leading to more data-efficient learning, better generalization, and stronger performance. We argue that these operations should feature more widely in neural networks in and of themselves, especially in the increasingly important setting of integrating multiple streams of information (including endogenously created streams e.g. in branching architectures). Our contributions are thus: (i) to re-explore multiplicative interactions and their design principles; (ii) to aid the community's understanding of other models (hypernetworks, gating, multiplicative RNNs) through them; (iii) to show their efficacy at representing certain solutions; and (iv) to empirically apply them to large scale sequence modeling and reinforcement learning problems, where we demonstrate state-of-the-art results. In this work we considered multiplicative interactions and various formulations thereof, connecting them to a variety of architectures, both older and modern, such as Hypernetworks, multplicative LSTMs or gating methods. We hypothesise that the ability of such networks to better represent a broader range of algorithmic primitives (e.g. conditional-statements or inner products) allows them to better integrate contextual or task-conditional information to fuse multiple stream of data. We first tested empirically this hypothesis in two controlled settings, in order to minimize the effect of confounding factors. We further show that we could match state-of-the-art methods on multiple domains with only LSTMs and multiplicative units. While we do not necessarily advocate for a specific instance of the above methods, we hope that this work leads to a broader understanding and consideration of such methods by practitioners, and in some cases replacing the standard practice of concatenation when using conditioning, contextual inputs, or additional sources of information. We believe there are many ways to explore this space of ideas more broadly, for instance looking at: the role of various approximations to these methods; ways to make their implementations more efficient; and their application to newer domains. Finally, while attention models use some of these multiplicative interactions, we hope that applying some of the lessons from this work (such as higher order interactions) will allow even greater integration of information in attention systems. Proof. Inclusion comes directly from the fact that if we split input into arbitrary parts [x; z] we get: which proves that H mlp ⊂ H mu . Thus, the only aspect of the theorm left to prove is that the inclusion is strict. Let us consider a 1D function x → x 2 , and for simplicity let x = z (a domain where context equals input). A single layer MLP with a single multiplicative unit can represent this function exactly, by using A = 0 and W = I, as then we obtain x T W x = x T x = x 2 . Since our function is positive, it is not affecting the multiplicative network output. For a regular MLP, let us first notice that we need at least one hidden layer, as otherwise MLP is a linear function and f is not. Lets denote by V, c and w, b weights and biases of second, and first layers respectively. Then we have to satisfy where g is transformation represented by all higher layers of the MLP (in particular if there are just 2 layers, then g(x) = x). Note that RHS is differentiable everywhere, while LHS is differentiable iff for each i and for each x we have w i x + b i = 0 (or f is independent from x, which x 2 does not satisfy). However, this is impossible, as if w i = 0, then we can always pick x = −b i /w i , and if all 2 , leading to a contradiction. Proof. Inclusion comes directly from the fact that only some activations are replaced, and in particular we can always replace none, thus leading to equality of hypotheses classes. To show that the inclusion is strict lets consider a Weierstrass function itself f (x) = σ w (x). We definitely have f ∈ H w as we can define 1 hidden layer network, with one hidden neuron and all the weights set to 1, and all biases to 0. Now, relu networks are piece-wise linear while the Weierstrass function is nowhere differentiable Weierstrass (1895) and thus not piece-wise linear. Similarly, network with an activation that is differentiable everywhere (e.g. sigmoid or tanh) is everywhere differentiable wrt. inputs, while Weierstrass function -nowhere Weierstrass (1895).","We explore the role of multiplicative interaction as a unifying framework to describe a range of classical and modern neural network architectural motifs, such as gating, attention layers, hypernetworks, and dynamic convolutions amongst others.",one ; LHS ; z ; first ; Hypernetworks ; second ; RL ; Wu et al. ; at least one ; RHS,attention systems ; multiplicative interactions ; multiplicative RNNs ; a linear function ; context ; they ; first ; ways ; this work ; the-art,one ; LHS ; z ; first ; Hypernetworks ; second ; RL ; Wu et al. ; at least one ; RHS,"We explore the role of multiplicative interaction as a unifying framework to describe classical and modern neural network architectural motifs, such as gating, attention layers, hypernetworks, and dynamic convolutions. We show that such layers strictly enrich representable function classes, while they offer powerful inductive bias when fusing multiple streams of information or when conditional computation is required. We argue that multiplicative interactions should be considered in many situations where multiple compute or information paths need to be combined, in place of the simple and oft-used concatenation operation. Finally, we back up our claims and demonstrate their potential by applying them in",/data/private/Advanded_Image_Analysis/Lab_4_and_5/Huzaifa/Models/bart-base_cs_KW,kw
