doc_id,question,cand_answer,qa_answer,f1,f1_ge_05,failure_reason,qg_model,qa_model,summary_sentence,source_excerpt,summary_len,source_len
0,False,a method,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,word2net is a method that replaces linear parametrization with neural networks.,"Word embeddings extract semantic features of words from large datasets of text.
 Most embedding methods rely on a log-bilinear model to predict the occurrence
 of a word in a context of other words. Here we propose word2net, a method that
 ",257,1204
0,True,that,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,word2net is a method that replaces linear parametrization with neural networks.,"Word embeddings extract semantic features of words from large datasets of text.
 Most embedding methods rely on a log-bilinear model to predict the occurrence
 of a word in a context of other words. Here we propose word2net, a method that
 ",257,1204
0,False,linear parametrization,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,word2net is a method that replaces linear parametrization with neural networks.,"Word embeddings extract semantic features of words from large datasets of text.
 Most embedding methods rely on a log-bilinear model to predict the occurrence
 of a word in a context of other words. Here we propose word2net, a method that
 ",257,1204
0,True,neural networks,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,word2net is a method that replaces linear parametrization with neural networks.,"Word embeddings extract semantic features of words from large datasets of text.
 Most embedding methods rely on a log-bilinear model to predict the occurrence
 of a word in a context of other words. Here we propose word2net, a method that
 ",257,1204
0,True,additional meta-data,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"word2net can incorporate additional meta-data, such as syntactic features, into the embedding model.","Word embeddings extract semantic features of words from large datasets of text.
 Most embedding methods rely on a log-bilinear model to predict the occurrence
 of a word in a context of other words. Here we propose word2net, a method that
 ",257,1204
0,True,syntactic features,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"word2net can incorporate additional meta-data, such as syntactic features, into the embedding model.","Word embeddings extract semantic features of words from large datasets of text.
 Most embedding methods rely on a log-bilinear model to predict the occurrence
 of a word in a context of other words. Here we propose word2net, a method that
 ",257,1204
0,True,the embedding model,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"word2net can incorporate additional meta-data, such as syntactic features, into the embedding model.","Word embeddings extract semantic features of words from large datasets of text.
 Most embedding methods rely on a log-bilinear model to predict the occurrence
 of a word in a context of other words. Here we propose word2net, a method that
 ",257,1204
0,False,popular embedding methods,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,word2net outperforms popular embedding methods on predicting held-out words.,"Word embeddings extract semantic features of words from large datasets of text.
 Most embedding methods rely on a log-bilinear model to predict the occurrence
 of a word in a context of other words. Here we propose word2net, a method that
 ",257,1204
1,True,natural language,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,natural language has a sequential overt form as spoken and written.,"Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. Whil",214,6762
1,True,a sequential overt form,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,natural language has a sequential overt form as spoken and written.,"Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. Whil",214,6762
1,True,the underlying structure,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,the underlying structure of language is not strictly sequential.,"Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. Whil",214,6762
1,True,language,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,the underlying structure of language is not strictly sequential.,"Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. Whil",214,6762
1,True,neurons LSTM,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,ordered neurons LSTM (ON-LSTM) achieves good performance on four different tasks.,"Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. Whil",214,6762
1,True,ON-LSTM,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,ordered neurons LSTM (ON-LSTM) achieves good performance on four different tasks.,"Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. Whil",214,6762
1,True,good performance,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,ordered neurons LSTM (ON-LSTM) achieves good performance on four different tasks.,"Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. Whil",214,6762
1,True,four different tasks,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,ordered neurons LSTM (ON-LSTM) achieves good performance on four different tasks.,"Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. Whil",214,6762
2,True,this paper,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,in this paper we develop a method for explaining the mistakes of a classifier model.,Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user ,229,4956
2,True,we,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,in this paper we develop a method for explaining the mistakes of a classifier model.,Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user ,229,4956
2,True,a method,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,in this paper we develop a method for explaining the mistakes of a classifier model.,Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user ,229,4956
2,True,the mistakes,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,in this paper we develop a method for explaining the mistakes of a classifier model.,Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user ,229,4956
2,True,a classifier model,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,in this paper we develop a method for explaining the mistakes of a classifier model.,Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user ,229,4956
2,True,what,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,we show what must be added to an image such that it is correctly classified.,Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user ,229,4956
2,True,an image,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,we show what must be added to an image such that it is correctly classified.,Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user ,229,4956
2,True,it,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,we show what must be added to an image such that it is correctly classified.,Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user ,229,4956
3,True,NLP,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,a lot of recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner.,A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general ,305,6814
3,True,a lot,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,a lot of recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner.,A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general ,305,6814
3,True,recent success,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,a lot of recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner.,A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general ,305,6814
3,True,natural language processing,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,a lot of recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner.,A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general ,305,6814
3,True,distributed vector representations,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,a lot of recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner.,A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general ,305,6814
3,True,words,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,a lot of recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner.,A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general ,305,6814
3,True,large amounts,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,a lot of recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner.,A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general ,305,6814
3,True,text,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,a lot of recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner.,A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general ,305,6814
4,True,Lyapunov,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"in this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems.","Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivale",323,4938
4,True,this paper,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"in this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems.","Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivale",323,4938
4,True,we,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"in this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems.","Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivale",323,4938
4,True,adversarial robustness,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"in this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems.","Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivale",323,4938
4,True,neural nets,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"in this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems.","Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivale",323,4938
4,True,Lyapunov stability,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"in this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems.","Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivale",323,4938
4,True,dynamical systems,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"in this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems.","Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivale",323,4938
4,True,this viewpoint,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"from this viewpoint, training neural nets is equivalent to finding an optimal control of the discrete dynamical system.","Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivale",323,4938
5,False,training,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available.,"Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining. Training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while the unsup",299,6698
5,False,aspect extraction,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available.,"Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining. Training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while the unsup",299,6698
5,False,ground truth aspect labels,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available.,"Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining. Training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while the unsup",299,6698
5,False,this work,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"in this work, we propose a weakly supervised approach for aspect extraction.","Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining. Training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while the unsup",299,6698
5,True,we,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"in this work, we propose a weakly supervised approach for aspect extraction.","Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining. Training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while the unsup",299,6698
5,True,a weakly supervised approach,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"in this work, we propose a weakly supervised approach for aspect extraction.","Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining. Training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while the unsup",299,6698
5,True,regularization,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,we show that regularization encourages the student to consider non-seed words for classification.,"Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining. Training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while the unsup",299,6698
5,False,the student,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,we show that regularization encourages the student to consider non-seed words for classification.,"Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining. Training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while the unsup",299,6698
6,True,Graph Convolutional Networks,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Graph Convolutional Networks (GCNs) have been shown to be quite successful in modeling graph-structured data.,"Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and ",263,5408
6,True,GCNs,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Graph Convolutional Networks (GCNs) have been shown to be quite successful in modeling graph-structured data.,"Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and ",263,5408
6,True,graph-structured data,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Graph Convolutional Networks (GCNs) have been shown to be quite successful in modeling graph-structured data.,"Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and ",263,5408
6,True,the primary focus,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,the primary focus has been on handling simple undirected graphs.,"Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and ",263,5408
6,True,simple undirected graphs,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,the primary focus has been on handling simple undirected graphs.,"Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and ",263,5408
6,False,the existing approaches,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,most of the existing approaches to handle such graphs suffer from over-parameterization.,"Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and ",263,5408
6,False,such graphs,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,most of the existing approaches to handle such graphs suffer from over-parameterization.,"Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and ",263,5408
6,True,over-parameterization,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,most of the existing approaches to handle such graphs suffer from over-parameterization.,"Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and ",263,5408
7,True,BERT,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"to accelerate future research, we will make our 24 pre-trained miniature BERT models publicly available.",Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to d,263,5785
7,True,authors,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,authors show that pre-training remains important in the context of smaller architectures.,Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to d,263,5785
7,True,pre,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,authors show that pre-training remains important in the context of smaller architectures.,Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to d,263,5785
7,True,training,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,authors show that pre-training remains important in the context of smaller architectures.,Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to d,263,5785
7,False,the context,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,authors show that pre-training remains important in the context of smaller architectures.,Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to d,263,5785
7,True,smaller architectures,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,authors show that pre-training remains important in the context of smaller architectures.,Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to d,263,5785
7,True,we,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,we explore transferring task knowledge from large fine-tuned models.,Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to d,263,5785
7,True,task knowledge,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,we explore transferring task knowledge from large fine-tuned models.,Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to d,263,5785
8,True,IP,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,analysis of deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently.,"Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate ",389,5918
8,True,analysis,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,analysis of deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently.,"Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate ",389,5918
8,True,deep neural networks,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,analysis of deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently.,"Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate ",389,5918
8,True,DNNs,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,analysis of deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently.,"Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate ",389,5918
8,True,information plane (IP) theory,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,analysis of deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently.,"Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate ",389,5918
8,True,tremendous attention,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,analysis of deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently.,"Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate ",389,5918
8,False,it,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"it is by no means obvious how to estimate mutual information (MI) between each hidden layer and the input/desired output, to construct the IP.","Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate ",389,5918
8,False,no means,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"it is by no means obvious how to estimate mutual information (MI) between each hidden layer and the input/desired output, to construct the IP.","Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate ",389,5918
9,True,we,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,we consider reinforcement learning in input-driven environments.,"We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with d",219,10274
9,True,reinforcement learning,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,we consider reinforcement learning in input-driven environments.,"We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with d",219,10274
9,True,input-driven environments,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,we consider reinforcement learning in input-driven environments.,"We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with d",219,10274
9,True,input processes,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"input processes arise in many applications, including queuing systems and robots.","We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with d",219,10274
9,True,many applications,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"input processes arise in many applications, including queuing systems and robots.","We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with d",219,10274
9,True,systems,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"input processes arise in many applications, including queuing systems and robots.","We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with d",219,10274
9,True,robots,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"input processes arise in many applications, including queuing systems and robots.","We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with d",219,10274
9,True,"a bias-free, input-dependent baseline",,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"we derive a bias-free, input-dependent baseline to reduce this variance.","We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with d",219,10274
