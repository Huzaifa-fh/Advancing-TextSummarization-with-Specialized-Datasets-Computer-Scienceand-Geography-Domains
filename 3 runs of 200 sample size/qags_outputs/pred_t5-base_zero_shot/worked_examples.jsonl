{"type": "failure", "doc_id": 0, "question": "False", "candidate_answer_from_summary": "a method", "qa_answer_from_source": "", "squad_f1": 0.0, "failure_reason": "empty_qa_answer", "qg_model": "iarfmoose/t5-base-question-generator", "qa_model": "deepset/roberta-base-squad2", "summary_sentence_used_for_qg": "word2net is a method that replaces linear parametrization with neural networks.", "source_excerpt_around_qa_answer": "Word embeddings extract semantic features of words from large datasets of text.\n Most embedding methods rely on a log-bilinear model to predict the occurrence\n of a word in a context of other words. Here we propose word2net, a method that\n "}
{"type": "failure", "doc_id": 1, "question": "True", "candidate_answer_from_summary": "natural language", "qa_answer_from_source": "", "squad_f1": 0.0, "failure_reason": "empty_qa_answer", "qg_model": "iarfmoose/t5-base-question-generator", "qa_model": "deepset/roberta-base-squad2", "summary_sentence_used_for_qg": "natural language has a sequential overt form as spoken and written.", "source_excerpt_around_qa_answer": "Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. Whil"}
{"type": "failure", "doc_id": 2, "question": "True", "candidate_answer_from_summary": "this paper", "qa_answer_from_source": "", "squad_f1": 0.0, "failure_reason": "empty_qa_answer", "qg_model": "iarfmoose/t5-base-question-generator", "qa_model": "deepset/roberta-base-squad2", "summary_sentence_used_for_qg": "in this paper we develop a method for explaining the mistakes of a classifier model.", "source_excerpt_around_qa_answer": "Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user "}
