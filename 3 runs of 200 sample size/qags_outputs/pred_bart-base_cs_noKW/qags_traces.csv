doc_id,question,cand_answer,qa_answer,f1,f1_ge_05,failure_reason,qg_model,qa_model,summary_sentence,source_excerpt,summary_len,source_len
0,True,linear,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Word2net, a method that combines linear parametrization with neural networks, employs hierarchical organization of word networks to incorporate additional meta-data into the embedding model.","Word embeddings extract semantic features of words from large datasets of text.
 Most embedding methods rely on a log-bilinear model to predict the occurrence
 of a word in a context of other words. Here we propose word2net, a method that
 ",377,1204
0,True,Word embeddings,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Word embeddings extract semantic features from large datasets of text.,"Word embeddings extract semantic features of words from large datasets of text.
 Most embedding methods rely on a log-bilinear model to predict the occurrence
 of a word in a context of other words. Here we propose word2net, a method that
 ",377,1204
0,True,semantic features,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Word embeddings extract semantic features from large datasets of text.,"Word embeddings extract semantic features of words from large datasets of text.
 Most embedding methods rely on a log-bilinear model to predict the occurrence
 of a word in a context of other words. Here we propose word2net, a method that
 ",377,1204
0,True,large datasets,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Word embeddings extract semantic features from large datasets of text.,"Word embeddings extract semantic features of words from large datasets of text.
 Most embedding methods rely on a log-bilinear model to predict the occurrence
 of a word in a context of other words. Here we propose word2net, a method that
 ",377,1204
0,True,text,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Word embeddings extract semantic features from large datasets of text.,"Word embeddings extract semantic features of words from large datasets of text.
 Most embedding methods rely on a log-bilinear model to predict the occurrence
 of a word in a context of other words. Here we propose word2net, a method that
 ",377,1204
0,True,Most embedding methods,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Most embedding methods rely on log-bilinear models to predict the occurrence of a word in a context of other words.,"Word embeddings extract semantic features of words from large datasets of text.
 Most embedding methods rely on a log-bilinear model to predict the occurrence
 of a word in a context of other words. Here we propose word2net, a method that
 ",377,1204
0,True,log-bilinear models,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Most embedding methods rely on log-bilinear models to predict the occurrence of a word in a context of other words.,"Word embeddings extract semantic features of words from large datasets of text.
 Most embedding methods rely on a log-bilinear model to predict the occurrence
 of a word in a context of other words. Here we propose word2net, a method that
 ",377,1204
0,True,the occurrence,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Most embedding methods rely on log-bilinear models to predict the occurrence of a word in a context of other words.,"Word embeddings extract semantic features of words from large datasets of text.
 Most embedding methods rely on a log-bilinear model to predict the occurrence
 of a word in a context of other words. Here we propose word2net, a method that
 ",377,1204
1,True,Natural language,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Natural language is hierarchically structured, with smaller units nested within larger units.","Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. Whil",679,6762
1,True,smaller units,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Natural language is hierarchically structured, with smaller units nested within larger units.","Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. Whil",679,6762
1,False,larger units,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Natural language is hierarchically structured, with smaller units nested within larger units.","Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. Whil",679,6762
1,True,a larger constituent,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"When a larger constituent ends, all of the smaller constituents that nested within it must also be closed.","Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. Whil",679,6762
1,True,all,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"When a larger constituent ends, all of the smaller constituents that nested within it must also be closed.","Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. Whil",679,6762
1,True,the smaller constituents,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"When a larger constituent ends, all of the smaller constituents that nested within it must also be closed.","Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. Whil",679,6762
1,"When a larger constituent ends, all of the smaller constituents that nested within it must also be closed",that,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"When a larger constituent ends, all of the smaller constituents that nested within it must also be closed.","Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. Whil",679,6762
1,True,it,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"When a larger constituent ends, all of the smaller constituents that nested within it must also be closed.","Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. Whil",679,6762
2,True,Gradient Descent,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"A method for explaining the mistakes of a classifier model by visually showing what must be added to an image such that it is correctly classified is called Gradient Descent on the input, which combines adversarial examples, generative modeling and a correction technique based on difference target propagation to create a technique that creates explanations of why an image is misclassified.",Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user ,681,4956
2,True,Neural networks,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Neural networks often are considered a black box.,Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user ,681,4956
2,True,a black box,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Neural networks often are considered a black box.,Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user ,681,4956
2,True,A method,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"A method for explaining the mistakes of a classifier model by visually showing what must be added to an image such that it is correctly classified is called Gradient Descent on the input, which combines adversarial examples, generative modeling and a correction technique based on difference target propagation to create a technique that creates explanations of why an image is misclassified.",Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user ,681,4956
2,True,the mistakes,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"A method for explaining the mistakes of a classifier model by visually showing what must be added to an image such that it is correctly classified is called Gradient Descent on the input, which combines adversarial examples, generative modeling and a correction technique based on difference target propagation to create a technique that creates explanations of why an image is misclassified.",Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user ,681,4956
2,True,a classifier model,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"A method for explaining the mistakes of a classifier model by visually showing what must be added to an image such that it is correctly classified is called Gradient Descent on the input, which combines adversarial examples, generative modeling and a correction technique based on difference target propagation to create a technique that creates explanations of why an image is misclassified.",Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user ,681,4956
2,Why does Gradient Descent on the input create a technique that creates explanations of why an image is misclassified?,what,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"A method for explaining the mistakes of a classifier model by visually showing what must be added to an image such that it is correctly classified is called Gradient Descent on the input, which combines adversarial examples, generative modeling and a correction technique based on difference target propagation to create a technique that creates explanations of why an image is misclassified.",Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user ,681,4956
2,True,an image,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"A method for explaining the mistakes of a classifier model by visually showing what must be added to an image such that it is correctly classified is called Gradient Descent on the input, which combines adversarial examples, generative modeling and a correction technique based on difference target propagation to create a technique that creates explanations of why an image is misclassified.",Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user ,681,4956
3,True,NLP,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in unsupervised manner.,A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general ,566,6814
3,True,Natural language processing,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in unsupervised manner.,A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general ,566,6814
3,True,distributed vector representations,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in unsupervised manner.,A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general ,566,6814
3,True,words,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in unsupervised manner.,A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general ,566,6814
3,True,large amounts,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in unsupervised manner.,A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general ,566,6814
3,True,text,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in unsupervised manner.,A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general ,566,6814
3,True,unsupervised manner,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in unsupervised manner.,A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general ,566,6814
3,True,These representations,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,These representations are used as general purpose features for words across a range of NLP problems.,A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general ,566,6814
4,True,Lyapunov,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Deep neural networks are vulnerable to adversarial perturbations due to their Lyapunov stability of dynamical systems.,"Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivale",573,4938
4,True,Pontryagin,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"In this paper, training neural nets is equivalent to finding an optimal control of the discrete dynamical system, which allows one to utilize methods of successive approximations, based on Pontryagin's maximum principle, to train neural nets.","Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivale",573,4938
4,True,Deep neural networks,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Deep neural networks are vulnerable to adversarial perturbations due to their Lyapunov stability of dynamical systems.,"Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivale",573,4938
4,True,adversarial perturbations,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Deep neural networks are vulnerable to adversarial perturbations due to their Lyapunov stability of dynamical systems.,"Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivale",573,4938
4,True,their Lyapunov stability,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Deep neural networks are vulnerable to adversarial perturbations due to their Lyapunov stability of dynamical systems.,"Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivale",573,4938
4,False,dynamical systems,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Deep neural networks are vulnerable to adversarial perturbations due to their Lyapunov stability of dynamical systems.,"Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivale",573,4938
4,True,this paper,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"In this paper, training neural nets is equivalent to finding an optimal control of the discrete dynamical system, which allows one to utilize methods of successive approximations, based on Pontryagin's maximum principle, to train neural nets.","Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivale",573,4938
4,True,neural nets,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"In this paper, training neural nets is equivalent to finding an optimal control of the discrete dynamical system, which allows one to utilize methods of successive approximations, based on Pontryagin's maximum principle, to train neural nets.","Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivale",573,4938
5,True,Pontiki,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Aspect extraction in online product reviews is a key task in sentiment analysis, opinion mining, and summarization BID11 BID8 Pontiki et al., 2016; BID0.","Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining. Training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while the unsup",642,6698
5,True,al.,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Aspect extraction in online product reviews is a key task in sentiment analysis, opinion mining, and summarization BID11 BID8 Pontiki et al., 2016; BID0.","Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining. Training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while the unsup",642,6698
5,True,Amazon,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Second, we demonstrate that a distillation approach for aspect extraction is outperformed by rule-based or traditional supervised learning approaches in six domains of Amazon product reviews, where the goal is to identify which features are discussed in individual segments of","Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining. Training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while the unsup",642,6698
5,True,extraction,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Aspect extraction in online product reviews is a key task in sentiment analysis, opinion mining, and summarization BID11 BID8 Pontiki et al., 2016; BID0.","Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining. Training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while the unsup",642,6698
5,True,online product reviews,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Aspect extraction in online product reviews is a key task in sentiment analysis, opinion mining, and summarization BID11 BID8 Pontiki et al., 2016; BID0.","Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining. Training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while the unsup",642,6698
5,True,a key task,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Aspect extraction in online product reviews is a key task in sentiment analysis, opinion mining, and summarization BID11 BID8 Pontiki et al., 2016; BID0.","Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining. Training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while the unsup",642,6698
5,True,sentiment analysis,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Aspect extraction in online product reviews is a key task in sentiment analysis, opinion mining, and summarization BID11 BID8 Pontiki et al., 2016; BID0.","Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining. Training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while the unsup",642,6698
5,True,opinion mining,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Aspect extraction in online product reviews is a key task in sentiment analysis, opinion mining, and summarization BID11 BID8 Pontiki et al., 2016; BID0.","Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining. Training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while the unsup",642,6698
6,True,Graph Convolutional Networks,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Graph Convolutional Networks (GCNs) have been successful in modeling graph-structured data.,"Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and ",655,5408
6,True,Knowledge Graph Embedding,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,CompGCN combines a variety of entity-relation composition operations from Knowledge Graph Embedding techniques and scales with the number of relations in a relational graph.,"Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and ",655,5408
6,False,Comp,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,The source code of Comp,"Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and ",655,5408
6,True,GCNs,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Graph Convolutional Networks (GCNs) have been successful in modeling graph-structured data.,"Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and ",655,5408
6,True,graph-structured data,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Graph Convolutional Networks (GCNs) have been successful in modeling graph-structured data.,"Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and ",655,5408
6,True,Multi-relational graphs,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Multi-relational graphs are a more general and prevalent form of graphs where each edge has a label and direction associated with it.,"Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and ",655,5408
6,True,a more general and prevalent form,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Multi-relational graphs are a more general and prevalent form of graphs where each edge has a label and direction associated with it.,"Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and ",655,5408
6,True,graphs,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Multi-relational graphs are a more general and prevalent form of graphs where each edge has a label and direction associated with it.,"Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and ",655,5408
7,True,NLP,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"However, this simple baseline has been overlooked by the NLP community due to the limited capacity of compact models, which can be capitalized better when focusing on the end task rather than a general language objective.",Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to d,671,5785
7,True,BERT,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"To accelerate future research, we will make our 24 pre-trained miniature BERT models publicly available.",Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to d,671,5785
7,True,Pre-trained language representations,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Pre-trained language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training and fine-tuning compact models.,Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to d,671,5785
7,True,large and expensive models,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Pre-trained language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training and fine-tuning compact models.,Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to d,671,5785
7,True,that,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Pre-trained language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training and fine-tuning compact models.,Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to d,671,5785
7,True,vast amounts,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Pre-trained language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training and fine-tuning compact models.,Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to d,671,5785
7,True,general-domain text,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Pre-trained language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training and fine-tuning compact models.,Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to d,671,5785
7,True,self-supervised pre,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Pre-trained language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training and fine-tuning compact models.,Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to d,671,5785
8,True,IP,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Deep neural networks (DNNs) via information plane (IP) theory have gained significant attention recently as a tool to gain insight into their generalization ability.,"Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate ",577,5918
8,True,Convolutional Neural Networks,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"The existing IP methods have not been able to study deep Convolutional Neural Networks (CNNs), such as the e.g.\ VGG-16.","Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate ",577,5918
8,True,Deep neural networks,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Deep neural networks (DNNs) via information plane (IP) theory have gained significant attention recently as a tool to gain insight into their generalization ability.,"Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate ",577,5918
8,True,DNNs,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Deep neural networks (DNNs) via information plane (IP) theory have gained significant attention recently as a tool to gain insight into their generalization ability.,"Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate ",577,5918
8,True,information plane (IP) theory,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Deep neural networks (DNNs) via information plane (IP) theory have gained significant attention recently as a tool to gain insight into their generalization ability.,"Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate ",577,5918
8,True,significant attention,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Deep neural networks (DNNs) via information plane (IP) theory have gained significant attention recently as a tool to gain insight into their generalization ability.,"Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate ",577,5918
8,True,a tool,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Deep neural networks (DNNs) via information plane (IP) theory have gained significant attention recently as a tool to gain insight into their generalization ability.,"Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate ",577,5918
8,True,insight,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Deep neural networks (DNNs) via information plane (IP) theory have gained significant attention recently as a tool to gain insight into their generalization ability.,"Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate ",577,5918
9,True,Reinforcement learning,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system.","We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with d",690,10274
9,True,input-driven environments,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system.","We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with d",690,10274
9,True,"an exogenous, stochastic input process",,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system.","We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with d",690,10274
9,True,the dynamics,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system.","We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with d",690,10274
9,False,the system,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system.","We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with d",690,10274
9,True,Input processes,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Input processes arise in many applications, including queuing systems, robotics control with disturbances, and object tracking.","We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with d",690,10274
9,True,many applications,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Input processes arise in many applications, including queuing systems, robotics control with disturbances, and object tracking.","We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with d",690,10274
9,True,queuing systems,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Input processes arise in many applications, including queuing systems, robotics control with disturbances, and object tracking.","We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with d",690,10274
