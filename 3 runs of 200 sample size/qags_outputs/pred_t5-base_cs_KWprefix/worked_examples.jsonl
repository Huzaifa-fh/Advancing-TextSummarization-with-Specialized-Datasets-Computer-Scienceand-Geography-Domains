{"type": "failure", "doc_id": 0, "question": "True", "candidate_answer_from_summary": "Word embeddings", "qa_answer_from_source": "", "squad_f1": 0.0, "failure_reason": "empty_qa_answer", "qg_model": "iarfmoose/t5-base-question-generator", "qa_model": "deepset/roberta-base-squad2", "summary_sentence_used_for_qg": "Word embeddings extract semantic features of words from large datasets of text.", "source_excerpt_around_qa_answer": "Word embeddings extract semantic features of words from large datasets of text.\n Most embedding methods rely on a log-bilinear model to predict the occurrence\n of a word in a context of other words. Here we propose word2net, a method that\n "}
{"type": "failure", "doc_id": 1, "question": "True", "candidate_answer_from_summary": "Natural language", "qa_answer_from_source": "", "squad_f1": 0.0, "failure_reason": "empty_qa_answer", "qg_model": "iarfmoose/t5-base-question-generator", "qa_model": "deepset/roberta-base-squad2", "summary_sentence_used_for_qg": "Natural language is hierarchically structured, with smaller units nested within larger units.", "source_excerpt_around_qa_answer": "Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. Whil"}
{"type": "failure", "doc_id": 2, "question": "False", "candidate_answer_from_summary": "Neural networks", "qa_answer_from_source": "", "squad_f1": 0.0, "failure_reason": "empty_qa_answer", "qg_model": "iarfmoose/t5-base-question-generator", "qa_model": "deepset/roberta-base-squad2", "summary_sentence_used_for_qg": "Neural networks make mistakes.", "source_excerpt_around_qa_answer": "Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user "}
