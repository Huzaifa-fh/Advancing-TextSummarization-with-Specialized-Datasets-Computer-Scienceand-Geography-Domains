doc_id,question,cand_answer,qa_answer,f1,f1_ge_05,failure_reason,qg_model,qa_model,summary_sentence,source_excerpt,summary_len,source_len
0,True,linear,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Word2net, a method that combines linear parametrization with neural networks, posits a neural network that takes context as input and outputs a probability of occurrence.","Word embeddings extract semantic features of words from large datasets of text.
 Most embedding methods rely on a log-bilinear model to predict the occurrence
 of a word in a context of other words. Here we propose word2net, a method that
 ",608,1204
0,True,Word embeddings,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Word embeddings extract semantic features from large datasets of text.,"Word embeddings extract semantic features of words from large datasets of text.
 Most embedding methods rely on a log-bilinear model to predict the occurrence
 of a word in a context of other words. Here we propose word2net, a method that
 ",608,1204
0,True,semantic features,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Word embeddings extract semantic features from large datasets of text.,"Word embeddings extract semantic features of words from large datasets of text.
 Most embedding methods rely on a log-bilinear model to predict the occurrence
 of a word in a context of other words. Here we propose word2net, a method that
 ",608,1204
0,True,large datasets,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Word embeddings extract semantic features from large datasets of text.,"Word embeddings extract semantic features of words from large datasets of text.
 Most embedding methods rely on a log-bilinear model to predict the occurrence
 of a word in a context of other words. Here we propose word2net, a method that
 ",608,1204
0,True,text,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Word embeddings extract semantic features from large datasets of text.,"Word embeddings extract semantic features of words from large datasets of text.
 Most embedding methods rely on a log-bilinear model to predict the occurrence
 of a word in a context of other words. Here we propose word2net, a method that
 ",608,1204
0,True,Most embedding methods,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Most embedding methods rely on a log-bilinear model to predict the occurrence of a word in a context of other words.,"Word embeddings extract semantic features of words from large datasets of text.
 Most embedding methods rely on a log-bilinear model to predict the occurrence
 of a word in a context of other words. Here we propose word2net, a method that
 ",608,1204
0,True,a log-bilinear model,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Most embedding methods rely on a log-bilinear model to predict the occurrence of a word in a context of other words.,"Word embeddings extract semantic features of words from large datasets of text.
 Most embedding methods rely on a log-bilinear model to predict the occurrence
 of a word in a context of other words. Here we propose word2net, a method that
 ",608,1204
0,True,the occurrence,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Most embedding methods rely on a log-bilinear model to predict the occurrence of a word in a context of other words.,"Word embeddings extract semantic features of words from large datasets of text.
 Most embedding methods rely on a log-bilinear model to predict the occurrence
 of a word in a context of other words. Here we propose word2net, a method that
 ",608,1204
1,False,un,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"The novel recurrent architecture achieves good performance on four different tasks: language modeling, un","Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. Whil",712,6762
1,True,Natural language,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Natural language is hierarchically structured, with smaller units nested within larger units.","Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. Whil",712,6762
1,True,smaller units,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Natural language is hierarchically structured, with smaller units nested within larger units.","Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. Whil",712,6762
1,False,larger units,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Natural language is hierarchically structured, with smaller units nested within larger units.","Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. Whil",712,6762
1,True,a larger constituent,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"When a larger constituent ends, all of the smaller constituents that nested within it must also be closed.","Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. Whil",712,6762
1,True,all,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"When a larger constituent ends, all of the smaller constituents that nested within it must also be closed.","Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. Whil",712,6762
1,True,the smaller constituents,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"When a larger constituent ends, all of the smaller constituents that nested within it must also be closed.","Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. Whil",712,6762
1,"When a larger constituent ends, all of the smaller constituents that nested within it must also be closed",that,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"When a larger constituent ends, all of the smaller constituents that nested within it must also be closed.","Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. Whil",712,6762
2,True,Gradient Descent,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,A method for explaining the mistakes of a classifier model by visually showing what must be added to an image such that it is correctly classified is called Gradient Descent on the input approach.,Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user ,667,4956
2,True,Neural networks,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Neural networks often are considered a black box due to their lack of explanation.,Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user ,667,4956
2,True,a black box,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Neural networks often are considered a black box due to their lack of explanation.,Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user ,667,4956
2,False,their lack,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Neural networks often are considered a black box due to their lack of explanation.,Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user ,667,4956
2,False,explanation,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Neural networks often are considered a black box due to their lack of explanation.,Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user ,667,4956
2,True,A method,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,A method for explaining the mistakes of a classifier model by visually showing what must be added to an image such that it is correctly classified is called Gradient Descent on the input approach.,Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user ,667,4956
2,True,the mistakes,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,A method for explaining the mistakes of a classifier model by visually showing what must be added to an image such that it is correctly classified is called Gradient Descent on the input approach.,Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user ,667,4956
2,True,a classifier model,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,A method for explaining the mistakes of a classifier model by visually showing what must be added to an image such that it is correctly classified is called Gradient Descent on the input approach.,Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user ,667,4956
3,True,NLP,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in unsupervised manner.,A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general ,747,6814
3,True,Natural language processing,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in unsupervised manner.,A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general ,747,6814
3,True,distributed vector representations,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in unsupervised manner.,A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general ,747,6814
3,True,words,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in unsupervised manner.,A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general ,747,6814
3,True,large amounts,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in unsupervised manner.,A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general ,747,6814
3,True,text,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in unsupervised manner.,A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general ,747,6814
3,True,unsupervised manner,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in unsupervised manner.,A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general ,747,6814
3,True,These representations,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,These representations are typically used as general purpose features for words across a range of NLP problems.,A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general ,747,6814
4,True,Lyapunov,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems.","Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivale",660,4938
4,True,Pontryagin,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"This approach is equivalent to finding an optimal control of the discrete dynamical system, which allows one to utilize successive approximations, based on Pontryagin's maximum principle, to train neural nets.","Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivale",660,4938
4,True,Deep neural networks,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Deep neural networks are known to be vulnerable to adversarial perturbations.,"Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivale",660,4938
4,True,adversarial perturbations,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Deep neural networks are known to be vulnerable to adversarial perturbations.,"Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivale",660,4938
4,True,this paper,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems.","Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivale",660,4938
4,True,we,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems.","Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivale",660,4938
4,True,adversarial robustness,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems.","Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivale",660,4938
4,True,neural nets,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems.","Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivale",660,4938
5,True,extraction,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining.,"Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining. Training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while the unsup",682,6698
5,True,online product reviews,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining.,"Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining. Training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while the unsup",682,6698
5,True,a key task,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining.,"Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining. Training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while the unsup",682,6698
5,True,sentiment analysis,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining.,"Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining. Training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while the unsup",682,6698
5,True,opinion mining,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining.,"Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining. Training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while the unsup",682,6698
5,False,training,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"However, training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while unsupervised neural topic models fail to capture the particular aspects of interest.","Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining. Training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while the unsup",682,6698
5,False,aspect extraction,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"However, training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while unsupervised neural topic models fail to capture the particular aspects of interest.","Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining. Training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while the unsup",682,6698
5,False,ground truth aspect labels,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"However, training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while unsupervised neural topic models fail to capture the particular aspects of interest.","Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining. Training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while the unsup",682,6698
6,True,Graph Convolutional Networks,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Graph Convolutional Networks (GCNs) have been successful in modeling graph-structured data.,"Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and ",657,5408
6,True,Knowledge,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,It combines entity-relation composition operations from Knowledge,"Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and ",657,5408
6,True,GCNs,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Graph Convolutional Networks (GCNs) have been successful in modeling graph-structured data.,"Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and ",657,5408
6,True,graph-structured data,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Graph Convolutional Networks (GCNs) have been successful in modeling graph-structured data.,"Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and ",657,5408
6,True,the primary focus,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"However, the primary focus has been on handling simple undirected graphs.","Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and ",657,5408
6,True,simple undirected graphs,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"However, the primary focus has been on handling simple undirected graphs.","Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and ",657,5408
6,True,Multi-relational graphs,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Multi-relational graphs are a more general and prevalent form of graphs where each edge has a label and direction associated with it.,"Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and ",657,5408
6,True,a more general and prevalent form,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Multi-relational graphs are a more general and prevalent form of graphs where each edge has a label and direction associated with it.,"Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and ",657,5408
7,True,NLP,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"This simple baseline has been overlooked by the NLP community, but now it can be explored.",Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to d,665,5785
7,True,BERT,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"To accelerate future research, we will make our 24 pre-trained miniature BERT models publicly available.",Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to d,665,5785
7,True,natural language representations,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"In natural language representations, large and expensive models have been used to leverage general-domain text through self-supervised pre-training.",Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to d,665,5785
7,True,large and expensive models,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"In natural language representations, large and expensive models have been used to leverage general-domain text through self-supervised pre-training.",Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to d,665,5785
7,True,general-domain text,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"In natural language representations, large and expensive models have been used to leverage general-domain text through self-supervised pre-training.",Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to d,665,5785
7,True,self-supervised pre,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"In natural language representations, large and expensive models have been used to leverage general-domain text through self-supervised pre-training.",Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to d,665,5785
7,True,training,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"In natural language representations, large and expensive models have been used to leverage general-domain text through self-supervised pre-training.",Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to d,665,5785
7,True,the cost,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"However, due to the cost of applying such models to down-stream tasks, several model compression techniques have been proposed, including Pre-trained Distillation.",Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to d,665,5785
8,True,The Mutual Information (MI,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,The Mutual Information (MI) between hidden layers and input/desired output is used to construct the IP.,"Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate ",631,5918
8,True,IP,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,The Mutual Information (MI) between hidden layers and input/desired output is used to construct the IP.,"Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate ",631,5918
8,True,Convolutional Neural Networks,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"However, there are limitations in understanding deep Convolutional Neural Networks (CNNs), such as the e.g.\ VGG-16.","Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate ",631,5918
8,True,R\'enyi,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"In this paper, we propose an IP analysis using the new matrix--based R\'enyi's entropy coupled with tensor kernels to represent properties of probability distribution independently of dimensionality.","Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate ",631,5918
8,True,The Mutual Information,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,The Mutual Information (MI) between hidden layers and input/desired output is used to construct the IP.,"Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate ",631,5918
8,True,(MI,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,The Mutual Information (MI) between hidden layers and input/desired output is used to construct the IP.,"Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate ",631,5918
8,False,hidden layers,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,The Mutual Information (MI) between hidden layers and input/desired output is used to construct the IP.,"Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate ",631,5918
8,True,input/desired output,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,The Mutual Information (MI) between hidden layers and input/desired output is used to construct the IP.,"Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate ",631,5918
9,True,Reinforcement learning,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system, arises in many applications, including queuing systems, robotics control with disturbances, and object tracking.","We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with d",689,10274
9,True,input-driven environments,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system, arises in many applications, including queuing systems, robotics control with disturbances, and object tracking.","We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with d",689,10274
9,True,"an exogenous, stochastic input process",,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system, arises in many applications, including queuing systems, robotics control with disturbances, and object tracking.","We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with d",689,10274
9,True,the dynamics,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system, arises in many applications, including queuing systems, robotics control with disturbances, and object tracking.","We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with d",689,10274
9,True,the system,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system, arises in many applications, including queuing systems, robotics control with disturbances, and object tracking.","We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with d",689,10274
9,True,many applications,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system, arises in many applications, including queuing systems, robotics control with disturbances, and object tracking.","We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with d",689,10274
9,True,queuing systems,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system, arises in many applications, including queuing systems, robotics control with disturbances, and object tracking.","We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with d",689,10274
9,True,robotics control,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system, arises in many applications, including queuing systems, robotics control with disturbances, and object tracking.","We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with d",689,10274
