doc_id,question,cand_answer,qa_answer,f1,f1_ge_05,failure_reason,qg_model,qa_model,summary_sentence,source_excerpt,summary_len,source_len
0,True,Word embeddings,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Word embeddings extract semantic features of words from large datasets of text.,"Word embeddings extract semantic features of words from large datasets of text.
 Most embedding methods rely on a log-bilinear model to predict the occurrence
 of a word in a context of other words. Here we propose word2net, a method that
 ",533,1204
0,True,semantic features,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Word embeddings extract semantic features of words from large datasets of text.,"Word embeddings extract semantic features of words from large datasets of text.
 Most embedding methods rely on a log-bilinear model to predict the occurrence
 of a word in a context of other words. Here we propose word2net, a method that
 ",533,1204
0,True,words,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Word embeddings extract semantic features of words from large datasets of text.,"Word embeddings extract semantic features of words from large datasets of text.
 Most embedding methods rely on a log-bilinear model to predict the occurrence
 of a word in a context of other words. Here we propose word2net, a method that
 ",533,1204
0,True,large datasets,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Word embeddings extract semantic features of words from large datasets of text.,"Word embeddings extract semantic features of words from large datasets of text.
 Most embedding methods rely on a log-bilinear model to predict the occurrence
 of a word in a context of other words. Here we propose word2net, a method that
 ",533,1204
0,True,text,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Word embeddings extract semantic features of words from large datasets of text.,"Word embeddings extract semantic features of words from large datasets of text.
 Most embedding methods rely on a log-bilinear model to predict the occurrence
 of a word in a context of other words. Here we propose word2net, a method that
 ",533,1204
0,True,Most embedding methods,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Most embedding methods rely on a log-bilinear model to predict the occurrence of a word in a context of other words.,"Word embeddings extract semantic features of words from large datasets of text.
 Most embedding methods rely on a log-bilinear model to predict the occurrence
 of a word in a context of other words. Here we propose word2net, a method that
 ",533,1204
0,True,a log-bilinear model,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Most embedding methods rely on a log-bilinear model to predict the occurrence of a word in a context of other words.,"Word embeddings extract semantic features of words from large datasets of text.
 Most embedding methods rely on a log-bilinear model to predict the occurrence
 of a word in a context of other words. Here we propose word2net, a method that
 ",533,1204
0,True,the occurrence,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Most embedding methods rely on a log-bilinear model to predict the occurrence of a word in a context of other words.,"Word embeddings extract semantic features of words from large datasets of text.
 Most embedding methods rely on a log-bilinear model to predict the occurrence
 of a word in a context of other words. Here we propose word2net, a method that
 ",533,1204
1,True,Natural language,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses).","Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. Whil",522,6762
1,True,smaller units,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses).","Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. Whil",522,6762
1,False,larger units,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses).","Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. Whil",522,6762
1,True,clauses,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses).","Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. Whil",522,6762
1,True,a larger constituent,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"When a larger constituent ends, all smaller constituents must also be closed.","Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. Whil",522,6762
1,True,all smaller constituents,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"When a larger constituent ends, all smaller constituents must also be closed.","Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. Whil",522,6762
1,True,The standard LSTM architecture,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,The standard LSTM architecture does not have an explicit bias towards modeling a hierarchy of constituents.,"Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. Whil",522,6762
1,False,an explicit bias,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,The standard LSTM architecture does not have an explicit bias towards modeling a hierarchy of constituents.,"Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. Whil",522,6762
2,False,Neural networks,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Neural networks make mistakes.,Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user ,527,4956
2,True,mistakes,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Neural networks make mistakes.,Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user ,527,4956
2,True,The reason,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,The reason why a mistake is made often remains a mystery.,Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user ,527,4956
2,True,a mistake,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,The reason why a mistake is made often remains a mystery.,Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user ,527,4956
2,True,a mystery,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,The reason why a mistake is made often remains a mystery.,Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user ,527,4956
2,Why would it be useful to have a method that can explain the mistakes of a classifier model by visually showing what needs to be added to an image such that it is correctly classified?,It,why an image is misclassified,0.0,False,low_f1,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,It would be useful to have a method that can explain the mistakes of a classifier model by visually showing what needs to be added to an image such that it is correctly classified.,sidered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user as to why an image is misclassified. In this paper we develop a method for explaining the mistakes of a classifier model by visually showing what must be a,527,4956
2,True,a method,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,It would be useful to have a method that can explain the mistakes of a classifier model by visually showing what needs to be added to an image such that it is correctly classified.,Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user ,527,4956
2,why would it be useful to have a method that can explain the mistakes of a classifier model by visually showing what needs to be added to an image such that it is correctly classified?,that,why an image is misclassified,0.0,False,low_f1,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,It would be useful to have a method that can explain the mistakes of a classifier model by visually showing what needs to be added to an image such that it is correctly classified.,sidered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user as to why an image is misclassified. In this paper we develop a method for explaining the mistakes of a classifier model by visually showing what must be a,527,4956
3,True,NLP,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner.,A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general ,490,6814
3,True,Recent success,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner.,A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general ,490,6814
3,True,natural language processing,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner.,A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general ,490,6814
3,True,distributed vector representations,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner.,A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general ,490,6814
3,True,words,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner.,A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general ,490,6814
3,True,large amounts,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner.,A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general ,490,6814
3,True,text,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner.,A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general ,490,6814
3,True,an unsupervised manner,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner.,A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general ,490,6814
4,True,Lyapunov,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems.","Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivale",422,4938
4,True,Deep neural networks,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Deep neural networks are known to be vulnerable to adversarial perturbations.,"Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivale",422,4938
4,True,adversarial perturbations,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Deep neural networks are known to be vulnerable to adversarial perturbations.,"Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivale",422,4938
4,True,this paper,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems.","Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivale",422,4938
4,True,we,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems.","Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivale",422,4938
4,True,adversarial robustness,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems.","Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivale",422,4938
4,True,neural nets,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems.","Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivale",422,4938
4,True,Lyapunov stability,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems.","Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivale",422,4938
5,True,Amazon,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Our proposed distillation approach outperforms previous weakly supervised approaches for aspect extraction in six domains of Amazon product reviews.,"Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining. Training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while the unsup",642,6698
5,True,extraction,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining.,"Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining. Training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while the unsup",642,6698
5,True,online product reviews,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining.,"Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining. Training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while the unsup",642,6698
5,True,a key task,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining.,"Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining. Training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while the unsup",642,6698
5,True,sentiment analysis,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining.,"Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining. Training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while the unsup",642,6698
5,True,opinion mining,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining.,"Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining. Training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while the unsup",642,6698
5,False,Training,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while unsupervised neural topic models fail to capture the particular aspects of interest.","Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining. Training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while the unsup",642,6698
5,False,neural networks,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while unsupervised neural topic models fail to capture the particular aspects of interest.","Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining. Training supervised neural networks for aspect extraction is not possible when ground truth aspect labels are not available, while the unsup",642,6698
6,True,Graph Convolutional Networks,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Graph Convolutional Networks (GCNs) have been shown to be quite successful in modeling graph-structured data.,"Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and ",563,5408
6,True,Knowledge Graph Embedding,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,CompGCN leverages entity-relation composition operations from Knowledge Graph Embedding techniques and generalizes existing multi-relational,"Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and ",563,5408
6,True,Graphs,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Graphs are one of the most expressive data-structures and have been used to model a variety of problems.,"Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and ",563,5408
6,True,the most expressive data-structures,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Graphs are one of the most expressive data-structures and have been used to model a variety of problems.,"Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and ",563,5408
6,True,a variety,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Graphs are one of the most expressive data-structures and have been used to model a variety of problems.,"Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and ",563,5408
6,True,problems,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Graphs are one of the most expressive data-structures and have been used to model a variety of problems.,"Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and ",563,5408
6,True,GCNs,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Graph Convolutional Networks (GCNs) have been shown to be quite successful in modeling graph-structured data.,"Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and ",563,5408
6,True,graph-structured data,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Graph Convolutional Networks (GCNs) have been shown to be quite successful in modeling graph-structured data.,"Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and ",563,5408
7,True,Recent developments,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training.,Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to d,636,5785
7,True,natural language representations,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training.,Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to d,636,5785
7,True,large and expensive models,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training.,Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to d,636,5785
7,True,that,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training.,Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to d,636,5785
7,True,vast amounts,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training.,Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to d,636,5785
7,True,general-domain text,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training.,Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to d,636,5785
7,True,self-supervised pre,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training.,Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to d,636,5785
7,True,training,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training.,Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to d,636,5785
8,True,IP,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into their generalization ability.,"Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate ",564,5918
8,True,deep neural networks,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into their generalization ability.,"Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate ",564,5918
8,True,DNNs,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into their generalization ability.,"Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate ",564,5918
8,True,information plane (IP) theory,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into their generalization ability.,"Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate ",564,5918
8,True,tremendous attention,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into their generalization ability.,"Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate ",564,5918
8,True,a tool,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into their generalization ability.,"Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate ",564,5918
8,True,insight,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into their generalization ability.,"Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate ",564,5918
8,True,their generalization ability,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into their generalization ability.,"Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate ",564,5918
9,False,We,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"We consider reinforcement learning in input-driven environments where an exogenous, stochastic input process affects the dynamics of the system.","We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with d",661,10274
9,True,reinforcement learning,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"We consider reinforcement learning in input-driven environments where an exogenous, stochastic input process affects the dynamics of the system.","We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with d",661,10274
9,True,input-driven environments,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"We consider reinforcement learning in input-driven environments where an exogenous, stochastic input process affects the dynamics of the system.","We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with d",661,10274
9,True,"an exogenous, stochastic input process",,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"We consider reinforcement learning in input-driven environments where an exogenous, stochastic input process affects the dynamics of the system.","We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with d",661,10274
9,True,the dynamics,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"We consider reinforcement learning in input-driven environments where an exogenous, stochastic input process affects the dynamics of the system.","We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with d",661,10274
9,False,the system,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"We consider reinforcement learning in input-driven environments where an exogenous, stochastic input process affects the dynamics of the system.","We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with d",661,10274
9,True,Input processes,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Input processes arise in many applications, including queuing systems, robotics control with disturbances, and object tracking.","We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with d",661,10274
9,True,many applications,,0.0,False,empty_qa_answer,iarfmoose/t5-base-question-generator,deepset/roberta-base-squad2,"Input processes arise in many applications, including queuing systems, robotics control with disturbances, and object tracking.","We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with d",661,10274
