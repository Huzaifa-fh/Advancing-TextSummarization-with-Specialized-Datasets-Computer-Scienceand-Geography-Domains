{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e38ff1-6d20-4090-904d-4685b92346a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_CSV = \"./Datasets/CS_Metrics_Test.csv\"                     # where scitldr is outputted \n",
    "\n",
    "# --------------------------\n",
    "# PATHS / COMMON SETTINGS \n",
    "# --------------------------\n",
    "DATA_X_PATH    = OUT_CSV\n",
    "SAVE_DIR       = \"./Models\"\n",
    "TOPICS_DF_DIR  = \"./Keywords\"\n",
    "DS_TAG         = \"_cs\"  # used in model folder names and cache tags\n",
    "\n",
    "MODEL_DIRS = [                                                 # all models you want to compute metrics for     \n",
    "    os.path.join(SAVE_DIR, f\"t5-base{DS_TAG}_noKW\"),\n",
    "    os.path.join(SAVE_DIR, f\"t5-base{DS_TAG}_KW\"),\n",
    "    os.path.join(SAVE_DIR, f\"t5-base{DS_TAG}_KWplus\"),\n",
    "    os.path.join(SAVE_DIR, f\"t5-base{DS_TAG}_KWprefix\"),\n",
    "    os.path.join(SAVE_DIR, f\"bart-base_{DS_TAG}_noKW\"),\n",
    "    os.path.join(SAVE_DIR, f\"bart-base_{DS_TAG}_KW\"),\n",
    "    os.path.join(SAVE_DIR, f\"bert2bert{DS_TAG}_noKW\"),\n",
    "    os.path.join(SAVE_DIR, f\"bert2bert{DS_TAG}_KW\"),\n",
    "]\n",
    "\n",
    "# After prediction col is run, you should have predictions saved at:\n",
    "GEN_OUT_ROOT = os.path.join(SAVE_DIR, \"eval_cs\")\n",
    "PRED_DIR     = os.path.join(GEN_OUT_ROOT, \"predictions\")\n",
    "COMBINED_PRED_CSV = os.path.join(PRED_DIR, \"predictions_all_models.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec72ea9e-60a1-4d55-bc39-6492c67ad80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ==========================\n",
    "# CHOOSE ONE:\n",
    "#   CHOICE = \"arxiv\"  or  \"pubmed\"  or  \"scitldr\"\n",
    "# ==========================\n",
    "CHOICE   = \"scitldr\"   # change to: \"pubmed\" or \"scitldr\"\n",
    "SAMPLE_N = 500       # keep small to run fast; bump if you like\n",
    "\n",
    "if CHOICE in (\"arxiv\", \"pubmed\"):\n",
    "    # Long-form scientific articles (plenty of CS in arXiv)\n",
    "    ds = load_dataset(\"scientific_papers\", CHOICE)\n",
    "    split = ds[\"validation\"] if \"validation\" in ds else ds[\"test\"]\n",
    "    if SAMPLE_N:\n",
    "        split = split.select(range(min(SAMPLE_N, len(split))))\n",
    "    df = pd.DataFrame({\n",
    "        \"Text\": split[\"article\"],\n",
    "        \"Abstractive\": split[\"abstract\"]\n",
    "    })\n",
    "\n",
    "elif CHOICE == \"scitldr\":\n",
    "    # TL;DRs for CS papers (short summaries)\n",
    "    # Try default; if a config is required on your mirror, fall back to \"AIC\"\n",
    "    try:\n",
    "        ds = load_dataset(\"allenai/scitldr\")\n",
    "    except:\n",
    "        ds = load_dataset(\"allenai/scitldr\", \"AIC\")\n",
    "    split = ds[\"validation\"] if \"validation\" in ds else ds[\"test\"]\n",
    "    if SAMPLE_N:\n",
    "        split = split.select(range(min(SAMPLE_N, len(split))))\n",
    "    # join list-of-sentences / list-of-tldrs into strings\n",
    "    def join_list(x):\n",
    "        if isinstance(x, list):\n",
    "            return \" \".join(x)\n",
    "        return str(x)\n",
    "    df = pd.DataFrame({\n",
    "        \"Text\": [join_list(x) for x in split[\"source\"]],\n",
    "        \"Abstractive\": [join_list(y[0] if isinstance(y, list) and len(y)>0 else y) for y in split[\"target\"]],\n",
    "    })\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Set CHOICE to 'arxiv', 'pubmed', or 'scitldr'.\")\n",
    "\n",
    "# Basic cleanup (your pipeline expects non-empty strings)\n",
    "df = df.dropna(subset=[\"Text\",\"Abstractive\"]).reset_index(drop=True)\n",
    "df[\"Text\"] = df[\"Text\"].astype(str).str.strip()\n",
    "df[\"Abstractive\"] = df[\"Abstractive\"].astype(str).str.strip()\n",
    "df = df[(df[\"Text\"].str.len() > 0) & (df[\"Abstractive\"].str.len() > 0)].reset_index(drop=True)\n",
    "\n",
    "print(df.head(2))\n",
    "print(\"Total rows:\", len(df))\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "print(\"Wrote:\", OUT_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a55233-f37b-4f48-bb5e-9fe2758d259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FACTUALITY EVAL (recompute topics) + KW/KW+ topic support\n",
    "# ============================================================\n",
    "import os, sys, gc, json, time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM  # (unchanged import; not used here)\n",
    "\n",
    "# -------- silence noisy logs (safe) --------\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "SEED = 42\n",
    "MAX_SOURCE_LEN = 512\n",
    "MAX_TARGET_LEN = 128\n",
    "GEN_KWARGS = dict(\n",
    "    max_new_tokens=MAX_TARGET_LEN,\n",
    "    min_new_tokens=5,\n",
    "    num_beams=4,\n",
    "    length_penalty=2.0,\n",
    "    early_stopping=True,\n",
    ")\n",
    "BATCH_GEN = 8\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Torch:\", torch.__version__, \"| device:\", DEVICE)\n",
    "\n",
    "# --------------------------\n",
    "# Data loading (.csv/.xlsx/.xls) \n",
    "# --------------------------\n",
    "def _ensure_xlrd_for_xls(path: str):\n",
    "    if str(path).lower().endswith(\".xls\"):\n",
    "        try:\n",
    "            import xlrd  # noqa\n",
    "        except Exception:\n",
    "            import subprocess, sys\n",
    "            print(\"Installing xlrd for .xls reading...\")\n",
    "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"xlrd==2.0.1\"], check=True)\n",
    "\n",
    "def load_dataframe(path: str) -> pd.DataFrame:\n",
    "    p = Path(path)\n",
    "    if p.suffix.lower() == \".csv\":\n",
    "        return pd.read_csv(path)\n",
    "    elif p.suffix.lower() == \".xlsx\":\n",
    "        return pd.read_excel(path, engine=\"openpyxl\")\n",
    "    elif p.suffix.lower() == \".xls\":\n",
    "        _ensure_xlrd_for_xls(path)\n",
    "        return pd.read_excel(path, engine=\"xlrd\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {p.suffix}\")\n",
    "\n",
    "df_full = load_dataframe(DATA_X_PATH)\n",
    "assert {\"Text\",\"Abstractive\"}.issubset(df_full.columns), \"Need 'Text' & 'Abstractive'.\"\n",
    "df_full = df_full.dropna(subset=[\"Text\",\"Abstractive\"]).reset_index(drop=True)\n",
    "\n",
    "# same 90/10 validation split used earlier  (UNCHANGED)\n",
    "from sklearn.model_selection import train_test_split\n",
    "_, val_df = train_test_split(df_full, test_size=0.10, random_state=SEED, shuffle=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "print(\"Eval size:\", len(val_df))\n",
    "\n",
    "# --------------------------\n",
    "# Recompute topics (no cache): KW and KW+\n",
    "#   - KW   = spaCy NER-only (fast, no parser)\n",
    "#   - KW+  = NER + noun chunks (needs dependency parser)\n",
    "#   - KWPREFIX = for your KWprefix model, we use the KW topics (to match training)\n",
    "# --------------------------\n",
    "import spacy\n",
    "try:\n",
    "    # KW (NER-only): exclude parser etc. for speed\n",
    "    nlp_kw = spacy.load(\"en_core_web_sm\", exclude=[\"parser\",\"attribute_ruler\",\"lemmatizer\",\"tagger\",\"senter\"])\n",
    "    if \"sentencizer\" not in nlp_kw.pipe_names:\n",
    "        nlp_kw.add_pipe(\"sentencizer\")\n",
    "    nlp_kw.max_length = 2_000_000\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"spaCy model missing. Run: python -m spacy download en_core_web_sm\") from e\n",
    "\n",
    "try:\n",
    "    # KW+ (needs noun_chunks -> requires dependency parser)\n",
    "    # keep parser; optionally exclude heavy components\n",
    "    nlp_kwplus = spacy.load(\"en_core_web_sm\", exclude=[\"attribute_ruler\",\"lemmatizer\",\"tagger\"])\n",
    "    if \"sentencizer\" not in nlp_kwplus.pipe_names:\n",
    "        nlp_kwplus.add_pipe(\"sentencizer\")\n",
    "    nlp_kwplus.max_length = 2_000_000\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"spaCy model missing for KW+. Run: python -m spacy download en_core_web_sm\") from e\n",
    "\n",
    "MAX_KW = 10\n",
    "\n",
    "def extract_topics_kw(text: str, max_kw: int = MAX_KW) -> str:\n",
    "    \"\"\"KW: NER-only (dedup, drop tokens containing digits).\"\"\"\n",
    "    doc = nlp_kw(str(text))\n",
    "    out, seen = [], set()\n",
    "    for ent in doc.ents:\n",
    "        tok = ent.text.strip()\n",
    "        if not tok or any(c.isdigit() for c in tok):\n",
    "            continue\n",
    "        low = tok.lower()\n",
    "        if low not in seen:\n",
    "            seen.add(low)\n",
    "            out.append(tok)\n",
    "        if len(out) >= max_kw:\n",
    "            break\n",
    "    return \" ; \".join(out)\n",
    "\n",
    "def extract_topics_kwplus(text: str, max_kw: int = MAX_KW) -> str:\n",
    "    \"\"\"KW+: NER first, then noun chunks (dedup, drop digits). Requires parser.\"\"\"\n",
    "    doc = nlp_kwplus(str(text))\n",
    "    out, seen = [], set()\n",
    "    # 1) Named entities first\n",
    "    for ent in doc.ents:\n",
    "        tok = ent.text.strip()\n",
    "        if not tok or any(c.isdigit() for c in tok):\n",
    "            continue\n",
    "        low = tok.lower()\n",
    "        if low not in seen:\n",
    "            seen.add(low)\n",
    "            out.append(tok)\n",
    "        if len(out) >= max_kw:\n",
    "            return \" ; \".join(out)\n",
    "    # 2) Noun chunks\n",
    "    for nc in doc.noun_chunks:\n",
    "        tok = nc.text.strip()\n",
    "        if not tok or any(c.isdigit() for c in tok):\n",
    "            continue\n",
    "        if 2 <= len(tok) <= 80:\n",
    "            low = tok.lower()\n",
    "            if low not in seen:\n",
    "                seen.add(low)\n",
    "                out.append(tok)\n",
    "            if len(out) >= max_kw:\n",
    "                break\n",
    "    return \" ; \".join(out)\n",
    "\n",
    "print(\"► Recomputing topics (KW & KW+) …\")\n",
    "val_df = val_df.copy()\n",
    "val_df[\"topics_kw\"]     = [extract_topics_kw(t)     for t in val_df[\"Text\"].tolist()]\n",
    "val_df[\"topics_kwplus\"] = [extract_topics_kwplus(t) for t in val_df[\"Text\"].tolist()]\n",
    "\n",
    "# For your KWprefix model, you trained with the *simple NER topics*.\n",
    "# To match training at inference time:\n",
    "val_df[\"topics_prefix\"] = val_df[\"topics_kw\"]\n",
    "\n",
    "# --------------------------\n",
    "# (Optional) refresh full-dataset topic caches on disk for future runs\n",
    "# --------------------------\n",
    "os.makedirs(TOPICS_DF_DIR, exist_ok=True)\n",
    "\n",
    "topics_kw_full     = [extract_topics_kw(t)     for t in df_full[\"Text\"].tolist()]\n",
    "topics_kwplus_full = [extract_topics_kwplus(t) for t in df_full[\"Text\"].tolist()]\n",
    "\n",
    "pd.DataFrame({\"topics_kw\": topics_kw_full}).to_parquet(os.path.join(TOPICS_DF_DIR, \"cs_topics_kw.parquet\"), index=False)\n",
    "pd.DataFrame({\"topics_kw\": topics_kw_full}).to_csv    (os.path.join(TOPICS_DF_DIR, \"cs_topics_kw.csv\"),         index=False)\n",
    "\n",
    "pd.DataFrame({\"topics_kwplus\": topics_kwplus_full}).to_parquet(os.path.join(TOPICS_DF_DIR, \"cs_topics_kwplus.parquet\"), index=False)\n",
    "pd.DataFrame({\"topics_kwplus\": topics_kwplus_full}).to_csv    (os.path.join(TOPICS_DF_DIR, \"cs_topics_kwplus.csv\"),         index=False)\n",
    "\n",
    "print(\"Saved fresh topic caches (KW + KW+).\")\n",
    "print(\"val_df now includes: ['topics_kw', 'topics_kwplus', 'topics_prefix']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b45e00-ec5f-4b76-b600-6f314a0605cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Generation helper (shared)\n",
    "# --------------------------\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "@torch.no_grad()\n",
    "def batched_generate(model, tok, inputs, max_src_len=MAX_SOURCE_LEN):\n",
    "    outs = []\n",
    "    for i in range(0, len(inputs), BATCH_GEN):\n",
    "        batch = inputs[i:i+BATCH_GEN]\n",
    "        enc = tok(batch, padding=True, truncation=True, max_length=max_src_len, return_tensors=\"pt\").to(DEVICE)\n",
    "        gen_ids = model.generate(**enc, **GEN_KWARGS)\n",
    "        outs.extend(tok.batch_decode(gen_ids, skip_special_tokens=True))\n",
    "    return outs\n",
    "\n",
    "def infer_one_model(model_dir: str, df_eval: pd.DataFrame) -> pd.DataFrame:\n",
    "    is_kw = model_dir.endswith(\"_KW\")\n",
    "    print(f\"\\n==> Generating with: {os.path.basename(model_dir)} | KW={is_kw}\")\n",
    "    tok = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_dir).to(DEVICE).eval()\n",
    "    inputs = [build_input(s, k, is_kw) for s,k in zip(df_eval[\"Text\"].tolist(), df_eval[\"topics\"].tolist())]\n",
    "    preds  = batched_generate(model, tok, inputs)\n",
    "    out = df_eval[[\"Text\",\"Abstractive\",\"topics\"]].copy()\n",
    "    out[\"prediction\"] = preds\n",
    "    out[\"model_dir\"]  = model_dir\n",
    "    out[\"kw\"]         = is_kw\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd79c05-14d0-437b-ba2a-ea6229d01bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# BERTScore (Precision vs. source)\n",
    "# =========================\n",
    "from bert_score import score as bertscore_score\n",
    "\n",
    "def bertscore_precision(summary_list, source_list, device: str):\n",
    "    P, R, F1 = bertscore_score(\n",
    "        cands=summary_list,\n",
    "        refs=source_list,\n",
    "        model_type=\"roberta-large\",  # good default\n",
    "        device=device,\n",
    "        lang=\"en\",\n",
    "        rescale_with_baseline=True\n",
    "    )\n",
    "    return P.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2929ad-7ca9-494c-bc13-4246f6f02cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# NLI (SummaC-style) with robust fallback\n",
    "# =========================\n",
    "import os, numpy as np, torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\", None)  # optional; set if your network requires auth\n",
    "LOCAL_ONLY = False\n",
    "\n",
    "# Smaller first (faster on CPU), then bigger:\n",
    "NLI_CANDIDATES = [\n",
    "    \"typeform/distilbert-base-uncased-mnli\",\n",
    "    \"textattack/roberta-base-MNLI\",\n",
    "    \"roberta-base-mnli\",\n",
    "    \"facebook/bart-large-mnli\",\n",
    "]\n",
    "\n",
    "_nli_tok = _nli_mod = None\n",
    "CT_IDX = NT_IDX = ET_IDX = None\n",
    "\n",
    "def _try_load_nli(rid: str):\n",
    "    tok = AutoTokenizer.from_pretrained(rid, use_fast=True, token=HF_TOKEN, local_files_only=LOCAL_ONLY)\n",
    "    mod = AutoModelForSequenceClassification.from_pretrained(rid, token=HF_TOKEN, local_files_only=LOCAL_ONLY).to(DEVICE).eval()\n",
    "    id2label = {int(k): v.lower() for k, v in mod.config.id2label.items()}\n",
    "    lbl = {v: k for k, v in id2label.items()}\n",
    "    # normalize possible variants\n",
    "    for k in list(lbl.keys()):\n",
    "        if \"contra\" in k and \"contradiction\" not in lbl:\n",
    "            lbl[\"contradiction\"] = lbl.pop(k)\n",
    "        if \"entail\" in k and \"entailment\" not in lbl:\n",
    "            lbl[\"entailment\"] = lbl.pop(k)\n",
    "        if \"neutral\" in k and \"neutral\" not in lbl:\n",
    "            lbl[\"neutral\"] = lbl[\"neutral\"]\n",
    "    need = {\"entailment\",\"neutral\",\"contradiction\"}\n",
    "    if not need.issubset(lbl):\n",
    "        raise ValueError(f\"Labels missing for {rid}: {id2label}\")\n",
    "    return tok, mod, lbl[\"contradiction\"], lbl[\"neutral\"], lbl[\"entailment\"], id2label\n",
    "\n",
    "last_err = None\n",
    "for rid in NLI_CANDIDATES:\n",
    "    try:\n",
    "        print(f\"Trying NLI model: {rid} ...\")\n",
    "        _nli_tok, _nli_mod, CT_IDX, NT_IDX, ET_IDX, labmap = _try_load_nli(rid)\n",
    "        print(f\"✅ Using NLI: {rid} | labels={labmap}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        last_err = e\n",
    "        print(f\"⚠️  {rid} failed: {e}\")\n",
    "\n",
    "if _nli_tok is None:\n",
    "    raise RuntimeError(\n",
    "        \"Could not load an MNLI model. If your network requires auth, do:\\n\"\n",
    "        \"  huggingface-cli login  (or set env HF_TOKEN)\\n\"\n",
    "        f\"Last error: {last_err}\"\n",
    "    )\n",
    "\n",
    "# sentence splitting + chunking\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "def sent_split(text: str):\n",
    "    return nltk.sent_tokenize(str(text).strip())\n",
    "\n",
    "def chunk_sentences(sents, k: int = 3, stride: int = 1):\n",
    "    if not sents:\n",
    "        return []\n",
    "    if len(sents) <= k:\n",
    "        return [\" \".join(sents)]\n",
    "    return [\" \".join(sents[i:i+k]) for i in range(0, len(sents)-k+1, stride)]\n",
    "\n",
    "@torch.no_grad()\n",
    "def nli_aggregate(source_text: str, summary_text: str,\n",
    "                  chunk_k: int = 3, stride: int = 1, batch_size: int = 16):\n",
    "    src_sents = sent_split(source_text)\n",
    "    sum_sents = sent_split(summary_text)\n",
    "    if len(sum_sents) == 0:\n",
    "        return dict(entail_mean=0.0, contra_mean=0.0, nli_score=0.0,\n",
    "                    support_rate=0.0, contradiction_rate=0.0, n_sum_sents=0)\n",
    "    src_chunks = chunk_sentences(src_sents, k=chunk_k, stride=stride) or [\" \".join(src_sents)]\n",
    "    max_entails, max_contras = [], []\n",
    "    SUP_T, CON_T = 0.5, 0.5\n",
    "    for s in sum_sents:\n",
    "        pairs = [(c, s) for c in src_chunks]\n",
    "        entail_scores, contra_scores = [], []\n",
    "        for i in range(0, len(pairs), batch_size):\n",
    "            batch = pairs[i:i+batch_size]\n",
    "            enc = _nli_tok([p for p,_ in batch], [h for _,h in batch],\n",
    "                           padding=True, truncation=True, max_length=384, return_tensors=\"pt\").to(DEVICE)\n",
    "            probs = torch.softmax(_nli_mod(**enc).logits, dim=-1).detach().cpu().numpy()\n",
    "            entail_scores.extend(probs[:, ET_IDX].tolist())\n",
    "            contra_scores.extend(probs[:, CT_IDX].tolist())\n",
    "        max_entails.append(float(np.max(entail_scores)))\n",
    "        max_contras.append(float(np.max(contra_scores)))\n",
    "    entail_mean = float(np.mean(max_entails))\n",
    "    contra_mean = float(np.mean(max_contras))\n",
    "    support_rate = float(np.mean([m > SUP_T for m in max_entails]))\n",
    "    contradiction_rate = float(np.mean([m > CON_T for m in max_contras]))\n",
    "    nli_score = entail_mean - contra_mean\n",
    "    return dict(\n",
    "        entail_mean=entail_mean,\n",
    "        contra_mean=contra_mean,\n",
    "        nli_score=nli_score,\n",
    "        support_rate=support_rate,\n",
    "        contradiction_rate=contradiction_rate,\n",
    "        n_sum_sents=len(sum_sents),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a70dbb5-f1f5-47d4-b90d-e9653eee84c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# QAGS-lite (QG + QA) with fallbacks\n",
    "# =========================\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoModelForQuestionAnswering\n",
    "\n",
    "# QG candidates\n",
    "QG_MODEL_IDS = [\n",
    "    \"iarfmoose/t5-base-question-generator\",\n",
    "    \"mrm8488/t5-base-finetuned-question-generation-ap\",\n",
    "]\n",
    "_qg_tok = _qg_mod = _qg_id = None\n",
    "for qgid in QG_MODEL_IDS:\n",
    "    try:\n",
    "        _qg_tok = AutoTokenizer.from_pretrained(qgid, use_fast=True, token=HF_TOKEN, local_files_only=LOCAL_ONLY)\n",
    "        _qg_mod = AutoModelForSeq2SeqLM.from_pretrained(qgid, token=HF_TOKEN, local_files_only=LOCAL_ONLY).to(DEVICE).eval()\n",
    "        _qg_id = qgid\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  QG model '{qgid}' failed: {e}\")\n",
    "if _qg_id is None:\n",
    "    raise RuntimeError(\"No QG model could be loaded.\")\n",
    "\n",
    "print(\"QG model:\", _qg_id)\n",
    "\n",
    "# QA candidates (span QA)\n",
    "QA_MODEL_IDS = [\n",
    "    \"deepset/roberta-base-squad2\",\n",
    "    \"distilbert-base-cased-distilled-squad\",\n",
    "]\n",
    "_qa_tok = _qa_mod = _qa_id = None\n",
    "for qaid in QA_MODEL_IDS:\n",
    "    try:\n",
    "        _qa_tok = AutoTokenizer.from_pretrained(qaid, use_fast=True, token=HF_TOKEN, local_files_only=LOCAL_ONLY)\n",
    "        _qa_mod = AutoModelForQuestionAnswering.from_pretrained(qaid, token=HF_TOKEN, local_files_only=LOCAL_ONLY).to(DEVICE).eval()\n",
    "        _qa_id = qaid\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  QA model '{qaid}' failed: {e}\")\n",
    "if _qa_id is None:\n",
    "    raise RuntimeError(\"No QA model could be loaded.\")\n",
    "\n",
    "print(\"QA model:\", _qa_id)\n",
    "\n",
    "# Helpers\n",
    "def normalize_text(s: str) -> str:\n",
    "    import string\n",
    "    s = s.lower().strip()\n",
    "    s = \"\".join(ch for ch in s if ch not in set(string.punctuation))\n",
    "    return \" \".join(s.split())\n",
    "\n",
    "def squad_f1(pred: str, truth: str) -> float:\n",
    "    pred_tokens  = normalize_text(pred).split()\n",
    "    truth_tokens = normalize_text(truth).split()\n",
    "    if len(pred_tokens) == 0 and len(truth_tokens) == 0:\n",
    "        return 1.0\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return 0.0\n",
    "    from collections import Counter\n",
    "    common = Counter(pred_tokens) & Counter(truth_tokens)\n",
    "    overlap = sum(common.values())\n",
    "    if overlap == 0:\n",
    "        return 0.0\n",
    "    precision = overlap / len(pred_tokens)\n",
    "    recall    = overlap / len(truth_tokens)\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def select_answer_spans(summary: str, max_ans: int = 8):\n",
    "    # reuse spaCy nlp from your first block\n",
    "    doc = nlp(summary)\n",
    "    items, seen = [], set()\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in {\"PERSON\",\"ORG\",\"GPE\",\"LOC\",\"NORP\",\"FAC\",\"PRODUCT\",\"EVENT\",\"WORK_OF_ART\"}:\n",
    "            a = ent.text.strip()\n",
    "            if a and a.lower() not in seen and not any(c.isdigit() for c in a):\n",
    "                seen.add(a.lower())\n",
    "                items.append((a, ent.sent.text.strip()))\n",
    "                if len(items) >= max_ans:\n",
    "                    return items\n",
    "    for nc in doc.noun_chunks:\n",
    "        a = nc.text.strip()\n",
    "        if 2 <= len(a) <= 80 and a.lower() not in seen and not any(c.isdigit() for c in a):\n",
    "            seen.add(a.lower())\n",
    "            items.append((a, nc.sent.text.strip()))\n",
    "            if len(items) >= max_ans:\n",
    "                break\n",
    "    return items\n",
    "\n",
    "@torch.no_grad()\n",
    "def qg_make_question(answer: str, context_sent: str, max_new_tokens: int = 48) -> str:\n",
    "    prompt = f\"answer: {answer}  context: {context_sent}\"\n",
    "    enc = _qg_tok([prompt], padding=True, truncation=True, max_length=256, return_tensors=\"pt\").to(DEVICE)\n",
    "    gen_ids = _qg_mod.generate(**enc, max_new_tokens=max_new_tokens, num_beams=4)\n",
    "    return _qg_tok.decode(gen_ids[0], skip_special_tokens=True).strip()\n",
    "\n",
    "@torch.no_grad()\n",
    "def qa_answer(context: str, question: str, max_len: int = 384) -> str:\n",
    "    enc = _qa_tok(question, context, truncation=\"only_second\", max_length=max_len, return_tensors=\"pt\").to(DEVICE)\n",
    "    out = _qa_mod(**enc)\n",
    "    start_idx = int(out.start_logits[0].argmax())\n",
    "    end_idx   = int(out.end_logits[0].argmax())\n",
    "    if end_idx < start_idx:\n",
    "        end_idx = start_idx\n",
    "    tokens = enc[\"input_ids\"][0].detach().cpu().tolist()\n",
    "    ans_ids = tokens[start_idx:end_idx+1]\n",
    "    return _qa_tok.decode(ans_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "def qags_example(source: str, summary: str, max_q: int = 8):\n",
    "    pairs = select_answer_spans(summary, max_ans=max_q)\n",
    "    if not pairs:\n",
    "        return dict(qags_doc_f1_mean=0.0, qags_doc_f1_median=0.0,\n",
    "                    qags_doc_f1_prop_ge_05=0.0, qags_nq=0)\n",
    "    f1s = []\n",
    "    for ans, sent in pairs:\n",
    "        q = qg_make_question(ans, sent)\n",
    "        if not q:\n",
    "            continue\n",
    "        a_doc = qa_answer(source, q)\n",
    "        f1s.append(squad_f1(a_doc, ans))\n",
    "    if not f1s:\n",
    "        return dict(qags_doc_f1_mean=0.0, qags_doc_f1_median=0.0,\n",
    "                    qags_doc_f1_prop_ge_05=0.0, qags_nq=0)\n",
    "    f1s = np.array(f1s, dtype=float)\n",
    "    return dict(\n",
    "        qags_doc_f1_mean=float(f1s.mean()),\n",
    "        qags_doc_f1_median=float(np.median(f1s)),\n",
    "        qags_doc_f1_prop_ge_05=float((f1s >= 0.5).mean()),\n",
    "        qags_nq=int(len(f1s)),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7457657f-9497-42a4-ae69-14b78a3045b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# GENERATION ONLY — save predictions (no topic recompute; skip existing)\n",
    "# ===================================\n",
    "import os, gc, glob\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Expected in scope: SAVE_DIR, DS_TAG, MODEL_DIRS, MAX_SOURCE_LEN, MAX_TARGET_LEN, GEN_KWARGS, BATCH_GEN\n",
    "# Also val_df (with columns: [\"Text\",\"Abstractive\",\"topics_kw\",\"topics_kwplus\",\"topics_prefix\"])\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "assert {\"Text\",\"Abstractive\"}.issubset(val_df.columns), \"val_df must have Text and Abstractive\"\n",
    "\n",
    "# Which topic columns do we actually need (based on existing model dirs)?\n",
    "bases = [os.path.basename(m).lower() for m in MODEL_DIRS if os.path.isdir(m)]\n",
    "need_kw      = any(b.endswith(\"_kw\")      for b in bases)\n",
    "need_kwplus  = any(b.endswith(\"_kwplus\")  for b in bases)\n",
    "need_kwpref  = any(b.endswith(\"_kwprefix\") for b in bases)\n",
    "\n",
    "if need_kw:     assert \"topics_kw\"     in val_df.columns, \"Run the Topic Builder block to create topics_kw\"\n",
    "if need_kwplus: assert \"topics_kwplus\" in val_df.columns, \"Run the Topic Builder block to create topics_kwplus\"\n",
    "if need_kwpref: assert \"topics_prefix\" in val_df.columns, \"Run the Topic Builder block to create topics_prefix\"\n",
    "\n",
    "# -------- input builders --------\n",
    "def build_input_kw(text: str, topics: str) -> str:\n",
    "    return f\"<TOPIC> {topics if isinstance(topics,str) else ''} <TEXT> {text}\".strip()\n",
    "\n",
    "def build_input_kwplus(text: str, topics_plus: str) -> str:\n",
    "    return f\"<TOPIC> {topics_plus if isinstance(topics_plus,str) else ''} <TEXT> {text}\".strip()\n",
    "\n",
    "def build_input_kwprefix(text: str, topics_prefix: str) -> str:\n",
    "    # Match KWprefix training prompt you used:\n",
    "    return f\"summarize: topics: {topics_prefix if isinstance(topics_prefix,str) else ''}  context: {text}\".strip()\n",
    "\n",
    "def needs_variant(model_dir: str) -> str:\n",
    "    \"\"\"Return 'none' | 'kw' | 'kwplus' | 'kwprefix' based on folder name.\"\"\"\n",
    "    base = os.path.basename(model_dir).lower()\n",
    "    if base.endswith(\"_kwplus\"):   return \"kwplus\"\n",
    "    if base.endswith(\"_kwprefix\"): return \"kwprefix\"\n",
    "    if base.endswith(\"_kw\"):       return \"kw\"\n",
    "    return \"none\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def batched_generate(model, tok, inputs: List[str]) -> List[str]:\n",
    "    outs = []\n",
    "    for i in range(0, len(inputs), BATCH_GEN):\n",
    "        batch = inputs[i:i+BATCH_GEN]\n",
    "        enc = tok(batch, padding=True, truncation=True, max_length=MAX_SOURCE_LEN, return_tensors=\"pt\").to(DEVICE)\n",
    "        gen_ids = model.generate(**enc, **GEN_KWARGS)\n",
    "        outs.extend(tok.batch_decode(gen_ids, skip_special_tokens=True))\n",
    "    return outs\n",
    "\n",
    "def infer_one_model(model_dir: str, df_eval: pd.DataFrame) -> pd.DataFrame:\n",
    "    variant = needs_variant(model_dir)\n",
    "    print(f\"\\n==> Generating with: {os.path.basename(model_dir)} | variant={variant}\")\n",
    "    tok = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_dir).to(DEVICE).eval()\n",
    "\n",
    "    if variant == \"none\":\n",
    "        inputs = df_eval[\"Text\"].astype(str).tolist()\n",
    "    elif variant == \"kw\":\n",
    "        inputs = [build_input_kw(s, k) for s, k in zip(df_eval[\"Text\"], df_eval[\"topics_kw\"])]\n",
    "    elif variant == \"kwplus\":\n",
    "        inputs = [build_input_kwplus(s, k) for s, k in zip(df_eval[\"Text\"], df_eval[\"topics_kwplus\"])]\n",
    "    elif variant == \"kwprefix\":\n",
    "        inputs = [build_input_kwprefix(s, k) for s, k in zip(df_eval[\"Text\"], df_eval[\"topics_prefix\"])]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown variant: {variant}\")\n",
    "\n",
    "    preds  = batched_generate(model, tok, [str(x) for x in inputs])\n",
    "    out = df_eval[[\"Text\",\"Abstractive\"]].copy()\n",
    "    # keep the topic columns for any downstream analysis\n",
    "    for col in [\"topics_kw\",\"topics_kwplus\",\"topics_prefix\"]:\n",
    "        if col in df_eval.columns:\n",
    "            out[col] = df_eval[col]\n",
    "    out[\"prediction\"] = preds\n",
    "    out[\"model_dir\"]  = model_dir\n",
    "    out[\"kw_variant\"] = variant\n",
    "    return out\n",
    "\n",
    "# -------- run only for missing per-model CSVs; then rebuild combined --------\n",
    "GEN_OUT_ROOT = os.path.join(SAVE_DIR, \"eval_cs\")\n",
    "PRED_DIR     = os.path.join(GEN_OUT_ROOT, \"predictions\")\n",
    "os.makedirs(PRED_DIR, exist_ok=True)\n",
    "\n",
    "generated_any = False\n",
    "for mdir in MODEL_DIRS:\n",
    "    if not os.path.isdir(mdir):\n",
    "        print(f\"⚠️  Skipping (not found): {mdir}\")\n",
    "        continue\n",
    "\n",
    "    tag = os.path.basename(mdir)\n",
    "    per_model_csv = os.path.join(PRED_DIR, f\"pred_{tag}.csv\")\n",
    "\n",
    "    if os.path.isfile(per_model_csv):\n",
    "        print(f\"✓ Already have predictions: {per_model_csv} — skipping\")\n",
    "        continue\n",
    "\n",
    "    df_pred = infer_one_model(mdir, val_df)\n",
    "    df_pred.to_csv(per_model_csv, index=False)\n",
    "    print(f\"Saved per-model predictions → {per_model_csv}\")\n",
    "    del df_pred\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    generated_any = True\n",
    "\n",
    "# Rebuild the combined predictions file from whatever is present\n",
    "csvs = sorted(glob.glob(os.path.join(PRED_DIR, \"pred_*.csv\")))\n",
    "if not csvs:\n",
    "    raise RuntimeError(\"No per-model prediction CSVs found. Generate at least one first.\")\n",
    "\n",
    "pred_all_df = pd.concat([pd.read_csv(p) for p in csvs], ignore_index=True)\n",
    "COMBINED_PRED_CSV = os.path.join(PRED_DIR, \"predictions_all_models.csv\")\n",
    "pred_all_df.to_csv(COMBINED_PRED_CSV, index=False)\n",
    "\n",
    "print(\"\\n✓ Combined predictions rebuilt:\")\n",
    "print(\" • Combined:\", COMBINED_PRED_CSV)\n",
    "print(\" • Included files:\")\n",
    "for p in csvs:\n",
    "    print(\"   -\", os.path.basename(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0986f237-96c0-4328-8011-923567d497f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# METRICS ONLY — fast, resume-safe (BERTScore, NLI, QAGS-lite)\n",
    "# ===================================\n",
    "import os, gc, json, math, time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# --- expected from earlier cells ---\n",
    "# SAVE_DIR, DS_TAG\n",
    "assert os.path.isfile(COMBINED_PRED_CSV), f\"Missing {COMBINED_PRED_CSV}. Run the generation block first.\"\n",
    "\n",
    "# ---------- Fast profile knobs (flip to False for full mode) ----------\n",
    "FAST_MODE = True\n",
    "if FAST_MODE:\n",
    "    BERTSCORE_MODEL = \"roberta-base\"\n",
    "    CHUNK_K, CHUNK_STRIDE = 2, 2\n",
    "    MAX_SUM_SENTS = 8         # cap sentences considered in the summary\n",
    "    NLI_MAXLEN = 256\n",
    "else:\n",
    "    BERTSCORE_MODEL = \"roberta-large\"\n",
    "    CHUNK_K, CHUNK_STRIDE = 3, 1\n",
    "    MAX_SUM_SENTS = None\n",
    "    NLI_MAXLEN = 384\n",
    "\n",
    "# ---------- Environment for stability ----------\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# ---------- Load predictions ----------\n",
    "results_df = pd.read_csv(COMBINED_PRED_CSV)\n",
    "print(\"Loaded predictions:\", results_df.shape)\n",
    "\n",
    "# ---------- spaCy: light sentence splitter + NER pipeline ----------\n",
    "import spacy\n",
    "\n",
    "# cheap sentence splitter (no NER) used for NLI chunking and QAGS sentence ops\n",
    "sent_nlp = spacy.blank(\"en\")\n",
    "if \"sentencizer\" not in sent_nlp.pipe_names:\n",
    "    sent_nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# NER pipeline used only where we truly need NER (QAGS answer candidates)\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\", exclude=[\"parser\",\"attribute_ruler\",\"lemmatizer\",\"tagger\",\"senter\"])\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"spaCy model missing. Run: python -m spacy download en_core_web_sm\") from e\n",
    "# ensure we can do `doc.sents` safely even in this pipeline (no parser)\n",
    "if \"sentencizer\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# ---------- BERTScore (Precision wrt source) ----------\n",
    "try:\n",
    "    from bert_score import score as bertscore_score\n",
    "except ImportError:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"bert-score\"])\n",
    "    from bert_score import score as bertscore_score\n",
    "\n",
    "def bertscore_precision(summary_list, source_list, device: str):\n",
    "    try:\n",
    "        P, R, F1 = bertscore_score(\n",
    "            cands=summary_list,\n",
    "            refs=source_list,\n",
    "            model_type=BERTSCORE_MODEL,\n",
    "            device=device,\n",
    "            lang=\"en\",\n",
    "            rescale_with_baseline=True,\n",
    "        )\n",
    "        return P.cpu().numpy()\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ BERTScore baseline failed ({e}); retrying without baseline…\")\n",
    "        P, R, F1 = bertscore_score(\n",
    "            cands=summary_list,\n",
    "            refs=source_list,\n",
    "            model_type=BERTSCORE_MODEL,\n",
    "            device=device,\n",
    "            lang=\"en\",\n",
    "            rescale_with_baseline=False,\n",
    "        )\n",
    "        return P.cpu().numpy()\n",
    "\n",
    "# ---------- NLI aggregation (SummaC-style), base backbones + fallbacks ----------\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "HF_TOKEN   = os.environ.get(\"HF_TOKEN\", None)\n",
    "LOCAL_ONLY = bool(os.environ.get(\"LOCAL_ONLY\", \"0\") == \"1\")\n",
    "\n",
    "NLI_MODEL_IDS = [\n",
    "    \"textattack/roberta-base-MNLI\",\n",
    "    \"cross-encoder/nli-roberta-base\",     # also 3-way MNLI head\n",
    "    \"typeform/distilbert-base-uncased-mnli\",\n",
    "]\n",
    "_nli_tok = _nli_mod = _nli_id = None\n",
    "for nid in NLI_MODEL_IDS:\n",
    "    try:\n",
    "        _nli_tok = AutoTokenizer.from_pretrained(nid, use_fast=True, token=HF_TOKEN, local_files_only=LOCAL_ONLY)\n",
    "        _nli_mod = AutoModelForSequenceClassification.from_pretrained(nid, token=HF_TOKEN, local_files_only=LOCAL_ONLY).to(DEVICE).eval()\n",
    "        _nli_id = nid\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  NLI model '{nid}' failed: {e}\")\n",
    "if _nli_id is None:\n",
    "    raise RuntimeError(\"No NLI model could be loaded.\")\n",
    "print(\"NLI model:\", _nli_id)\n",
    "CT_IDX, NT_IDX, ET_IDX = 0, 1, 2  # (contradiction, neutral, entailment)\n",
    "\n",
    "def sent_split(text: str) -> List[str]:\n",
    "    return [s.text.strip() for s in sent_nlp(str(text)).sents if s.text.strip()]\n",
    "\n",
    "def chunk_sentences(sents: List[str], k: int, stride: int) -> List[str]:\n",
    "    if not sents:\n",
    "        return []\n",
    "    if MAX_SUM_SENTS is not None:\n",
    "        sents = sents[:MAX_SUM_SENTS]\n",
    "    if len(sents) <= k:\n",
    "        return [\" \".join(sents)]\n",
    "    out = []\n",
    "    for i in range(0, len(sents)-k+1, stride):\n",
    "        out.append(\" \".join(sents[i:i+k]))\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def nli_aggregate(source_text: str, summary_text: str,\n",
    "                  chunk_k: int = CHUNK_K, stride: int = CHUNK_STRIDE, batch_size: int = 16) -> Dict[str,float]:\n",
    "    src_sents = sent_split(source_text)\n",
    "    sum_sents = sent_split(summary_text)\n",
    "    if not sum_sents:\n",
    "        return dict(entail_mean=0.0, contra_mean=0.0, nli_score=0.0,\n",
    "                    support_rate=0.0, contradiction_rate=0.0, n_sum_sents=0)\n",
    "    src_chunks = chunk_sentences(src_sents, k=chunk_k, stride=stride) or [\" \".join(src_sents)]\n",
    "    max_entails, max_contras = [], []\n",
    "    SUP_T, CON_T = 0.5, 0.5\n",
    "    for s in sum_sents:\n",
    "        pairs = [(c, s) for c in src_chunks]\n",
    "        entail_scores, contra_scores = [], []\n",
    "        for i in range(0, len(pairs), batch_size):\n",
    "            batch = pairs[i:i+batch_size]\n",
    "            enc = _nli_tok([p for p,_ in batch], [h for _,h in batch],\n",
    "                           padding=True, truncation=True, max_length=NLI_MAXLEN, return_tensors=\"pt\").to(DEVICE)\n",
    "            probs = torch.softmax(_nli_mod(**enc).logits, dim=-1).detach().cpu().numpy()\n",
    "            entail_scores.extend(probs[:, ET_IDX].tolist())\n",
    "            contra_scores.extend(probs[:, CT_IDX].tolist())\n",
    "        max_entails.append(float(np.max(entail_scores)))\n",
    "        max_contras.append(float(np.max(contra_scores)))\n",
    "    entail_mean = float(np.mean(max_entails))\n",
    "    contra_mean = float(np.mean(max_contras))\n",
    "    support_rate = float(np.mean([m > SUP_T for m in max_entails]))\n",
    "    contradiction_rate = float(np.mean([m > CON_T for m in max_contras]))\n",
    "    nli_score = entail_mean - contra_mean\n",
    "    return dict(\n",
    "        entail_mean=entail_mean,\n",
    "        contra_mean=contra_mean,\n",
    "        nli_score=nli_score,\n",
    "        support_rate=support_rate,\n",
    "        contradiction_rate=contradiction_rate,\n",
    "        n_sum_sents=len(sum_sents),\n",
    "    )\n",
    "\n",
    "# ---------- QAGS-lite (QG + QA) with safe spaCy usage (no noun_chunks) ----------\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "# QG (fallbacks)\n",
    "QG_MODEL_IDS = [\n",
    "    \"iarfmoose/t5-base-question-generator\",\n",
    "    \"mrm8488/t5-base-finetuned-question-generation-ap\",\n",
    "]\n",
    "_qg_tok = _qg_mod = _qg_id = None\n",
    "for qgid in QG_MODEL_IDS:\n",
    "    try:\n",
    "        _qg_tok = AutoTokenizer.from_pretrained(qgid, use_fast=True, token=HF_TOKEN, local_files_only=LOCAL_ONLY)\n",
    "        _qg_mod = AutoModelForSeq2SeqLM.from_pretrained(qgid, token=HF_TOKEN, local_files_only=LOCAL_ONLY).to(DEVICE).eval()\n",
    "        _qg_id = qgid\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  QG model '{qgid}' failed: {e}\")\n",
    "if _qg_id is None:\n",
    "    raise RuntimeError(\"No QG model could be loaded.\")\n",
    "print(\"QG model:\", _qg_id)\n",
    "\n",
    "# QA (fallbacks)\n",
    "QA_MODEL_IDS = [\n",
    "    \"deepset/roberta-base-squad2\",\n",
    "    \"distilbert-base-cased-distilled-squad\",\n",
    "]\n",
    "_qa_tok = _qa_mod = _qa_id = None\n",
    "for qaid in QA_MODEL_IDS:\n",
    "    try:\n",
    "        _qa_tok = AutoTokenizer.from_pretrained(qaid, use_fast=True, token=HF_TOKEN, local_files_only=LOCAL_ONLY)\n",
    "        _qa_mod = AutoModelForQuestionAnswering.from_pretrained(qaid, token=HF_TOKEN, local_files_only=LOCAL_ONLY).to(DEVICE).eval()\n",
    "        _qa_id = qaid\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  QA model '{qaid}' failed: {e}\")\n",
    "if _qa_id is None:\n",
    "    raise RuntimeError(\"No QA model could be loaded.\")\n",
    "print(\"QA model:\", _qa_id)\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    import string\n",
    "    s = s.lower().strip()\n",
    "    s = \"\".join(ch for ch in s if ch not in set(string.punctuation))\n",
    "    return \" \".join(s.split())\n",
    "\n",
    "def squad_f1(pred: str, truth: str) -> float:\n",
    "    pt, tt = normalize_text(pred).split(), normalize_text(truth).split()\n",
    "    if not pt and not tt: return 1.0\n",
    "    if not pt or not tt:  return 0.0\n",
    "    from collections import Counter\n",
    "    common = Counter(pt) & Counter(tt)\n",
    "    overlap = sum(common.values())\n",
    "    if overlap == 0: return 0.0\n",
    "    prec = overlap / len(pt)\n",
    "    rec  = overlap / len(tt)\n",
    "    return 2 * prec * rec / (prec + rec)\n",
    "\n",
    "def _capitalized_phrases(doc):\n",
    "    phrases, buf = [], []\n",
    "    for t in doc:\n",
    "        if t.is_alpha and t.text[0].isupper() and len(t.text) > 1:\n",
    "            buf.append(t.text)\n",
    "        else:\n",
    "            if buf:\n",
    "                phrases.append(\" \".join(buf))\n",
    "                buf = []\n",
    "    if buf:\n",
    "        phrases.append(\" \".join(buf))\n",
    "    return phrases\n",
    "\n",
    "def select_answer_spans_safe(summary: str, max_ans: int = 8):\n",
    "    doc = nlp(summary)\n",
    "    items, seen = [], set()\n",
    "    # Named entities first\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in {\"PERSON\",\"ORG\",\"GPE\",\"LOC\",\"NORP\",\"FAC\",\"PRODUCT\",\"EVENT\",\"WORK_OF_ART\"}:\n",
    "            a = ent.text.strip()\n",
    "            if a and a.lower() not in seen and not any(c.isdigit() for c in a):\n",
    "                seen.add(a.lower())\n",
    "                items.append((a, ent.sent.text.strip() if ent.sent is not None else summary.strip()))\n",
    "                if len(items) >= max_ans:\n",
    "                    return items\n",
    "    # Heuristic capitalized phrases (no parser required)\n",
    "    caps = _capitalized_phrases(doc)\n",
    "    for a in caps:\n",
    "        if 2 <= len(a) <= 80 and a.lower() not in seen and not any(c.isdigit() for c in a):\n",
    "            seen.add(a.lower())\n",
    "            # use nearest sentence via sentencizer\n",
    "            ss = list(doc.sents)\n",
    "            context = ss[0].text.strip() if ss else summary.strip()\n",
    "            items.append((a, context))\n",
    "            if len(items) >= max_ans:\n",
    "                break\n",
    "    return items\n",
    "\n",
    "@torch.no_grad()\n",
    "def qg_make_question(answer: str, context_sent: str, max_new_tokens: int = 48) -> str:\n",
    "    prompt = f\"answer: {answer}  context: {context_sent}\"\n",
    "    enc = _qg_tok([prompt], padding=True, truncation=True, max_length=256, return_tensors=\"pt\").to(DEVICE)\n",
    "    gen_ids = _qg_mod.generate(**enc, max_new_tokens=max_new_tokens, num_beams=4)\n",
    "    return _qg_tok.decode(gen_ids[0], skip_special_tokens=True).strip()\n",
    "\n",
    "@torch.no_grad()\n",
    "def qa_answer(context: str, question: str, max_len: int = 384) -> str:\n",
    "    enc = _qa_tok(question, context, truncation=\"only_second\", max_length=max_len, return_tensors=\"pt\").to(DEVICE)\n",
    "    out = _qa_mod(**enc)\n",
    "    start_idx = int(out.start_logits[0].argmax())\n",
    "    end_idx   = int(out.end_logits[0].argmax())\n",
    "    if end_idx < start_idx: end_idx = start_idx\n",
    "    tokens = enc[\"input_ids\"][0].detach().cpu().tolist()\n",
    "    ans_ids = tokens[start_idx:end_idx+1]\n",
    "    return _qa_tok.decode(ans_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "def qags_example_safe(source: str, summary: str, max_q: int = 8) -> Dict[str, float]:\n",
    "    try:\n",
    "        pairs = select_answer_spans_safe(summary, max_ans=max_q)\n",
    "        if not pairs:\n",
    "            return dict(qags_doc_f1_mean=0.0, qags_doc_f1_median=0.0,\n",
    "                        qags_doc_f1_prop_ge_05=0.0, qags_nq=0)\n",
    "        f1s = []\n",
    "        for ans, sent in pairs:\n",
    "            q = qg_make_question(ans, sent)\n",
    "            if not q: continue\n",
    "            a_doc = qa_answer(source, q)\n",
    "            f1s.append(squad_f1(a_doc, ans))\n",
    "        if not f1s:\n",
    "            return dict(qags_doc_f1_mean=0.0, qags_doc_f1_median=0.0,\n",
    "                        qags_doc_f1_prop_ge_05=0.0, qags_nq=0)\n",
    "        f1s = np.array(f1s, dtype=float)\n",
    "        return dict(\n",
    "            qags_doc_f1_mean=float(f1s.mean()),\n",
    "            qags_doc_f1_median=float(np.median(f1s)),\n",
    "            qags_doc_f1_prop_ge_05=float((f1s >= 0.5).mean()),\n",
    "            qags_nq=int(len(f1s)),\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # never crash the run\n",
    "        return dict(qags_doc_f1_mean=0.0, qags_doc_f1_median=0.0,\n",
    "                    qags_doc_f1_prop_ge_05=0.0, qags_nq=0, qags_error=str(e))\n",
    "\n",
    "# ---------- resume-safe helpers ----------\n",
    "OUT_DIR = GEN_OUT_ROOT\n",
    "SCORE_PART_DIR = os.path.join(OUT_DIR, \"scored_partial_fast\" if FAST_MODE else \"scored_partial_full\")\n",
    "os.makedirs(SCORE_PART_DIR, exist_ok=True)\n",
    "FINAL_PER_EXAMPLE = os.path.join(OUT_DIR, \"factuality_per_example_with_qags.csv\")\n",
    "FINAL_SUMMARY     = os.path.join(OUT_DIR, \"factuality_summary_with_qags.csv\")\n",
    "\n",
    "def part_path(tag): return os.path.join(SCORE_PART_DIR, f\"scored_{tag}.csv\")\n",
    "\n",
    "def model_name_from_dir(p):\n",
    "    base = os.path.basename(p)\n",
    "    if base.endswith(\"_KW\"):\n",
    "        return base.replace(DS_TAG + \"_KW\", \"\"), \"KW\"\n",
    "    elif base.endswith(\"_noKW\"):\n",
    "        return base.replace(DS_TAG + \"_noKW\", \"\"), \"noKW\"\n",
    "    return base, \"?\"\n",
    "\n",
    "# ---------- score per model, saving after each metric ----------\n",
    "all_groups = []\n",
    "for mdir, group in results_df.groupby(\"model_dir\", as_index=False):\n",
    "    mdir_name = mdir if isinstance(mdir, str) else group[\"model_dir\"].iloc[0]\n",
    "    tag = os.path.basename(mdir_name)\n",
    "    out_csv = part_path(tag)\n",
    "\n",
    "    # resume: skip if fully scored already\n",
    "    if os.path.isfile(out_csv):\n",
    "        try:\n",
    "            prev = pd.read_csv(out_csv)\n",
    "            # heuristic: if qags columns are present, consider done\n",
    "            if {\"qags_doc_f1_mean\",\"qags_doc_f1_median\",\"qags_doc_f1_prop_ge_05\",\"qags_nq\"}.issubset(prev.columns):\n",
    "                print(f\"✓ already scored: {tag} → {out_csv}\")\n",
    "                all_groups.append(prev)\n",
    "                continue\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    group = group.copy().reset_index(drop=True)\n",
    "    print(f\"\\n==> Scoring ({'fast' if FAST_MODE else 'full'}): {tag} (n={len(group)})\")\n",
    "\n",
    "    # 1) BERTScore\n",
    "    print(f\"  • BERTScore-P ({BERTSCORE_MODEL}) …\")\n",
    "    P = bertscore_precision(group[\"prediction\"].tolist(), group[\"Text\"].tolist(), device=DEVICE)\n",
    "    group[\"bertscore_precision_src\"] = P\n",
    "    group.to_csv(out_csv, index=False)\n",
    "    print(f\"    ✓ saved (BERTScore) → {out_csv}\")\n",
    "\n",
    "    # 2) NLI\n",
    "    print(f\"  • NLI aggregation (k={CHUNK_K}/stride={CHUNK_STRIDE}, maxlen={NLI_MAXLEN}) …\")\n",
    "    nli_rows = [nli_aggregate(src, hyp) for src, hyp in zip(group[\"Text\"].tolist(), group[\"prediction\"].tolist())]\n",
    "    group = pd.concat([group, pd.DataFrame(nli_rows)], axis=1)\n",
    "    group.to_csv(out_csv, index=False)\n",
    "    print(f\"    ✓ saved (NLI) → {out_csv}\")\n",
    "\n",
    "    # 3) QAGS-lite (safe)\n",
    "    print(\"  • QAGS-lite …\")\n",
    "    qrows = []\n",
    "    for i, (src, hyp) in enumerate(zip(group[\"Text\"].tolist(), group[\"prediction\"].tolist())):\n",
    "        qres = qags_example_safe(src, hyp, max_q=8)\n",
    "        qrows.append(qres)\n",
    "        # periodic flush to disk (every 10)\n",
    "        if (i+1) % 10 == 0:\n",
    "            tmp = pd.concat([group.iloc[:i+1].reset_index(drop=True), pd.DataFrame(qrows)], axis=1)\n",
    "            tmp.to_csv(out_csv, index=False)\n",
    "    group = pd.concat([group, pd.DataFrame(qrows)], axis=1)\n",
    "    group.to_csv(out_csv, index=False)\n",
    "    print(f\"    ✓ saved (QAGS) → {out_csv}\")\n",
    "\n",
    "    all_groups.append(group)\n",
    "    del group, P, nli_rows, qrows\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# ---------- final combine + aggregates ----------\n",
    "scored_df = pd.concat(all_groups, axis=0, ignore_index=True)\n",
    "scored_df.to_csv(FINAL_PER_EXAMPLE, index=False)\n",
    "\n",
    "agg = (scored_df\n",
    "       .assign(model=lambda d: d[\"model_dir\"].apply(lambda x: model_name_from_dir(x)[0]),\n",
    "               kw=lambda d: d[\"model_dir\"].apply(lambda x: model_name_from_dir(x)[1]))\n",
    "       .groupby([\"model\",\"kw\"], as_index=False)\n",
    "       .agg(\n",
    "           n=(\"prediction\",\"count\"),\n",
    "           bertscore_precision_src=(\"bertscore_precision_src\",\"mean\"),\n",
    "           nli_score=(\"nli_score\",\"mean\"),\n",
    "           entail_mean=(\"entail_mean\",\"mean\"),\n",
    "           contra_mean=(\"contra_mean\",\"mean\"),\n",
    "           support_rate=(\"support_rate\",\"mean\"),\n",
    "           contradiction_rate=(\"contradiction_rate\",\"mean\"),\n",
    "           qags_doc_f1_mean=(\"qags_doc_f1_mean\",\"mean\"),\n",
    "           qags_doc_f1_median=(\"qags_doc_f1_median\",\"mean\"),\n",
    "           qags_doc_f1_prop_ge_05=(\"qags_doc_f1_prop_ge_05\",\"mean\"),\n",
    "           qags_nq=(\"qags_nq\",\"mean\"),\n",
    "       ))\n",
    "\n",
    "agg.to_csv(FINAL_SUMMARY, index=False)\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\" • Per-example:\", FINAL_PER_EXAMPLE)\n",
    "print(\" • Summary    :\", FINAL_SUMMARY)\n",
    "\n",
    "try:\n",
    "    display(agg)\n",
    "except Exception:\n",
    "    print(agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1d97f3-4e9e-4e23-82ff-b196e57a99ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# BASELINE: vanilla t5-base (no fine-tuning)\n",
    "# Fast metrics (BERTScore, NLI, QAGS-lite)\n",
    "# ============================================\n",
    "import os, gc, json, time, glob\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "MAX_SOURCE_LEN = 512\n",
    "MAX_TARGET_LEN = 128\n",
    "BATCH_GEN = 8\n",
    "GEN_KWARGS = dict(max_new_tokens=MAX_TARGET_LEN, min_new_tokens=5, num_beams=4, length_penalty=2.0, early_stopping=True)\n",
    "\n",
    "# Fast profile knobs\n",
    "BERTSCORE_MODEL = \"roberta-base\"\n",
    "CHUNK_K, CHUNK_STRIDE = 2, 2        # NLI chunking\n",
    "MAX_SUM_SENTS = 8                   # cap #summary sents examined\n",
    "NLI_MAXLEN = 256\n",
    "\n",
    "# ---------- Environment ----------\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# ---------- Data loading ----------\n",
    "def _ensure_xlrd_for_xls(path: str):\n",
    "    if str(path).lower().endswith(\".xls\"):\n",
    "        try:\n",
    "            import xlrd  # noqa\n",
    "        except Exception:\n",
    "            import subprocess, sys\n",
    "            print(\"Installing xlrd for .xls reading...\")\n",
    "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"xlrd==2.0.1\"], check=True)\n",
    "\n",
    "def load_dataframe(path: str) -> pd.DataFrame:\n",
    "    p = Path(path)\n",
    "    if p.suffix.lower() == \".csv\":\n",
    "        return pd.read_csv(path)\n",
    "    elif p.suffix.lower() == \".xlsx\":\n",
    "        return pd.read_excel(path, engine=\"openpyxl\")\n",
    "    elif p.suffix.lower() == \".xls\":\n",
    "        _ensure_xlrd_for_xls(path)\n",
    "        return pd.read_excel(path, engine=\"xlrd\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {p.suffix}\")\n",
    "\n",
    "df_full = load_dataframe(DATA_X_PATH)\n",
    "assert {\"Text\",\"Abstractive\"}.issubset(df_full.columns), \"Need 'Text' and 'Abstractive'.\"\n",
    "df_full = df_full.dropna(subset=[\"Text\",\"Abstractive\"]).reset_index(drop=True)\n",
    "\n",
    "# same 90/10 split used elsewhere\n",
    "from sklearn.model_selection import train_test_split\n",
    "_, val_df = train_test_split(df_full, test_size=0.10, random_state=SEED, shuffle=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "print(\"Eval size:\", len(val_df))\n",
    "\n",
    "# ---------- Baseline generation with vanilla t5-base ----------\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "@torch.no_grad()\n",
    "def batched_generate(model, tok, inputs: List[str]) -> List[str]:\n",
    "    outs = []\n",
    "    for i in range(0, len(inputs), BATCH_GEN):\n",
    "        batch = inputs[i:i+BATCH_GEN]\n",
    "        enc = tok(batch, padding=True, truncation=True, max_length=MAX_SOURCE_LEN, return_tensors=\"pt\").to(DEVICE)\n",
    "        gen_ids = model.generate(**enc, **GEN_KWARGS)\n",
    "        outs.extend(tok.batch_decode(gen_ids, skip_special_tokens=True))\n",
    "    return outs\n",
    "\n",
    "BASE_TAG = \"t5-base_zero_shot\"  # clear, unique tag for baseline\n",
    "GEN_OUT_ROOT = os.path.join(SAVE_DIR, \"eval_cs\")\n",
    "PRED_DIR = os.path.join(GEN_OUT_ROOT, \"predictions\")\n",
    "os.makedirs(PRED_DIR, exist_ok=True)\n",
    "per_model_csv = os.path.join(PRED_DIR, f\"pred_{BASE_TAG}.csv\")\n",
    "\n",
    "if os.path.isfile(per_model_csv):\n",
    "    print(f\"✓ Baseline predictions already exist: {per_model_csv}\")\n",
    "else:\n",
    "    print(\"\\n==> Generating baseline with vanilla t5-base…\")\n",
    "    tok = AutoTokenizer.from_pretrained(\"t5-base\", use_fast=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\").to(DEVICE).eval()\n",
    "    # T5 works better zero-shot with the task prefix:\n",
    "    inputs = [f\"summarize: {t}\" for t in val_df[\"Text\"].tolist()]\n",
    "    preds = batched_generate(model, tok, inputs)\n",
    "    base_df = val_df[[\"Text\",\"Abstractive\"]].copy()\n",
    "    base_df[\"topics_used\"] = \"\"     # noKW baseline\n",
    "    base_df[\"prediction\"]  = preds\n",
    "    base_df[\"model_dir\"]   = BASE_TAG\n",
    "    base_df[\"kw_like\"]     = False\n",
    "    base_df.to_csv(per_model_csv, index=False)\n",
    "    print(f\"✓ Saved baseline predictions → {per_model_csv}\")\n",
    "    del model, tok, preds, base_df\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Rebuild combined predictions if others exist\n",
    "combined_csv = os.path.join(PRED_DIR, \"predictions_all_models.csv\")\n",
    "try:\n",
    "    # gather all per-model pred CSVs (including baseline)\n",
    "    csvs = sorted(glob.glob(os.path.join(PRED_DIR, \"pred_*.csv\")))\n",
    "    pred_all_df = pd.concat([pd.read_csv(p) for p in csvs], ignore_index=True)\n",
    "    pred_all_df.to_csv(combined_csv, index=False)\n",
    "    print(f\"✓ Combined predictions rebuilt → {combined_csv}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not rebuild combined predictions: {e}\")\n",
    "\n",
    "# ---------- spaCy setup for sentence ops & light NER ----------\n",
    "import spacy\n",
    "# cheap splitter for NLI/QAGS sentence ops\n",
    "sent_nlp = spacy.blank(\"en\")\n",
    "if \"sentencizer\" not in sent_nlp.pipe_names:\n",
    "    sent_nlp.add_pipe(\"sentencizer\")\n",
    "# light NER pipeline (no parser); add sentencizer to ensure doc.sents\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\", exclude=[\"parser\",\"attribute_ruler\",\"lemmatizer\",\"tagger\",\"senter\"])\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"spaCy model missing. Run: python -m spacy download en_core_web_sm\") from e\n",
    "if \"sentencizer\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "def sent_split(text: str) -> List[str]:\n",
    "    return [s.text.strip() for s in sent_nlp(str(text)).sents if s.text.strip()]\n",
    "\n",
    "# ---------- BERTScore (Precision wrt source) ----------\n",
    "try:\n",
    "    from bert_score import score as bertscore_score\n",
    "except ImportError:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"bert-score\"])\n",
    "    from bert_score import score as bertscore_score\n",
    "\n",
    "def bertscore_precision(summary_list, source_list, device: str):\n",
    "    try:\n",
    "        P, R, F1 = bertscore_score(\n",
    "            cands=summary_list, refs=source_list,\n",
    "            model_type=BERTSCORE_MODEL, device=device, lang=\"en\",\n",
    "            rescale_with_baseline=True,\n",
    "        )\n",
    "        return P.cpu().numpy()\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ BERTScore baseline failed ({e}); retrying without baseline…\")\n",
    "        P, R, F1 = bertscore_score(\n",
    "            cands=summary_list, refs=source_list,\n",
    "            model_type=BERTSCORE_MODEL, device=device, lang=\"en\",\n",
    "            rescale_with_baseline=False,\n",
    "        )\n",
    "        return P.cpu().numpy()\n",
    "\n",
    "# ---------- NLI aggregation (fast) ----------\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "HF_TOKEN   = os.environ.get(\"HF_TOKEN\", None)\n",
    "LOCAL_ONLY = bool(os.environ.get(\"LOCAL_ONLY\", \"0\") == \"1\")\n",
    "\n",
    "NLI_MODEL_IDS = [\n",
    "    \"textattack/roberta-base-MNLI\",\n",
    "    \"cross-encoder/nli-roberta-base\",\n",
    "    \"typeform/distilbert-base-uncased-mnli\",\n",
    "]\n",
    "_nli_tok = _nli_mod = _nli_id = None\n",
    "for nid in NLI_MODEL_IDS:\n",
    "    try:\n",
    "        _nli_tok = AutoTokenizer.from_pretrained(nid, use_fast=True, token=HF_TOKEN, local_files_only=LOCAL_ONLY)\n",
    "        _nli_mod = AutoModelForSequenceClassification.from_pretrained(nid, token=HF_TOKEN, local_files_only=LOCAL_ONLY).to(DEVICE).eval()\n",
    "        _nli_id = nid\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  NLI model '{nid}' failed: {e}\")\n",
    "if _nli_id is None:\n",
    "    raise RuntimeError(\"No NLI model could be loaded.\")\n",
    "print(\"NLI model:\", _nli_id)\n",
    "CT_IDX, NT_IDX, ET_IDX = 0, 1, 2\n",
    "\n",
    "def chunk_sentences(sents: List[str], k: int, stride: int) -> List[str]:\n",
    "    if not sents: return []\n",
    "    if MAX_SUM_SENTS is not None:\n",
    "        sents = sents[:MAX_SUM_SENTS]\n",
    "    if len(sents) <= k:\n",
    "        return [\" \".join(sents)]\n",
    "    return [\" \".join(sents[i:i+k]) for i in range(0, len(sents)-k+1, stride)]\n",
    "\n",
    "@torch.no_grad()\n",
    "def nli_aggregate(source_text: str, summary_text: str,\n",
    "                  k: int = CHUNK_K, stride: int = CHUNK_STRIDE, batch_size: int = 16) -> Dict[str,float]:\n",
    "    src_sents = sent_split(source_text)\n",
    "    sum_sents = sent_split(summary_text)\n",
    "    if not sum_sents:\n",
    "        return dict(entail_mean=0.0, contra_mean=0.0, nli_score=0.0,\n",
    "                    support_rate=0.0, contradiction_rate=0.0, n_sum_sents=0)\n",
    "    src_chunks = chunk_sentences(src_sents, k=k, stride=stride) or [\" \".join(src_sents)]\n",
    "    max_entails, max_contras = [], []\n",
    "    SUP_T, CON_T = 0.5, 0.5\n",
    "    for s in sum_sents:\n",
    "        pairs = [(c, s) for c in src_chunks]\n",
    "        entail_scores, contra_scores = [], []\n",
    "        for i in range(0, len(pairs), batch_size):\n",
    "            batch = pairs[i:i+batch_size]\n",
    "            enc = _nli_tok([p for p,_ in batch], [h for _,h in batch],\n",
    "                           padding=True, truncation=True, max_length=NLI_MAXLEN, return_tensors=\"pt\").to(DEVICE)\n",
    "            probs = torch.softmax(_nli_mod(**enc).logits, dim=-1).detach().cpu().numpy()\n",
    "            entail_scores.extend(probs[:, ET_IDX].tolist())\n",
    "            contra_scores.extend(probs[:, CT_IDX].tolist())\n",
    "        max_entails.append(float(np.max(entail_scores)))\n",
    "        max_contras.append(float(np.max(contra_scores)))\n",
    "    entail_mean = float(np.mean(max_entails))\n",
    "    contra_mean = float(np.mean(max_contras))\n",
    "    support_rate = float(np.mean([m > SUP_T for m in max_entails]))\n",
    "    contradiction_rate = float(np.mean([m > CON_T for m in max_contras]))\n",
    "    nli_score = entail_mean - contra_mean\n",
    "    return dict(entail_mean=entail_mean, contra_mean=contra_mean, nli_score=nli_score,\n",
    "                support_rate=support_rate, contradiction_rate=contradiction_rate, n_sum_sents=len(sum_sents))\n",
    "\n",
    "# ---------- QAGS-lite (safe: no parser dependency) ----------\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "# QG\n",
    "QG_MODEL_IDS = [\"iarfmoose/t5-base-question-generator\", \"mrm8488/t5-base-finetuned-question-generation-ap\"]\n",
    "_qg_tok = _qg_mod = _qg_id = None\n",
    "for qgid in QG_MODEL_IDS:\n",
    "    try:\n",
    "        _qg_tok = AutoTokenizer.from_pretrained(qgid, use_fast=True, token=HF_TOKEN, local_files_only=LOCAL_ONLY)\n",
    "        _qg_mod = AutoModelForSeq2SeqLM.from_pretrained(qgid, token=HF_TOKEN, local_files_only=LOCAL_ONLY).to(DEVICE).eval()\n",
    "        _qg_id = qgid\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  QG model '{qgid}' failed: {e}\")\n",
    "if _qg_id is None:\n",
    "    raise RuntimeError(\"No QG model could be loaded.\")\n",
    "print(\"QG model:\", _qg_id)\n",
    "\n",
    "# QA\n",
    "QA_MODEL_IDS = [\"deepset/roberta-base-squad2\", \"distilbert-base-cased-distilled-squad\"]\n",
    "_qa_tok = _qa_mod = _qa_id = None\n",
    "for qaid in QA_MODEL_IDS:\n",
    "    try:\n",
    "        _qa_tok = AutoTokenizer.from_pretrained(qaid, use_fast=True, token=HF_TOKEN, local_files_only=LOCAL_ONLY)\n",
    "        _qa_mod = AutoModelForQuestionAnswering.from_pretrained(qaid, token=HF_TOKEN, local_files_only=LOCAL_ONLY).to(DEVICE).eval()\n",
    "        _qa_id = qaid\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  QA model '{qaid}' failed: {e}\")\n",
    "if _qa_id is None:\n",
    "    raise RuntimeError(\"No QA model could be loaded.\")\n",
    "print(\"QA model:\", _qa_id)\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    import string\n",
    "    s = s.lower().strip()\n",
    "    s = \"\".join(ch for ch in s if ch not in set(string.punctuation))\n",
    "    return \" \".join(s.split())\n",
    "\n",
    "def squad_f1(pred: str, truth: str) -> float:\n",
    "    pt, tt = normalize_text(pred).split(), normalize_text(truth).split()\n",
    "    if not pt and not tt: return 1.0\n",
    "    if not pt or not tt:  return 0.0\n",
    "    from collections import Counter\n",
    "    common = Counter(pt) & Counter(tt)\n",
    "    overlap = sum(common.values())\n",
    "    if overlap == 0: return 0.0\n",
    "    prec = overlap / len(pt); rec = overlap / len(tt)\n",
    "    return 2*prec*rec/(prec+rec)\n",
    "\n",
    "def _capitalized_phrases(doc):\n",
    "    phrases, buf = [], []\n",
    "    for t in doc:\n",
    "        if t.is_alpha and t.text[0].isupper() and len(t.text) > 1:\n",
    "            buf.append(t.text)\n",
    "        else:\n",
    "            if buf:\n",
    "                phrases.append(\" \".join(buf))\n",
    "                buf = []\n",
    "    if buf: phrases.append(\" \".join(buf))\n",
    "    return phrases\n",
    "\n",
    "def select_answer_spans_safe(summary: str, max_ans: int = 8):\n",
    "    doc = nlp(summary)\n",
    "    items, seen = [], set()\n",
    "    # NER first\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in {\"PERSON\",\"ORG\",\"GPE\",\"LOC\",\"NORP\",\"FAC\",\"PRODUCT\",\"EVENT\",\"WORK_OF_ART\"}:\n",
    "            a = ent.text.strip()\n",
    "            if a and a.lower() not in seen and not any(c.isdigit() for c in a):\n",
    "                seen.add(a.lower())\n",
    "                context = ent.sent.text.strip() if ent.sent is not None else summary.strip()\n",
    "                items.append((a, context))\n",
    "                if len(items) >= max_ans: return items\n",
    "    # Heuristic capitalized phrases\n",
    "    caps = _capitalized_phrases(doc)\n",
    "    ss = list(doc.sents)\n",
    "    default_context = (ss[0].text.strip() if ss else summary.strip())\n",
    "    for a in caps:\n",
    "        if 2 <= len(a) <= 80 and a.lower() not in seen and not any(c.isdigit() for c in a):\n",
    "            seen.add(a.lower())\n",
    "            items.append((a, default_context))\n",
    "            if len(items) >= max_ans: break\n",
    "    return items\n",
    "\n",
    "@torch.no_grad()\n",
    "def qg_make_question(answer: str, context_sent: str, max_new_tokens: int = 48) -> str:\n",
    "    prompt = f\"answer: {answer}  context: {context_sent}\"\n",
    "    enc = _qg_tok([prompt], padding=True, truncation=True, max_length=256, return_tensors=\"pt\").to(DEVICE)\n",
    "    gen_ids = _qg_mod.generate(**enc, max_new_tokens=max_new_tokens, num_beams=4)\n",
    "    return _qg_tok.decode(gen_ids[0], skip_special_tokens=True).strip()\n",
    "\n",
    "@torch.no_grad()\n",
    "def qa_answer(context: str, question: str, max_len: int = 384) -> str:\n",
    "    enc = _qa_tok(question, context, truncation=\"only_second\", max_length=max_len, return_tensors=\"pt\").to(DEVICE)\n",
    "    out = _qa_mod(**enc)\n",
    "    start_idx = int(out.start_logits[0].argmax()); end_idx = int(out.end_logits[0].argmax())\n",
    "    if end_idx < start_idx: end_idx = start_idx\n",
    "    tokens = enc[\"input_ids\"][0].detach().cpu().tolist()\n",
    "    ans_ids = tokens[start_idx:end_idx+1]\n",
    "    return _qa_tok.decode(ans_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "def qags_example_safe(source: str, summary: str, max_q: int = 8) -> Dict[str, float]:\n",
    "    try:\n",
    "        pairs = select_answer_spans_safe(summary, max_ans=max_q)\n",
    "        if not pairs:\n",
    "            return dict(qags_doc_f1_mean=0.0, qags_doc_f1_median=0.0,\n",
    "                        qags_doc_f1_prop_ge_05=0.0, qags_nq=0)\n",
    "        f1s = []\n",
    "        for ans, sent in pairs:\n",
    "            q = qg_make_question(ans, sent)\n",
    "            if not q: continue\n",
    "            a_doc = qa_answer(source, q)\n",
    "            f1s.append(squad_f1(a_doc, ans))\n",
    "        if not f1s:\n",
    "            return dict(qags_doc_f1_mean=0.0, qags_doc_f1_median=0.0,\n",
    "                        qags_doc_f1_prop_ge_05=0.0, qags_nq=0)\n",
    "        f1s = np.array(f1s, dtype=float)\n",
    "        return dict(\n",
    "            qags_doc_f1_mean=float(f1s.mean()),\n",
    "            qags_doc_f1_median=float(np.median(f1s)),\n",
    "            qags_doc_f1_prop_ge_05=float((f1s >= 0.5).mean()),\n",
    "            qags_nq=int(len(f1s)),\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return dict(qags_doc_f1_mean=0.0, qags_doc_f1_median=0.0,\n",
    "                    qags_doc_f1_prop_ge_05=0.0, qags_nq=0, qags_error=str(e))\n",
    "\n",
    "# ---------- Load baseline predictions ----------\n",
    "pred_df = pd.read_csv(per_model_csv)\n",
    "print(\"Loaded baseline predictions:\", pred_df.shape)\n",
    "\n",
    "# ---------- Metrics: resume-safe, save after each metric ----------\n",
    "SCORE_PART_DIR = os.path.join(GEN_OUT_ROOT, \"scored_partial_fast\")\n",
    "os.makedirs(SCORE_PART_DIR, exist_ok=True)\n",
    "baseline_scored_csv = os.path.join(SCORE_PART_DIR, f\"scored_{BASE_TAG}.csv\")\n",
    "\n",
    "# Resume if possible\n",
    "if os.path.isfile(baseline_scored_csv):\n",
    "    try:\n",
    "        tmp = pd.read_csv(baseline_scored_csv)\n",
    "        already_have_qags = {\"qags_doc_f1_mean\",\"qags_doc_f1_median\",\"qags_doc_f1_prop_ge_05\",\"qags_nq\"}.issubset(tmp.columns)\n",
    "        if already_have_qags:\n",
    "            print(f\"✓ Baseline already fully scored → {baseline_scored_csv}\")\n",
    "        pred_df = tmp\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# 1) BERTScore\n",
    "if \"bertscore_precision_src\" not in pred_df.columns:\n",
    "    print(\"  • BERTScore-P (roberta-base) …\")\n",
    "    P = bertscore_precision(pred_df[\"prediction\"].tolist(), pred_df[\"Text\"].tolist(), device=DEVICE)\n",
    "    pred_df[\"bertscore_precision_src\"] = P\n",
    "    pred_df.to_csv(baseline_scored_csv, index=False)\n",
    "    print(f\"    ✓ saved (BERTScore) → {baseline_scored_csv}\")\n",
    "\n",
    "# 2) NLI\n",
    "if \"nli_score\" not in pred_df.columns:\n",
    "    print(f\"  • NLI aggregation (k={CHUNK_K}/stride={CHUNK_STRIDE}, maxlen={NLI_MAXLEN}) …\")\n",
    "    nli_rows = [nli_aggregate(src, hyp) for src, hyp in zip(pred_df[\"Text\"].tolist(), pred_df[\"prediction\"].tolist())]\n",
    "    pred_df = pd.concat([pred_df, pd.DataFrame(nli_rows)], axis=1)\n",
    "    pred_df.to_csv(baseline_scored_csv, index=False)\n",
    "    print(f\"    ✓ saved (NLI) → {baseline_scored_csv}\")\n",
    "\n",
    "# 3) QAGS-lite\n",
    "if \"qags_doc_f1_mean\" not in pred_df.columns:\n",
    "    print(\"  • QAGS-lite …\")\n",
    "    qrows = []\n",
    "    for i, (src, hyp) in enumerate(zip(pred_df[\"Text\"].tolist(), pred_df[\"prediction\"].tolist())):\n",
    "        qrows.append(qags_example_safe(src, hyp, max_q=8))\n",
    "        if (i+1) % 10 == 0:\n",
    "            tmp = pd.concat([pred_df.iloc[:i+1].reset_index(drop=True), pd.DataFrame(qrows)], axis=1)\n",
    "            tmp.to_csv(baseline_scored_csv, index=False)\n",
    "    pred_df = pd.concat([pred_df, pd.DataFrame(qrows)], axis=1)\n",
    "    pred_df.to_csv(baseline_scored_csv, index=False)\n",
    "    print(f\"    ✓ saved (QAGS) → {baseline_scored_csv}\")\n",
    "\n",
    "# ---------- Aggregate just the baseline ----------\n",
    "def model_name_from_dir(p):\n",
    "    base = os.path.basename(str(p))\n",
    "    if base.endswith(\"_KW\"):\n",
    "        return base.replace(DS_TAG + \"_KW\", \"\"), \"KW\"\n",
    "    elif base.endswith(\"_noKW\"):\n",
    "        return base.replace(DS_TAG + \"_noKW\", \"\"), \"noKW\"\n",
    "    return base, \"?\"\n",
    "\n",
    "baseline_agg = (pred_df\n",
    "    .assign(model=lambda d: d[\"model_dir\"].apply(lambda x: model_name_from_dir(x)[0]),\n",
    "            kw=lambda d: d[\"model_dir\"].apply(lambda x: model_name_from_dir(x)[1]))\n",
    "    .groupby([\"model\",\"kw\"], as_index=False)\n",
    "    .agg(\n",
    "        n=(\"prediction\",\"count\"),\n",
    "        bertscore_precision_src=(\"bertscore_precision_src\",\"mean\"),\n",
    "        nli_score=(\"nli_score\",\"mean\"),\n",
    "        entail_mean=(\"entail_mean\",\"mean\"),\n",
    "        contra_mean=(\"contra_mean\",\"mean\"),\n",
    "        support_rate=(\"support_rate\",\"mean\"),\n",
    "        contradiction_rate=(\"contradiction_rate\",\"mean\"),\n",
    "        qags_doc_f1_mean=(\"qags_doc_f1_mean\",\"mean\"),\n",
    "        qags_doc_f1_median=(\"qags_doc_f1_median\",\"mean\"),\n",
    "        qags_doc_f1_prop_ge_05=(\"qags_doc_f1_prop_ge_05\",\"mean\"),\n",
    "        qags_nq=(\"qags_nq\",\"mean\"),\n",
    "    ))\n",
    "\n",
    "# Save per-example and baseline summary\n",
    "FINAL_PER_EXAMPLE = os.path.join(GEN_OUT_ROOT, f\"factuality_per_example_{BASE_TAG}.csv\")\n",
    "FINAL_SUMMARY     = os.path.join(GEN_OUT_ROOT, f\"factuality_summary_{BASE_TAG}.csv\")\n",
    "pred_df.to_csv(FINAL_PER_EXAMPLE, index=False)\n",
    "baseline_agg.to_csv(FINAL_SUMMARY, index=False)\n",
    "\n",
    "print(\"\\nSaved baseline files:\")\n",
    "print(\" • Per-example:\", FINAL_PER_EXAMPLE)\n",
    "print(\" • Summary    :\", FINAL_SUMMARY)\n",
    "\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    display(baseline_agg)\n",
    "except Exception:\n",
    "    print(baseline_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566a2541-a72e-4518-90fc-e315173f978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# LEXICAL METRICS (ROUGE-1/2/L, BLEU)\n",
    "# Per-example + per-model summary\n",
    "# =========================================\n",
    "import os, sys, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "GEN_OUT_ROOT = os.path.join(SAVE_DIR, \"eval_cs\")\n",
    "PRED_DIR     = os.path.join(GEN_OUT_ROOT, \"predictions\")\n",
    "COMBINED_PRED_CSV = os.path.join(PRED_DIR, \"predictions_all_models.csv\")\n",
    "assert os.path.isfile(COMBINED_PRED_CSV), f\"Missing {COMBINED_PRED_CSV}. Run your generation step first.\"\n",
    "\n",
    "results_df = pd.read_csv(COMBINED_PRED_CSV)\n",
    "print(\"Loaded predictions:\", results_df.shape)\n",
    "assert {\"Text\",\"Abstractive\",\"prediction\",\"model_dir\"}.issubset(results_df.columns)\n",
    "\n",
    "# ------------------------\n",
    "# Ensure deps are present\n",
    "# ------------------------\n",
    "try:\n",
    "    from rouge_score import rouge_scorer\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"rouge-score\"])\n",
    "    from rouge_score import rouge_scorer\n",
    "\n",
    "try:\n",
    "    import sacrebleu\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"sacrebleu\"])\n",
    "    import sacrebleu\n",
    "\n",
    "# ------------------------\n",
    "# Metric helpers\n",
    "# ------------------------\n",
    "# ROUGE scorer: use stemming; Lsum = sentence-level L with newlines split by scorer\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeLsum\"], use_stemmer=True)\n",
    "\n",
    "def safe_text(x):\n",
    "    return \"\" if not isinstance(x, str) else x.strip()\n",
    "\n",
    "def compute_rouge(ref: str, hyp: str):\n",
    "    ref, hyp = safe_text(ref), safe_text(hyp)\n",
    "    scores = scorer.score(ref, hyp)\n",
    "    # Return F1 for 1/2/L (common in summarization)\n",
    "    return (\n",
    "        float(scores[\"rouge1\"].fmeasure),\n",
    "        float(scores[\"rouge2\"].fmeasure),\n",
    "        float(scores[\"rougeLsum\"].fmeasure),\n",
    "    )\n",
    "\n",
    "def compute_bleu(ref: str, hyp: str):\n",
    "    ref, hyp = safe_text(ref), safe_text(hyp)\n",
    "    # sacreBLEU sentence score in [0, 100]\n",
    "    try:\n",
    "        return float(sacrebleu.sentence_bleu(hyp, [ref], smooth_method=\"exp\").score)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "# ------------------------\n",
    "# Compute per-example\n",
    "# ------------------------\n",
    "R1, R2, RL, BL = [], [], [], []\n",
    "for ref, hyp in zip(results_df[\"Abstractive\"].tolist(), results_df[\"prediction\"].tolist()):\n",
    "    r1, r2, rl = compute_rouge(ref, hyp)\n",
    "    R1.append(r1); R2.append(r2); RL.append(rl)\n",
    "    BL.append(compute_bleu(ref, hyp))\n",
    "\n",
    "lex_df = results_df.copy()\n",
    "lex_df[\"rouge1_f\"] = R1\n",
    "lex_df[\"rouge2_f\"] = R2\n",
    "lex_df[\"rougeL_f\"] = RL\n",
    "lex_df[\"bleu\"]     = BL   # sacreBLEU sentence-level, 0–100\n",
    "\n",
    "# ------------------------\n",
    "# Save per-example\n",
    "# ------------------------\n",
    "os.makedirs(GEN_OUT_ROOT, exist_ok=True)\n",
    "per_example_csv = os.path.join(GEN_OUT_ROOT, \"lexical_per_example.csv\")\n",
    "lex_df.to_csv(per_example_csv, index=False)\n",
    "print(\"✓ Saved per-example lexical metrics →\", per_example_csv)\n",
    "\n",
    "# ------------------------\n",
    "# Aggregate by (model, KW/noKW)\n",
    "# ------------------------\n",
    "def model_name_from_dir(p):\n",
    "    base = os.path.basename(str(p))\n",
    "    if base.endswith(\"_KW\"):\n",
    "        return base.replace(DS_TAG + \"_KW\", \"\"), \"KW\"\n",
    "    elif base.endswith(\"_noKW\"):\n",
    "        return base.replace(DS_TAG + \"_noKW\", \"\"), \"noKW\"\n",
    "    # allow custom tags (e.g., zero-shot baseline)\n",
    "    return base, (\"KW\" if base.lower().endswith(\"kw\") else (\"noKW\" if \"nokw\" in base.lower() else \"?\"))\n",
    "\n",
    "agg = (lex_df\n",
    "       .assign(model=lambda d: d[\"model_dir\"].apply(lambda x: model_name_from_dir(x)[0]),\n",
    "               kw=lambda d: d[\"model_dir\"].apply(lambda x: model_name_from_dir(x)[1]))\n",
    "       .groupby([\"model\",\"kw\"], as_index=False)\n",
    "       .agg(\n",
    "           n=(\"prediction\",\"count\"),\n",
    "           rouge1_f=(\"rouge1_f\",\"mean\"),\n",
    "           rouge2_f=(\"rouge2_f\",\"mean\"),\n",
    "           rougeL_f=(\"rougeL_f\",\"mean\"),\n",
    "           bleu=(\"bleu\",\"mean\"),   # average of sentence-level BLEU (0–100)\n",
    "       ))\n",
    "\n",
    "summary_csv = os.path.join(GEN_OUT_ROOT, \"lexical_summary.csv\")\n",
    "agg.to_csv(summary_csv, index=False)\n",
    "print(\"✓ Saved summary lexical metrics →\", summary_csv)\n",
    "\n",
    "# Optional: show summary\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    display(agg)\n",
    "except Exception:\n",
    "    print(agg)\n",
    "\n",
    "# Cleanup\n",
    "del results_df, lex_df, agg\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
