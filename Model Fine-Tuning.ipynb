{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d51330-5814-439e-9e59-aae16b96a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Config ----------------\n",
    "DATA_CSV_PATH = \"./Datasets/CS_Summ.xlsx\"     # <-- set this to your CS-Summ CSV\n",
    "SAVE_DIR      = \"./Models\"                    # where to save checkpoints\n",
    "TOPICS_DF_DIR = \"./Keywords\"                  # where keyword cache is stored\n",
    "DS_TAG        = \"_cs\"                         # dataset tag in saved folder names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629a8526-0fca-4fb6-aef6-fd56c8963dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Silence tokenizers + tame BLAS/OpenMP BEFORE any imports ---\n",
    "import os\n",
    "\n",
    "# This both disables tokenizers' thread pool and silences the fork warning.\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Keep BLAS/OpenMP single-threaded (prevents the OpenBLAS warning & potential hangs)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "# Reduce OpenBLAS chatter (won’t fix issues, just hides logs)\n",
    "os.environ[\"OPENBLAS_VERBOSE\"] = \"0\"\n",
    "\n",
    "# Optional: quiet HF logs (cosmetic)\n",
    "import logging, warnings\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"datasets\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(\"Silencing set: TOKENIZERS_PARALLELISM=false, *NUM_THREADS=1, OPENBLAS_VERBOSE=0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f7c3b8-a96d-4293-b375-e6d7fabbdd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fair/equal hyperparameters across models\n",
    "SEED = 42\n",
    "NUM_EPOCHS = 10                 \n",
    "TRAIN_BATCH_SIZE = 8\n",
    "EVAL_BATCH_SIZE  = 8\n",
    "LEARNING_RATE = 5e-5\n",
    "MAX_SOURCE_LEN = 512\n",
    "MAX_TARGET_LEN = 128\n",
    "GRAD_ACCUM_STEPS = 1\n",
    "WARMUP_RATIO = 0.03\n",
    "ES_PATIENCE = 2\n",
    "\n",
    "# Keyword special tokens\n",
    "SPECIAL_TOKENS = {\"additional_special_tokens\": [\"<TEXT>\", \"<TOPIC>\"]}\n",
    "USE_KEYWORD_CONFIGS = [False, True]  # train noKW and KW variants\n",
    "\n",
    "# ---------------- Repro ----------------\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "set_seed(SEED)\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(\"Torch:\", torch.__version__, \"| CUDA:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# ---------------- spaCy NER ----------------\n",
    "import spacy\n",
    "try:\n",
    "    # Keep the pipeline lean; we only need NER for keyword cues\n",
    "    nlp = spacy.load(\"en_core_web_sm\", exclude=[\"parser\",\"attribute_ruler\",\"lemmatizer\",\"tagger\",\"senter\"])\n",
    "    nlp.max_length = 2_000_000\n",
    "    print(\"spaCy loaded:\", nlp.pipe_names)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"spaCy model not installed. Run: python -m spacy download en_core_web_sm\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad47d342-d5f9-4fbb-9c7c-1571bc991737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# B) Load & split data\n",
    "# ===============================\n",
    "df = pd.read_excel(DATA_CSV_PATH)\n",
    "assert {\"Text\",\"Abstractive\"}.issubset(df.columns), \\\n",
    "    \"CSV must have 'Text' and 'Abstractive' columns.\"\n",
    "\n",
    "# basic cleanup\n",
    "df = df.dropna(subset=[\"Text\",\"Abstractive\"]).reset_index(drop=True)\n",
    "\n",
    "# 90/10 split\n",
    "train_df, val_df = train_test_split(df, test_size=0.10, random_state=SEED, shuffle=True)\n",
    "print(f\"Train size: {len(train_df)} | Val size: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dc3711-a2ee-40fe-b78b-a73966e35051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# C) Topic cache settings (no file re-read)\n",
    "# ===============================\n",
    "import os, numpy as np, pandas as pd\n",
    "\n",
    "os.makedirs(TOPICS_DF_DIR, exist_ok=True)\n",
    "TOPICS_CACHE_PARQUET = os.path.join(TOPICS_DF_DIR, f\"topics_cache{DS_TAG}.parquet\")\n",
    "TOPICS_CACHE_CSV     = os.path.join(TOPICS_DF_DIR, f\"topics_cache{DS_TAG}.csv\")\n",
    "\n",
    "# If you want to force a re-extraction once, flip to True\n",
    "FORCE_RECOMPUTE_TOPICS = True\n",
    "MAX_KW = 10  # top entities to keep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433b6d12-184f-4255-855c-ec45820024ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# D) Topic extraction (once) and caching — NO re-reading of XLS/CSV\n",
    "# ===============================\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def _extract_keywords_spacy(text: str, nlp, max_kw: int = MAX_KW) -> str:\n",
    "    \"\"\"\n",
    "    NER-based cues (deduped, no digits). Returns a single string 'k1 ; k2 ; ...'.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    out, seen = [], set()\n",
    "    for ent in doc.ents:\n",
    "        tok = ent.text.strip()\n",
    "        if not tok:\n",
    "            continue\n",
    "        if any(c.isdigit() for c in tok):\n",
    "            continue\n",
    "        key = tok.lower()\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            out.append(tok)\n",
    "        if len(out) >= max_kw:\n",
    "            break\n",
    "    return \" ; \".join(out)\n",
    "\n",
    "# Work strictly from the df you already loaded earlier\n",
    "df_orig = df.copy()\n",
    "df_orig = df_orig.dropna(subset=[\"Text\",\"Abstractive\"])\n",
    "\n",
    "# we’ll key the cache by the **original row index** (orig_idx)\n",
    "df_orig = df_orig.reset_index().rename(columns={\"index\": \"orig_idx\"})\n",
    "\n",
    "use_cache = False\n",
    "if (not FORCE_RECOMPUTE_TOPICS) and os.path.exists(TOPICS_CACHE_PARQUET):\n",
    "    try:\n",
    "        cached = pd.read_parquet(TOPICS_CACHE_PARQUET)\n",
    "        # cache valid if it covers all rows once\n",
    "        if {\"orig_idx\",\"topics\"}.issubset(cached.columns) and cached[\"orig_idx\"].nunique() == len(df_orig):\n",
    "            topics_df = cached[[\"orig_idx\",\"topics\"]].copy()\n",
    "            use_cache = True\n",
    "            print(\"► Using cached topics.\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if not use_cache:\n",
    "    print(\"► Extracting topics with spaCy NER (one-time)...\")\n",
    "    topics = []\n",
    "    for txt in tqdm(df_orig[\"Text\"].tolist(), total=len(df_orig)):\n",
    "        topics.append(_extract_keywords_spacy(str(txt), nlp, max_kw=MAX_KW))\n",
    "    topics_df = pd.DataFrame({\"orig_idx\": df_orig[\"orig_idx\"], \"topics\": topics})\n",
    "    topics_df.to_parquet(TOPICS_CACHE_PARQUET, index=False)\n",
    "    topics_df.to_csv(TOPICS_CACHE_CSV, index=False)\n",
    "    print(f\"Saved topics cache: {TOPICS_CACHE_PARQUET}\")\n",
    "\n",
    "# Attach topics to your current split frames by **matching original indexes**\n",
    "train_df = train_df.copy()\n",
    "val_df   = val_df.copy()\n",
    "\n",
    "train_df[\"orig_idx\"] = train_df.index\n",
    "val_df[\"orig_idx\"]   = val_df.index\n",
    "\n",
    "train_df = train_df.merge(topics_df, on=\"orig_idx\", how=\"left\")\n",
    "val_df   = val_df.merge(topics_df,   on=\"orig_idx\", how=\"left\")\n",
    "\n",
    "# tidy\n",
    "train_df = train_df.drop(columns=[\"orig_idx\"]).reset_index(drop=True)\n",
    "val_df   = val_df.drop(columns=[\"orig_idx\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49eb9f1-dc72-49fd-8085-b18b0965ed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# E) Build inputs (KW vs noKW)  — <TOPIC> first, then <TEXT>\n",
    "# ===============================\n",
    "from datasets import Dataset\n",
    "\n",
    "def build_input(text: str, topics: str, use_keywords: bool) -> str:\n",
    "    \"\"\"\n",
    "    KW:  \"<TOPIC> k1 ; k2 ; ... <TEXT> original\"\n",
    "    noKW: original text only\n",
    "    \"\"\"\n",
    "    if not use_keywords:\n",
    "        return text\n",
    "    topics = (topics or \"\").strip()\n",
    "    return f\"<TOPIC> {topics} <TEXT> {text}\".strip()\n",
    "\n",
    "def df_to_hf_dataset(df_: pd.DataFrame, use_keywords: bool) -> Dataset:\n",
    "    src, tgt = [], []\n",
    "    for s, t, k in zip(df_[\"Text\"].tolist(), df_[\"Abstractive\"].tolist(), df_[\"topics\"].tolist()):\n",
    "        src.append(build_input(str(s), str(k), use_keywords=use_keywords))\n",
    "        tgt.append(str(t))\n",
    "    return Dataset.from_dict({\"source\": src, \"target\": tgt})\n",
    "\n",
    "train_ds_no_kw = df_to_hf_dataset(train_df, use_keywords=False)\n",
    "val_ds_no_kw   = df_to_hf_dataset(val_df,   use_keywords=False)\n",
    "train_ds_kw    = df_to_hf_dataset(train_df, use_keywords=True)\n",
    "val_ds_kw      = df_to_hf_dataset(val_df,   use_keywords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6482a3a2-2dc1-4a8c-be76-12c75588f760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# F) Tokenization helpers\n",
    "# ===============================\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "@dataclass\n",
    "class TokenizeConfig:\n",
    "    tokenizer: AutoTokenizer\n",
    "    max_source_len: int\n",
    "    max_target_len: int\n",
    "\n",
    "def tokenize_function(samples: Dict[str, List[str]], cfg: TokenizeConfig):\n",
    "    tok = cfg.tokenizer(\n",
    "        samples[\"source\"],\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=MAX_SOURCE_LEN,\n",
    "    )\n",
    "    labels = cfg.tokenizer(\n",
    "        text_target=samples[\"target\"],\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=MAX_TARGET_LEN,\n",
    "    )\n",
    "    tok[\"labels\"] = labels[\"input_ids\"]\n",
    "    return tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbb3c85-37fc-419d-aa76-45ce852ca267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# G) Trainer with early stopping on eval_loss\n",
    "# ===============================\n",
    "import torch, os\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
    "    EncoderDecoderModel, BertTokenizer,\n",
    "    DataCollatorForSeq2Seq, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "PATIENCE = 2  # epochs with no improvement\n",
    "\n",
    "def train_and_save_seq2seq(\n",
    "    model_name: str,\n",
    "    pretrained_id: str,\n",
    "    tokenizer_cls,\n",
    "    model_cls,\n",
    "    train_ds: Dataset,\n",
    "    val_ds: Dataset,\n",
    "    add_special_tokens: bool,\n",
    "    save_subdir: str,\n",
    "):\n",
    "    import os\n",
    "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, EncoderDecoderModel, BertTokenizer\n",
    "    os.makedirs(save_subdir, exist_ok=True)\n",
    "\n",
    "    # 1) Tokenizer\n",
    "    if tokenizer_cls is None:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(pretrained_id, use_fast=True)\n",
    "    else:\n",
    "        tokenizer = tokenizer_cls.from_pretrained(pretrained_id, use_fast=True)\n",
    "\n",
    "    # 2) Model\n",
    "    if model_name in (\"t5\", \"bart\"):\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(pretrained_id)\n",
    "    elif model_name == \"bert2bert\":\n",
    "        enc_id = \"bert-base-uncased\"\n",
    "        dec_id = \"bert-base-uncased\"\n",
    "        model = EncoderDecoderModel.from_encoder_decoder_pretrained(enc_id, dec_id)\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
    "        model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "        model.config.eos_token_id = tokenizer.sep_token_id\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        model.config.max_length = MAX_TARGET_LEN\n",
    "        model.config.min_length = 5\n",
    "        model.config.no_repeat_ngram_size = 3\n",
    "        model.config.early_stopping = True\n",
    "        model.config.length_penalty = 2.0\n",
    "        model.config.num_beams = 4\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model_name\")\n",
    "\n",
    "    # 3) Special tokens for keyword format\n",
    "    if add_special_tokens:\n",
    "        tokenizer.add_special_tokens(SPECIAL_TOKENS)\n",
    "        if model_name == \"bert2bert\":\n",
    "            new_size = len(tokenizer)\n",
    "            model.encoder.resize_token_embeddings(new_size)\n",
    "            model.decoder.resize_token_embeddings(new_size)\n",
    "        else:\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # 4) Tokenize\n",
    "    @dataclass\n",
    "    class TokenizeConfig:\n",
    "        tokenizer: AutoTokenizer\n",
    "        max_source_len: int\n",
    "        max_target_len: int\n",
    "\n",
    "    def tokenize_function(samples: Dict[str, List[str]], cfg: \"TokenizeConfig\"):\n",
    "        tok = cfg.tokenizer(\n",
    "            samples[\"source\"], padding=False, truncation=True, max_length=cfg.max_source_len,\n",
    "        )\n",
    "        with cfg.tokenizer.as_target_tokenizer():\n",
    "            labels = cfg.tokenizer(\n",
    "                samples[\"target\"], padding=False, truncation=True, max_length=cfg.max_target_len,\n",
    "            )\n",
    "        tok[\"labels\"] = labels[\"input_ids\"]\n",
    "        return tok\n",
    "\n",
    "    cfg = TokenizeConfig(tokenizer=tokenizer, max_source_len=MAX_SOURCE_LEN, max_target_len=MAX_TARGET_LEN)\n",
    "    tokenized_train = train_ds.map(lambda x: tokenize_function(x, cfg), batched=True, remove_columns=train_ds.column_names)\n",
    "    tokenized_val   = val_ds.map(lambda x: tokenize_function(x, cfg), batched=True, remove_columns=val_ds.column_names)\n",
    "\n",
    "    # 5) Trainer with EVAL EACH EPOCH + EARLY STOPPING + EPOCH LOGGING\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "    args = TrainingArguments(\n",
    "        output_dir=os.path.join(save_subdir, \"hf_runs\"),\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "        per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=0.01,\n",
    "        gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "        warmup_ratio=WARMUP_RATIO,\n",
    "\n",
    "        # 👇 these lines drive the “Epoch | Training | Validation” pattern\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        save_total_limit=2,\n",
    "\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=\"none\",\n",
    "        seed=SEED,\n",
    "        dataloader_num_workers=0,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=ES_PATIENCE, early_stopping_threshold=0.0)],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    model.save_pretrained(save_subdir)\n",
    "    tokenizer.save_pretrained(save_subdir)\n",
    "    print(f\"✓ Saved: {save_subdir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ddd545-7c93-4e92-b1e2-917dfaad90cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BART base is safer VRAM-wise; switch to \"facebook/bart-large-cnn\" if your GPU can handle it\n",
    "BART_ID = \"facebook/bart-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf0237d-2aad-4373-a9e4-6304c920c068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# H) Run training per model (isolated try/except per run)\n",
    "# ===============================\n",
    "\n",
    "# ---- T5-base: noKW & KW ----\n",
    "for use_kw, tag in [(False,\"noKW\"), (True,\"KW\")]:\n",
    "    try:\n",
    "        subdir = os.path.join(SAVE_DIR, f\"t5-base{DS_TAG}_{tag}_NEW\")\n",
    "        train_and_save_seq2seq(\n",
    "            model_name=\"t5\",\n",
    "            pretrained_id=\"t5-base\",\n",
    "            tokenizer_cls=None,\n",
    "            model_cls=AutoModelForSeq2SeqLM,   # keep arg to satisfy signature\n",
    "            train_ds=(train_ds_kw if use_kw else train_ds_no_kw),\n",
    "            val_ds=(val_ds_kw   if use_kw else val_ds_no_kw),\n",
    "            add_special_tokens=use_kw,\n",
    "            save_subdir=subdir,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"❌ T5 {tag} failed:\", repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933e66ea-c6e0-47fd-82e6-25f6a0249882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Two more T5 variants — KW+ and Prefix\n",
    "# ===============================\n",
    "import re\n",
    "from typing import List\n",
    "import spacy\n",
    "\n",
    "# ---- (1) A \"full\" spaCy pipeline for KW+ (needs parser for noun_chunks)\n",
    "try:\n",
    "    nlp_kwplus = spacy.load(\"en_core_web_sm\")  # full pipeline, includes parser\n",
    "    nlp_kwplus.max_length = 2_000_000\n",
    "    # cheap sentence splitter just in case (safe if already present)\n",
    "    if \"sentencizer\" not in nlp_kwplus.pipe_names and \"senter\" not in nlp_kwplus.pipe_names:\n",
    "        nlp_kwplus.add_pipe(\"sentencizer\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"spaCy full model not available. Install with:\\n\"\n",
    "        \"python -m spacy download en_core_web_sm\"\n",
    "    ) from e\n",
    "\n",
    "def normalize_kw(s: str) -> str:\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def extract_keywords_spacy_plus(text: str, max_kw: int = MAX_KW) -> str:\n",
    "    \"\"\"\n",
    "    KW+ : NER + noun chunks (short noun phrases), lowercase/dedup, no digits.\n",
    "    Order: entities first, then NP chunks; cap to max_kw.\n",
    "    \"\"\"\n",
    "    doc = nlp_kwplus(str(text))\n",
    "    cands: List[str] = []\n",
    "\n",
    "    # 1) Named entities (prioritize salient types)\n",
    "    keep_ent = {\"PERSON\",\"ORG\",\"GPE\",\"LOC\",\"NORP\",\"FAC\",\"PRODUCT\",\"EVENT\",\"WORK_OF_ART\",\"LAW\",\"LANGUAGE\"}\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in keep_ent:\n",
    "            cands.append(ent.text)\n",
    "\n",
    "    # 2) Noun chunks\n",
    "    for nc in doc.noun_chunks:\n",
    "        cands.append(nc.text)\n",
    "\n",
    "    # 3) Clean/dedup\n",
    "    out, seen = [], set()\n",
    "    for c in cands:\n",
    "        c = normalize_kw(c)\n",
    "        if not c or any(ch.isdigit() for ch in c):\n",
    "            continue\n",
    "        lc = c.lower()\n",
    "        if lc not in seen:\n",
    "            seen.add(lc)\n",
    "            out.append(c)\n",
    "        if len(out) >= max_kw:\n",
    "            break\n",
    "    return \" ; \".join(out)\n",
    "\n",
    "# ---- (2) T5-style prefix formatter with keyword dropout (training only)\n",
    "def keyword_dropout(topics: str, p_drop: float = 0.3) -> str:\n",
    "    \"\"\"\n",
    "    Randomly drop some keywords during TRAINING to improve robustness.\n",
    "    During EVAL we won't drop (p_drop=0).\n",
    "    \"\"\"\n",
    "    toks = [t.strip() for t in (topics or \"\").split(\";\") if t.strip()]\n",
    "    if not toks:\n",
    "        return \"\"\n",
    "    import random\n",
    "    kept = [t for t in toks if random.random() > p_drop]\n",
    "    if not kept:\n",
    "        kept = toks[: max(1, len(toks)//2)]\n",
    "    return \" ; \".join(kept)\n",
    "\n",
    "def build_input_kwplus(text: str, topics_plus: str) -> str:\n",
    "    # same control tokens as your KW baseline, but with better topics\n",
    "    return f\"<TOPIC> {topics_plus} <TEXT> {text}\".strip()\n",
    "\n",
    "def build_input_prefix(text: str, topics: str, train_mode: bool) -> str:\n",
    "    # classic T5 style with natural language fields\n",
    "    # IMPORTANT: no special tokens, so we set add_special_tokens=False for this run.\n",
    "    t = keyword_dropout(topics, 0.3 if train_mode else 0.0)\n",
    "    return f\"summarize: topics: {t}  context: {text}\".strip()\n",
    "\n",
    "# -----------------------------------------\n",
    "# Compute KW+ topics for CURRENT splits only\n",
    "# (does not touch your original NER cache)\n",
    "# -----------------------------------------\n",
    "print(\"► Building KW+ topics for train/val …\")\n",
    "train_df = train_df.copy()\n",
    "val_df   = val_df.copy()\n",
    "\n",
    "train_df[\"topics_plus\"] = [extract_keywords_spacy_plus(x, MAX_KW) for x in tqdm(train_df[\"Text\"].tolist())]\n",
    "val_df[\"topics_plus\"]   = [extract_keywords_spacy_plus(x, MAX_KW) for x in tqdm(val_df[\"Text\"].tolist())]\n",
    "\n",
    "# Fallback if base 'topics' column is missing (recompute simple NER topics)\n",
    "if \"topics\" not in train_df.columns or train_df[\"topics\"].isna().any():\n",
    "    from tqdm.auto import tqdm\n",
    "    print(\"► Base 'topics' not found — recomputing simple NER topics for Prefix variant …\")\n",
    "    train_df[\"topics\"] = [_extract_keywords_spacy(x, nlp, MAX_KW) for x in tqdm(train_df[\"Text\"].tolist())]\n",
    "    val_df[\"topics\"]   = [_extract_keywords_spacy(x, nlp, MAX_KW) for x in tqdm(val_df[\"Text\"].tolist())]\n",
    "\n",
    "# -----------------------------------------\n",
    "# Build HF datasets for the two new variants\n",
    "# -----------------------------------------\n",
    "from datasets import Dataset\n",
    "\n",
    "# KW+ datasets (use control tokens; will require SPECIAL_TOKENS)\n",
    "def df_to_hf_kwplus(df_):\n",
    "    src, tgt = [], []\n",
    "    for s, t, k in zip(df_[\"Text\"].tolist(), df_[\"Abstractive\"].tolist(), df_[\"topics_plus\"].tolist()):\n",
    "        src.append(build_input_kwplus(str(s), str(k)))\n",
    "        tgt.append(str(t))\n",
    "    return Dataset.from_dict({\"source\": src, \"target\": tgt})\n",
    "\n",
    "# Prefix datasets (T5 prompt, no special tokens)\n",
    "def df_to_hf_prefix(df_, train_mode: bool):\n",
    "    src, tgt = [], []\n",
    "    for s, t, k in zip(df_[\"Text\"].tolist(), df_[\"Abstractive\"].tolist(), df_[\"topics\"].tolist()):\n",
    "        src.append(build_input_prefix(str(s), str(k), train_mode=train_mode))\n",
    "        tgt.append(str(t))\n",
    "    return Dataset.from_dict({\"source\": src, \"target\": tgt})\n",
    "\n",
    "train_ds_kwplus = df_to_hf_kwplus(train_df)\n",
    "val_ds_kwplus   = df_to_hf_kwplus(val_df)\n",
    "\n",
    "train_ds_prefix = df_to_hf_prefix(train_df, train_mode=True)\n",
    "val_ds_prefix   = df_to_hf_prefix(val_df,   train_mode=False)\n",
    "\n",
    "# -----------------------------------------\n",
    "# Train: T5-base_KWplus  (needs special tokens)\n",
    "# -----------------------------------------\n",
    "try:\n",
    "    subdir = os.path.join(SAVE_DIR, f\"t5-base{DS_TAG}_KWplus\")\n",
    "    train_and_save_seq2seq(\n",
    "        model_name=\"t5\",\n",
    "        pretrained_id=\"t5-base\",\n",
    "        tokenizer_cls=None,\n",
    "        model_cls=AutoModelForSeq2SeqLM,\n",
    "        train_ds=train_ds_kwplus,\n",
    "        val_ds=val_ds_kwplus,\n",
    "        add_special_tokens=True,      # <TOPIC>/<TEXT> tokens required\n",
    "        save_subdir=subdir,\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"❌ T5 KWplus failed:\", repr(e))\n",
    "\n",
    "# -----------------------------------------\n",
    "# Train: T5-base_Prefix (T5 task-style, no special tokens)\n",
    "# -----------------------------------------\n",
    "try:\n",
    "    subdir = os.path.join(SAVE_DIR, f\"t5-base{DS_TAG}_KWprefix\")\n",
    "    train_and_save_seq2seq(\n",
    "        model_name=\"t5\",\n",
    "        pretrained_id=\"t5-base\",\n",
    "        tokenizer_cls=None,\n",
    "        model_cls=AutoModelForSeq2SeqLM,\n",
    "        train_ds=train_ds_prefix,\n",
    "        val_ds=val_ds_prefix,\n",
    "        add_special_tokens=False,     # plain text prompt, no extra tokens\n",
    "        save_subdir=subdir,\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"❌ T5 KWprefix failed:\", repr(e))\n",
    "\n",
    "print(\"Done training new T5 variants: KWplus & KWprefix.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bfba2c-b288-41c7-aa73-88230dae504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- BART: noKW & KW ----\n",
    "for use_kw, tag in [(False,\"noKW\"), (True,\"KW\")]:\n",
    "    try:\n",
    "        subdir = os.path.join(SAVE_DIR, f\"{os.path.basename(BART_ID)}{DS_TAG}_{tag}_NEW\")\n",
    "        train_and_save_seq2seq(\n",
    "            model_name=\"bart\",\n",
    "            pretrained_id=BART_ID,\n",
    "            tokenizer_cls=None,\n",
    "            model_cls=AutoModelForSeq2SeqLM,\n",
    "            train_ds=(train_ds_kw if use_kw else train_ds_no_kw),\n",
    "            val_ds=(val_ds_kw   if use_kw else val_ds_no_kw),\n",
    "            add_special_tokens=use_kw,\n",
    "            save_subdir=subdir,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"❌ BART {tag} failed:\", repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b21cb5-0876-42b4-93e6-0dc58dc27c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- BERT2BERT: noKW & KW (re-run with the fixed helper) ----\n",
    "for use_kw, tag in [(False, \"noKW\"), (True, \"KW\")]:\n",
    "    try:\n",
    "        subdir = os.path.join(SAVE_DIR, f\"bert2bert{DS_TAG}_{tag}_NEW\")\n",
    "        train_and_save_seq2seq(\n",
    "            model_name=\"bert2bert\",\n",
    "            pretrained_id=\"bert-base-uncased\",\n",
    "            tokenizer_cls=BertTokenizer,            # ensures BERT tokenizer\n",
    "            model_cls=EncoderDecoderModel,          # not used inside now, but kept for signature compatibility\n",
    "            train_ds=(train_ds_kw if use_kw else train_ds_no_kw),\n",
    "            val_ds=(val_ds_kw   if use_kw else val_ds_no_kw),\n",
    "            add_special_tokens=use_kw,              # triggers encoder/decoder resize on KW\n",
    "            save_subdir=subdir,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"❌ BERT2BERT {tag} failed:\", repr(e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
